var documenterSearchIndex = {"docs":
[{"location":"predictive_asymmetry/#Predictive-asymmetry","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"","category":"section"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"predictive_asymmetry","category":"page"},{"location":"predictive_asymmetry/#CausalityTools.PredictiveAsymmetry.predictive_asymmetry","page":"Predictive asymmetry","title":"CausalityTools.PredictiveAsymmetry.predictive_asymmetry","text":"General interface\n\npredictive_asymmetry(s, t, [c], \n    estimator::TransferEntropyEstimator, Œ∑s; \n    dùíØ = 1, dT = 1, dS = 1, œÑT = -1, œÑS = -1, \n    [dC = 1, œÑC = -1,],\n    normalize::Bool = false, f::Real = 1.0) ‚Üí Vector{Float64}\n\nCompute the predictive asymmetry[Haaga2020] ùî∏(s ‚Üí t) for source time series s and  target time series t over prediction lags Œ∑s, using the given estimator and embedding  parameters dùíØ, dT, dS, œÑT, œÑS. \n\nIf a conditional time series c is provided, compute ùî∏(s ‚Üí t |¬†c). Then, dC and  œÑC controls the embedding dimension and embedding lag for the conditional variable.\n\nReturns\n\nReturns a vector containing the predictive asymmetry for each value of Œ∑s.\n\nNormalization (hypothesis test)\n\nIf normalize == true (the default), then compute the normalized predictive asymmetry ùíú. \n\nIn this case, for each eta in Œ∑s, compute ùíú(Œ∑) by normalizing ùî∏(Œ∑) to some fraction f of the  mean transfer entropy over prediction lags -eta  eta (exluding lag 0). \n\nHaaga et al. (2020)[Haaga2020] uses a normalization with f=1.0 as a built-in hypothesis test,  avoiding more computationally costly surrogate testing.\n\nEstimators\n\nAny estimator that works for transferentropy will also work with  predictive_asymmetry. It is recommended to use either the rectangular  binning-based methods or the symbolic estimators for the fastest computations. \n\nExamples\n\nusing CausalityTools \n\n# Some example time series\nx, y = rand(100), rand(100)\n\n# ùî∏(x ‚Üí y) over prediction lags 1:5\nùî∏reg  = predictive_asymmetry(x, y, VisitationFrequency(RectangularBinning(5)), 1:5) \n\n[Haaga2020]: Haaga, Kristian Agas√∏ster, David Diego, Jo Brendryen, and Bjarte Hannisdal. \"A simple test for causality in complex systems.\" arXiv preprint arXiv:2005.01860 (2020).\n\n\n\n\n\n","category":"function"},{"location":"predictive_asymmetry/#Example:-no-coupling","page":"Predictive asymmetry","title":"Example: no coupling","text":"","category":"section"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"Here, we'll compute the predictive asymmetry on 100 different sets of random time series. Because there is no dynamical coupling between the time series, we expect the predictive asymmetry to be zero.","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"We'll use a visitation frequency estimator.","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"using CausalityTools, Plots, StatsBase\n\n# Define prediction lags and estimator\nŒ∑s = 1:10\nest = VisitationFrequency(RectangularBinning(5)) # guess that 5 bins along each coordinate axis is sufficient\n\nnreps = 100\nùî∏xy = zeros(nreps, length(Œ∑s))\nùî∏yx = zeros(nreps, length(Œ∑s))\n\nfor i = 1:nreps\n    # Some example time series\n    x, y = rand(1000), rand(1000)\n    ùî∏xy[i, :] = predictive_asymmetry(x, y, est, Œ∑s) \n    ùî∏yx[i, :] = predictive_asymmetry(y, x, est, Œ∑s) \nend\n\nplot()\nplot!(Œ∑s, dropdims(mean(ùî∏xy, dims = 1), dims = 1), label = \"X to Y\", c = :black)\nplot!(Œ∑s, dropdims(mean(ùî∏yx, dims = 1), dims = 1), label = \"Y to X\", c = :red)\nylims!((-0.1, 0.1))\nxlabel!(\"Prediction lag (Œ∑)\")\nylabel!(\"Predictive asymmetry (bits)\")\nsavefig(\"random-pa.svg\"); nothing # hide","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"(Image: )","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"As expected, because there is no dynamical coupling between the variables, the predictive asymmetry is around zero for all prediction lags.","category":"page"},{"location":"predictive_asymmetry/#Example:-coupled-Ulam-maps","page":"Predictive asymmetry","title":"Example: coupled Ulam maps","text":"","category":"section"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"Here, we'll use the same example as in Schreiber's transfer entropy paper and see if the predictive asymmetry  is able to decipher the directional coupling in the Ulam system.","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"Let's start by defining the Ulam system. Then, we generate time series for different values of the  parameter Œµ and compute the predictive asymmetry for prediction lags Œ∑s = 1:10.","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"We expect that the predictive asymmetry is positive if there is a directional coupling, and zero or negative  if there is no directional coupling.","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"using DynamicalSystems, CausalityTools, Plots, Random, StatsBase, TimeseriesSurrogates\n\nRandom.seed!(12234)\n\nfunction ulam_system(dx, x, p, t)\n    f(x) = 2 - x^2\n    Œµ = p[1]\n    dx[1] = f(Œµ*x[length(dx)] + (1-Œµ)*x[1])\n    for i in 2:length(dx)\n        dx[i] = f(Œµ*x[i-1] + (1-Œµ)*x[i])\n    end\nend\n\nds = DiscreteDynamicalSystem(ulam_system, rand(100) .- 0.5, [0.04])\ntrajectory(ds, 1000; Ttr = 1000)\n\nŒµs = 0.02:0.02:1.0\nŒ∑s = 1:10\npas_x1x2 = zeros(length(Œµs), length(Œ∑s)); \npas_x2x1 = zeros(length(Œµs), length(Œ∑s))\n\nfor (i, Œµ) in enumerate(Œµs)\n    set_parameter!(ds, 1, Œµ)\n    # Use time series consisting of 1000 points\n    tr = trajectory(ds, 1000; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    binning = RectangularBinning(0.2) # guess an appropriate bin width of 0.2\n    pas_x1x2[i, :] = predictive_asymmetry(X1, X2, VisitationFrequency(binning), Œ∑s)\n    pas_x2x1[i, :] = predictive_asymmetry(X2, X1, VisitationFrequency(binning), Œ∑s)\nend","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"Let's pick the predictive asymmetry at prediction lag Œ∑ = 10. ","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"# Pick predictive asymmetry at Œ∑ = 10\npa_x1x2 = [pas_x1x2[i, end] for i = 1:length(Œµs)]\npa_x2x1 = [pas_x2x1[i, end] for i = 1:length(Œµs)]\n\nplot()\nplot(Œµs, pa_x1x2, label = \"X1 to X2\", c = :black, lw = 1.5)\nplot!(Œµs, pa_x2x1, label = \"X2 to X1\", c = :red)\nxlabel!(\"epsilon\")\nylabel!(\"Predictive asymmetry (bits)\")\nhline!([0], ls = :dash, label = \"\", c = :grey)\nsavefig(\"predasym.svg\") # hide","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"(Image: )","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"As expected, predictive asymmetry is positive in the direction X_1 to X_2, and  negative in the direction X_2 to X_1.","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"Are the results significant? We can test that using a surrogate test. We'll use random shuffle surrogates from the TimeseriesSurrogates.jl package. ","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"Using the same parameters, as before, we generate time series for the same values of  Œµs, but for each Œµ, we generate 100 different shuffled versions of the source time  series. For each of those surrogate time series, we compute the predictive asymmetry and pick the value at Œ∑ = 10","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"n_surr = 100\npas_x1x2_surr = zeros(length(Œµs), n_surr); \npas_x2x1_surr = zeros(length(Œµs), n_surr)\n\nfor (i, Œµ) in enumerate(Œµs)\n    set_parameter!(ds, 1, Œµ)\n    tr = trajectory(ds, 1000; Ttr = 5000)\n    X1 = tr[:, 1]; @assert !any(isnan, X1)\n    X2 = tr[:, 2]; @assert !any(isnan, X2)\n    S1 = surrogenerator(X1, RandomShuffle())\n    S2 = surrogenerator(X2, RandomShuffle())\n\n    binning = RectangularBinning(0.2) # guess an appropriate bin width of 0.2\n\n    pas_x1x2_surr[i, :] = [predictive_asymmetry(S1(), X2, VisitationFrequency(binning), Œ∑s)[end] for k = 1:n_surr]\n    pas_x2x1_surr[i, :] = [predictive_asymmetry(S2(), X1, VisitationFrequency(binning), Œ∑s)[end] for k = 1:n_surr]\nend","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"We'll plot the 95th percentile of the surrogate ensemble predictive asymmetries (dashed lines) along with the original predictive asymmetries (solid lines). If the predictive asymmetry computed on the original time series exceed that of the 95th percentile of the surrogate ensemble, then the predictive asymmetry is significant.","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"uq_x1x2 = [quantile(pas_x1x2_surr[i, :], 0.95) for i = 1:length(Œµs)]\nuq_x2x1 = [quantile(pas_x2x1_surr[i, :], 0.95) for i = 1:length(Œµs)]\nymax = maximum(abs.([pa_x1x2; pa_x2x1; uq_x1x2; uq_x2x1]))*1.1\nplot(ylims = (-ymax, ymax))\nplot!(Œµs, pa_x1x2, label = \"X1 to X2\", c = :black, lw = 1.5)\nplot!(Œµs, uq_x1x2, label = \"X1 to X2\", c = :black, ls = :dot)\nplot!(Œµs, pa_x2x1, label = \"X2 to X1\", c = :red)\nplot!(Œµs, uq_x2x1, label = \"X2 to X1\", c = :red, ls = :dot)\n\nxlabel!(\"epsilon\")\nylabel!(\"Predictive asymmetry (bits)\")\nhline!([0], ls = :dash, label = \"\", c = :grey)\nsavefig(\"predasym_withsurr.svg\") # hide","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"(Image: )","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"The predictive asymmetry in the direction X_1 to X_2 is above the 95th percentile of the surrogates for most parameters. Thus, we reject the null hypothesis that X_1 does not influence X_2 and accept the alternative hypothesis that X_1 does in fact influence X_2. In the context of causal inference, this means that there is evidence of directional dynamical coupling from X_1 to X_2 and that X_1 is causally coupled to X_2. ","category":"page"},{"location":"predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"In the oppposite direction, X_2 to X_1, where there should not be any directional coupling (by definition), the predictive asymmetries are below the surrogates, so we cannot reject the null hypothesis that X_2 does not influence X_1. Hence, there is no evidence of directional dynamical coupling from X_2 to  X_1.","category":"page"},{"location":"TransferEntropy/#[Transfer-entropy](@ref-transferentropy)","page":"Transfer entropy","title":"Transfer entropy","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The following transferentropy function computes transfer entropy \"manually\", that is, in addition to specifying an estimator, you have to specify embedding parameters.","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"transferentropy","category":"page"},{"location":"TransferEntropy/#TransferEntropy.transferentropy","page":"Transfer entropy","title":"TransferEntropy.transferentropy","text":"transferentropy(s, t, [c,] est; base = 2, q = 1, \n    œÑT = -1, œÑS = -1, Œ∑ùíØ = 1, dT = 1, dS = 1, dùíØ = 1, [œÑC = -1, dC = 1]\n)\n\nEstimate transfer entropy[Schreiber2000] from source s to target t, TE^q(s to t), using the  provided entropy/probability estimator est with logarithms to the given base. Optionally, condition  on c and estimate the conditional transfer entropy TE^q(s to t  c). The input series s, t, and c must be equal-length real-valued vectors.\n\nCompute either Shannon transfer entropy (q = 1, which is the default) or the order-q  R√©nyi transfer entropy[Jizba2012] by setting q different from 1.\n\nAll possible estimators that can be used are described in the online documentation.\n\nKeyword Arguments\n\nKeyword arguments tune the embedding that will be done to each of the timeseries (with more details following below). In short, the embedding lags œÑT, œÑS, œÑC must be zero or negative, the  prediction lag Œ∑ùíØ must be positive, and the embedding dimensions dT, dS, dC, dùíØ  must be greater than or equal to 1. Thus, the convention is to use negative lags to  indicate embedding delays for past state vectors (for the T, S and C marginals,  detailed below), and positive lags to indicate embedding delays for future state vectors  (for the mathcal T marginal, also detailed below). \n\nThe default behaviour is to use scalar timeseries for past state vectors (in that case, the œÑT, œÑS or œÑC does not affect the analysis).\n\nDescription\n\nTransfer entropy on scalar time series\n\nTransfer entropy[Schreiber2000] between two simultaneously measured scalar time series s(n) and t(n),   s(n) =  s_1 s_2 ldots s_N  and t(n) =  t_1 t_2 ldots t_N , is is defined as \n\nTE(s to t) = sum_i p(s_i t_i t_i+eta) log left( dfracp(t_i+eta ¬†t_i s_i)p(t_i+eta ¬†t_i) right)\n\nTransfer entropy on generalized embeddings\n\nBy defining the vector-valued time series, it is possible to include more than one  historical/future value for each marginal (see 'Uniform vs. non-uniform embeddings' below for embedding details):\n\nmathcalT^(d_mathcal T eta_mathcal T) = t_i^(d_mathcal T eta_mathcal T) _i=1^N, \nT^(d_T tau_T) = t_i^(d_T tau_T) _i=1^N, \nS^(d_S tau_S) = s_i^(d_T tau_T) _i=1^N,  and \nC^(d_C tau_C) = s_i^(d_C tau_C) _i=1^N.\n\nThe non-conditioned generalized and conditioned generalized forms of the transfer entropy are then\n\nTE(s to t) = sum_i p(ST mathcalT) log left( dfracp(mathcalT ¬†T S)p(mathcalT ¬†T) right)\n\nTE(s to t ¬†c) = sum_i p(ST mathcalT C) log left( dfracp(mathcalT ¬†T S C)p(mathcalT ¬†T C) right)\n\nUniform vs. non-uniform embeddings\n\nThe N state vectors for each marginal are either \n\nuniform, of the form x_i^(d omega) = (x_i x_i+omega x_i+2omega ldots x_i+(d - 1)omega),    with equally spaced state vector entries. Note: When constructing marginals for T, S and C,    we need omega leq 0 to get present/past values, while omega  0 is necessary to get future states    when constructing mathcalT.\nnon-uniform, of the form x_i^(d omega) = (x_i x_i+omega_1 x_i+omega_2 ldots x_i+omega_d),   with non-equally spaced state vector entries omega_1 omega_2 ldots omega_d,   which can be freely chosen. Note: When constructing marginals for T, S and C,    we need omega_i leq 0 for all omega_i to get present/past values, while omega_i  0 for all omega_i    is necessary to get future states when constructing mathcalT.\n\nIn practice, the dT-dimensional, dS-dimensional and dC-dimensional state vectors  comprising T, S and C are constructed with embedding lags œÑT,  œÑS, and œÑC, respectively. The dùíØ-dimensional future states mathcalT^(d_mathcal T eta_mathcal T) are constructed with prediction lag Œ∑ùíØ (i.e. predictions go from present/past states to  future states spanning a maximum of dùíØ*Œ∑ùíØ time steps). Note: in Schreiber's paper, only the historical states are defined as  potentially higher-dimensional, while the future states are always scalar.\n\nEstimation\n\nTransfer entropy is here estimated by rewriting the above expressions as a sum of marginal  entropies, and extending the definitions above to use R√©nyi generalized entropies of order  q as\n\nTE^q(s to t) = H^q(mathcal T T) + H^q(T S) - H^q(T) - H^q(mathcal T T S)\n\nTE^q(s to t  c) = H^q(mathcal T T C) + H^q(T S C) - H^q(T C) - H^q(mathcal T T S C)\n\nwhere H^q(cdot) is the generalized R√©nyi entropy of order q. This is equivalent to the R√©nyi transfer entropy implementation in Jizba et al. (2012)[Jizba2012].\n\nExamples\n\nDefault estimation (scalar marginals): \n\n# Symbolic estimator, motifs of length 4, uniform delay vectors with lag 1\nest = SymbolicPermutation(m = 4, œÑ = 1) \n\nx, y = rand(100), rand(100)\ntransferentropy(x, y, est)\n\nIncreasing the dimensionality of the T marginal (present/past states of the target  variable):\n\n# Binning-based estimator\nest = VisitationFrequency(RectangularBinning(4)) \nx, y = rand(100), rand(100)\n\n# Uniform delay vectors when `œÑT` is an integer (see explanation above)\n# Here t_{i}^{(dT, œÑT)} = (t_i, t_{i+œÑ}, t_{i+2œÑ}, \\ldots t_{i+(dT-1)œÑ})\n# = (t_i, t_{i-2}, t_{i-4}, \\ldots t_{i-6œÑ}), so we need zero/negative values for `œÑT`.\ntransferentropy(x, y, est, dT = 4, œÑT = -2)\n\n# Non-uniform delay vectors when `œÑT` is a vector of integers\n# Here t_{i}^{(dT, œÑT)} = (t_i, t_{i+œÑ_{1}}, t_{i+œÑ_{2}}, \\ldots t_{i+œÑ_{dT}})\n# = (t_i, t_{i-7}, t_{i-25}), so we need zero/negative values for `œÑT`.\ntransferentropy(x, y, est, dT = 3, œÑT = [0, -7, -25])\n\nLogarithm bases and the order of the R√©nyi entropy can also be tuned:\n\nx, y = rand(100), rand(100)\nest = NaiveKernel(0.3)\ntransferentropy(x, y, est, base = MathConstants.e, q = 2) # TE in nats, order-2 R√©nyi entropy\n\n[Schreiber2000]: Schreiber, T. (2000). Measuring information transfer. Physical review letters, 85(2), 461.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). R√©nyi‚Äôs information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"function"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The bbnue function optimizes embedding parameters using an iterative procedure for  variable selection, and performs null hypothesis testing as part of that procedure.","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"bbnue","category":"page"},{"location":"TransferEntropy/#TransferEntropy.bbnue","page":"Transfer entropy","title":"TransferEntropy.bbnue","text":"bbnue(source, target, [cond], est::BBNUE; q = 0.95, Œ∑ = 1, \n    nsurr = 100, uq = 0.95, \n    include_instantaneous = true, \n    method_delay = \"ac_min\", \n    maxlag::Union{Int, Float64} = 0.05\n    ) ‚Üí te, js, œÑs, idxs_source, idxs_target, idxs_cond\n\nEstimate transfer entropy using the bootstrap-based non-uniform embedding (BBNUE) estimator,  which uses a bootstrap-basedcriterion to identify the most relevant and minimally redundant  variables from the present/past of source, present/past cond (if given) and the past of  target that contribute most to target's future. Œ∑ is the forward prediction lag.  Multivariate source, target and cond (if given) are all possible.\n\nFor significance testing of a variable, nsurr circular shift surrogates are generated,  and if transfer entropy for the original variables exceeds the uq-quantile of that of the  surrogate ensemble, then the variable is included.\n\nIf instantaneous is true, then instantaneous interactions are also considered, i.e. effects like  source(t) ‚Üí target(t) are allowed.\n\nIn this implementation, the maximum lag for each embedding variable is determined using estimate_delay  from DelayEmbeddings. The keywords method_delay (default is \"ac_min\") controls the method  for estimating the delay, and maxlag is the maximum allowed delay (if maxlag ‚àà [0, 1] is a fraction,  then the maximum lag is that fraction of the input time series length, and if maxlag is an integer,  then the maximum lag is maxlag).\n\nImplementation details\n\nCurrently, only this implementation is optimized for the bin-estimator approach from  Montalto et al. (2014)[Montalto2014], which uses a conditional entropy minimization criterion for  select variables.  Their bin-estimator approach corresponds to using  the VisitationFrequency estimator with bins whose sides are equal-length, e.g.  VisitationFrequency(RectangularBinning(0.5)). Here, you can use any desired rectangular binning.\n\nIt is also possible to use other entropy estimators than VisitationFrequency to compute entropies,  but this implementation will use conditional entropy minimization regardless of the choice of  estimator.\n\nReturns\n\nA 6-tuple is returned, consisting of:\n\nte: The computed transfer entropy value. If no relevant variables were selected, then te = 0.0.\njs: The indices of the selected variables. js[i] is the i-th entry in the array [idxs_source..., idxs_target..., idxs_cond...,].\nœÑs: The embedding lags of the selected variables. œÑs[i] corresponds to js[i].\nidxs_source: The indices of the source variables.\nidxs_target: The indices of the target variables.\nidxs_cond: The indices of the conditional variables (empty if cond is not given).\n\nExample\n\nusing CausalityTools, DynamicalSystems\nsys = ExampleSystems.logistic2_unidir(c_xy = 1.5)\norbit = trajectory(sys, 10000, Ttr = 10000)\nx, y = columns(orbit)\n\n# Use a coarse-grained rectangular binning with subdivisions in each dimension,\n# to keep computation costs low and to ensure the probability distributions \n# over the bins don't approach the uniform distribution (need enough points \n# to fill bins).\nest = VisitationFrequency(RectangularBinning(3))\nte_xy, params_xy = bbnue(x, y, BBNUE(est))\nte_yx, params_yx = bbnue(y, x, BBNUE(est))\n\nte_xy, te_yx\n\n[Montalto2014]: Montalto, A.; Faes, L.; Marinazzo, D. MuTE: A MATLAB toolbox to compare established and novel estimators of the multivariate transfer entropy. PLoS ONE 2014, 9, e109462.\n\n\n\n\n\n","category":"function"},{"location":"TransferEntropy/#Example:-Reproducing-Schreiber-(2000)","page":"Transfer entropy","title":"Example: Reproducing Schreiber (2000)","text":"","category":"section"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Let's try to reproduce the results from Schreiber's original paper[Schreiber2000] on transfer entropy. We'll use a  visitation frequency estimator, which computes entropies by counting visits of the system's orbit to discrete portions  of its reconstructed state space.","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"using DynamicalSystems, CausalityTools, Plots, Random, StatsBase\n\nRandom.seed!(12234)\n\nfunction ulam_system(dx, x, p, t)\n    f(x) = 2 - x^2\n    Œµ = p[1]\n    dx[1] = f(Œµ*x[length(dx)] + (1-Œµ)*x[1])\n    for i in 2:length(dx)\n        dx[i] = f(Œµ*x[i-1] + (1-Œµ)*x[i])\n    end\nend\n\nds = DiscreteDynamicalSystem(ulam_system, rand(100) .- 0.5, [0.04])\ntrajectory(ds, 1000; Ttr = 1000)\n\nŒµs = 0.02:0.02:1.0\nte_x1x2 = zeros(length(Œµs)); te_x2x1 = zeros(length(Œµs))\n\nfor (i, Œµ) in enumerate(Œµs)\n    set_parameter!(ds, 1, Œµ)\n    tr = trajectory(ds, 2000; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    binning = RectangularBinning(0.2) # guess an appropriate bin width of 0.2\n    te_x1x2[i] = transferentropy(X1, X2, VisitationFrequency(binning), base = 2)\n    te_x2x1[i] = transferentropy(X2, X1, VisitationFrequency(binning), base = 2)\nend\n\nplot()\nplot(Œµs, te_x1x2, label = \"X1 to X2\", c = :black, lw = 1.5)\nplot!(Œµs, te_x2x1, label = \"X2 to X1\", c = :red)\nxlabel!(\"epsilon\")\nylabel!(\"Transfer entropy (bits)\")\nsavefig(\"ulam-te.svg\"); nothing # hide","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"(Image: )","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"As expected, transfer entropy from X1 to X2 is higher than from X2 to X1 across parameter values for Œµ. But, by our definition of the ulam system, dynamical coupling only occurs from X1 to X2. The results, however,  show nonzero transfer entropy in both directions. What does this mean? ","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Computing transfer entropy from finite time series introduces bias, and so does any particular choice of entropy estimator used to calculate it. To determine whether a transfer entropy estimate should be trusted, we can employ surrogate testing. We'll generate surrogate using TimeseriesSurrogates.jl.","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"In the example below, we continue with the same time series generated above. However, at each value of Œµ, we also compute transfer entropy for nsurr = 50 different randomly shuffled (permuted) versions of the source process.  If the original transfer entropy exceeds that of some percentile the transfer entropy estimates of the surrogate ensemble, we will take that as \"significant\" transfer entropy.","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"nsurr = 50\nte_x1x2 = zeros(length(Œµs)); te_x2x1 = zeros(length(Œµs))\nte_x1x2_surr = zeros(length(Œµs), nsurr); te_x2x1_surr = zeros(length(Œµs), nsurr)\n\nfor (i, Œµ) in enumerate(Œµs)\n    set_parameter!(ds, 1, Œµ)\n    tr = trajectory(ds, 1000; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    binning = RectangularBinning(0.2) # guess an appropriate bin width of 0.2\n    est = VisitationFrequency(binning)\n    te_x1x2[i] = transferentropy(X1, X2, est, base = 2)\n    te_x2x1[i] = transferentropy(X2, X1, est, base = 2)\n    s1 = surrogenerator(X1, RandomShuffle()); s2 = surrogenerator(X2, RandomShuffle())\n\n    for j = 1:nsurr\n        te_x1x2_surr[i, j] =  transferentropy(s1(), X2, est, base = 2)\n        te_x2x1_surr[i, j] =  transferentropy(s2(), X1, est, base = 2)\n    end\nend\n\n# Compute 95th percentiles of the surrogates for each Œµ\nqs_x1x2 = [quantile(te_x1x2_surr[i, :], 0.95) for i = 1:length(Œµs)]\nqs_x2x1 = [quantile(te_x2x1_surr[i, :], 0.95) for i = 1:length(Œµs)]\n\nplot(xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\", legend = :topleft)\nplot!(Œµs, te_x1x2, label = \"X1 to X2\", c = :black, lw = 1.5)\nplot!(Œµs, qs_x1x2, label = \"\", c = :black, ls = :dot, lw = 1.5)\nplot!(Œµs, te_x2x1, label = \"X2 to X1\", c = :red)\nplot!(Œµs, qs_x2x1, label = \"\", c = :red, ls = :dot)\n\nsavefig(\"ulam-te-surr.svg\"); nothing # hide","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"(Image: )","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The plot above shows the original transfer entropies (solid lines) and the 95th percentile transfer entropies of the surrogate ensembles (dotted lines). As expected, using the surrogate test, the transfer entropies from X1 to X2 are mostly significant (solid black line is above dashed black line). The transfer entropies from X2 to X1, on the other hand, are mostly not significant (red solid line is below red dotted line).","category":"page"},{"location":"TransferEntropy/","page":"Transfer entropy","title":"Transfer entropy","text":"[Schreiber2000](Schreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.)","category":"page"},{"location":"invariant_measure/#Invariant-measures-and-transfer-operators","page":"Invariant measures and transfer operators","title":"Invariant measures and transfer operators","text":"","category":"section"},{"location":"invariant_measure/","page":"Invariant measures and transfer operators","title":"Invariant measures and transfer operators","text":"invariantmeasure\nInvariantMeasure\ntransfermatrix","category":"page"},{"location":"invariant_measure/#Entropies.invariantmeasure","page":"Invariant measures and transfer operators","title":"Entropies.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, œµ::RectangularBinning) ‚Üí iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into  rectangular boxes dictated by the binning scheme œµ, then approximate the transfer  (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator,  compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, Entropies\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins \ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) ‚Üí (œÅ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins.  The element œÅ[i] is the probability of visitation to the box bins[i]. Analogous to  binhist. \n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain  probabilities? In fact, the naive histogram approach and the  transfer operator approach are equivalent in the limit of long enough time series  (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that  orbits visit a certain region of the state space. The transfer operator encodes that  information too, but comes with the added benefit of knowing the transition  probabilities between states (see transfermatrix). \n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"invariant_measure/#Entropies.InvariantMeasure","page":"Invariant measures and transfer operators","title":"Entropies.InvariantMeasure","text":"InvariantMeasure(to, œÅ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant  measure œÅ, as well as the transfer operator to from which it is computed (including  bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"invariant_measure/#Entropies.transfermatrix","page":"Invariant measures and transfer operators","title":"Entropies.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) ‚Üí (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds  to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the  probability of jumping from the state defined by bins[i] to the state defined by  bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Example-systems","page":"Example systems","title":"Example systems","text":"","category":"section"},{"location":"example_systems/#continuous_systems","page":"Example systems","title":"Continuous","text":"","category":"section"},{"location":"example_systems/#Mediated-link","page":"Example systems","title":"Mediated link","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"ExampleSystems.mediated_link(;u‚ÇÄ = rand(9), œâx = 1, œây = 1.015, œâz = 0.985,\n    k = 0.15, l = 0.2, m = 10.0, c = 0.06)","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.mediated_link-Tuple{}","page":"Example systems","title":"CausalityTools.ExampleSystems.mediated_link","text":"mediated_link(;u‚ÇÄ = rand(9), œâx = 1, œây = 1.015, œâz = 0.985,\n    k = 0.15, l = 0.2, m = 10.0, \n    c = 0.06) ‚Üí ContinuousDynamicalSystem\n\nInitialise a three-subsystem dynamical system where X and Y are driven by Z. At the default value of the coupling constant c = 0.06, the responses X and Y are already synchronized to the driver Z.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndx_1 = -omega_x x_2 - x_3 + c*(z_1 - x_1) \ndx_2 = omega_x x_1 + k*x_2  \ndx_3 = l + x_3(x_1 - m)  \ndy_1 = -omega_y y_2 - y_3 + c*(z_1 - y_1)  \ndy_2 = omega_y y_1 + k*y_2  \ndy_3 = l + y_3(y_1 - m)  \ndz_1 = -omega_z z_2 - z_3  \ndz_2 = omega_z z_1 + k*z_2  \ndz_3 = l + z_3(z_1 - m)\nendaligned\n\nReferences\n\nKrakovsk√°, Anna, et al. \"Comparison of six methods for the detection of   causality in a bivariate time series.\" Physical Review E 97.4 (2018): 042207\n\n\n\n\n\n","category":"method"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-Lorenz-systems","page":"Example systems","title":"Two bidirectionally coupled 3D Lorenz systems","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"ExampleSystems.lorenz_lorenz_bidir","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.lorenz_lorenz_bidir","page":"Example systems","title":"CausalityTools.ExampleSystems.lorenz_lorenz_bidir","text":"lorenz_lorenz_bidir(; u0 = rand(6), \n    c_xy = 0.2, c_yx = 0.2, \n    a‚ÇÅ = 10, a‚ÇÇ = 28, a‚ÇÉ = 8/3, \n    b‚ÇÅ = 10, b‚ÇÇ = 28, b‚ÇÉ = 9/3) ‚Üí ContinuousDynamicalSystem\n\nInitialise a bidirectionally coupled Lorenz-Lorenz system, where each  subsystem is a 3D Lorenz system [1]. Default values for the parameters  a‚ÇÅ, a‚ÇÇ, a‚ÇÉ, b‚ÇÅ, b‚ÇÇ, b‚ÇÉ are as in [1].\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = -a_1 (x_1 - x_2) + c_yx(y_1 - x_1) \ndotx_2 = -x_1 x_3 + a_2 x_1 - x_2 \ndotx_3 = x_1 x_2 - a_3 x_3 \ndoty_1 = -b_1 (y_1 - y_2) + c_xy (x_1 - y_1) \ndoty_2 = -y_1 y_3 + b_2 y_1 - y_2 \ndoty_3 = y_1 y_2 - b_3 y_3\nendaligned\n\nReferences\n\nAmig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-Lorenz-systems-forced-by-another-3D-Lorenz-system","page":"Example systems","title":"Two bidirectionally coupled 3D Lorenz systems forced by another 3D Lorenz system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"lorenz_lorenz_lorenz_bidir_forced","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_bidir_forced","page":"Example systems","title":"CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_bidir_forced","text":"lorenz_lorenz_lorenz_bidir_forced(; u0 = rand(9), \n    c_xy = 0.1, c_yx = 0.1,\n    c_zx = 0.05, c_zy = 0.05, \n    a‚ÇÅ = 10, a‚ÇÇ = 28, a‚ÇÉ = 8/3,\n    b‚ÇÅ = 10, b‚ÇÇ = 28, b‚ÇÉ = 8/3,\n    c‚ÇÅ = 10, c‚ÇÇ = 28, c‚ÇÉ = 8/3)\n\nInitialise a system consisting of two bidirectionally coupled 3D Lorenz  systems forced by an external 3D Lorenz system, giving a 9D system.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = - a_1 (x_1 - x_2) + c_yx(y_1 - x_1) + c_zx(z_1 - x_1) \ndotx_2 = - x_1 x_3 + a_2 x_1 - x_2 \ndotx_3 = x_1 x_2 - a_3 x_3 \ndoty_1 = -b_1 (y_1 - y_2) + c_xy (x_1 - y_1) + c_zy(z_1 - y_1) \ndoty_2 = - y_1 y_3 + b_2 y_1 - y_2 \ndoty_3 = y_1 y_2 - b_3 y_3 \ndotz_1 = - c_1 (z_1 - z_2) \ndotz_2 = - z_1 z_3 + c_2 z_1 - z_2 \ndotz_3 = z_1 z_2 - c_3 z_3 \nendaligned\n\nReferences\n\nAmig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Three-transitively-connected-3D-Lorenz-systems","page":"Example systems","title":"Three transitively connected 3D Lorenz systems","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"lorenz_lorenz_lorenz_transitive","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_transitive","page":"Example systems","title":"CausalityTools.ExampleSystems.lorenz_lorenz_lorenz_transitive","text":"lorenz_lorenz_lorenz_transitive(;u‚ÇÄ=rand(9),\n            œÉ‚ÇÅ = 10.0, œÉ‚ÇÇ = 10.0, œÉ‚ÇÉ = 10.0,\n            œÅ‚ÇÅ = 28.0, œÅ‚ÇÇ = 28.0, œÅ‚ÇÉ = 28.0,\n            Œ≤‚ÇÅ = 8/3,  Œ≤‚ÇÇ = 8/3,  Œ≤‚ÇÉ = 8.3,\n            c‚ÇÅ‚ÇÇ = 1.0, c‚ÇÇ‚ÇÉ = 1.0) ‚Üí ContinuousDynamicalSystem\n\nInitalise a dynamical system consisting of three coupled Lorenz attractors with a transitive causality chain where X‚ÇÅ ‚Üí X‚ÇÇ and X‚ÇÇ ‚Üí X‚ÇÉ. In total, the three 3D-subsystems create a 9-dimensional dynamical system.\n\nThe strength of the forcing X‚ÇÅ ‚Üí X‚ÇÇ is controlled by the parameter c‚ÇÅ, and the forcing from X‚ÇÇ ‚Üí X‚ÇÉ by c‚ÇÇ. The remaining parameters are the usual parameters for the Lorenz system, where the subscript i refers to the subsystem X·µ¢. \n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = sigma_1(y_1 - x_1) \ndoty_1 = rho_1 x_1 - y_1 - x_1 z_1 \ndotz_1 = x_1 y_1 - beta_1 z_1 \ndotx_2 =  sigma_2 (y_2 - x_2) + c_12(x_1 - x_2) \ndoty_2 = rho_2 x_2 - y_2 - x_2 z_2 \ndotz_2 = x_2 y_2 - beta_2 z_2 \ndotx_3 = sigma_3 (y_3 - x_3) + c_23 (x_2 - x_3) \ndoty_3 = rho_3 x_3 - y_3 - x_3 z_3 \ndotz_3 = x_3 y_3 - beta_3 z_3\nendaligned\n\nUsage in literature\n\nThis system was studied by Papana et al. (2013) for coupling strengths  c_12 = 0 1 3 5 and c_23 = 0 1 3 5.\n\nReferences\n\nPapana et al., Simulation Study of Direct Causality Measures in Multivariate   Time Series. Entropy 2013, 15(7), 2635-2661; doi:10.3390/e15072635\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-R√∂ssler-systems","page":"Example systems","title":"Two bidirectionally coupled 3D R√∂ssler systems","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"rossler_rossler_bidir","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.rossler_rossler_bidir","page":"Example systems","title":"CausalityTools.ExampleSystems.rossler_rossler_bidir","text":"rossler_rossler_bidir(; u0 = rand(6), \n    œâ‚ÇÅ = 1.015, œâ‚ÇÇ = 0.985, \n    c_xy = 0.1, c_yx = 0.1, \n    a‚ÇÅ = 0.15, a‚ÇÇ = 0.2, a‚ÇÉ = 10,\n    b‚ÇÅ = 0.15, b‚ÇÇ = 0.2, b‚ÇÉ = 10)\n\nInitialise a system of two bidirectionally coupled 3D R√∂ssler systems.  This system has been modified from [1] to allow other parameterisations,  but default parameters are as in [1].\n\nThe X and Y subsystems are mostly synchronized for  c_xy > 0.1 or c_yx > 0.1.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = -omega_1(x_2 + x_3) + c_yx(y_1 - x_1) \ndotx_2 = omega_1 x_1 + a_1 x_2 \ndotx_3 = a_2 + x_3 (x_1 - a_3) \ndoty_1 = -omega_2 (y_2 + y_3) + c_xy(x_1 - y_1) \ndoty_2 = omega_2 y_1 + b_1 y_2 \ndoty_3 = b_2 + y_3 (y_1 - b_3)\nendaligned\n\nReferences\n\nAmig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Two-bidirectionally-coupled-3D-R√∂ssler-systems-forced-by-another-3D-R√∂ssler-system","page":"Example systems","title":"Two bidirectionally coupled 3D R√∂ssler systems forced by another 3D R√∂ssler system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"rossler_rossler_rossler_bidir_forced","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.rossler_rossler_rossler_bidir_forced","page":"Example systems","title":"CausalityTools.ExampleSystems.rossler_rossler_rossler_bidir_forced","text":"rossler_rossler_rossler_bidir_forced(; u0 = rand(9), \n    œâ‚ÇÅ = 1.015, œâ‚ÇÇ = 0.985, œâ‚ÇÉ = 0.95,\n    c_xy = 0.1, c_yx = 0.1,\n    c_zx = 0.05, c_zy = 0.05, \n    a‚ÇÅ = 0.15, a‚ÇÇ = 0.2, a‚ÇÉ = 10,\n    b‚ÇÅ = 0.15, b‚ÇÇ = 0.2, b‚ÇÉ = 10,\n    c‚ÇÅ = 0.15, c‚ÇÇ = 0.2, c‚ÇÉ = 10)\n\nEquations of motion for a system consisting of three coupled 3D R√∂ssler systems  (X, Y, Z), giving a 9D system [1]. The external system  Z influences both X and Y (controlled by c_zx and c_zy).  Simultaneously, the subsystems  X and Y bidirectionally  influences each other (controlled by c_xy and c_yx).\n\nThe X and Y subsystems are mostly synchronized for c_xy > 0.1 or  c_yx > 0.1.\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = -omega_1 (x_2 + x_3) + c_yx(y_1 - x_1) + c_zx(z_1 - x_1) \ndotx_2 = omega_1 x_1 + a_1 x_2 \ndotx_3 = a_2 + x_3 (x_1 - a_3) \ndoty_1 = -omega_1 (y_2 + y_3) + c_xy(x_1 - y_1) + c_zy(z_1 - y_1) \ndotx_2 = omega_2 y_1 + b_1 y_2 \ndotx_3 = b_2 + x_3 (y_1 - b_3) \ndoty_1 = -omega_2 (z_2  + z_3) \ndotx_2 = omega_2 z_1 + c_1 z_2 \ndotx_3 = c_2 + z_3 (z_1 - c_3)\nendaligned\n\nReferences\n\nAmig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302. \n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Unidirectonal-forcing-from-a-3D-R√∂ssler-system-to-a-3D-Lorenz-system","page":"Example systems","title":"Unidirectonal forcing from a 3D R√∂ssler system to a 3D Lorenz system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"rossler_lorenz","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.rossler_lorenz","page":"Example systems","title":"CausalityTools.ExampleSystems.rossler_lorenz","text":"rossler_lorenz(;u‚ÇÄ = rand(6), a‚ÇÅ = -6, a‚ÇÇ = 6, a‚ÇÉ = 2.0, \n    b‚ÇÅ = 10, b‚ÇÇ = 28, b‚ÇÉ = 8/3, c_xy = 1) ‚Üí ContinuousDynamicalSystem\n\nInitialise a R√∂ssler-Lorenz system consisting of two independent 3D subsystems: one R√∂ssler system and one Lorenz system. They are coupled such that the second component (x‚ÇÇ) of the R√∂ssler system unidirectionally forces the second component (y‚ÇÇ) of the Lorenz system. \n\nThe parameter c_xy controls the coupling strength. The implementation here also  allows for tuning the parameters of each subsystem by introducing the constants  a‚ÇÅ, a‚ÇÇ, a‚ÇÉ, b‚ÇÅ, b‚ÇÇ, b‚ÇÉ. Default values for these parameters are  as in [1].\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndot x_1 = a_1(x_2 + x_3) \ndot x_2 = a_2(x_1 + 02x_2) \ndot x_3 = a_2(02 + x_3(x_1 - a_3)) \ndot y_1 = b_1(y_2 - y_1) \ndot y_2 = y_1(b_2 - y_3) - y_2 +c_xy(x_2)^2 \ndot y_3 = y_1 y_2 - b_3y_3\nendaligned\n\nwith the coupling constant c_xy geq 0.\n\nReferences\n\nKrakovsk√°, Anna, et al. \"Comparison of six methods for the detection of causality in a   bivariate time series.\" Physical Review E 97.4 (2018):042207.   https://journals.aps.org/pre/abstract/10.1103/PhysRevE.97.042207\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#N-scroll-Chua-attractors","page":"Example systems","title":"N-scroll Chua attractors","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"chuacircuit_nscroll_sine","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.chuacircuit_nscroll_sine","page":"Example systems","title":"CausalityTools.ExampleSystems.chuacircuit_nscroll_sine","text":"chuacircuit_nscroll_sine(;u‚ÇÄ = [0.0, 0.0, 0.28695],\n    Œ± = 10.814, Œ≤ = 14, Œ≥ = 0, a = 1.3, b = 0.11, c = 2,\n    œÉx = 0.0, œÉy = 0.0, œÉz = 0.0)\n\nInitialise an adjusted Chua system giving rise to n-scroll attractors [1].\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx = alpha (y - fx) + eta x \ndoty = x - y + z + eta y \ndotz = -beta y - gamma z + eta z \nendaligned\n\nwhere eta x, eta z, and eta z are drawn independently from  normal distributions with zero mean and standard deviations œÉx, œÉy  and œÉz at each iteration.\n\nfx is given by the following conditions: \n\nn::Int = c + 1\n\nif x >= 2*a*c\n    fx = (b*pi/2*a)*(x - 2*a*c)\nelseif -2*a*c < x < 2*a*c\n    d = ifelse(isodd(n), pi, 0)\n    fx = -b*sin((pi*x/2*a) + d)\nelseif x <= -2*a*c\n    fx = (b*pi/2*a)*(x + 2*a*c)\nend\n\nReferences\n\nTang, Wallace KS, et al. \"Generation of n-scroll attractors via   sine function.\" IEEE Transactions on Circuits and Systems I:   Fundamental Theory and Applications 48.11 (2001): 1369-1372.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#discrete_systems","page":"Example systems","title":"Discrete","text":"","category":"section"},{"location":"example_systems/#ulam","page":"Example systems","title":"Ulam map","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"ulam","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.ulam","page":"Example systems","title":"CausalityTools.ExampleSystems.ulam","text":"ulam(D::Int = 10; u‚ÇÄ = rand(D), Œµ::Real = 0.10) ‚Üí DiscreteDynamicalSystem\n\nA lattice of D unidirectionally coupled ulam maps[Schreiber2000] defined as \n\nx^m_t+1 = f(epsilon x^m-1_t + (1 - epsilon) x_t^m)\n\nwhere m = 1 2 ldots D and f(x) = 2 - x^2. In this system, information transfer  happens only in the direction of increasing m.\n\n[Schreiber2000]: Schreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#system_ar1","page":"Example systems","title":"Autoregressive order one 2D system","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"ar1_unidir","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.ar1_unidir","page":"Example systems","title":"CausalityTools.ExampleSystems.ar1_unidir","text":"ar1_unidir(u‚ÇÄ, a‚ÇÅ = 0.90693, b‚ÇÅ = 0.40693, c_xy = 0.5, \n    œÉ = 0.40662) ‚Üí DiscreteDynamicalSystem\n\nA bivariate, order one autoregressive model, where x to y [1].\n\nEquations of motion\n\nbeginaligned\nx(t+1) = a_1 x(t) + xi_1 \ny(t+1) = b_1 y(t) - c_xy x + xi_2\nendaligned\n\nwhere xi_1 and xi_2 are drawn from normal distributions  with zero mean and standard deviation œÉ at each iteration.\n\nReferences\n\nPalu≈°, M., Krakovsk√°, A., Jakub√≠k, J., & Chvostekov√°, M. (2018). Causality,  dynamical systems and the arrow of time. Chaos: An Interdisciplinary Journal of  Nonlinear Science, 28(7), 075307. http://doi.org/10.1063/1.5019944\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#system_nonlinear3d","page":"Example systems","title":"Nonlinear 3D system with nonlinear coupling","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"nonlinear3d","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.nonlinear3d","page":"Example systems","title":"CausalityTools.ExampleSystems.nonlinear3d","text":"nonlinear3d(;u‚ÇÄ = rand(3), \n    œÉ‚ÇÅ = 1.0, œÉ‚ÇÇ = 1.0, œÉ‚ÇÉ = 1.0, \n    a‚ÇÅ = 3.4, a‚ÇÇ = 3.4, a‚ÇÉ = 3.4, \n    b‚ÇÅ = 0.4, b‚ÇÇ = 0.4, b‚ÇÉ = 0.4, \n    c‚ÇÅ‚ÇÇ = 0.5, c‚ÇÇ‚ÇÉ = 0.3, c‚ÇÅ‚ÇÉ = 0.5) ‚Üí DiscreteDynamicalSystem\n\nA 3d nonlinear system with nonlinear couplings x_1 to x_2,  x_2 to x_3 and x_1 to x_3. Modified from [1]. \n\nEquations of motion\n\nThe equations of motion are\n\nbeginaligned\nx_1(t+1) = a_1 x_1 (1-x_1(t))^2  e^-x_2(t)^2 + 04 xi_1(t) \nx_2(t+1) = a_1 x_2 (1-x_2(t))^2  e^-x_2(t)^2 + 04 xi_2(t) + b x_1 x_2 \nx_3(t+1) = a_3 x_3 (1-x_3(t))^2  e^-x_3(t)^2 + 04 xi_3(t) + c x_2(t) + d x_1(t)^2\nendaligned\n\nReferences\n\nGour√©vitch, B., Le Bouquin-Jeann√®s, R., & Faucon, G. (2006). Linear and nonlinear   causality between signals: methods, examples and neurophysiological   applications. Biological Cybernetics, 95(4), 349‚Äì369.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#system_logistic2_unidir","page":"Example systems","title":"Unidirectionally coupled 2D logistic maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic2_unidir","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic2_unidir","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic2_unidir","text":"logistic2(;u‚ÇÄ = rand(2), c_xy = 0.1, œÉ = 0.05,\n    r‚ÇÅ = 3.78, r‚ÇÇ = 3.66) ‚Üí DiscreteDynamicalSystem\n\nInitialise a system consisting of two coupled logistic maps where X unidirectionally influences Y. By default, the parameters r‚ÇÅ and r‚ÇÇ are set to values yielding chaotic behaviour.\n\nEquations of motion\n\nThe equations of motion are\n\nbeginaligned\nx(t+1) = r_1 x(t)(1 - x(t)) \ny(t+1) = r_2 f(xy)(1 - f(xy))\nendaligned\n\nwith\n\nbeginaligned\nf(xy) = dfracy + fracc_xy(x xi )21 + fracc_xy2(1+ sigma )\nendaligned\n\nThe parameter c_xy controls how strong the dynamical forcing is. If œÉ > 0, dynamical noise masking the influence of  x on y equivalent to sigma cdot xi is added at each iteration. Here,xi is a draw from a flat distribution on 0 1. Thus, setting œÉ = 0.05 is equivalent to add dynamical noise corresponding to a maximum of 5  of the possible range of values of the logistic map.\n\nReferences\n\nDiego, David, Kristian Agas√∏ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#system_logistic2_bidir","page":"Example systems","title":"Bidirectionally coupled 2D logistic maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic2_bidir","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic2_bidir","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic2_bidir","text":"logistic2_bidir(;u‚ÇÄ = rand(2), c_xy = 0.1, c_yx = 0.1, \n    r‚ÇÅ = 3.78, r‚ÇÇ = 3.66, œÉ_xy = 0.05, œÉ_yx = 0.05)\n\nA bidirectional logistic model for the chaotic population dynamics of two interacting  species [1].\n\nEquations of motion\n\nThe equations of motion are \n\nbeginalign\nx(t+1) = r_1 f_yx^t(1 - f_yx^t) \ny(t+1) = r_2 f_xy^t(1 - f_xy^t) \nf_xy^t = dfracy(t) + c_xy(x(t) + sigma_xy xi_xy^t )1 + c_xy (1 + sigma_xy )  \nf_yx^t = dfracx(t) + c_yx(y(t) + sigma_yx xi_yx^t )1 + c_yx (1 + sigma_yx )\nendalign\n\nwhere the coupling strength c_xy controls how strongly species x influences species  y, and vice versa for c_yx. To simulate time-varying influence of unobserved  processes, we use the dynamical noise terms xi_xy^t and xi_yx^t, drawn from a  uniform distribution with support on 0 1. If sigma_xy  0, then the influence  of x on y is masked by dynamical noise equivalent to sigma_xy xi_xy^t at  the t-th iteration of the map, and vice versa for sigma_yx.\n\nReferences\n\nDiego, David, Kristian Agas√∏ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Forcing-of-two-independent-logistic-maps-from-common-logistic-map-driver","page":"Example systems","title":"Forcing of two independent logistic maps from common logistic map driver","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic3","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic3","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic3","text":"logistic3(;u‚ÇÄ = rand(3), r = 4,\n    œÉx = 0.05, œÉy = 0.05, œÉz = 0.05) ‚Üí DiscreteDynamicalSystem\n\nInitialise a dynamical system consisting of three coupled logistic map representing the response of two independent dynamical variables to the forcing from a common driver. The dynamical influence goes in the directions Z to X and Z to Y.\n\nEquations of motion\n\nThe equations of motion are\n\nbeginaligned\nx(t+1) = (x(t)(r - r_1 x(t) - z(t) + œÉ_x Œ∑_x)) mod 1 \ny(t+1) = (y(t)(r - r_2 y(t) - z(t) + œÉ_y Œ∑_y)) mod 1 \nz(t+1) = (z(t)(r - r_3 z(t) + œÉ_z Œ∑_z)) mod 1\nendaligned\n\nDynamical noise may be added to each of the dynamical variables by tuning the parameters œÉz, œÉx and œÉz. Default values for the parameters r‚ÇÅ, r‚ÇÇ and r‚ÇÉ are set such that the system exhibits chaotic behaviour, with r‚ÇÅ = r‚ÇÇ = r‚ÇÉ = 4.\n\nReferences\n\nRunge, Jakob. Causal network reconstruction from time series: From theoretical   assumptions to practical estimation, Chaos 28, 075310 (2018);   doi: 10.1063/1.5025050\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Unidirectional,-transitive-chain-of-logistic-maps","page":"Example systems","title":"Unidirectional, transitive chain of logistic maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"logistic4","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.logistic4","page":"Example systems","title":"CausalityTools.ExampleSystems.logistic4","text":"logistic4(;u‚ÇÄ = rand(4), r‚ÇÅ = 3.9, r‚ÇÇ = 3.6, r‚ÇÉ = 3.6, r‚ÇÑ = 3.8,\n    c‚ÇÅ‚ÇÇ = 0.4, c‚ÇÇ‚ÇÉ = 0.4, c‚ÇÉ‚ÇÑ = 0.35) ‚Üí DiscreteDynamicalSystem\n\nInitialise a system of a transitive chain of four unidirectionally coupled logistic maps, where y_1 to y_2 to y_3 to y_4 [1]. Default  parameters are as in [1].\n\nNote: With the default parameters which are as in [1], for some initial conditions,  this system wanders off to pm infty for some of the variables. Make sure that  you have a good realisation before using the orbit for anything.\n\nEquations of motion\n\nbeginaligned\ny_1(t+1) = y_1(t)(r_1 - r_1 y_1) \ny_2(t+1) = y_2(t)(r_2 - c_12 y_1 - r_2 y_2) \ny_3(t+1) = y_3(t)(r_3 - c_23 y_2 - r_3 y_3) \ny_4(t+1) = y_4(t)(r_4 - c_34 y_3 - r_4 y_4)\nendaligned\n\nReferences\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using  convergent cross mapping.\" Scientific reports 5 (2015): 14750\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#henon2d","page":"Example systems","title":"Two unidirectionally coupled Henon maps","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"henon2","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.henon2","page":"Example systems","title":"CausalityTools.ExampleSystems.henon2","text":"henon2(;u‚ÇÄ = rand(4), c_xy = 2.0) ‚Üí DiscreteDynamicalSystem\n\nInitialize a 2D Henon system consisting of two identical Henon maps with unidirectional forcing X to Y [1].\n\nEquations of motion\n\nThe equations of motion are \n\nbeginaligned\nx_1(t+1) = 14 - x_1^2(t) + 03x_2(t) \nx_2(t+1) = x_1(t) \ny_1(t+1) = 14 - c_xy x_1(t) y_1(t) + (1-c_xy) y_1^2(t) + 03 y_2(t) \ny_2(t+1) = y_1(t)\nendaligned\n\nReferences\n\nKrakovsk√°, A., Jakub√≠k, J., Chvostekov√°, M., Coufal, D., Jajcay, N., & Palu≈°, M. (2018).   Comparison of six methods for the detection of causality in a bivariate time series.   Physical Review E, 97(4), 042207.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Strange,-nonchaotic-attractors","page":"Example systems","title":"Strange, nonchaotic attractors","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"anishchenko1","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.anishchenko1","page":"Example systems","title":"CausalityTools.ExampleSystems.anishchenko1","text":"anishchenko1(;u‚ÇÄ = rand(2), Œ± =3.277, s=0.1, œâ=0.5*(sqrt(5)-1)) ‚Üí DiscreteDynamicalSystem\n\nInitialise the system defined by eq. 13 in [1], which can give strange,  nonchaotic attractors.\n\nEquations of motion\n\nThe equations of motion are \n\nbeginaligned\ndx = alpha (1-s cos (2 pi phi )) cdot x(1-x) \ndœï = (phi + omega ) mod1\nendaligned\n\nReferences\n\nAnishchenko, Vadim S., and Galina I. Strelkova. \"Irregular attractors.\"  Discrete dynamics in Nature and Society 2.1 (1998): 53-72.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#ikeda2d_bidir","page":"Example systems","title":"Bidirectional Ikeda map","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"ikeda","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.ikeda","page":"Example systems","title":"CausalityTools.ExampleSystems.ikeda","text":"ikeda(; u‚ÇÄ = rand(2), c_xy = 1.0, c_yx = 1.0, a = 0.8, b = 12, c = 0.9,\n    r‚ÇÅ = rand(Uniform(0.01, 0.3)), r‚ÇÇ = rand(Uniform(0.01, 0.3)), œÉ = 0.05)\n\nInitialise a discrete two-dimensional Ikeda map system, adapted from [1] by adding a noise term and allowing the influences from x to y (c_xy) and  from y to x (c_yx) to be adjusted.\n\nAs a rule-of-thumb, if parameters a, b, and c are drawn from uniform  distributions on [0.8, 1.5], [10, 14] and [0.1, 0.9].\n\nThe difference equations are\n\nbeginaligned\nx(t+1) = 1 + mu(x cos(theta) - c_yx y sin(theta)) - min(dfracsigma xi_t^(1))(1-x) xi_t^(2) \ny(t+1) = mu(y cos(theta) - c_xy x sin(theta)) - min(dfracsigma zeta_t^(1))(1-y) zeta_t^(2)\nendaligned\n\nReferences\n\nCao, Liangyue, Alistair Mees, and Kevin Judd. \"Modeling and predicting   non-stationary time series.\" International Journal of Bifurcation and   Chaos 7.08 (1997): 1823-1831.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/#Noise","page":"Example systems","title":"Noise","text":"","category":"section"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"noise_uu","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.noise_uu","page":"Example systems","title":"CausalityTools.ExampleSystems.noise_uu","text":"noise_uu(n::Int, lo = - 1, hi = 1)\n\nGenerate a signal consisting of n steps of uncorrelated uniform noise from  a uniform distribution on [lo, hi].\n\n\n\n\n\n","category":"function"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"noise_ug","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.noise_ug","page":"Example systems","title":"CausalityTools.ExampleSystems.noise_ug","text":"noise_ug(n::Int; Œº = 0, œÉ = 1)\n\nGenerate a signal consisting of n steps of uncorrelated Gaussian noise from a normal distribution with mean Œº and standard deviation œÉ.\n\n\n\n\n\n","category":"function"},{"location":"example_systems/","page":"Example systems","title":"Example systems","text":"noise_brownian","category":"page"},{"location":"example_systems/#CausalityTools.ExampleSystems.noise_brownian","page":"Example systems","title":"CausalityTools.ExampleSystems.noise_brownian","text":"noise_brownian(n::Int; lo = - 1, hi = 1)\nnoise_brownian(d::Distribution, n::Int)\n\nGenerate a signal consisting of n steps of Brownian noise, generated as the zero-mean and unit standard deviation normalised cumulative sum of noise generated from a uniform distribution on [lo, hi]. Optionally, a distribution d from which to sample can be provided.\n\nExamples\n\n# Based on uncorrelated uniform noise\nnoise_brownian(100)\nnoise_brownian(100, lo = -2, hi = 2)\nnoise_brownian(Uniform(-3, 3), 100)\n\n# Based on uncorrelated Gaussian noise\nŒº, œÉ = 0, 2\nnoise_brownian(Normal(Œº, œÉ), 100)\n\n\n\n\n\n","category":"function"},{"location":"generalized_entropy/#Generalized-entropy","page":"Generalized entropy","title":"Generalized entropy","text":"","category":"section"},{"location":"generalized_entropy/","page":"Generalized entropy","title":"Generalized entropy","text":"The following two functions are used for probability and entropy estimation:","category":"page"},{"location":"generalized_entropy/","page":"Generalized entropy","title":"Generalized entropy","text":"probabilities which computes probability distributions of given datasets\ngenentropy which uses the output of probabilities, or a set of   pre-computed Probabilities, to calculate entropies.","category":"page"},{"location":"generalized_entropy/","page":"Generalized entropy","title":"Generalized entropy","text":"See the Entropies.jl documentation for  details.","category":"page"},{"location":"generalized_entropy/","page":"Generalized entropy","title":"Generalized entropy","text":"Entropies.genentropy","category":"page"},{"location":"generalized_entropy/#Entropies.genentropy","page":"Generalized entropy","title":"Entropies.genentropy","text":"genentropy(p::Probabilities; q = 1.0, base = MathConstants.e)\n\nCompute the generalized order-q entropy of some probabilities returned by the probabilities function. Alternatively, compute entropy from pre-computed Probabilities.\n\ngenentropy(x::Vector_or_Dataset, est; q = 1.0, base)\n\nA convenience syntax, which calls first probabilities(x, est) and then calculates the entropy of the result (and thus est can be a ProbabilitiesEstimator or simply Œµ::Real).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the generalized (R√©nyi) entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\n[R√©nyi1960]: A. R√©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"function"},{"location":"generalized_entropy/","page":"Generalized entropy","title":"Generalized entropy","text":"Probabilities\nprobabilities\nprobabilities!\nProbabilitiesEstimator","category":"page"},{"location":"generalized_entropy/#Entropies.Probabilities","page":"Generalized entropy","title":"Entropies.Probabilities","text":"Probabilities(x) ‚Üí p\n\nA simple wrapper type around an x::AbstractVector which ensures that p sums to 1. Behaves identically to Vector.\n\n\n\n\n\n","category":"type"},{"location":"generalized_entropy/#Entropies.probabilities","page":"Generalized entropy","title":"Entropies.probabilities","text":"probabilities(x::Vector_or_Dataset, est::ProbabilitiesEstimator) ‚Üí p::Probabilities\n\nCalculate probabilities representing x based on the provided estimator and return them as a Probabilities container (Vector-like). The probabilities are typically unordered and may or may not contain 0s, see the documentation of the individual estimators for more.\n\nThe configuration options are always given as arguments to the chosen estimator.\n\nprobabilities(x::Vector_or_Dataset, Œµ::AbstractFloat) ‚Üí p::Probabilities\n\nConvenience syntax which provides probabilities for x based on rectangular binning (i.e. performing a histogram). In short, the state space is divided into boxes of length Œµ, and formally we use est = VisitationFrequency(RectangularBinning(Œµ)) as an estimator, see VisitationFrequency.\n\nThis method has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes Œµ without memory overflow and with maximum performance. To obtain the bin information along with p, use binhist.\n\nprobabilities(x::Vector_or_Dataset, n::Integer) ‚Üí p::Probabilities\n\nSame as the above method, but now each dimension of the data is binned into n::Int equal sized bins instead of bins of length Œµ::AbstractFloat.\n\nprobabilities(x::Vector_or_Dataset) ‚Üí p::Probabilities\n\nDirectly count probabilities from the elements of x without any discretization, binning, or other processing (mostly useful when x contains categorical or integer data).\n\n\n\n\n\n","category":"function"},{"location":"generalized_entropy/#Entropies.probabilities!","page":"Generalized entropy","title":"Entropies.probabilities!","text":"probabilities!(args...)\n\nIdentical to probabilities(args...), but allows pre-allocation of temporarily used containers.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"generalized_entropy/#Entropies.ProbabilitiesEstimator","page":"Generalized entropy","title":"Entropies.ProbabilitiesEstimator","text":"An abstract type for probabilities estimators.\n\n\n\n\n\n","category":"type"},{"location":"generalized_entropy/","page":"Generalized entropy","title":"Generalized entropy","text":"binhist","category":"page"},{"location":"generalized_entropy/#Entropies.binhist","page":"Generalized entropy","title":"Entropies.binhist","text":"binhist(x::AbstractDataset, Œµ::Real) ‚Üí p, bins\nbinhist(x::AbstractDataset, Œµ::RectangularBinning) ‚Üí p, bins\n\nHyper-optimized histogram calculation for x with rectangular binning Œµ. Returns the probabilities p of each bin of the histogram as well as the bins. Notice that bins are the starting corners of each bin. If Œµ isa Real, then the actual bin size is Œµ across each dimension. If Œµ isa RectangularBinning, then the bin size for each dimension will depend on the binning scheme.\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"function"},{"location":"dataset/#Multivariate-[Dataset](@ref)s","page":"Multivariate Datasets","title":"Multivariate Datasets","text":"","category":"section"},{"location":"dataset/","page":"Multivariate Datasets","title":"Multivariate Datasets","text":"Univariate timeseries are given as AbstractVector{<:Real}. Multivariate time series can be represented by the Dataset-type from DelayEmbeddings.jl, where each observation is a D-dimensional data point represented by a static vector. See the DynamicalSystems.jl documentation for more info.","category":"page"},{"location":"dataset/","page":"Multivariate Datasets","title":"Multivariate Datasets","text":"Dataset","category":"page"},{"location":"dataset/#DelayEmbeddings.Dataset","page":"Multivariate Datasets","title":"DelayEmbeddings.Dataset","text":"Dataset{D, T} <: AbstractDataset{D,T}\n\nA dedicated interface for datasets. It contains equally-sized datapoints of length D, represented by SVector{D, T}. These data are contained in the field .data of a dataset, as a standard Julia Vector{SVector}.\n\nWhen indexed with 1 index, a dataset is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.\n\nDataset also supports most sensible operations like append!, push!, hcat, eachrow, among others, and when iterated over, it iterates over its contained points.\n\nDescription of indexing\n\nIn the following let i, j be integers,  typeof(data) <: AbstractDataset and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges, and for massive performance benefits make v2 an SVector{X, Int}).\n\ndata[i] == data[i, :] gives the ith datapoint (returns an SVector)\ndata[v1] == data[v1, :], returns a Dataset with the points in those indices.\ndata[:, j] gives the jth variable timeseries, as Vector\ndata[v1, v2], data[:, v2] returns a Dataset with the appropriate entries (first indices being \"time\"/point index, while second being variables)\ndata[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(dataset) or Dataset(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like Dataset(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#Mutual-information","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"mutualinfo","category":"page"},{"location":"mutualinfo/#TransferEntropy.mutualinfo","page":"Mutual information","title":"TransferEntropy.mutualinfo","text":"mutualinfo(x, y, est; base = 2, q = 1)\n\nEstimate mutual information between x and y, I^q(x y), using the provided  entropy/probability estimator est from Entropies.jl, and R√©nyi entropy of order q (defaults to q = 1, which is the Shannon entropy), with logarithms to the given base.\n\nBoth x and y can be vectors or (potentially multivariate) Datasets.\n\nWorth highlighting here are the estimators that compute entropies directly, e.g. nearest-neighbor based methhods. The choice is between naive  estimation using the KozachenkoLeonenko or Kraskov entropy estimators,  or the improved Kraskov1 and Kraskov2 dedicated I estimators. The  latter estimators reduce bias compared to the naive estimators.\n\nNote: only Shannon entropy is possible to use for nearest neighbor estimators, so the  keyword q cannot be provided; it is hardcoded as q = 1. \n\nDescription\n\nMutual information I between X and Y  is defined as \n\nI(X Y) = sum_y in Y sum_x in X p(x y) log left( dfracp(x y)p(x)p(y) right)\n\nHere, we rewrite this expression as the sum of the marginal entropies, and extend the  definition of I to use generalized R√©nyi entropies\n\nI^q(X Y) = H^q(X) + H^q(Y) - H^q(X Y)\n\nwhere H^q(cdot) is the generalized Renyi entropy of order q, i.e., the genentropy function from Entropies.jl.\n\n\n\n\n\n","category":"function"},{"location":"mutualinfo/#Synthetic-systems-example","page":"Mutual information","title":"Synthetic systems example","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In this example we generate realizations of two different systems where we know the strength of coupling between the variables. Our aim is to compute mutual information I(X Y) between time series of each variable and assess how the magnitude of I(X Y) changes as we change the strength of coupling between X and Y.","category":"page"},{"location":"mutualinfo/#Defining-the-systems","page":"Mutual information","title":"Defining the systems","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here we implement two of the example systems that come with the CausalityTools.jl package:","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"A stochastic system consisting of two unidirectionally coupled first-order autoregressive processes (ar1_unidir)\nA deterministic, chaotic system consisting of two unidirectionally coupled logistic maps (logistic2_unidir)","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"We use the default input parameter values (see ar1_unidir and logistic2_unidir for details) and below we toggle only the random initial conditions and the coupling strength parameter c_xy. For each value of c_xy we generate 1,000 unique realizations of the system and obtain 500-point time series of the coupled variables.","category":"page"},{"location":"mutualinfo/#Estimating-mutual-information","page":"Mutual information","title":"Estimating mutual information","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here we use the binning-based VisitationFrequency estimator. We summarize the distribution of I(X Y) values across all realizations using the median and quantiles encompassing 95 % of the values.","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools, Statistics, Plots\n\n# Span a range of x-y coupling strengths\nc = 0.0:0.1:1.0\n\n# Number of observations in each time series\nnpts = 500\n\n# Number of unique realizations of each system\nn_realizations = 1000\n\n# Get MI for multiple realizations of two systems, \n# saving three quantiles for each c value\nmi = zeros(length(c), 3, 2)\n\n# Define an estimator for MI\nb = RectangularBinning(4)\nmi_estimator = VisitationFrequency(b)\n\nfor i in 1 : length(c)\n    \n    tmp = zeros(n_realizations, 2)\n    \n    for k in 1 : n_realizations\n        \n        # Obtain time series realizations of the two 2D systems \n        # for a given coupling strength and random initial conditions\n        lmap = trajectory(logistic2_unidir(u‚ÇÄ = rand(2), c_xy = c[i]), npts - 1, Ttr = 1000)\n        ar1 = trajectory(ar1_unidir(u‚ÇÄ = rand(2), c_xy = c[i]), npts - 1)\n        \n        # Compute the MI between the two coupled components of each system\n        tmp[k, 1] = mutualinfo(lmap[:, 1], lmap[:, 2], mi_estimator)\n        tmp[k, 2] = mutualinfo(ar1[:, 1], ar1[:, 2], mi_estimator)\n    end\n    \n    # Compute lower, middle, and upper quantiles of MI for each coupling strength\n    mi[i, :, 1] = quantile(tmp[:, 1], [0.025, 0.5, 0.975])\n    mi[i, :, 2] = quantile(tmp[:, 2], [0.025, 0.5, 0.975])\nend\n\n# Plot distribution of MI values as a function of coupling strength for both systems\nplot(c, mi[:, 2, 1], label = \"2D chaotic logistic maps\", lc = \"black\",\n    ribbon = (mi[:, 2, 1]¬†- mi[:, 1, 1], mi[:, 3, 1] - mi[:, 2, 1]), c = \"black\", fillalpha = 0.3,\n    legend = :topleft)\nplot!(c, mi[:, 2, 2], label = \"2D order-1 autoregressive\", lc = \"red\",\n    ribbon = (mi[:, 2, 2]¬†- mi[:, 1, 2], mi[:, 3, 2] - mi[:, 2, 2]), c = \"red\", fillalpha = 0.3)\nxlabel!(\"Coupling strength\")\nylabel!(\"Mutual information\")\nsavefig(\"./mutualinfo_example.svg\") # hide","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"(Image: )","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"As expected, I(X Y) increases with coupling strength in a system-specific manner.","category":"page"},{"location":"pairwise_asymmetric_inference/#Pairwise-asymmetric-inference","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"","category":"section"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"pai","category":"page"},{"location":"pairwise_asymmetric_inference/#CausalityTools.CrossMappings.pai","page":"Pairwise asymmetric inference","title":"CausalityTools.CrossMappings.pai","text":"pai(x, y, d, œÑ; w = 0, correspondence_measure = Statistics.cor) ‚Üí Float64\npai(x, y, d, œÑ, bootstrap_method::Symbol; w = 0, correspondence_measure = Statistics.cor,\n    method = :segment, L = ceil(Int, (length(x)-d*œÑ)*0.2), nreps = 100) ‚Üí Vector{Float64}\n\nCompute the pairwise asymmetric inference (PAI; McCracken, 2014)[McCracken2014] between x and y. Returns the correspondence between original and cross mapped values (the default is  œÅ = correspondence_measure(y(t), yÃÉ(t) | M_xy)).\n\nPAI is a modification to Sugihara et al. (2012)'s CCM algorithm[Sugihara2012], where instead of  using completely out-of-sample prediction when trying to predict y(t), values about both variables  are included in the embedding used to make predictions. Specifically, PAI computes the  correspondence between the values y(t) and the cross-map estimated values y(t)  M_xy, where the tildey(t) are the values estimated using the embedding M_xy =  ( x_t x_t-tau x_t-2tau ldots x_t-(d - 1)tau ) . *Note: a d+1-dimensional embedding is used, rather than the d-dimensional embedding used for CCM.\n\nLike for the CCM algorithm, the Theiler window r indicates how many temporal neighbors of the predictee is to be excluded  during the nearest neighbors search (the default r = 0 excludes only the predictee itself, while  r = 2 excludes the point itself plus its two nearest neighbors in time).\n\nIf bootstrap_method is specified, then nreps different bootstrapped estimates of  correspondence_measure(y(t), yÃÉ(t) | M_x) are returned. The following bootstrap methods are available:\n\nbootstrap_method = :random selects training sets of length L consisting of randomly selected    points from the embedding M_x  (time ordering does not matter). This is method 3 from Luo    et al. (2015)[Luo2015], which critiqued the original Sugihara et al. methodology.\nbootstrap_method = :segment selects training sets consisting of time-contiguous segments    (each of lenght L) of embedding vectors in M_x (time ordering matters). This is    method 2 from Luo et al. (2015)[Luo2015].\n\n[McCracken2014]: McCracken, James M., and Robert S. Weigel. \"Convergent cross-mapping and pairwise asymmetric inference.\" Physical Review E 90.6 (2014): 062903.\n\n[Sugihara2012]: Sugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079.http://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou\n\n\n\n\n\n","category":"function"},{"location":"pairwise_asymmetric_inference/#Example:-nonlinear-system","page":"Pairwise asymmetric inference","title":"Example: nonlinear system","text":"","category":"section"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"Let's try to reproduce figure 8 in McCracken & Weigel (2014)[McCracken2014]. We'll start by defining the their example B (equations 6-7). This system consists of two variables X and Y, where X drives Y.","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"using CausalityTools, DynamicalSystems, Plots, StatsBase, Statistics, Distributions; gr()\n\nfunction eom_nonlinear_sindriver(dx, x, p, n)\n    a, b, c, t, Œît = (p...,)\n    x, y = x[1], x[2]\n    ùí© = Normal(0, 1)\n    \n    dx[1] = sin(t)\n    dx[2] = a*x * (1 - b*x) + c*rand(ùí©)\n    p[end-1] += 1 # update t\n\n    return\nend\n\nfunction nonlinear_sindriver(;u‚ÇÄ = rand(2), a = 1.0, b = 1.0, c = 2.0, Œît = 1)\n    DiscreteDynamicalSystem(eom_nonlinear_sindriver, u‚ÇÄ, [a, b, c, 0, Œît])\nend\n\n# Create a system of nonidentical logistic maps where coupling from variable x to variable y\n# is stronger than vice versa.\nsys = nonlinear_sindriver(a = 1.0, b = 1.0, c = 2.0)\nnpts = 100\norbit = trajectory(sys, npts, Ttr = 10000)\nx, y = columns(orbit);\nplot(xlabel = \"Time step\", ylabel = \"Value\")\n# Standardize and plot data\nplot!((x .- mean(x)) ./ std(x), label = \"x\")\nplot!((y .- mean(y)) ./ std(y), label = \"y\")\nsavefig(\"pai_ts.svg\") # hide","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"(Image: )","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"Now, let's generate such time series for many different values of the parameters a and b, and compute PAI for fixed p = 2.0. This will replicate the upper right panel of figure 8 in the original paper.","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"as = 0.25:0.25:4.0\nbs = 0.25:0.25:4.0\n\npai_xys = zeros(length(as), length(bs))\npai_yxs = zeros(length(as), length(bs))\nc = 2.0\nnpts = 2000\nd, œÑ = 2, 1\nfor (i, a) in enumerate(as)\n    for (j, b) in enumerate(bs)\n        s = nonlinear_sindriver(a = a, b = a, c = c)\n        X, Y = columns(trajectory(s, npts, Ttr = 10000))\n        # Use the segment bootstrap estimator, take the mean of 50 reps over segments of length L = 200\n        pai_xys[i, j] = pai(X, Y, d, œÑ, :segment, L = 200, nreps = 50) |> mean\n        pai_yxs[i, j] = pai(Y, X, d, œÑ, :segment, L = 200, nreps = 50) |> mean\n    end\nend","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"Now that we have computed the PAI in both directions, we define a measure of directionality as the difference between PAI in the X to Y direction and in the Y to X direction, so that if X drives Y, then Delta  0.","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"Œî = pai_xys .- pai_yxs\n\nclr = cgrad(:magma, categorical = true)\nplot(xlabel = \"a\", ylabel = \"b\")\nyticks!((1:length(as), string.(as)))\nxticks!((1:length(bs), string.(bs)))\nheatmap!(Œî, c = clr, logscale = true)\nsavefig(\"heatmap_pai.svg\") # hide","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"(Image: )","category":"page"},{"location":"pairwise_asymmetric_inference/","page":"Pairwise asymmetric inference","title":"Pairwise asymmetric inference","text":"As expected, Delta  0 for all parameter combinations, implying that X \"PAI drives\" Y.","category":"page"},{"location":"s_measure/#Smeasure_overview","page":"S-measure","title":"S-measure","text":"","category":"section"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"s_measure","category":"page"},{"location":"s_measure/#CausalityTools.SMeasure.s_measure","page":"S-measure","title":"CausalityTools.SMeasure.s_measure","text":"s_measure(x::AbstractDataset, y::AbstractDataset; \n    K::Int = 2, metric = SqEuclidean(), tree_metric = Euclidean()) ‚Üí Float64\ns_measure(x::AbstractVector, y::AbstractVector; \n    K::Int = 2, metric = SqEuclidean(), tree_metric = Euclidean(),\n    dx::Int = 2, my::Int = 2, œÑx::Int = 1, œÑy::Int = 1) ‚Üí Float64\ns_measure(x::AbstractDataset, y::AbstractVector; \n    K::Int = 2, metric = SqEuclidean(), tree_metric = Euclidean(),\n    dx::Int = 2, œÑx::Int = 1) ‚Üí Float64\ns_measure(x::AbstractDataset, y::AbstractVector; \n    K::Int = 2, metric = SqEuclidean(), tree_metric = Euclidean(),\n    dy::Int = 2, œÑy::Int = 1) ‚Üí Float64\n\nS-measure [Grassberger1999][Quiroga2000] for the directional dependence between  univariate/multivariate time series x and y. \n\nReturns a scalar s ‚àà [0, 1], where s = 0 indicates independence between x and y,  and higher values indicate synchronization between x and y, with complete  synchronization for s = 1.0.\n\nInput data\n\nThe algorithm is slightly modified from [1] to allow univariate time series as input. In that case, these time series are embedded using the parameters mx/œÑx (dimension/lag  for time series x) or my/œÑy (dimension/lag for time series y). For custom embeddings, do the embedding yourself and input Datasets as both x and y.\n\nIf x and y are dx and dy-dimensional Datasets, respectively,    then use x and y as is. \nIf x and y are scalar time series, then create dx and dy dimensional embeddings,   respectively, of both x and y, resulting in N different m-dimensional embedding points   X = x_1 x_2 ldots x_N  and Y = y_1 y_2 ldots y_N .   œÑx and œÑy control the embedding lags for x and y. \nIf x is a scalar-valued vector and y is a Dataset, or vice versa,    then create an embedding of the scalar time series using parameters dx/œÑx or dy/œÑy.\n\nIn all three cases, input datasets are length-matched by eliminating points at the end of  the longest dataset (after the embedding step, if relevant) before analysis.\n\nDescription\n\nLet r_ij and s_ij be the indices of the K-th nearest neighbors   of x_i and y_i, respectively.\nCompute the the mean squared Euclidean distance to the K nearest neighbors   for each x_i, using the indices r_i j.\n\nR_i^(k)(x) = dfrac1k sum_i=1^k(x_i x_r_ij)^2\n\nCompute the y-conditioned mean squared Euclidean distance to the K nearest    neighbors for each x_i, now using the indices s_ij.\n\nR_i^(k)(xy) = dfrac1k sum_i=1^k(x_i x_s_ij)^2\n\nDefine the following measure of independence, where 0 leq S leq 1, and    low values indicate independence and values close to one occur for    synchronized signals.\n\nS^(k)(xy) = dfrac1N sum_i=1^N dfracR_i^(k)(x)R_i^(k)(xy)\n\nExample\n\nusing CausalityTools\n\n# A two-dimensional Ulam lattice map\nsys = ulam(2)\n\n# Sample 1000 points after discarding 5000 transients\norbit = trajectory(sys, 1000, Ttr = 5000)\nx, y = orbit[:, 1], orbit[:, 2]\n\n# 4-dimensional embedding for `x`, 5-dimensional embedding for `y`\ns_measure(x, y, dx = 4, œÑx = 3, dy = 5, œÑy = 1)\n\n[Quiroga2000]: Quian Quiroga, R., Arnhold, J. & Grassberger, P. [2000] ‚ÄúLearning driver-response relationships from synchronization patterns,‚Äù Phys. Rev. E61(5), 5142‚Äì5148.\n\n[Grassberger1999]: Arnhold, J., Grassberger, P., Lehnertz, K., & Elger, C. E. (1999). A robust method for detecting interdependences: application to intracranially recorded EEG. Physica D: Nonlinear Phenomena, 134(4), 419-430.\n\n\n\n\n\n","category":"function"},{"location":"s_measure/#Example:-random-data-vs-Henon-maps","page":"S-measure","title":"Example: random data vs Henon maps","text":"","category":"section"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"Here, we'll compute the S-measure between random time series (uniform noise), between time series of a dynamical system (coupled Henon maps).","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"We start by generating the time series.","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"using CausalityTools, DynamicalSystems, Plots, Distributions\nnpts, Ttr = 10000, 5000\nx, y = columns(trajectory(henon2(c_xy = 0.87), npts - 1, Ttr = Ttr))\nxr, yr = rand(Uniform(-1, 1), npts), rand(Uniform(-1, 1), npts)\n[x y xr yr]","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"Let's plot the time series.","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"p_det = plot(xlabel = \"\", ylabel = \"Value\", title = \"Coupled Henon maps\")\nplot!(x[1:100], label = \"x\", marker = stroke(:black), c = :black)\nplot!(y[1:100], label = \"y\", marker = stroke(:red), c = :red)\np_rand = plot(xlabel = \"Time step\", ylabel = \"Value\", title = \"Random time series\")\nplot!(xr[1:100], label = \"xr\", c = :blue)\nplot!(yr[1:100], label = \"yr\", c = :purple)\n\nplot(p_det, p_rand, layout = grid(2, 1), size = (382*2, 400), legend = :bottomright, \n    tickfont = font(13), guidefont = font(13), legendfont = font(13))\nsavefig(\"henon_random_timeseries.svg\"); nothing # hide","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"(Image: )","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"Now we compute the S-measure between the random time series, both from x to y and from y to x. We'll also do the same for the Henon maps. ","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"The test parameters are embedding dimensions (dx for the source and dy for the target), the embedding lags (œÑx for the source and œÑy for the target), and the number of nearest neighbors K. We'll compute the test with fixed embedding parameters, but a varying number of nearest neighbors (ks = 2:10).","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"For the sake of demonstration, we'll use the same embedding parameters both for the source and for the target, for all analyses. For a real use case, embedding parameters should be chosen more carefully, and will, in general, be different for the source and for the target.","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"dx, dy = 5, 5\nœÑx, œÑy = 1, 1\n\n# Compute the s-measures for different values of k\nks = 2:10\nss_r_xy = [s_measure(xr, yr, dx = dx, œÑx = œÑx, dy = dy, œÑy = œÑy, K = k) for k in ks]\nss_r_yx = [s_measure(yr, xr, dx = dx, œÑx = œÑx, dy = dy, œÑy = œÑy, K = k) for k in ks]\nss_henon_xy = [s_measure(x, y, dx = dx, œÑx = œÑx, dy = dy, œÑy = œÑy, K = k) for k in ks]\nss_henon_yx = [s_measure(y, x, dx = dx, œÑx = œÑx, dy = dy, œÑy = œÑy, K = k) for k in ks]\n\nplot(xlabel = \"# nearest neighbors (k)\", ylabel = \"S\", ylims = (-0.05, 1.05))\nplot!(ks, ss_r_xy,  label = \"random uncoupled system (x -> y)\", marker = stroke(2), c = :black)\nplot!(ks, ss_r_yx,  label = \"random uncoupled system (y -> x)\", marker = stroke(2), c = :red)\nplot!(ks, ss_henon_xy, marker = stroke(2), label = \"henon unidir (x -> y)\")\nplot!(ks, ss_henon_yx, marker = stroke(2), label = \"henon unidir (y -> x)\")\nsavefig(\"smeasure_random_henon.svg\"); nothing # hide","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"(Image: )","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"For uncoupled time series, we expect the value of S to be close to zero. For strongly coupled time series, the value of S should be nonzero and approaching 1. This is exactly what we get: for time random time series, the value of S is close to zero and for the Henon maps, it's clearly non-zero.","category":"page"},{"location":"s_measure/","page":"S-measure","title":"S-measure","text":"Note that the actual dynamical coupling in the Henon system is unidirectional from x to y. The results (positive S in both directions), however, indicate that the coupling is bidirectional, with the coupling being stronger in one direction. This disagreement between results and ground truth highlights the importance of employing causal hypothesis testing. For this, we could use TimeseriesSurrogates.jl to  generate surrogate time series. An in-depth example on how to use surrogate testing can be found in the transfer entropy example.","category":"page"},{"location":"cross_mapping/#Cross-mapping","page":"Cross mapping","title":"Cross mapping","text":"","category":"section"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"crossmap","category":"page"},{"location":"cross_mapping/#CausalityTools.CrossMappings.crossmap","page":"Cross mapping","title":"CausalityTools.CrossMappings.crossmap","text":"crossmap(x, y, d, œÑ; r = 0, correspondence_measure = Statistics.cor) ‚Üí Float64\ncrossmap(x, y, d, œÑ, bootstrap_method::Symbol; r = 0, correspondence_measure = Statistics.cor,\n    method = :segment, L = ceil(Int, (length(x)-d*œÑ)*0.2), nreps = 100) ‚Üí Vector{Float64}\n\nCompute the cross mapping [Sugihara2012] between x and y, which is the correspondence (computed using  correspondence measure) between the values y(t) and the cross-map estimated values y(t)  M_x. Returns the correspondence between original and cross mapped values (the default is  œÅ = correspondence_measure(y(t), yÃÉ(t) | M_x)).\n\nHere, y(t) are the raw values of the time series y, and y(t) are the predicted values  computed from the out-of-sample embedding M_X constructed from the time series x with  embedding dimension d and embedding lag œÑ.\n\nThe Theiler window r indicates how many temporal neighbors of the predictee is to be excluded  during the nearest neighbors search (the default r = 0 excludes only the predictee itself, while  r = 2 excludes the point itself plus its two nearest neighbors in time).\n\nIf bootstrap_method is specified, then nreps different bootstrapped estimates of  correspondence_measure(y(t), yÃÉ(t) | M_x) are returned. The following bootstrap methods are available:\n\nbootstrap_method = :random selects training sets of length L consisting of randomly selected    points from the embedding M_x  (time ordering does not matter). This is method 3 from Luo    et al. (2015)[Luo2015], which critiqued the original Sugihara et al. methodology.\nbootstrap_method = :segment selects training sets consisting of time-contiguous segments    (each of lenght L) of embedding vectors in M_x (time ordering matters). This is    method 2 from Luo et al. (2015)[Luo2015].\n\n[Sugihara2012]: Sugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079.http://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou\n\n\n\n\n\n","category":"function"},{"location":"cross_mapping/#Example:-reproducing-Sugihara-et-al.-(2012)","page":"Cross mapping","title":"Example: reproducing Sugihara et al. (2012)","text":"","category":"section"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"Let's try to reproduce figure 3A in Sugihara et al. We start by defining the bidirectionally coupled  logistic maps they use for that example (the parameters used can be found in their supplementary material).","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"using CausalityTools, DynamicalSystems, Plots, StatsBase, Statistics; gr()\n\nfunction eom_logistic_bidir_sugihara(dx, x, p, n)\n    c_xy, c_yx, rx, ry = (p...,)\n    x, y = x[1], x[2]\n    \n    dx[1] = x * (rx - rx * x - c_yx * y)\n    dx[2] = y * (ry - ry * y - c_xy * x)\n    return\nend\n\nfunction logistic_bidir_sugihara(;u‚ÇÄ = rand(2), c_xy = 0.5, c_yx = 0.5, rx = 3.78, ry = 3.66)\n    DiscreteDynamicalSystem(eom_logistic_bidir_sugihara, u‚ÇÄ, [c_xy, c_yx, rx, ry])\nend\n\n# Create a system of nonidentical logistic maps where coupling from variable x to variable y\n# is stronger than vice versa.\nsys = logistic_bidir_sugihara(u‚ÇÄ = [0.4, 0.2], c_xy = 0.1, c_yx = 0.02, rx = 3.8, ry = 3.5)\nnpts = 5000\norbit = trajectory(sys, npts, Ttr = 10000)\nx, y = columns(orbit);\nplot(xlabel = \"Time step\", ylabel = \"Value\")\nplot!(x[1:200], label = \"x\")\nplot!(y[1:200], label = \"y\")\nsavefig(\"ccm_sugihara_ts.svg\") # hide","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"(Image: )","category":"page"},{"location":"cross_mapping/#Naive-estimation","page":"Cross mapping","title":"Naive estimation","text":"","category":"section"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"The most naive form sampling for the cross mapping procedure is to construct embeddings from all  points in the original time series. This yields a single cross map estimate. ","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"# Embedding dimension 2, embedding lag 1\nd, œÑ = 2, 1\n\n# Cross map estimates from x to y, and from y to x\ncm_xy, cm_yx = crossmap(x, y, d, œÑ), crossmap(y, x, d, œÑ)","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"The term convergent cross mapping does not come into play until we compute the cross mapping for  different time series length L.","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"If cor(y(t) y(t)  M_x) converges with increasing time  series length[Sugihara2012], or library size L in the case of bootstrapping (see below), then  M_x contains information about y. In the context of causal inference, this means that x causally influences y, and there is a directional dynamical coupling from x to y.  Conversely, if cor(x(t) x(t)  M_y)  converges with increasing time series length or L, then that is interpreted as dynamical coupling from y to x.","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"Let's see if either is true for these example time series. We'll increase L by simply subsampling the time  series from index 1 to L for increasing L.","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"# Select some time series lengths\nLs = [10:10:100; 120:20:300; 350:50:1000]\n\ncs_xy_increasing_segments = [crossmap(x[1:L], y[1:L], d, œÑ) for L in Ls]\ncs_yx_increasing_segments = [crossmap(y[1:L], x[1:L], d, œÑ) for L in Ls]\n\np_naive_segments = plot(legend = :bottomright, ylabel = \"Correlation (œÅ)\", xlabel = \"Library size (L)\")\nplot!(Ls, cs_xy_increasing_segments, c = :black, lw = 2, ls = :dot, label = \"cor(Y(t), YÃÉ(t) | Mx), segments from start to L\")\nplot!(Ls, cs_yx_increasing_segments, c = :green, lw = 2, ls = :dash, label = \"cor(X(t), XÃÉ(t) | My), segments from start to L\")\nylims!((-0.05, 1.05))\nsavefig(\"ccm_naive.svg\") # hide","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"(Image: )","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"As in the original CCM paper, we observe that the correlation between observed and  cross-map estimated values is stronger and converges faster when using M_y to  estimate x(t) (green), than when using M_x to estimate y(t) (black).  Why? In this example, we set c_xy = 0.1 and c_yx = 0.02. Therefore, \"... the much stronger effect of species X on Y implies faster convergence for predicting X than for Y\" (Sugihara et al., 2012)[Sugihara2012].","category":"page"},{"location":"cross_mapping/#Bootstrapping","page":"Cross mapping","title":"Bootstrapping","text":"","category":"section"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"Computing a single estimate for the cross mapping is essentially the same as computing a single estimate for the mean of a sample. To more robustly estimate the cross map statistic, we can use bootstrapping.  To do so, we simply employ the five-argument version of the crossmap function, where the fifth argument is  a symbol indicating the type of bootstrapping.","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"Using the same library sizes as before, we repeat the cross mapping procedure, but using bootstrapping. For each L, we obtain nreps = 100 different estimates of œÅ and plot the 95 percentile confidence intervals.","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"Ls = [10:10:100; 120:20:300; 350:50:1000]\nnreps = 200\ncs_xy = [crossmap(x, y, d, œÑ, :random, L = L, nreps = nreps) for L in Ls]\ncs_xy_seg = [crossmap(x, y, d, œÑ, :segment, L = L, nreps = nreps) for L in Ls]\ncs_yx = [crossmap(y, x, d, œÑ, L = L, :random, nreps = nreps) for L in Ls]\ncs_yx_seg = [crossmap(y, x, d, œÑ, :segment, L = L, nreps = nreps) for L in Ls]\n\ncs_xy_lower = median.(cs_xy) .- [quantile(cs_xy[i], 0.025) for i = 1:length(Ls)]\ncs_xy_upper = [quantile(cs_xy[i], 0.975) for i = 1:length(Ls)] .- median.(cs_xy)\ncs_yx_lower = median.(cs_yx) .- [quantile(cs_yx[i], 0.025) for i = 1:length(Ls)]\ncs_yx_upper = [quantile(cs_yx[i], 0.975) for i = 1:length(Ls)] .- median.(cs_yx)\n\ncs_xy_seg_lower = median.(cs_xy_seg) .- [quantile(cs_xy_seg[i], 0.025) for i = 1:length(Ls)]\ncs_xy_seg_upper = [quantile(cs_xy_seg[i], 0.975) for i = 1:length(Ls)] .- median.(cs_xy_seg)\n\ncs_yx_seg_lower = median.(cs_yx_seg) .- [quantile(cs_yx_seg[i], 0.025) for i = 1:length(Ls)]\ncs_yx_seg_upper = [quantile(cs_yx_seg[i], 0.975) for i = 1:length(Ls)] .- median.(cs_yx_seg)\n\np_random = plot(legend = :bottomright, ylabel = \"Correlation (œÅ)\", xlabel = \"Library size (L)\")\nplot!(Ls, median.(cs_xy), ribbon = (cs_xy_lower, cs_xy_upper),\n    label = \"cor(Y(t), YÃÉ(t)|Mx), libraries: random points\", c = :black)\n    plot!(Ls, median.(cs_xy_seg), ribbon = (cs_xy_seg_lower, cs_xy_seg_upper),\n    label = \"cor(Y(t), YÃÉ(t)|Mx), libraries: random segments\", c = :black, ls = :dash)\nylims!((-0.05, 1.05))\n\nplot!(Ls, median.(cs_yx), ribbon = (cs_yx_lower, cs_yx_upper),\n    label = \"cor(X(t), XÃÉ(t)|My), libraries: random points\", c = :green)\nplot!(Ls, median.(cs_yx_seg), ribbon = (cs_yx_seg_lower, cs_yx_seg_upper), \n    label = \"cor(X(t), XÃÉ(t)|My), libraries: random segments\", c = :green, ls = :dot)\nylims!((-0.05, 1.05))\n\n#plot(p_random, p_segments, layout = grid(1, 2), size = (800, 350))\nsavefig(\"ccm_bootstrap.svg\") # hide","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"(Image: )","category":"page"},{"location":"cross_mapping/","page":"Cross mapping","title":"Cross mapping","text":"When using bootstrapping, we get the same results as when contiguous segments of the  original time series: using M_y to estimate x(t) converges faster than in the opposite direction due to the stronger influence from  x to y than vice versa. ","category":"page"},{"location":"surrogate/#Surrogate-data","page":"Surrogate data","title":"Surrogate data","text":"","category":"section"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"Surrogate time series can be used for hypothesis testing in time series causality analyses. ","category":"page"},{"location":"surrogate/","page":"Surrogate data","title":"Surrogate data","text":"The TimeseriesSurrogates.jl package provides methods for generating surrogate time series. Relevant methods are re-exported here for convenience. For more details, see its package documentation.","category":"page"},{"location":"info_estimators/#estimators","page":"Estimators","title":"Estimators","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"Information theoretic causality measures in this package are calculated using entropy estimation. To do so, it uses estimators and genentropy from the Entropies.jl package. However, additional estimators are also available for some of the higher-level methods.","category":"page"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"The following method-estimator combinations are possible.","category":"page"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"Estimator genentropy mutualinfo transferentropy predictive_asymmetry\nVisitationFrequency ‚úì ‚úì ‚úì ‚úì\nTransferOperator ‚úì ‚úì ‚úì ‚úì\nNaiveKernel ‚úì ‚úì ‚úì ‚úì\nSymbolicPermutation ‚úì ‚úì ‚úì ‚úì\nSymbolicAmplitudeAwarePermutation ‚úì   \nSymbolicWeightedPermutation ‚úì   \nKozachenkoLeonenko ‚úì ‚úì ‚úì ‚úì\nKraskov ‚úì ‚úì ‚úì ‚úì\nKraskov1 ‚úì ‚úì  \nKraskov2 ‚úì ‚úì  \nHilbert ‚úì  ‚úì ‚úì\nTimeScaleMODWT ‚úì   ","category":"page"},{"location":"info_estimators/#Binning-based","page":"Estimators","title":"Binning based","text":"","category":"section"},{"location":"info_estimators/#Visitation-frequency","page":"Estimators","title":"Visitation frequency","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"VisitationFrequency\nRectangularBinning","category":"page"},{"location":"info_estimators/#Entropies.VisitationFrequency","page":"Estimators","title":"Entropies.VisitationFrequency","text":"VisitationFrequency(r::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by the binning scheme r.\n\nExample\n\n# Construct boxes by dividing each coordinate axis into 5 equal-length chunks.\nb = RectangularBinning(5)\n\n# A probabilities estimator that, when applied a dataset, computes visitation frequencies\n# over the boxes of the binning\nest = VisitationFrequency(b)\n\nSee also: RectangularBinning.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#Entropies.RectangularBinning","page":"Estimators","title":"Entropies.RectangularBinning","text":"RectangularBinning(œµ) <: RectangularBinningScheme\n\nInstructions for creating a rectangular box partition using the binning scheme œµ.  Binning instructions are deduced from the type of œµ.\n\nRectangular binnings may be automatically adjusted to the data in which the RectangularBinning  is applied, as follows:\n\nœµ::Int divides each coordinate axis into œµ equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.\nœµ::Float64 divides each coordinate axis into intervals of fixed size œµ, starting   from the axis minima until the data is completely covered by boxes.\nœµ::Vector{Int} divides the i-th coordinate axis into œµ[i] equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.\nœµ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size œµ[i], starting   from the axis minima until the data is completely covered by boxes.\n\nRectangular binnings may also be specified on arbitrary min-max ranges. \n\nœµ::Tuple{Vector{Tuple{Float64,Float64}},Int64} creates intervals   along each coordinate axis from ranges indicated by a vector of (min, max) tuples, then divides   each coordinate axis into an integer number of equal-length intervals. Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored).\n\nExample 1: Grid deduced automatically from data (partition guaranteed to cover data points)\n\nFlexible box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into 10 equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.\n\nusing Entropies\nRectangularBinning(10)\n\nNow, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into 10 equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into 5 equal-length intervals. This gives (hyper-)rectangular boxes.\n\nusing Entropies\nRectangularBinning([10, 5])\n\nFixed box sizes\n\nThe following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size 0.5 until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.\n\nusing Entropies\nRectangularBinning(0.5)\n\nAgain, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.\n\nThe following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size 0.3, and the range along the second axis into equal-length intervals of size 0.1 (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. \n\nusing Entropies\nRectangularBinning([0.3, 0.1])\n\nExample 2: Custom grids (partition not guaranteed to cover data points):\n\nAssume the data consists of 3-dimensional points (x, y, z), and that we want a grid  that is fixed over the intervals [x‚ÇÅ, x‚ÇÇ] for the first dimension, over [y‚ÇÅ, y‚ÇÇ] for the second dimension, and over [z‚ÇÅ, z‚ÇÇ] for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded). \n\nThe following binning specification produces the desired (hyper-)rectangular boxes. \n\nusing Entropies, DelayEmbeddings\n\nD = Dataset(rand(100, 3));\n\nx‚ÇÅ, x‚ÇÇ = 0.5, 1 # not completely covering the data, which are on [0, 1]\ny‚ÇÅ, y‚ÇÇ = -2, 1.5 # covering the data, which are on [0, 1]\nz‚ÇÅ, z‚ÇÇ = 0, 0.5 # not completely covering the data, which are on [0, 1]\n\nœµ = [(x‚ÇÅ, x‚ÇÇ), (y‚ÇÅ, y‚ÇÇ), (z‚ÇÅ, z‚ÇÇ)], 4 # [interval 1, interval 2, ...], n_subdivisions\n\nRectangularBinning(œµ)\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#Transfer-operator","page":"Estimators","title":"Transfer operator","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"TransferOperator","category":"page"},{"location":"info_estimators/#Entropies.TransferOperator","page":"Estimators","title":"Entropies.TransferOperator","text":"TransferOperator(œµ::RectangularBinning) <: BinningProbabilitiesEstimator\n\nA probability estimator based on binning data into rectangular boxes dictated by  the binning scheme œµ, then approxmating the transfer (Perron-Frobenius) operator  over the bins, then taking the invariant measure associated with that transfer operator  as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition  probabilities between the states defined by the partition elements, where N is the  number of boxes in the partition that is visited by the orbit/points. \n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over  which the transfer operator is approximated,  C_k=1^N  are the N different  partition elements (as dictated by œµ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n ¬†phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points  that are initially in box C_i end up in box C_j when the points in C_i are  projected one step forward in time. Thus, the row P_ik^N where  k in 1 2 ldots N  gives the probability  of jumping from the state defined by box C_i to any of the other N states. It  follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right  stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where  mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row  eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution  mathbfrho^N approximates the invariant density of the system subject to the  partition œµ, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using  invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N.  The resulting length-N distribution is then applied to P^N again. This process  repeats until the difference between the distributions over consecutive iterations is  below some threshold. \n\nProbability and entropy estimation\n\nprobabilities(x::AbstractDataset, est::TransferOperator{RectangularBinning}) estimates    probabilities for the bins defined by the provided binning (est.œµ)\ngenentropy(x::AbstractDataset, est::TransferOperator{RectangularBinning}) does the same,    but computes generalized entropy using the probabilities.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#Kernel-density-based","page":"Estimators","title":"Kernel density based","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"NaiveKernel","category":"page"},{"location":"info_estimators/#Entropies.NaiveKernel","page":"Estimators","title":"Entropies.NaiveKernel","text":"NaiveKernel(œµ::Real, ss = KDTree; w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by  counting how many other points occupy the space spanned by  a hypersphere of radius œµ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nThe search structure ss is any search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\n\nThe keyword w stands for the Theiler window, and excludes indices s that are within i - s  w from the given point X_i.\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#Nearest-neighbor-based","page":"Estimators","title":"Nearest neighbor based","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"KozachenkoLeonenko\nKraskov\nKraskov1\nKraskov2","category":"page"},{"location":"info_estimators/#Entropies.KozachenkoLeonenko","page":"Estimators","title":"Entropies.KozachenkoLeonenko","text":"KozachenkoLeonenko(; k::Int = 1, w::Int = 0) <: EntropyEstimator\n\nEntropy estimator based on nearest neighbors. This implementation is based on Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzy≈Ñska and Gambin (2016)[Charzy≈Ñska2016].\n\nw is the Theiler window (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\ninfo: Info\nThis estimator is only available for entropy estimation. Probabilities cannot be obtained directly.\n\n[Charzy≈Ñska2016]: Charzy≈Ñska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#Entropies.Kraskov","page":"Estimators","title":"Entropies.Kraskov","text":"k-th nearest neighbour(kNN) based\n\nKraskov(; k::Int = 1, w::Int = 0) <: EntropyEstimator\n\nEntropy estimator based on k-th nearest neighbor searches[Kraskov2004]. w is the Theiler window.\n\ninfo: Info\nThis estimator is only available for entropy estimation.  Probabilities cannot be obtained directly.\n\n[Kraskov2004]: Kraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#TransferEntropy.Kraskov1","page":"Estimators","title":"TransferEntropy.Kraskov1","text":"Kraskov1(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) <: MutualInformationEstimator\n\nThe I^(1) nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using k nearest neighbors. The distance metric for  the marginals x and y can be chosen separately, while the Chebyshev metric  is always used for the z = (x, y) joint space.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#TransferEntropy.Kraskov2","page":"Estimators","title":"TransferEntropy.Kraskov2","text":"Kraskov2(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) <: MutualInformationEstimator\n\nThe I^(2)(x y) nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using k nearest neighbors. The distance metric for  the marginals x and y can be chosen separately, while the Chebyshev metric  is always used for the z = (x, y) joint space.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#symbolic","page":"Estimators","title":"Permutation based","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"SymbolicPermutation","category":"page"},{"location":"info_estimators/#Entropies.SymbolicPermutation","page":"Estimators","title":"Entropies.SymbolicPermutation","text":"SymbolicPermutation(; œÑ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicWeightedPermutation(; œÑ = 1, m = 3, lt = Entropies.isless_rand) <: ProbabilityEstimator\nSymbolicAmplitudeAwarePermutation(; œÑ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) <: ProbabilityEstimator\n\nSymbolic, permutation-based probabilities/entropy estimators. m is the permutation order (or the symbol size or the embedding dimension) and  œÑ is the delay time (or lag).\n\nRepeated values during symbolization\n\nIn the original implementation of permutation entropy [BandtPompe2002], equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution [Zunino2017]. Here, we resolve this issue by letting the user provide a custom \"less-than\" function. The keyword lt accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (lt = Entropies.isless_rand). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.\n\nTo get the behaviour described in Bandt and Pompe (2002), use lt = Base.isless).\n\nProperties of original signal preserved\n\nSymbolicPermutation: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt & Pompe et al. (2002)[BandtPompe2002] and   Berger et al. (2019) [Berger2019].\nSymbolicWeightedPermutation: Like SymbolicPermutation, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)[Fadlallah2013].\nSymbolicAmplitudeAwarePermutation: Like SymbolicPermutation, but also encodes   amplitude information by considering a weighted combination of absolute amplitudes   of state vectors, and relative differences between elements of state vectors. See   description below for explanation of the weighting parameter A. This implementation   is based on Azami & Escudero (2016) [Azami2016].\n\nProbability estimation\n\nUnivariate time series\n\nTo estimate probabilities or entropies from univariate time series, use the following methods:\n\nprobabilities(x::AbstractVector, est::SymbolicProbabilityEstimator). Constructs state vectors   from x using embedding lag œÑ and embedding dimension m, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.\ngenentropy(x::AbstractVector, est::SymbolicProbabilityEstimator; Œ±=1, base = 2) computes   probabilities by calling probabilities(x::AbstractVector, est),   then computer the order-Œ± generalized entropy to the given base.\n\nSpeeding up repeated computations\n\nA pre-allocated integer symbol array s can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.\n\nNote: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that length(x) == length(s) if x is a Dataset, or length(s) == length(x) - (m-1)œÑ if x is a univariate signal that is to be embedded first.\n\nUse the following signatures (only works for SymbolicPermutation).\n\nprobabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) ‚Üí ps::Probabilities\nprobabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) ‚Üí ps::Probabilities\n\nMultivariate datasets\n\nAlthough not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension ‚â• 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the L existing state vectors mathbfx_1 mathbfx_2 ldots mathbfx_L.\n\nprobabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator). Compute ordinal patterns of the   state vectors of x directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.\ngenentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator). Computes probabilities from   symbol frequencies using probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator),   then computes the order-Œ± generalized (permutation) entropy to the given base.\n\nCaveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for Datasets are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.\n\nDescription\n\nAll symbolic estimators use the same underlying approach to estimating probabilities.\n\nEmbedding, ordinal patterns and symbolization\n\nConsider the n-element univariate time series x(t) = x_1 x_2 ldots x_n. Let mathbfx_i^m tau = x_j x_j+tau ldots x_j+(m-1)tau for j = 1 2 ldots n - (m-1)tau be the i-th state vector in a delay reconstruction with embedding dimension m and reconstruction lag tau. There are then N = n - (m-1)tau state vectors.\n\nFor an m-dimensional vector, there are m possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a motif. Let pi_i^m tau denote the motif associated with the m-dimensional state vector mathbfx_i^m tau, and let R be the number of distinct motifs that can be constructed from the N state vectors. Then there are at most R motifs; R = N precisely when all motifs are unique, and R = 1 when all motifs are the same.\n\nEach unique motif pi_i^m tau can be mapped to a unique integer symbol 0 leq s_i leq M-1. Let S(pi)  mathbbR^m to mathbbN_0 be the function that maps the motif pi to its symbol s, and let Pi denote the set of symbols Pi =  s_i _iin  1 ldots R.\n\nProbability computation\n\nSymbolicPermutation\n\nThe probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from [Fadlallah2013]),\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) sum_k=1^N mathbf1_uS(u) in Pi left(mathbfx_k^m tau right) = dfracsum_k=1^N mathbf1_uS(u) = s_i left(mathbfx_k^m tau right) N\n\nwhere the function mathbf1_A(u) is the indicator function of a set A. That     is, mathbf1_A(u) = 1 if u in A, and mathbf1_A(u) = 0 otherwise.\n\nSymbolicAmplitudeAwarePermutation\n\nAmplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N mathbf1_uS(u) in Pi left( mathbfx_k^m tau right) a_k = dfracsum_k=1^N mathbf1_uS(u) = s_i left( mathbfx_k^m tau right)  a_ksum_k=1^N a_k\n\nThe weights encoding amplitude information about state vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) are\n\na_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1 sum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\nSymbolicWeightedPermutation\n\nWeighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:\n\np(pi_i^m tau) = dfracsum_k=1^N mathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)\n w_ksum_k=1^N mathbf1_uS(u) in Pi\nleft( mathbfx_k^m tau right) w_k = dfracsum_k=1^N\nmathbf1_uS(u) = s_i\nleft( mathbfx_k^m tau right)  w_ksum_k=1^N w_k\n\nThe weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (w_j = beta  forall  j leq N and beta  0). Weights are dictated by the variance of the state vectors.\n\nLet the aritmetic mean of state vector mathbfx_i be denoted by\n\nmathbfhatx_j^m tau = frac1m sum_k=1^m x_j + (k+1)tau\n\nWeights are then computed as\n\nw_j = dfrac1msum_k=1^m (x_j+(k+1)tau - mathbfhatx_j^m tau)^2\n\nNote: in equation 7, section III, of the original paper, the authors write\n\nw_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2\n\nBut given the formula they give for the arithmetic mean, this is not the variance of mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms \"vector\" and \"neighboring vector\" (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for mathbfx_i.\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102.\n\n[Berger2019]: Berger, Sebastian, et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[R√©nyi1960]: A. R√©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n[Fadlallah2013]: Fadlallah, Bilal, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#Wavelet-based","page":"Estimators","title":"Wavelet based","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"TimeScaleMODWT","category":"page"},{"location":"info_estimators/#Entropies.TimeScaleMODWT","page":"Estimators","title":"Entropies.TimeScaleMODWT","text":"TimeScaleMODWT <: WaveletProbabilitiesEstimator\nTimeScaleMODWT(wl::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{12}())\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. This implementation is based on Rosso et al. (2001)[Rosso2001]. Optionally specify a wavelet to be used.\n\nThe probability p[i] is the relative/total energy for the i-th wavelet scale.\n\nExample\n\nManually picking a wavelet is done as follows.\n\nusing Entropies, Wavelets\nN = 200\na = 10\nt = LinRange(0, 2*a*œÄ, N)\nx = sin.(t .+  cos.(t/0.1)) .- 0.1;\n\n# Pick a wavelet (if no wavelet provided, defaults to Wavelets.WL.Daubechies{12}())\nwl = Wavelets.WT.Daubechies{12}()\n\n# Compute the probabilities (relative energies) at the different wavelet scales\nprobabilities(x, TimeScaleMODWT(wl))\n\n[Rosso2001]: Rosso, O. A., Blanco, S., Yordanova, J., Kolev, V., Figliola, A., Sch√ºrmann, M., & Ba≈üar, E. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#Hilbert","page":"Estimators","title":"Hilbert","text":"","category":"section"},{"location":"info_estimators/","page":"Estimators","title":"Estimators","text":"Hilbert\nAmplitude\nPhase","category":"page"},{"location":"info_estimators/#TransferEntropy.Hilbert","page":"Estimators","title":"TransferEntropy.Hilbert","text":"Hilbert(est; \n    source::InstantaneousSignalProperty = Phase(),\n    target::InstantaneousSignalProperty = Phase(),\n    cond::InstantaneousSignalProperty = Phase())\n) <: TransferEntropyEstimator\n\nCompute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are  obtained by first applying the Hilbert transform to each signal, then extracting the  phases/amplitudes of the resulting complex numbers[Palus2014]. Original time series are  thus transformed to instantaneous phase/amplitude time series. Transfer  entropy is then estimated using the provided est on those phases/amplitudes (use e.g.  VisitationFrequency, or SymbolicPermutation).\n\ninfo: Info\nDetails on estimation of the transfer entropy (conditional mutual information)  following the phase/amplitude extraction step is not given in Palus (2014). Here,  after instantaneous phases/amplitudes have been obtained, these are treated as regular  time series, from which transfer entropy is then computed as usual.\n\nSee also: Phase, Amplitude.\n\n[Palus2014]: Palu≈°, M. (2014). Cross-scale interactions and information transfer. Entropy, 16(10), 5263-5289.\n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#TransferEntropy.Amplitude","page":"Estimators","title":"TransferEntropy.Amplitude","text":"Amplitude <: InstantaneousSignalProperty\n\nIndicates that the instantaneous amplitudes of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"info_estimators/#TransferEntropy.Phase","page":"Estimators","title":"TransferEntropy.Phase","text":"Phase <: InstantaneousSignalProperty\n\nIndicates that the instantaneous phases of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"joint_distance_distribution/#joint_distance_distribution_overview","page":"Joint distance distribution","title":"Joint distance distribution","text":"","category":"section"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"jdd(::Any, ::Any)","category":"page"},{"location":"joint_distance_distribution/#CausalityTools.JointDistanceDistribution.jdd-Tuple{Any, Any}","page":"Joint distance distribution","title":"CausalityTools.JointDistanceDistribution.jdd","text":"jdd(source, target; distance_metric = SqEuclidean(), \n    B::Int = 10, D::Int = 2, œÑ::Int = 1) ‚Üí Vector{Float64}\n\nCompute the joint distance distribution [1] from source to target using  the provided distance_metric, with B controlling the number of subintervals,  D the embedding dimension and œÑ the embedding lag.\n\nExample\n\nusing CausalityTools\nx, y = rand(1000), rand(1000)\n\njdd(x, y)\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.    Defaults to SqEuclidean().\nB: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances. \nD: Embedding dimension.\nœÑ: Embedding delay.\n\nReferences\n\n[1] Amig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"method"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"For the joint distance distribution to indicate a causal influence, it must be significantly  skewed towards positive values.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Providing the OneSampleTTest type as the first  argument to jdd yields a one sample t-test on the joint distance distribution. From this test, you can extract p-values and obtain  confidence intervals like in HypothesisTests.jl as usual.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"jdd(::Type{OneSampleTTest}, ::Any, ::Any)","category":"page"},{"location":"joint_distance_distribution/#CausalityTools.JointDistanceDistribution.jdd-Tuple{Type{OneSampleTTest}, Any, Any}","page":"Joint distance distribution","title":"CausalityTools.JointDistanceDistribution.jdd","text":"jdd(test::OneSampleTTest, source, target;\n    distance_metric = SqEuclidean(), B::Int = 10, D::Int = 2, œÑ::Int = 1, \n    Œº0 = 0.0) ‚Üí OneSampleTTest\n\nPerform a one sample t-test to check that the joint distance distribution[Amigo] computed from source to target is biased towards positive values, using the null  hypothesis that the mean of the distribution is Œº0.\n\nThe interpretation of the t-test is that if we can reject the null, then the  joint distance distribution is biased towards positive values, and then there  exists an underlying coupling from source to target. \n\nExample\n\nusing CausalityTools, HypothesisTests\nx, y = rand(1000), rand(1000)\n\njdd(OneSampleTTest, x, y)\n\nwhich gives \n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         0.0\n    point estimate:          0.06361857324022721\n    95% confidence interval: (0.0185, 0.1087)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0082\n\nDetails:\n    number of observations:   20\n    t-statistic:              2.9517208721082873\n    degrees of freedom:       19\n    empirical standard error: 0.0215530451545668\n\nThe lower bound of the confidence interval for the mean of the joint  distance distribution is 0.0185 at confidence level Œ± = 0.05. The  meaning that the test falsely detected causality from x to y between these two random time series. \n\nTo get the confidence intervals at confidence level Œ±, run \n\nconfinf(jdd(OneSampleTTest, x, y), Œ±). \n\nIf you just want the p-value at 95% confidence, run \n\npvalue(jdd(OneSampleTTest, x, y), tail = :right)\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.    Defaults to SqEuclidean().\nB: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances. \nD: Embedding dimension.\nœÑ: Embedding delay.\nŒº0: The hypothetical mean value of the joint distance distribution if there    is no coupling between x and y (default is Œº0 = 0.0).\n\nReferences\n\n[Amigo]: Amig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"method"},{"location":"joint_distance_distribution/#Example:-two-coupled-Lorenz-attractors","page":"Joint distance distribution","title":"Example: two coupled Lorenz attractors","text":"","category":"section"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"The joint distance distribution was introduced in Amigo et al. (2018)[Amigo2018]. Here, we'll attempt to reproduce  figures 1a and 1b from their paper, but first we'll do a couple of simple examples.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Let's start by generating some time series. We'll use the lorenz_lorenz_bidir example system that ships with CausalityTools, which implements their bidirectionally coupled Lorenz systems. ","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"using CausalityTools, Plots, DynamicalSystems\n\n# Time series of length 10000, sampled every 0.1 time steps.\nŒîT, npts, Ttr = 0.1, 1000, 100\nT = npts * ŒîT\n\n# Let there be unidirectional coupling from x to y\ncxy, cyx = 1.5, 0.0\nsys = lorenz_lorenz_bidir(c_xy = cxy, c_yx = cyx)\norbit = trajectory(sys, T, ŒîT = ŒîT, Ttr = 500)\nx1, x2, x3, y1, y2, y3 = columns(orbit)\nplot(xlabel = \"Time step\", ylabel = \"Value\", size = (800, 220))\nplot!(x1, label = \"x1\")\nplot!(y1, label = \"y1\")\nsavefig(\"lorenz_lorenz_timeseries.svg\") # hide","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"(Image: )","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Let's compute the joint distance distribution from x to y. For this example, we'll use an embedding dimension  of 10 and embedding lag of 10. In real applications, these parameters should be optimized.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"jxy = jdd(x1, y1, D = 10, œÑ = 10)","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"If jxy is biased towards positive values, then in the framework of the joint distance distribution, the variables are coupled and there exists a continuous functional dependence x = phi(y). In the context of causal inference, this is is the same as saying that x drives y. Likewise, if jxy is not biased towards positive values, then there exists no such relationship, and the variables are not coupled.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"To formally test whether the distribution jxy is skewed torwards positive values, we can use a one-sample t-test to test the null hypothesis mean(jxy) == 0, and compute the right-sided p-value for the test. If p < 0.05 for the test, we take that as rejection of the null hypothesis, and accept the alternative hypothesis that mean(jxy) is skewed towards positive values.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"pvalue(jdd(OneSampleTTest, x1, y1, D = 10, œÑ = 10), tail = :right)","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Similarly, we can also see if the joint distance distribution test provides evidence of a coupling from y to x:","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"pvalue(jdd(OneSampleTTest, y1, x1, D = 10, œÑ = 10), tail = :right)","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"If the test works perfectly, then we expect p < 0.05 when computing the joint distance distribution from x to y and p >= 0.05 when computing the distribution from y to x. However, for a single finite time series realization, this might not necessarily always be the case.  ","category":"page"},{"location":"joint_distance_distribution/#Reproducing-Amigo-et-al.-(2018)","page":"Joint distance distribution","title":"Reproducing Amigo et al. (2018)","text":"","category":"section"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Here, we attempt reproduce figures 1a and 1b from Amigo et al., which computes p-values for a range of coupling strengths in both directions. We'll use a coarser coupling resolution and shorter time series to limit computation time.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"using CausalityTools, Plots, DynamicalSystems\n\ncxys = 0.0:0.25:2.0\ncyxs = 0.0:0.25:2.0\n# We'll use time series 2000 points long, to limit computation time\nŒîT, npts, Ttr = 0.1, 200, 100\nT = npts * ŒîT\n\npvals_xy = zeros(length(cyxs), length(cxys))\npvals_yx = zeros(length(cyxs), length(cxys))\n\nfor (i, cxy) in enumerate(cxys)\n    for (j, cyx) in enumerate(cyxs)\n        sys = lorenz_lorenz_bidir(c_xy = cxy, c_yx = cyx)\n        orbit = trajectory(sys, T, ŒîT = ŒîT, Ttr = Ttr)\n        x1, x2, x3, y1, y2, y3 = columns(orbit)\n\n        # The original paper uses an embedding dimension of 10 and embedding lag of 10\n        pvals_xy[j, i] = pvalue(jdd(OneSampleTTest, x1, y1, D = 10, œÑ = 10), tail = :right)\n        pvals_yx[j, i] = pvalue(jdd(OneSampleTTest, y1, x1, D = 10, œÑ = 10), tail = :right)\n    end\nend","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Plotting the p-values for the tests in the direction x to y, and in the direction y to x, as a function of coupling strengths in both directions, we get the following:","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"z = [0, 0.0001, 0.001, 0.01, 0.05, 1.0]\nc = cgrad(:magma, categorical = true)\n\np_xy = plot(xlabel = \"Coupling from x to y\", ylabel = \"Coupling from y to x\")\nyticks!((1:length(cyxs), string.(cyxs)))\nxticks!((1:length(cxys), string.(cxys)))\nheatmap!(pvals_xy, c = c, logscale = true)\n\np_yx = plot(xlabel = \"Coupling from x to y\", ylabel = \"\")\nyticks!((1:length(cyxs), string.(cyxs)))\nxticks!((1:length(cxys), string.(cxys)))\nheatmap!(pvals_yx, c = c, logscale = true)\n\nplot(p_xy, p_yx, layout = grid(1, 2), size = (900, 300))\nsavefig(\"jdd_heatmap.svg\") # hide","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"(Image: )","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Values on the x-axis are the coupling strengths from x to y, and those on the y-axis are the coupling strengths from y to x. The colorbar indicates the p-value. ","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"For sufficiently high coupling strengths, the test successfully identifies the couplings both from x to y and vice versa. For lower coupling strengths, we cannot reject the null hypothesis. ","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"Note that using longer time series and more carefully tuned embeddings would likely lead to better performance.","category":"page"},{"location":"joint_distance_distribution/","page":"Joint distance distribution","title":"Joint distance distribution","text":"[Amigo2018]: Amig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.","category":"page"},{"location":"#CausalityTools.jl","page":"Overview","title":"CausalityTools.jl","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"CausalityTools is a Julia package that provides algorithms for detecting  dynamical influences and causal inference based on time series data. ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"For an updated overview of the field of causal inference, see for example Runge et al. (2019)[Runge2019].","category":"page"},{"location":"#Getting-started","page":"Overview","title":"Getting started","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Examples showing how to use the causal inference methods are provided in their respective documentation pages.","category":"page"},{"location":"#Geometric-methods","page":"Overview","title":"Geometric methods","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Geometrically based methods rely on delay reconstructions of the time series, and numerical  properties of these delay reconstructions, to infer causal/dynamical relationships. They take as input the time series, and embedding parameters (given as keyword arguments).","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Cross mapping (crossmap)\nPairwise asymmetric inference (pai)\nJoint distance distribution (jdd)\nS-measure (s_measure)","category":"page"},{"location":"#Information-theoretic-methods","page":"Overview","title":"Information theoretic methods","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Entropy based methods for causal inference take as inputs the time series in question, and an entropy estimator of choice. Additional parameters are given as keyword arguments. ","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Transfer entropy (transferentropy)\nPredictive asymmetry (predictive_asymmetry)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"We use entropy estimators from Entropies.jl.  Which estimator should you use? See the list of estimators.  A good choice is to start with a VisitationFrequency estimator.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"We also provide estimators for generalized entropy and mutual information, though these are not causal inference methods per se.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Generalized entropy (genentropy) and fast histograms (binhist)\nMutual information (mutualinfo)","category":"page"},{"location":"#Contributions/issues","page":"Overview","title":"Contributions/issues","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Do you want additional methods or example systems to be implemented? Make a PR to the  master branch in the  CausalityTools repo, or open an  issue describing the desired feature.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"[Runge2019]: Runge, J., Bathiany, S., Bollt, E., Camps-Valls, G., Coumou, D., Deyle, E., Glymour, C., Kretschmer, M., Mahecha, M. D., Mu√±oz-Mar√≠, J., van Nes, E. H., Peters, J., Quax, R., Reichstein, M., Scheffer, M., Sch√∂lkopf, B., Spirtes, P., Sugihara, G., Sun, J., ‚Ä¶ Zscheischler, J. (2019). Inferring causation from time series in Earth system sciences. Nature Communications, 10(1), 1‚Äì13. https://doi.org/10.1038/s41467-019-10105-3","category":"page"}]
}
