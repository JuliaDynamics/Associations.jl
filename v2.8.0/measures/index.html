<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Association measures · CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="CausalityTools.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="CausalityTools.jl logo"/></a><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li class="is-active"><a class="tocitem" href>Association measures</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Correlation-measures"><span>Correlation measures</span></a></li><li><a class="tocitem" href="#Closeness-measures"><span>Closeness measures</span></a></li><li><a class="tocitem" href="#Cross-map-measures"><span>Cross-map measures</span></a></li><li><a class="tocitem" href="#Recurrence-based"><span>Recurrence-based</span></a></li><li><a class="tocitem" href="#information_measures"><span>Information measures</span></a></li></ul></li><li><a class="tocitem" href="../independence/">Independence testing</a></li><li><a class="tocitem" href="../causal_graphs/">Causal graphs</a></li><li><a class="tocitem" href="../api/">APIs and estimators</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><a class="tocitem" href="../coupled_systems/">Predefined systems</a></li><li><a class="tocitem" href="../experimental/">Experimental</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Association measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Association measures</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/measures.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="association_measure"><a class="docs-heading-anchor" href="#association_measure">Association measures</a><a id="association_measure-1"></a><a class="docs-heading-anchor-permalink" href="#association_measure" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.AssociationMeasure" href="#CausalityTools.AssociationMeasure"><code>CausalityTools.AssociationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AssociationMeasure</code></pre><p>The supertype of all association measures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/core.jl#L9-L13">source</a></section></article><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><table><tr><th style="text-align: right">Type</th><th style="text-align: right">Measure</th><th style="text-align: center">Pairwise</th><th style="text-align: center">Conditional</th><th style="text-align: right">Function version</th></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#CausalityTools.PearsonCorrelation"><code>PearsonCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="#CausalityTools.pearson_correlation"><code>pearson_correlation</code></a></td></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#CausalityTools.DistanceCorrelation"><code>DistanceCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="#CausalityTools.distance_correlation"><code>distance_correlation</code></a></td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.SMeasure"><code>SMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="#CausalityTools.s_measure"><code>s_measure</code></a></td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.HMeasure"><code>HMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="#CausalityTools.h_measure"><code>h_measure</code></a></td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.MMeasure"><code>MMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="#CausalityTools.m_measure"><code>m_measure</code></a></td></tr><tr><td style="text-align: right">Closeness (ranks)</td><td style="text-align: right"><a href="#CausalityTools.LMeasure"><code>LMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="#CausalityTools.l_measure"><code>l_measure</code></a></td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="#CausalityTools.jdd"><code>jdd</code></a></td></tr><tr><td style="text-align: right">Cross-mapping</td><td style="text-align: right"><a href="#CausalityTools.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="../api/api_crossmap/#CausalityTools.crossmap"><code>crossmap</code></a></td></tr><tr><td style="text-align: right">Cross-mapping</td><td style="text-align: right"><a href="#CausalityTools.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="../api/api_crossmap/#CausalityTools.crossmap"><code>crossmap</code></a></td></tr><tr><td style="text-align: right">Conditional recurrence</td><td style="text-align: right"><a href="#CausalityTools.MCR"><code>MCR</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="../api/api_recurrence/#CausalityTools.mcr"><code>mcr</code></a></td></tr><tr><td style="text-align: right">Conditional recurrence</td><td style="text-align: right"><a href="#CausalityTools.RMCD"><code>RMCD</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_recurrence/#CausalityTools.rmcd"><code>rmcd</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.PartialCorrelation"><code>PartialCorrelation</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="#CausalityTools.partial_correlation"><code>partial_correlation</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="@ref"><code>CMIRenyiSarbu</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a></td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a></td></tr><tr><td style="text-align: right">Information transfer</td><td style="text-align: right"><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_transferentropy/#CausalityTools.transferentropy"><code>transferentropy</code></a></td></tr><tr><td style="text-align: right">Information transfer</td><td style="text-align: right"><a href="#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_transferentropy/#CausalityTools.transferentropy"><code>transferentropy</code></a></td></tr><tr><td style="text-align: right">Part mutual information</td><td style="text-align: right"><a href="#CausalityTools.PMI"><code>PMI</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_pmi/#CausalityTools.pmi"><code>pmi</code></a></td></tr><tr><td style="text-align: right">Information asymmetry</td><td style="text-align: right"><a href="#CausalityTools.PA"><code>PA</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: right"><a href="../api/api_predictive_asymmetry/#CausalityTools.asymmetry"><code>asymmetry</code></a></td></tr></table><h2 id="Correlation-measures"><a class="docs-heading-anchor" href="#Correlation-measures">Correlation measures</a><a id="Correlation-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Correlation-measures" title="Permalink"></a></h2><h3 id="Pearson-correlation"><a class="docs-heading-anchor" href="#Pearson-correlation">Pearson correlation</a><a id="Pearson-correlation-1"></a><a class="docs-heading-anchor-permalink" href="#Pearson-correlation" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.PearsonCorrelation" href="#CausalityTools.PearsonCorrelation"><code>CausalityTools.PearsonCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PearsonCorrelation</code></pre><p>The Pearson correlation of two variables.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="#CausalityTools.pearson_correlation"><code>pearson_correlation</code></a> to compute the raw correlation coefficient.</li></ul><p><strong>Description</strong></p><p>The sample <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation coefficient</a> for real-valued random variables <span>$X$</span> and <span>$Y$</span> with associated samples <span>$\{x_i\}_{i=1}^N$</span> and <span>$\{y_i\}_{i=1}^N$</span> is defined as</p><p class="math-container">\[\rho_{xy} = \dfrac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }{\sqrt{\sum_{i=1}^N (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^N (y_i - \bar{y})^2}},\]</p><p>where <span>$\bar{x}$</span> and <span>$\bar{y}$</span> are the means of the observations <span>$x_k$</span> and <span>$y_k$</span>, respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/correlation/pearson_correlation.jl#L4-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.pearson_correlation" href="#CausalityTools.pearson_correlation"><code>CausalityTools.pearson_correlation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">pearson_correlation(x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)</code></pre><p>Compute the <a href="#CausalityTools.PearsonCorrelation"><code>PearsonCorrelation</code></a> between <code>x</code> and <code>y</code>, which must each be 1-dimensional.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/correlation/pearson_correlation.jl#L29-L34">source</a></section></article><h3 id="Partial-correlation"><a class="docs-heading-anchor" href="#Partial-correlation">Partial correlation</a><a id="Partial-correlation-1"></a><a class="docs-heading-anchor-permalink" href="#Partial-correlation" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.PartialCorrelation" href="#CausalityTools.PartialCorrelation"><code>CausalityTools.PartialCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PartialCorrelation &lt;: AssociationMeasure</code></pre><p>The correlation of two variables, with the effect of a set of conditioning variables removed.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for   conditional dependence.</li><li>Use with <a href="#CausalityTools.partial_correlation"><code>partial_correlation</code></a> to compute the raw correlation coefficient.</li></ul><p><strong>Description</strong></p><p>There are several ways of estimating the partial correlation. We follow the <a href="https://en.wikipedia.org/wiki/Partial_correlation">matrix inversion method</a>, because for <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s, we can very efficiently compute the required joint covariance matrix <span>$\Sigma$</span> for the random variables.</p><p>Formally, let <span>$X_1, X_2, \ldots, X_n$</span> be a set of <span>$n$</span> real-valued random variables. Consider the joint precision matrix,<span>$P = (p_{ij}) = \Sigma^-1$</span>. The partial correlation of any pair of variables <span>$(X_i, X_j)$</span>, given the remaining variables <span>$\bf{Z} = \{X_k\}_{i=1, i \neq i, j}^n$</span>, is defined as</p><p class="math-container">\[\rho_{X_i X_j | \bf{Z}} = -\dfrac{p_ij}{\sqrt{ p_{ii} p_{jj} }}\]</p><p>In practice, we compute the estimate</p><p class="math-container">\[\hat{\rho}_{X_i X_j | \bf{Z}} =
-\dfrac{\hat{p}_ij}{\sqrt{ \hat{p}_{ii} \hat{p}_{jj} }},\]</p><p>where <span>$\hat{P} = \hat{\Sigma}^{-1}$</span> is the sample precision matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/correlation/partial_correlation.jl#L4-L40">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.partial_correlation" href="#CausalityTools.partial_correlation"><code>CausalityTools.partial_correlation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">partial_correlation(x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet,
    z::VectorOrStateSpaceSet...)</code></pre><p>Compute the <a href="#CausalityTools.PartialCorrelation"><code>PartialCorrelation</code></a> between <code>x</code> and <code>y</code>, given <code>z</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/correlation/partial_correlation.jl#L43-L48">source</a></section></article><h3 id="Distance-correlation"><a class="docs-heading-anchor" href="#Distance-correlation">Distance correlation</a><a id="Distance-correlation-1"></a><a class="docs-heading-anchor-permalink" href="#Distance-correlation" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.DistanceCorrelation" href="#CausalityTools.DistanceCorrelation"><code>CausalityTools.DistanceCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DistanceCorrelation</code></pre><p>The distance correlation (Székely et al., 2007)<sup class="footnote-reference"><a id="citeref-Székely2007" href="#footnote-Székely2007">[Székely2007]</a></sup> measure quantifies potentially nonlinear associations between pairs of variables. If applied to three variables, the partial distance correlation (Székely and Rizzo, 2014)<sup class="footnote-reference"><a id="citeref-Székely2014" href="#footnote-Székely2014">[Székely2014]</a></sup> is computed.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for   pairwise dependence.</li><li>Use with <a href="#CausalityTools.distance_correlation"><code>distance_correlation</code></a> to compute the raw distance correlation   coefficient.</li></ul><div class="admonition is-category-warn"><header class="admonition-header">Warn</header><div class="admonition-body"><p>A partial distance correlation <code>distance_correlation(X, Y, Z) = 0</code> doesn&#39;t always guarantee conditional independence <code>X ⫫ Y | Z</code>. See Székely and Rizzo (2014) for in-depth discussion.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/correlation/distance_correlation.jl#L8-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.distance_correlation" href="#CausalityTools.distance_correlation"><code>CausalityTools.distance_correlation</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">distance_correlation(x, y) → dcor ∈ [0, 1]
distance_correlation(x, y, z) → pdcor</code></pre><p>Compute the empirical/sample distance correlation (Székely et al., 2007)<sup class="footnote-reference"><a id="citeref-Székely2007" href="#footnote-Székely2007">[Székely2007]</a></sup>, here called <code>dcor</code>, between StateSpaceSets <code>x</code> and <code>y</code>. Alternatively, compute the partial distance correlation <code>pdcor</code> (Székely and Rizzo, 2014)<sup class="footnote-reference"><a id="citeref-Székely2014" href="#footnote-Székely2014">[Székely2014]</a></sup>.</p><p>See also: <a href="#CausalityTools.DistanceCorrelation"><code>DistanceCorrelation</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/correlation/distance_correlation.jl#L37-L53">source</a></section></article><h2 id="Closeness-measures"><a class="docs-heading-anchor" href="#Closeness-measures">Closeness measures</a><a id="Closeness-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Closeness-measures" title="Permalink"></a></h2><h3 id="Joint-distance-distribution"><a class="docs-heading-anchor" href="#Joint-distance-distribution">Joint distance distribution</a><a id="Joint-distance-distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-distance-distribution" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.JointDistanceDistribution" href="#CausalityTools.JointDistanceDistribution"><code>CausalityTools.JointDistanceDistribution</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointDistanceDistribution &lt;: AssociationMeasure end
JointDistanceDistribution(; metric = Euclidean(), B = 10, D = 2, τ = -1, μ = 0.0)</code></pre><p>The joint distance distribution (JDD) measure (Amigó &amp; Hirata, 2018)<sup class="footnote-reference"><a id="citeref-Amigo2018" href="#footnote-Amigo2018">[Amigo2018]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li><li>Use with <a href="#CausalityTools.jdd"><code>jdd</code></a> to compute the joint distance distribution <code>Δ</code> from Amigó &amp; Hirata (2018).</li></ul><p><strong>Keyword arguments</strong></p><ul><li><strong><code>distance_metric::Metric</code></strong>: An instance of a valid distance metric from <code>Distances.jl</code>.   Defaults to <code>Euclidean()</code>.</li><li><strong><code>B::Int</code></strong>: The number of equidistant subintervals to divide the interval <code>[0, 1]</code> into   when comparing the normalised distances.</li><li><strong><code>D::Int</code></strong>: Embedding dimension.</li><li><strong><code>τ::Int</code></strong>: Embedding delay. By convention, <code>τ</code> is negative.</li><li><strong><code>μ</code></strong>: The hypothetical mean value of the joint distance distribution if there   is no coupling between <code>x</code> and <code>y</code> (default is <code>μ = 0.0</code>).</li></ul><p><strong>Description</strong></p><p>From input time series <span>$x(t)$</span> and <span>$y(t)$</span>, we first construct the delay embeddings (note the positive sign in the embedding lags; therefore the input parameter <code>τ</code> is by convention negative).</p><p class="math-container">\[\begin{align*}
\{\bf{x}_i \} &amp;= \{(x_i, x_{i+\tau}, \ldots, x_{i+(d_x - 1)\tau}) \} \\
\{\bf{y}_i \} &amp;= \{(y_i, y_{i+\tau}, \ldots, y_{i+(d_y - 1)\tau}) \} \\
\end{align*}\]</p><p>The algorithm then proceeds to analyze the distribution of distances between points of these embeddings, as described in Amigó &amp; Hirata (2018)<sup class="footnote-reference"><a id="citeref-Amigo2018" href="#footnote-Amigo2018">[Amigo2018]</a></sup>.</p><p><strong>Examples</strong></p><ul><li><a href="../examples/examples_closeness/#quickstart_jdd">Computing the JDD</a></li><li><a href="../examples/examples_independence/#quickstart_jddtest">Independence testing using JDD</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/JointDistanceDistribution.jl#L19-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.jdd" href="#CausalityTools.jdd"><code>CausalityTools.jdd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">jdd(measure::JointDistanceDistribution, source, target) → Δ</code></pre><p>Compute the joint distance distribution (Amigó &amp; Hirata, 2018<sup class="footnote-reference"><a id="citeref-Amigo2018" href="#footnote-Amigo2018">[Amigo2018]</a></sup>) from <code>source</code> to <code>target</code> using the given <a href="#CausalityTools.JointDistanceDistribution"><code>JointDistanceDistribution</code></a> measure.</p><p>Returns the distribution <code>Δ</code> from the paper directly (<a href="../examples/examples_closeness/#quickstart_jdd">example</a>). Use <a href="../independence/#CausalityTools.JointDistanceDistributionTest"><code>JointDistanceDistributionTest</code></a> to perform a formal indepencence test.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/deprecations/joint_distance_distribution.jl#L2-L15">source</a></section></article><h3 id="S-measure"><a class="docs-heading-anchor" href="#S-measure">S-measure</a><a id="S-measure-1"></a><a class="docs-heading-anchor-permalink" href="#S-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.SMeasure" href="#CausalityTools.SMeasure"><code>CausalityTools.SMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SMeasure &lt; AssociationMeasure
SMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p><code>SMeasure</code> is a bivariate association measure from Grassberger et al. (1999)<sup class="footnote-reference"><a id="citeref-Grassberger1999" href="#footnote-Grassberger1999">[Grassberger1999]</a></sup> and Quiroga et al. (2000) <sup class="footnote-reference"><a id="citeref-Quiroga2000" href="#footnote-Quiroga2000">[Quiroga2000]</a></sup> that measure directional dependence between two input (potentially multivariate) time series.</p><p>Note that <code>τx</code> and <code>τy</code> are negative; see explanation below.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li><li>Use with <a href="#CausalityTools.s_measure"><code>s_measure</code></a> to compute the raw s-measure statistic.</li></ul><p><strong>Description</strong></p><p>The steps of the algorithm are:</p><ol><li>From input time series <span>$x(t)$</span> and <span>$y(t)$</span>, construct the delay embeddings (note  the positive sign in the embedding lags; therefore inputs parameters  <code>τx</code> and <code>τy</code> are by convention negative).</li></ol><p class="math-container">\[\begin{align*}
\{\bf{x}_i \} &amp;= \{(x_i, x_{i+\tau_x}, \ldots, x_{i+(d_x - 1)\tau_x}) \} \\
\{\bf{y}_i \} &amp;= \{(y_i, y_{i+\tau_y}, \ldots, y_{i+(d_y - 1)\tau_y}) \} \\
\end{align*}\]</p><ol><li><p>Let <span>$r_{i,j}$</span> and <span>$s_{i,j}$</span> be the indices of the <code>K</code>-th nearest neighbors  of <span>$\bf{x}_i$</span> and <span>$\bf{y}_i$</span>, respectively. Neighbors closed than <code>w</code> time indices  are excluded during searches (i.e. <code>w</code> is the Theiler window).</p></li><li><p>Compute the the mean squared Euclidean distance to the <span>$K$</span> nearest neighbors  for each <span>$x_i$</span>, using the indices <span>$r_{i, j}$</span>.</p></li></ol><p class="math-container">\[R_i^{(k)}(x) = \dfrac{1}{k} \sum_{i=1}^{k}(\bf{x}_i, \bf{x}_{r_{i,j}})^2\]</p><ul><li>Compute the y-conditioned mean squared Euclidean distance to the <span>$K$</span> nearest   neighbors for each <span>$x_i$</span>, now using the indices <span>$s_{i,j}$</span>.</li></ul><p class="math-container">\[R_i^{(k)}(x|y) = \dfrac{1}{k} \sum_{i=1}^{k}(\bf{x}_i, \bf{x}_{s_{i,j}})^2\]</p><ul><li>Define the following measure of independence, where <span>$0 \leq S \leq 1$</span>, and   low values indicate independence and values close to one occur for   synchronized signals.</li></ul><p class="math-container">\[S^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N} \dfrac{R_i^{(k)}(x)}{R_i^{(k)}(x|y)}\]</p><p><strong>Input data</strong></p><p>The algorithm is slightly modified from <sup class="footnote-reference"><a id="citeref-Grassberger1999" href="#footnote-Grassberger1999">[Grassberger1999]</a></sup> to allow univariate timeseries as input.</p><ul><li>If <code>x</code> and <code>y</code> are <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s then use <code>x</code> and <code>y</code> as is and ignore the parameters   <code>dx</code>/<code>τx</code> and <code>dy</code>/<code>τy</code>.</li><li>If <code>x</code> and <code>y</code> are scalar time series, then create <code>dx</code> and <code>dy</code> dimensional embeddings,   respectively, of both <code>x</code> and <code>y</code>, resulting in <code>N</code> different <code>m</code>-dimensional embedding points   <span>$X = \{x_1, x_2, \ldots, x_N \}$</span> and <span>$Y = \{y_1, y_2, \ldots, y_N \}$</span>.   <code>τx</code> and <code>τy</code> control the embedding lags for <code>x</code> and <code>y</code>.</li><li>If <code>x</code> is a scalar-valued vector and <code>y</code> is a <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, or vice versa,   then create an embedding of the scalar timeseries using parameters <code>dx</code>/<code>τx</code> or <code>dy</code>/<code>τy</code>.</li></ul><p>In all three cases, input StateSpaceSets are length-matched by eliminating points at the end of the longest StateSpaceSet (after the embedding step, if relevant) before analysis.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/SMeasure.jl#L10-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.s_measure" href="#CausalityTools.s_measure"><code>CausalityTools.s_measure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">s_measure(measure::SMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)</code></pre><p>Compute the <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> from source <code>x</code> to target <code>y</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/SMeasure.jl#L101-L105">source</a></section><section><div><pre><code class="nohighlight hljs">s_measure(measure::SMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet) → s ∈ [0, 1]</code></pre><p>Compute the given <code>measure</code> to quantify the directional dependence between univariate/multivariate time series <code>x</code> and <code>y</code>.</p><p>Returns a scalar <code>s</code> where <code>s = 0</code> indicates independence between <code>x</code> and <code>y</code>, and higher values indicate synchronization between <code>x</code> and <code>y</code>, with complete synchronization for <code>s = 1.0</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools

# A two-dimensional Ulam lattice map
sys = ulam(2)

# Sample 1000 points after discarding 5000 transients
orbit = trajectory(sys, 1000, Ttr = 5000)
x, y = orbit[:, 1], orbit[:, 2]

# 4-dimensional embedding for `x`, 5-dimensional embedding for `y`
m = SMeasure(dx = 4, τx = 3, dy = 5, τy = 1)
s_measure(m, x, y)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/deprecations/smeasure.jl#L4-L31">source</a></section></article><h3 id="H-measure"><a class="docs-heading-anchor" href="#H-measure">H-measure</a><a id="H-measure-1"></a><a class="docs-heading-anchor-permalink" href="#H-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.HMeasure" href="#CausalityTools.HMeasure"><code>CausalityTools.HMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HMeasure &lt;: AssociationMeasure
HMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>HMeasure</code> (Grassberger et al., 1999)<sup class="footnote-reference"><a id="citeref-Grassberger1999" href="#footnote-Grassberger1999">[Grassberger1999]</a></sup> is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li><li>Use with <a href="#CausalityTools.h_measure"><code>h_measure</code></a> to compute the raw h-measure statistic.</li></ul><p><strong>Description</strong></p><p>The <code>HMeasure</code> (Grassberger et al., 1999)<sup class="footnote-reference"><a id="citeref-Grassberger1999" href="#footnote-Grassberger1999">[Grassberger1999]</a></sup> is similar to the <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>, but the numerator of the formula is replaced by <span>$R_i(x)$</span>, the mean squared Euclidean distance to <em>all other points</em>, and there is a <span>$\log$</span>-term inside the sum:</p><p class="math-container">\[H^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{R_i(x)}{R_i^{(k)}(x|y)} \right).\]</p><p>Parameters are the same and <span>$R_i^{(k)}(x|y)$</span> is computed as for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/HMeasure.jl#L10-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.h_measure" href="#CausalityTools.h_measure"><code>CausalityTools.h_measure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">h_measure(measure::HMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)</code></pre><p>Compute the <a href="#CausalityTools.HMeasure"><code>HMeasure</code></a> from source <code>x</code> to target <code>y</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/HMeasure.jl#L56-L60">source</a></section></article><h3 id="M-measure"><a class="docs-heading-anchor" href="#M-measure">M-measure</a><a id="M-measure-1"></a><a class="docs-heading-anchor-permalink" href="#M-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MMeasure" href="#CausalityTools.MMeasure"><code>CausalityTools.MMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MMeasure &lt;: AssociationMeasure
MMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>MMeasure</code> (Andrzejak et al., 2003)<sup class="footnote-reference"><a id="citeref-Andrzejak2003" href="#footnote-Andrzejak2003">[Andrzejak2003]</a></sup> is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li><li>Use with <a href="#CausalityTools.m_measure"><code>m_measure</code></a> to compute the raw m-measure statistic.</li></ul><p><strong>Description</strong></p><p>The <code>MMeasure</code> is based on <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> and <a href="#CausalityTools.HMeasure"><code>HMeasure</code></a>. It is given by</p><p class="math-container">\[M^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{R_i(x) - R_i^{(k)}(x|y)}{R_i(x) - R_i^k(x)} \right),\]</p><p>where <span>$R_i(x)$</span> is computed as for <a href="#CausalityTools.HMeasure"><code>HMeasure</code></a>, while <span>$R_i^k(x)$</span> and <span>$R_i^{(k)}(x|y)$</span> is computed as for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>. Parameters also have the same meaning as for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>/<a href="#CausalityTools.HMeasure"><code>HMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/MMeasure.jl#L10-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.m_measure" href="#CausalityTools.m_measure"><code>CausalityTools.m_measure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">m_measure(measure::MMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)</code></pre><p>Compute the <a href="#CausalityTools.MMeasure"><code>MMeasure</code></a> from source <code>x</code> to target <code>y</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/MMeasure.jl#L55-L59">source</a></section></article><h3 id="L-measure"><a class="docs-heading-anchor" href="#L-measure">L-measure</a><a id="L-measure-1"></a><a class="docs-heading-anchor-permalink" href="#L-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.LMeasure" href="#CausalityTools.LMeasure"><code>CausalityTools.LMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LMeasure &lt;: AssociationMeasure
LMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>LMeasure</code> (Chicharro &amp; Andrzejak, 2009)[^^Chicharro20093] is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li><li>Use with <a href="#CausalityTools.l_measure"><code>l_measure</code></a> to compute the raw l-measure statistic.</li></ul><p><strong>Description</strong></p><p><code>LMeasure</code> is similar to <a href="#CausalityTools.MMeasure"><code>MMeasure</code></a>, but uses distance ranks instead of the raw distances.</p><p>Let <span>$\bf{x_i}$</span> be an embedding vector, and let <span>$g_{i,j}$</span> denote the rank that the distance between <span>$\bf{x_i}$</span> and some other vector <span>$\bf{x_j}$</span> in a sorted ascending list of distances between <span>$\bf{x_i}$</span> and <span>$\bf{x_{i \neq j}}$</span> In other words, <span>$g_{i,j}$</span> this is just the <span>$N-1$</span> nearest neighbor distances sorted )</p><p><code>LMeasure</code> is then defined as</p><p class="math-container">\[L^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{G_i(x) - G_i^{(k)}(x|y)}{G_i(x) - G_i^k(x)} \right),\]</p><p>where <span>$G_i(x) = \frac{N}{2}$</span> and <span>$G_i^K(x) = \frac{k+1}{2}$</span> are the mean and minimal rank, respectively.</p><p>The <span>$y$</span>-conditioned mean rank is defined as</p><p class="math-container">\[G_i^{(k)}(x|y) = \dfrac{1}{K}\sum_{j=1}^{K} g_{i,w_{i, j}},\]</p><p>where <span>$w_{i,j}$</span> is the index of the <span>$j$</span>-th nearest neighbor of <span>$\bf{y_i}$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/LMeasure.jl#L10-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.l_measure" href="#CausalityTools.l_measure"><code>CausalityTools.l_measure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">l_measure(measure::LMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)</code></pre><p>Compute the <a href="#CausalityTools.LMeasure"><code>LMeasure</code></a> from source <code>x</code> to target <code>y</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/closeness/LMeasure.jl#L69-L73">source</a></section></article><h2 id="Cross-map-measures"><a class="docs-heading-anchor" href="#Cross-map-measures">Cross-map measures</a><a id="Cross-map-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-map-measures" title="Permalink"></a></h2><p>See also the <a href="@ref crossmap_api">cross mapping API</a> for estimators.</p><h3 id="Convergent-cross-mapping"><a class="docs-heading-anchor" href="#Convergent-cross-mapping">Convergent cross mapping</a><a id="Convergent-cross-mapping-1"></a><a class="docs-heading-anchor-permalink" href="#Convergent-cross-mapping" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.ConvergentCrossMapping" href="#CausalityTools.ConvergentCrossMapping"><code>CausalityTools.ConvergentCrossMapping</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConvergentCrossMapping &lt;: CrossmapMeasure
ConvergentCrossMapping(; d::Int = 2, τ::Int = -1, w::Int = 0,
    f = Statistics.cor, embed_warn = true)</code></pre><p>The convergent <a href="../api/api_crossmap/#cross_mapping_api">cross mapping</a> (CCM) measure (Sugihara et al., 2012)<sup class="footnote-reference"><a id="citeref-Sugihara2012" href="#footnote-Sugihara2012">[Sugihara2012]</a></sup>).</p><p>Specifies embedding dimension <code>d</code>, embedding lag <code>τ</code> to be used, as described below, with <a href="../api/api_crossmap/#CausalityTools.predict"><code>predict</code></a> or <a href="../api/api_crossmap/#CausalityTools.crossmap"><code>crossmap</code></a>. The Theiler window <code>w</code> controls how many temporal neighbors are excluded during neighbor searches (<code>w = 0</code> means that only the point itself is excluded). <code>f</code> is a function that computes the agreement between observations and predictions (the default, <code>f = Statistics.cor</code>, gives the Pearson correlation coefficient).</p><p><strong>Embedding</strong></p><p>Let <code>S(i)</code> be the source time series variable and <code>T(i)</code> be the target time series variable. This version produces regular embeddings with fixed dimension <code>d</code> and embedding lag <code>τ</code> as follows:</p><p class="math-container">\[( S(i), S(i+\tau), S(i+2\tau), \ldots, S(i+(d-1)\tau, T(i))_{i=1}^{N-(d-1)\tau}.\]</p><p>In this joint embedding, neighbor searches are performed in the subspace spanned by the first <code>D-1</code> variables, while the last (<code>D</code>-th) variable is to be predicted.</p><p>With this convention, <code>τ &lt; 0</code> implies &quot;past/present values of source used to predict target&quot;, and <code>τ &gt; 0</code> implies &quot;future/present values of source used to predict target&quot;. The latter case may not be meaningful for many applications, so by default, a warning will be given if <code>τ &gt; 0</code> (<code>embed_warn = false</code> turns off warnings).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/crossmappings/ccm-like/ConvergentCrossMapping.jl#L6-L43">source</a></section></article><h3 id="Pairwise-asymmetric-inference"><a class="docs-heading-anchor" href="#Pairwise-asymmetric-inference">Pairwise asymmetric inference</a><a id="Pairwise-asymmetric-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Pairwise-asymmetric-inference" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.PairwiseAsymmetricInference" href="#CausalityTools.PairwiseAsymmetricInference"><code>CausalityTools.PairwiseAsymmetricInference</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PairwiseAsymmetricInference &lt;: CrossmapMeasure
PairwiseAsymmetricInference(; d::Int = 2, τ::Int = -1, w::Int = 0,
    f = Statistics.cor, embed_warn = true)</code></pre><p>The pairwise asymmetric inference (PAI) <a href="../api/api_crossmap/#cross_mapping_api">cross mapping</a> measure (McCracken &amp; Weigel (2014)<sup class="footnote-reference"><a id="citeref-McCracken2014" href="#footnote-McCracken2014">[McCracken2014]</a></sup>) is a version of <a href="#CausalityTools.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a> that searches for neighbors in <em>mixed</em> embeddings (i.e. both source and target variables included); otherwise, the algorithms are identical.</p><p>Specifies embedding dimension <code>d</code>, embedding lag <code>τ</code> to be used, as described below, with <a href="../api/api_crossmap/#CausalityTools.predict"><code>predict</code></a> or <a href="../api/api_crossmap/#CausalityTools.crossmap"><code>crossmap</code></a>. The Theiler window <code>w</code> controls how many temporal neighbors are excluded during neighbor searches (<code>w = 0</code> means that only the point itself is excluded). <code>f</code> is a function that computes the agreement between observations and predictions (the default, <code>f = Statistics.cor</code>, gives the Pearson correlation coefficient).</p><p><strong>Embedding</strong></p><p>There are many possible ways of defining the embedding for PAI. Currently, we only implement the <em>&quot;add one non-lagged source timeseries to an embedding of the target&quot;</em> approach, which is used as an example in McCracken &amp; Weigel&#39;s paper. Specifically: Let <code>S(i)</code> be the source time series variable and <code>T(i)</code> be the target time series variable. <code>PairwiseAsymmetricInference</code> produces regular embeddings with fixed dimension <code>d</code> and embedding lag <code>τ</code> as follows:</p><p class="math-container">\[(S(i), T(i+(d-1)\tau, \ldots, T(i+2\tau), T(i+\tau), T(i)))_{i=1}^{N-(d-1)\tau}.\]</p><p>In this joint embedding, neighbor searches are performed in the subspace spanned by the first <code>D</code> variables, while the last variable is to be predicted.</p><p>With this convention, <code>τ &lt; 0</code> implies &quot;past/present values of source used to predict target&quot;, and <code>τ &gt; 0</code> implies &quot;future/present values of source used to predict target&quot;. The latter case may not be meaningful for many applications, so by default, a warning will be given if <code>τ &gt; 0</code> (<code>embed_warn = false</code> turns off warnings).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/crossmappings/ccm-like/PairwiseAsymmetricInference.jl#L5-L48">source</a></section></article><h2 id="Recurrence-based"><a class="docs-heading-anchor" href="#Recurrence-based">Recurrence-based</a><a id="Recurrence-based-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrence-based" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MCR" href="#CausalityTools.MCR"><code>CausalityTools.MCR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MCR &lt;: AssociationMeasure
MCR(; r, metric = Euclidean())</code></pre><p>An association measure based on mean conditional probabilities of recurrence (MCR) introduced by Romano et al. (2007)<sup class="footnote-reference"><a id="citeref-Romano2007" href="#footnote-Romano2007">[Romano2007]</a></sup>.</p><p><code>r</code> is  mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of <code>AbstractRecurrenceType</code> from <a href="https://juliadynamics.github.io/RecurrenceAnalysis.jl/stable/">RecurrenceAnalysis.jl</a>. To use any <code>r</code> that is not a real number, you have to do <code>using RecurrenceAnalysis</code> first. The <code>metric</code> is any valid metric from <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   association.</li><li>Use with <a href="../api/api_recurrence/#CausalityTools.mcr"><code>mcr</code></a> to compute the raw MCR for pairwise association.</li></ul><p><strong>Description</strong></p><p>For input variables <code>X</code> and <code>Y</code>, the conditional probability of recurrence is defined as</p><p class="math-container">\[M(X | Y) = \dfrac{1}{N} \sum_{i=1}^N p(\bf{y_i} | \bf{x_i}) =
\dfrac{1}{N} \sum_{i=1}^N \dfrac{\sum_{i=1}^N J_{R_{i, j}}^{X, Y}}{\sum_{i=1}^N R_{i, j}^X},\]</p><p>where <span>$R_{i, j}^X$</span> is the recurrence matrix and <span>$J_{R_{i, j}}^{X, Y}$</span> is the joint recurrence matrix, constructed using the given <code>metric</code>. The measure <span>$M(Y | X)$</span> is defined analogously.</p><p>Romano et al. (2007)&#39;s interpretation of this quantity is that if <code>X</code> drives <code>Y</code>, then <code>M(X|Y) &gt; M(Y|X)</code>, if <code>Y</code> drives <code>X</code>, then <code>M(Y|X) &gt; M(X|Y)</code>, and if coupling is symmetric,  then <code>M(Y|X) = M(X|Y)</code>.</p><p><strong>Input data</strong></p><p><code>X</code> and <code>Y</code> can be either both univariate timeseries, or both multivariate <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/recurrence/MCR.jl#L7-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.RMCD" href="#CausalityTools.RMCD"><code>CausalityTools.RMCD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMCD &lt;: AssociationMeasure
RMCD(; r, metric = Euclidean(), base = 2)</code></pre><p>The recurrence measure of conditional dependence, or RMCD (Ramos et al., 2017)<sup class="footnote-reference"><a id="citeref-Ramos2017" href="#footnote-Ramos2017">[Ramos2017]</a></sup>, is a recurrence-based measure that mimics the conditional mutual information, but uses recurrence probabilities.</p><p><code>r</code> is a mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of <code>AbstractRecurrenceType</code> from <a href="https://juliadynamics.github.io/RecurrenceAnalysis.jl/stable/">RecurrenceAnalysis.jl</a>. To use any <code>r</code> that is not a real number, you have to do <code>using RecurrenceAnalysis</code> first. The <code>metric</code> is any valid metric from <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a>.</p><p>Both the pairwise and conditional RMCD is non-negative, but due to round-off error, negative values may occur. If that happens, an RMCD value of <code>0.0</code> is returned.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   or conditional association.</li><li>Use with <a href="../api/api_recurrence/#CausalityTools.rmcd"><code>rmcd</code></a> to compute the raw RMCD for pairwise or conditional association.</li></ul><p><strong>Description</strong></p><p>The RMCD measure is defined by</p><p class="math-container">\[I_{RMCD}(X; Y | Z) = \dfrac{1}{N}
\sum_{i} \left[
\dfrac{1}{N} \sum_{j} R_{ij}^{X, Y, Z}
\log \left(
    \dfrac{\sum_{j} R_{ij}^{X, Y, Z} \sum_{j} R_{ij}^{Z} }{\sum_{j} \sum_{j} R_{ij}^{X, Z} \sum_{j} \sum_{j} R_{ij}^{Y, Z}}
    \right)
\right],\]</p><p>where  <code>base</code> controls the base of the logarithm. <span>$I_{RMCD}(X; Y | Z)$</span> is zero when <span>$Z = X$</span>, <span>$Z = Y$</span> or when <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span> are mutually independent.</p><p>Our implementation allows dropping the third/last argument, in which case the following mutual information-like quantitity is computed (not discussed in Ramos et al., 2017).</p><p class="math-container">\[
I_{RMCD}(X; Y) = \dfrac{1}{N}
\sum_{i} \left[
\dfrac{1}{N} \sum_{j} R_{ij}^{X, Y}
\log \left(
    \dfrac{\sum_{j} R_{ij}^{X}  R_{ij}^{Y} }{\sum_{j} R_{ij}^{X, Y}}
    \right)
\right]\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/recurrence/RMCD.jl#L5-L67">source</a></section></article><h2 id="information_measures"><a class="docs-heading-anchor" href="#information_measures">Information measures</a><a id="information_measures-1"></a><a class="docs-heading-anchor-permalink" href="#information_measures" title="Permalink"></a></h2><p>Association measures that are information-based are listed here. Available estimators are listed in the <a href="../api/api_information_overview/#information_api">information API</a>.</p><h3 id="Mutual-information-(Shannon)"><a class="docs-heading-anchor" href="#Mutual-information-(Shannon)">Mutual information (Shannon)</a><a id="Mutual-information-(Shannon)-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information-(Shannon)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MIShannon" href="#CausalityTools.MIShannon"><code>CausalityTools.MIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIShannon &lt;: MutualInformation
MIShannon(; base = 2)</code></pre><p>The Shannon mutual information <span>$I^S(X; Y)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> to compute the raw mutual information.</li></ul><p><strong>Discrete definition</strong></p><p>There are many equivalent formulations of discrete Shannon mutual information. In this package, we currently use the double-sum and the three-entropies formulations.</p><p><strong>Double sum formulation</strong></p><p>Assume we observe samples <span>$\bar{\bf{X}}_{1:N_y} = \{\bar{\bf{X}}_1, \ldots, \bar{\bf{X}}_n \}$</span> and <span>$\bar{\bf{Y}}_{1:N_x} = \{\bar{\bf{Y}}_1, \ldots, \bar{\bf{Y}}_n \}$</span> from two discrete random variables <span>$X$</span> and <span>$Y$</span> with finite supports <span>$\mathcal{X} = \{ x_1, x_2, \ldots, x_{M_x} \}$</span> and <span>$\mathcal{Y} = y_1, y_2, \ldots, x_{M_y}$</span>. The double-sum estimate is obtained by replacing the double sum</p><p class="math-container">\[\hat{I}_{DS}(X; Y) =
 \sum_{x_i \in \mathcal{X}, y_i \in \mathcal{Y}} p(x_i, y_j) \log \left( \dfrac{p(x_i, y_i)}{p(x_i)p(y_j)} \right)\]</p><p>where  <span>$\hat{p}(x_i) = \frac{n(x_i)}{N_x}$</span>, <span>$\hat{p}(y_i) = \frac{n(y_j)}{N_y}$</span>, and <span>$\hat{p}(x_i, x_j) = \frac{n(x_i)}{N}$</span>, and <span>$N = N_x N_y$</span>. This definition is used by <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> when called with a <a href="../api/api_contingency_table/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>.</p><p><strong>Three-entropies formulation</strong></p><p>An equivalent formulation of discrete Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = H^S(X) + H_q^S(Y) - H^S(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint discrete Shannon entropies. This definition is used by <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> when called with a <a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Differential mutual information</strong></p><p>One possible formulation of differential Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = h^S(X) + h_q^S(Y) - h^S(X, Y),\]</p><p>where <span>$h^S(\cdot)$</span> and <span>$h^S(\cdot, \cdot)$</span> are the marginal and joint differential Shannon entropies. This definition is used by <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> when called with a <a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p><p>See also: <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/mutualinfo/MIShannon.jl#L6-L67">source</a></section></article><h3 id="Mutual-information-(Tsallis,-Furuichi)"><a class="docs-heading-anchor" href="#Mutual-information-(Tsallis,-Furuichi)">Mutual information (Tsallis, Furuichi)</a><a id="Mutual-information-(Tsallis,-Furuichi)-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information-(Tsallis,-Furuichi)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MITsallisFuruichi" href="#CausalityTools.MITsallisFuruichi"><code>CausalityTools.MITsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisFuruichi &lt;: MutualInformation
MITsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from Furuichi (2006)<sup class="footnote-reference"><a id="citeref-Furuichi2006" href="#footnote-Furuichi2006">[Furuichi2006]</a></sup>, which in that paper is called the <em>mutual entropy</em>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> to compute the raw mutual information.</li></ul><p><strong>Description</strong></p><p>Furuichi&#39;s Tsallis mutual entropy between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_q^T(X; Y) = H_q^T(X) - H_q^T(X | Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X, Y),\]</p><p>where <span>$H^T(\cdot)$</span> and <span>$H^T(\cdot, \cdot)$</span> are the marginal and joint Tsallis entropies, and <code>q</code> is the <a href="../api/api_entropies/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter. ```</p><p>See also: <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/mutualinfo/MITsallisFuruichi.jl#L3-L34">source</a></section></article><h3 id="Mutual-information-(Tsallis,-Martin)"><a class="docs-heading-anchor" href="#Mutual-information-(Tsallis,-Martin)">Mutual information (Tsallis, Martin)</a><a id="Mutual-information-(Tsallis,-Martin)-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information-(Tsallis,-Martin)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MITsallisMartin" href="#CausalityTools.MITsallisMartin"><code>CausalityTools.MITsallisMartin</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisMartin &lt;: MutualInformation
MITsallisMartin(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from Martin et al. (2005)<sup class="footnote-reference"><a id="citeref-Martin2004" href="#footnote-Martin2004">[Martin2004]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> to compute the raw mutual information. </li></ul><p><strong>Description</strong></p><p>Martin et al.&#39;s Tsallis mutual information between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_{\text{Martin}}^T(X, Y, q) := H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint Shannon entropies, and <code>q</code> is the <a href="../api/api_entropies/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter.</p><p>See also: <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/mutualinfo/MITsallisMartin.jl#L3-L32">source</a></section></article><h3 id="Mutual-information-(Rényi,-Sarbu)"><a class="docs-heading-anchor" href="#Mutual-information-(Rényi,-Sarbu)">Mutual information (Rényi, Sarbu)</a><a id="Mutual-information-(Rényi,-Sarbu)-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information-(Rényi,-Sarbu)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MIRenyiSarbu" href="#CausalityTools.MIRenyiSarbu"><code>CausalityTools.MIRenyiSarbu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiSarbu &lt;: MutualInformation
MIRenyiSarbu(; base = 2, q = 1.5)</code></pre><p>The discrete Rényi mutual information from Sarbu (2014)<sup class="footnote-reference"><a id="citeref-Sarbu2014" href="#footnote-Sarbu2014">[Sarbu2014]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> to compute the raw mutual information.</li></ul><p><strong>Description</strong></p><p>Sarbu (2014) defines discrete Rényi mutual information as the Rényi <span>$\alpha$</span>-divergence between the conditional joint probability mass function <span>$p(x, y)$</span> and the product of the conditional marginals, <span>$p(x) \cdot p(y)$</span>:</p><p class="math-container">\[I(X, Y)^R_q =
\dfrac{1}{q-1}
\log \left(
    \sum_{x \in X, y \in Y}
    \dfrac{p(x, y)^q}{\left( p(x)\cdot p(y) \right)^{q-1}}
\right)\]</p><p>See also: <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/mutualinfo/MIRenyiSarbu.jl#L3-L34">source</a></section></article><h3 id="Mutual-information-(Rényi,-Jizba)"><a class="docs-heading-anchor" href="#Mutual-information-(Rényi,-Jizba)">Mutual information (Rényi, Jizba)</a><a id="Mutual-information-(Rényi,-Jizba)-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information-(Rényi,-Jizba)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MIRenyiJizba" href="#CausalityTools.MIRenyiJizba"><code>CausalityTools.MIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiJizba &lt;: MutualInformation</code></pre><p>The Rényi mutual information <span>$I_q^{R_{J}}(X; Y)$</span> defined in Jizba et al. (2012)<sup class="footnote-reference"><a id="citeref-Jizba2012" href="#footnote-Jizba2012">[Jizba2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_mutualinfo/#CausalityTools.mutualinfo"><code>mutualinfo</code></a> to compute the raw mutual information. </li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y) = S_q^{R}(X) + S_q^{R}(Y) - S_q^{R}(X, Y),\]</p><p>where <span>$S_q^{R}(\cdot)$</span> and <span>$S_q^{R}(\cdot, \cdot)$</span> the <a href="@ref"><code>Rényi</code></a> entropy and the joint Rényi entropy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/mutualinfo/MIRenyiJizba.jl#L3-L27">source</a></section></article><h3 id="Conditional-mutual-information-(Shannon)"><a class="docs-heading-anchor" href="#Conditional-mutual-information-(Shannon)">Conditional mutual information (Shannon)</a><a id="Conditional-mutual-information-(Shannon)-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-information-(Shannon)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CMIShannon" href="#CausalityTools.CMIShannon"><code>CausalityTools.CMIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIShannon &lt;: ConditionalMutualInformation
CMIShannon(; base = 2)</code></pre><p>The Shannon conditional mutual information (CMI) <span>$I^S(X; Y | Z)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a> to compute the raw conditional mutual information.</li></ul><p><strong>Supported definitions</strong></p><p>Consider random variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span>, given <span>$Z \in \mathbb{R}^{d_Z}$</span>. The Shannon conditional mutual information is defined as</p><p class="math-container">\[\begin{align*}
I(X; Y | Z)
&amp;= H^S(X, Z) + H^S(Y, z) - H^S(X, Y, Z) - H^S(Z) \\
&amp;= I^S(X; Y, Z) + I^S(X; Y)
\end{align*},\]</p><p>where <span>$I^S(\cdot; \cdot)$</span> is the Shannon mutual information <a href="#CausalityTools.MIShannon"><code>MIShannon</code></a>, and <span>$H^S(\cdot)$</span> is the <a href="../api/api_entropies/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>Differential Shannon CMI is obtained by replacing the entropies by differential entropies.</p><p>See also: <a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/condmutualinfo/CMIShannon.jl#L6-L38">source</a></section></article><h3 id="Conditional-mutual-information-(Rényi,-Jizba)"><a class="docs-heading-anchor" href="#Conditional-mutual-information-(Rényi,-Jizba)">Conditional mutual information (Rényi, Jizba)</a><a id="Conditional-mutual-information-(Rényi,-Jizba)-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-information-(Rényi,-Jizba)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CMIRenyiJizba" href="#CausalityTools.CMIRenyiJizba"><code>CausalityTools.CMIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiJizba &lt;: ConditionalMutualInformation</code></pre><p>The Rényi conditional mutual information <span>$I_q^{R_{J}}(X; Y | Z$</span> defined in Jizba et al. (2012)<sup class="footnote-reference"><a id="citeref-Jizba2012" href="#footnote-Jizba2012">[Jizba2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a> to compute the raw conditional mutual information.</li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y | Z) = I_q^{R_{J}}(X; Y, Z) - I_q^{R_{J}}(X; Z),\]</p><p>where <span>$I_q^{R_{J}}(X; Z)$</span> is the <a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a> mutual information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/condmutualinfo/CMIRenyiJizba.jl#L4-L27">source</a></section></article><h3 id="Conditional-mutual-information-(Rényi,-Poczos)"><a class="docs-heading-anchor" href="#Conditional-mutual-information-(Rényi,-Poczos)">Conditional mutual information (Rényi, Poczos)</a><a id="Conditional-mutual-information-(Rényi,-Poczos)-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-information-(Rényi,-Poczos)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CMIRenyiPoczos" href="#CausalityTools.CMIRenyiPoczos"><code>CausalityTools.CMIRenyiPoczos</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiPoczos &lt;: ConditionalMutualInformation</code></pre><p>The differential Rényi conditional mutual information <span>$I_q^{R_{P}}(X; Y | Z)$</span> defined in (Póczos &amp; Schneider, 2012)<sup class="footnote-reference"><a id="citeref-Póczos2012" href="#footnote-Póczos2012">[Póczos2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a> to compute the raw conditional mutual information. </li></ul><p><strong>Definition</strong></p><p class="math-container">\[\begin{align*}
I_q^{R_{P}}(X; Y | Z) = \dfrac{1}{q-1}
\int \int \int \dfrac{p_Z(z) p_{X, Y | Z}^q}{( p_{X|Z}(x|z) p_{Y|Z}(y|z) )^{q-1}} \\
\mathbb{E}_{X, Y, Z} \sim p_{X, Y, Z}
\left[ \dfrac{p_{X, Z}^{1-q}(X, Z) p_{Y, Z}^{1-q}(Y, Z) }{p_{X, Y, Z}^{1-q}(X, Y, Z) p_Z^{1-q}(Z)} \right]
\end{align*}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/condmutualinfo/CMIRenyiPoczos.jl#L3-L29">source</a></section></article><h3 id="Transfer-entropy-(Shannon)"><a class="docs-heading-anchor" href="#Transfer-entropy-(Shannon)">Transfer entropy (Shannon)</a><a id="Transfer-entropy-(Shannon)-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy-(Shannon)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.TEShannon" href="#CausalityTools.TEShannon"><code>CausalityTools.TEShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TEShannon &lt;: TransferEntropy
TEShannon(; base = 2; embedding = EmbeddingTE()) &lt;: TransferEntropy</code></pre><p>The Shannon-type transfer entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li><li>Use with <a href="../api/api_transferentropy/#CausalityTools.transferentropy"><code>transferentropy</code></a> to compute the raw transfer entropy.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I^S(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I^S(T^+; S^- | T^-, C^-)
\end{align*}\]</p><p>where <span>$I(T^+; S^- | T^-)$</span> is the Shannon conditional mutual information (<a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a>). The variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> are described in the docstring for <a href="../api/api_transferentropy/#CausalityTools.transferentropy"><code>transferentropy</code></a>.</p><p><strong>Compatible estimators</strong></p><p>Shannon-type transfer entropy can be estimated using a range of different estimators, which all boil down to computing conditional mutual information, except for <a href="../api/api_transferentropy/#CausalityTools.TransferEntropyEstimator"><code>TransferEntropyEstimator</code></a>, which compute transfer entropy using some direct method.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Type</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></th></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>SymbolicPermuation</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Lord"><code>Lord</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>GaussanMI</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Parametric</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>KSG1</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Continuous</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>KSG2</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Continuous</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Mixed</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Continuous</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.FPVP"><code>FPVP</code></a></td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.MesnerShalizi"><code>MesnerShalizi</code></a></td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a></td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_transferentropy/#CausalityTools.Zhu1"><code>Zhu1</code></a></td><td style="text-align: right"><a href="../api/api_transferentropy/#CausalityTools.TransferEntropyEstimator"><code>TransferEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_transferentropy/#CausalityTools.Lindner"><code>Lindner</code></a></td><td style="text-align: right"><a href="../api/api_transferentropy/#CausalityTools.TransferEntropyEstimator"><code>TransferEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/transferentropy/TEShannon.jl#L4-L61">source</a></section></article><h3 id="Transfer-entropy-(Rényi,-Jizba)"><a class="docs-heading-anchor" href="#Transfer-entropy-(Rényi,-Jizba)">Transfer entropy (Rényi, Jizba)</a><a id="Transfer-entropy-(Rényi,-Jizba)-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy-(Rényi,-Jizba)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.TERenyiJizba" href="#CausalityTools.TERenyiJizba"><code>CausalityTools.TERenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TERenyiJizba() &lt;: TransferEntropy</code></pre><p>The Rényi transfer entropy from Jizba et al. (2012)<sup class="footnote-reference"><a id="citeref-Jizba2012" href="#footnote-Jizba2012">[Jizba2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li><li>Use with <a href="../api/api_transferentropy/#CausalityTools.transferentropy"><code>transferentropy</code></a> to compute the raw transfer entropy.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I_q^{R_J}(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I_q^{R_J}(T^+; S^- | T^-, C^-),
\end{align*},\]</p><p>where <span>$I_q^{R_J}(T^+; S^- | T^-)$</span> is Jizba et al. (2012)&#39;s definition of conditional mutual information (<a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a>). The variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> are described in the docstring for <a href="../api/api_transferentropy/#CausalityTools.transferentropy"><code>transferentropy</code></a>.</p><p><strong>Compatible estimators</strong></p><p>Jizba&#39;s formulation of Renyi-type transfer entropy can currently be estimated using selected probabilities estimators and differential entropy estimators, which under the hood compute the transfer entropy as Jizba&#39;s formulation of Rényi conditional mutual information.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Type</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a></th></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/transferentropy/TERenyiJizba.jl#L3-L47">source</a></section></article><h3 id="Part-mutual-information"><a class="docs-heading-anchor" href="#Part-mutual-information">Part mutual information</a><a id="Part-mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Part-mutual-information" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.PMI" href="#CausalityTools.PMI"><code>CausalityTools.PMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PMI &lt;: AssociationMeasure
PMI(; base = 2)</code></pre><p>The partial mutual information (PMI) measure of association (Zhao et al., 2016)<sup class="footnote-reference"><a id="citeref-Zhao2016" href="#footnote-Zhao2016">[Zhao2016]</a></sup>.</p><p><strong>Definition</strong></p><p>PMI is defined as for variables <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span> as</p><p class="math-container">\[PMI(X; Y | Z) = D(p(x, y, z) || p^{*}(x|z) p^{*}(y|z) p(z)),\]</p><p>where <span>$p(x, y, z)$</span> is the joint distribution for <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span>, and <span>$D(\cdot, \cdot)$</span> is the extended Kullback-Leibler divergence from <span>$p(x, y, z)$</span> to <span>$p^{*}(x|z) p^{*}(y|z) p(z)$</span>. See Zhao et al. (2016) for details.</p><p><strong>Estimation</strong></p><p>PMI can be estimated using any <a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> that implements <a href="../api/api_contingency_table/#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>. This allows estimation of 3D contingency matrices, from which relevant probabilities for the PMI formula are extracted. See also <a href="../api/api_pmi/#CausalityTools.pmi"><code>pmi</code></a>.</p><p><strong>Properties</strong></p><p>For the discrete case, the following identities hold in theory (when estimating PMI, they may not).</p><ul><li><code>PMI(X, Y, Z) &gt;= CMI(X, Y, Z)</code> (where CMI is the Shannon CMI). Holds in theory, but   when estimating PMI, the identity may not hold.</li><li><code>PMI(X, Y, Z) &gt;= 0</code>. Holds both in theory and for estimation using   <a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s.</li><li><code>X ⫫ Y | Z =&gt; PMI(X, Y, Z) = CMI(X, Y, Z) = 0</code> (in theory, but not necessarily for   estimation).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/pmi.jl#L4-L44">source</a></section></article><h3 id="Predictive-asymmetry"><a class="docs-heading-anchor" href="#Predictive-asymmetry">Predictive asymmetry</a><a id="Predictive-asymmetry-1"></a><a class="docs-heading-anchor-permalink" href="#Predictive-asymmetry" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.PA" href="#CausalityTools.PA"><code>CausalityTools.PA</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PA &lt;: CausalityTools.AssociationMeasure
PA(ηT = 1:5, τS = 1, τC = 1)</code></pre><p>The modified predictive asymmetry measure (Haaga et al., in revision).</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This is an experimental measure. It is part of an ongoing paper submission revision, but is provided here for convenience.</p></div></div><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   or conditional directional dependence.</li><li>Use with <a href="../api/api_predictive_asymmetry/#CausalityTools.asymmetry"><code>asymmetry</code></a> to compute the raw asymmetry distribution.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>ηT</code>. The prediction lags for the target variable.</li><li><code>τS</code>. The embedding delay(s) for the source variable.</li><li><code>τC</code>. The embedding delay(s) for the conditional variable(s).</li></ul><p>All parameters are given as a single integer or multiple integers greater than zero.</p><p><strong>Compatible estimators</strong></p><p><code>PA</code>/<a href="../api/api_predictive_asymmetry/#CausalityTools.asymmetry"><code>asymmetry</code></a> uses <a href="../api/api_condmutualinfo/#CausalityTools.condmutualinfo"><code>condmutualinfo</code></a> under the hood. Any estimator that can be used for <a href="../api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> can therefore, in principle, be used with the predictive asymmetry. We recommend to use <a href="../api/api_condmutualinfo/#CausalityTools.FPVP"><code>FPVP</code></a>, or one of the other dedicated conditional mutual information estimators.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Type</th><th style="text-align: right">Principle</th><th style="text-align: center">Pairwise</th><th style="text-align: center">Conditional</th></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right"><a href="../api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.Lord"><code>Lord</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right"><a href="../api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>GaussanMI</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Parametric</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>KSG1</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Continuous</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>KSG2</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Continuous</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Mixed</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></td><td style="text-align: right"><a href="../api/api_mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></td><td style="text-align: right">Continuous</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.FPVP"><code>FPVP</code></a></td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.MesnerShalizi"><code>MesnerShalizi</code></a></td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a></td><td style="text-align: right"><a href="../api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr></table><p><strong>Examples</strong></p><ul><li><a href="../examples/examples_predictive_asymmetry/#examples_pa_asymmetry_dist">Computing the asymmetry distribution</a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/4ef4cee2908d0cfdd7bc2e73fd6c7693d753ef55/src/methods/infomeasures/predictive_asymmetry/PA.jl#L7-L62">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Székely2007"><a class="tag is-link" href="#citeref-Székely2007">Székely2007</a>Székely, G. J., Rizzo, M. L., &amp; Bakirov, N. K. (2007). Measuring and testing dependence by correlation of distances. The annals of statistics, 35(6), 2769-2794.</li><li class="footnote" id="footnote-Székely2014"><a class="tag is-link" href="#citeref-Székely2014">Székely2014</a>Székely, G. J., &amp; Rizzo, M. L. (2014). Partial distance correlation with methods for dissimilarities.</li><li class="footnote" id="footnote-Székely2007"><a class="tag is-link" href="#citeref-Székely2007">Székely2007</a>Székely, G. J., Rizzo, M. L., &amp; Bakirov, N. K. (2007). Measuring and testing dependence by correlation of distances. The annals of statistics, 35(6), 2769-2794.</li><li class="footnote" id="footnote-Székely2014"><a class="tag is-link" href="#citeref-Székely2014">Székely2014</a>Székely, G. J., &amp; Rizzo, M. L. (2014). Partial distance correlation with methods for dissimilarities.</li><li class="footnote" id="footnote-Amigo2018"><a class="tag is-link" href="#citeref-Amigo2018">Amigo2018</a>Amigó, José M., and Yoshito Hirata. &quot;Detecting directional couplings from multivariate flows by the joint distance distribution.&quot; Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.</li><li class="footnote" id="footnote-Amigo2018"><a class="tag is-link" href="#citeref-Amigo2018">Amigo2018</a>Amigó, José M., and Yoshito Hirata. &quot;Detecting directional couplings from multivariate flows by the joint distance distribution.&quot; Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.</li><li class="footnote" id="footnote-Quiroga2000"><a class="tag is-link" href="#citeref-Quiroga2000">Quiroga2000</a>Quian Quiroga, R., Arnhold, J. &amp; Grassberger, P. [2000] “Learning driver-response relationships from synchronization patterns,” Phys. Rev. E61(5), 5142–5148.</li><li class="footnote" id="footnote-Grassberger1999"><a class="tag is-link" href="#citeref-Grassberger1999">Grassberger1999</a>Arnhold, J., Grassberger, P., Lehnertz, K., &amp; Elger, C. E. (1999). A robust method for detecting interdependences: application to intracranially recorded EEG. Physica D: Nonlinear Phenomena, 134(4), 419-430.</li><li class="footnote" id="footnote-Grassberger1999"><a class="tag is-link" href="#citeref-Grassberger1999">Grassberger1999</a>Arnhold, J., Grassberger, P., Lehnertz, K., &amp; Elger, C. E. (1999). A robust method for detecting interdependences: application to intracranially recorded EEG. Physica D: Nonlinear Phenomena, 134(4), 419-430.</li><li class="footnote" id="footnote-Andrzejak2003"><a class="tag is-link" href="#citeref-Andrzejak2003">Andrzejak2003</a>Andrzejak, R. G., Kraskov, A., Stögbauer, H., Mormann, F., &amp; Kreuz, T. (2003). Bivariate surrogate techniques: necessity, strengths, and caveats. Physical review E, 68(6), 066202.</li><li class="footnote" id="footnote-Chicharro2009"><a class="tag is-link" href="#citeref-Chicharro2009">Chicharro2009</a>Chicharro, D., &amp; Andrzejak, R. G. (2009). Reliable detection of directional couplings using rank statistics. Physical Review E, 80(2), 026217.</li><li class="footnote" id="footnote-Sugihara2012"><a class="tag is-link" href="#citeref-Sugihara2012">Sugihara2012</a>Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., &amp; Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.</li><li class="footnote" id="footnote-McCracken2014"><a class="tag is-link" href="#citeref-McCracken2014">McCracken2014</a>McCracken, J. M., &amp; Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E, 90(6), 062903.</li><li class="footnote" id="footnote-Romano2007"><a class="tag is-link" href="#citeref-Romano2007">Romano2007</a>Romano, M. C., Thiel, M., Kurths, J., &amp; Grebogi, C. (2007). Estimation of the direction of the coupling by conditional probabilities of recurrence. Physical Review E, 76(3), 036211.</li><li class="footnote" id="footnote-Ramos2017"><a class="tag is-link" href="#citeref-Ramos2017">Ramos2017</a>Ramos, A. M., Builes-Jaramillo, A., Poveda, G., Goswami, B., Macau, E. E., Kurths, J., &amp; Marwan, N. (2017). Recurrence measure of conditional dependence and applications. Physical Review E, 95(5), 052206.</li><li class="footnote" id="footnote-Furuichi2006"><a class="tag is-link" href="#citeref-Furuichi2006">Furuichi2006</a>Furuichi, S. (2006). Information theoretical properties of Tsallis entropies. Journal of Mathematical Physics, 47(2), 023302.</li><li class="footnote" id="footnote-Martin2004"><a class="tag is-link" href="#citeref-Martin2004">Martin2004</a>Martin, S., Morison, G., Nailon, W., &amp; Durrani, T. (2004). Fast and accurate image registration using Tsallis entropy and simultaneous perturbation stochastic approximation. Electronics Letters, 40(10), 1.</li><li class="footnote" id="footnote-Sarbu2014"><a class="tag is-link" href="#citeref-Sarbu2014">Sarbu2014</a>Sarbu, S. (2014, May). Rényi information transfer: Partial Rényi transfer entropy and partial Rényi mutual information. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5666-5670). IEEE.</li><li class="footnote" id="footnote-Jizba2012"><a class="tag is-link" href="#citeref-Jizba2012">Jizba2012</a>Jizba, P., Kleinert, H., &amp; Shefaat, M. (2012). Rényi&#39;s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.</li><li class="footnote" id="footnote-Jizba2012"><a class="tag is-link" href="#citeref-Jizba2012">Jizba2012</a>Jizba, P., Kleinert, H., &amp; Shefaat, M. (2012). Rényi’s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.</li><li class="footnote" id="footnote-Póczos2012"><a class="tag is-link" href="#citeref-Póczos2012">Póczos2012</a>Póczos, B., &amp; Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.</li><li class="footnote" id="footnote-Jizba2012"><a class="tag is-link" href="#citeref-Jizba2012">Jizba2012</a>Jizba, P., Kleinert, H., &amp; Shefaat, M. (2012). Rényi’s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.</li><li class="footnote" id="footnote-Zhao2016"><a class="tag is-link" href="#citeref-Zhao2016">Zhao2016</a>Zhao, J., Zhou, Y., Zhang, X., &amp; Chen, L. (2016). Part mutual information for quantifying direct associations in networks. Proceedings of the National Academy of Sciences, 113(18), 5130-5135.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Overview</a><a class="docs-footer-nextpage" href="../independence/">Independence testing »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Saturday 26 August 2023 18:26">Saturday 26 August 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
