<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Single-variable information API · Associations.jl</title><meta name="title" content="Single-variable information API · Associations.jl"/><meta property="og:title" content="Single-variable information API · Associations.jl"/><meta property="twitter:title" content="Single-variable information API · Associations.jl"/><meta name="description" content="Documentation for Associations.jl."/><meta property="og:description" content="Documentation for Associations.jl."/><meta property="twitter:description" content="Documentation for Associations.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Associations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Associations.jl</a></li><li><span class="tocitem">Core API reference</span><ul><li><a class="tocitem" href="../../associations/">Association measures</a></li><li><a class="tocitem" href="../../independence/">Independence</a></li><li><a class="tocitem" href="../../causal_graphs/">Network/graph inference</a></li></ul></li><li><span class="tocitem">Extended API reference</span><ul><li><a class="tocitem" href="../discretization_counts_probs_api/">Discretization API</a></li><li><a class="tocitem" href="../counts_and_probabilities_api/">Multivariate counts and probabilities API</a></li><li class="is-active"><a class="tocitem" href>Single-variable information API</a><ul class="internal"><li><a class="tocitem" href="#Single-variable-information-measures"><span>Single-variable information measures</span></a></li><li><a class="tocitem" href="#Discrete-information-estimators"><span>Discrete information estimators</span></a></li><li><a class="tocitem" href="#Differential-information-estimators"><span>Differential information estimators</span></a></li></ul></li><li><a class="tocitem" href="../information_multivariate_api/">Multivariate information API</a></li><li><a class="tocitem" href="../cross_map_api/">Cross-map API</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/examples_associations/">Associations</a></li><li><a class="tocitem" href="../../examples/examples_independence/">Independence testing</a></li><li><a class="tocitem" href="../../examples/examples_infer_graphs/">Causal graph inference</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Extended API reference</a></li><li class="is-active"><a href>Single-variable information API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Single-variable information API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Associations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Associations.jl/blob/main/docs/src/api/information_single_variable_api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Single-variable-information-API"><a class="docs-heading-anchor" href="#Single-variable-information-API">Single-variable information API</a><a id="Single-variable-information-API-1"></a><a class="docs-heading-anchor-permalink" href="#Single-variable-information-API" title="Permalink"></a></h1><p>Below we list some relevant functions types from <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl">ComplexityMeasures.jl</a> that  are used for the <a href="../information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> estimator.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.information" href="#ComplexityMeasures.information"><code>ComplexityMeasures.information</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">information([die::DiscreteInfoEstimator,] [est::ProbabilitiesEstimator,] o::OutcomeSpace, x) → h::Real
information(o::OutcomeSpace, x) → h::Real</code></pre><p>Estimate a discrete information measure from input data <code>x</code> using the provided <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> and <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> over the given <a href="../discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>.</p><p>As an alternative, you can provide an <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> for the first argument (<code>die</code>) which will default to <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimation) for the information estimation. You may also skip the first argument (<code>die</code>), in which case <code>Shannon()</code> will be used. You may also skip the second argument (<code>est</code>), which will default to the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> probabilities estimator. Note that some information measure estimators (e.g., <a href="#ComplexityMeasures.GeneralizedSchuermann"><code>GeneralizedSchuermann</code></a>) operate directly on counts and hence ignore <code>est</code>.</p><pre><code class="nohighlight hljs">information([e::DiscreteInfoEstimator,] p::Probabilities) → h::Real
information([e::DiscreteInfoEstimator,] c::Counts) → h::Real</code></pre><p>Like above, but estimate the information measure from the pre-computed <a href="../counts_and_probabilities_api/#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> <code>p</code> or <a href="../counts_and_probabilities_api/#ComplexityMeasures.Counts"><code>Counts</code></a>. Counts are converted into probabilities using <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a>, unless the estimator <code>e</code> uses counts directly.</p><p>See also: <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.information_maximum"><code>information_maximum</code></a>, <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.information_normalized"><code>information_normalized</code></a> for a normalized version.</p><p><strong>Examples (naive estimation)</strong></p><p>The simplest way to estimate a discrete measure is to provide the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> directly in combination with an <a href="../discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>. This will use the &quot;naive&quot; <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator for the measure, and the &quot;naive&quot; <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimator for the probabilities.</p><pre><code class="language-julia hljs">x = randn(100) # some input data
o = ValueBinning(RectangularBinning(5)) # a 5-bin histogram outcome space
h_s = information(Shannon(), o, x)</code></pre><p>Here are some more examples:</p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = information(ps) # gives 1, about 1 bit by definition (Shannon entropy by default)
h = information(Shannon(), ps) # syntactically equivalent to the above
h = information(Shannon(), UniqueElements(), x) # syntactically equivalent to above
h = information(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss
h = information(OrdinalPatterns(;m=3), x) # gives about 2, again by definition</code></pre><p><strong>Examples (bias-corrected estimation)</strong></p><p>It is known that both <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimation for information measures and <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimation for probabilities are biased. The scientific literature abounds with estimators that correct for this bias, both on the measure-estimation level and on the probability-estimation level. We thus provide the option to use any <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a> in combination with any <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> for improved estimates. Note that custom probabilites estimators will only work with counting-compatible <a href="../discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>.</p><pre><code class="language-julia hljs">x = randn(100)
o = ValueBinning(RectangularBinning(5))

# Estimate Shannon entropy estimation using various dedicated estimators
h_s = information(MillerMadow(Shannon()), RelativeAmount(), o, x)
h_s = information(HorvitzThompson(Shannon()), Shrinkage(), o, x)
h_s = information(Schuermann(Shannon()), Shrinkage(), o, x)

# Estimate information measures using the generic `Jackknife` estimator
h_r = information(Jackknife(Renyi()), Shrinkage(), o, x)
j_t = information(Jackknife(TsallisExtropy()), BayesianRegularization(), o, x)
j_r = information(Jackknife(RenyiExtropy()), RelativeAmount(), o, x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/core/information_functions.jl#L14-L92">source</a></section><section><div><pre><code class="language-julia hljs">information(est::DifferentialInfoEstimator, x) → h::Real</code></pre><p>Estimate a <strong>differential information measure</strong> using the provided <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> and input data <code>x</code>.</p><p><strong>Description</strong></p><p>The overwhelming majority of differential estimators estimate the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy. If the same estimator can estimate different information measures (e.g. it can estimate both <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> and <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>), then the information measure is provided as an argument to the estimator itself.</p><p>See the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#table_diff_ent_est">table of differential information measure estimators</a> in the docs for all differential information measure estimators.</p><p>Currently, unlike for the discrete information measures, this method doesn&#39;t involve explicitly first computing a probability density function and then passing this density to an information measure definition. But in the future, we want to establish a <code>density</code> API similar to the <a href="../counts_and_probabilities_api/#ComplexityMeasures.probabilities-Tuple{OutcomeSpace}"><code>probabilities</code></a> API.</p><p><strong>Examples</strong></p><p>To compute the differential version of a measure, give it as the first argument to a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> and pass it to <a href="#ComplexityMeasures.information"><code>information</code></a>.</p><pre><code class="language-julia hljs">x = randn(1000)
h_sh = information(Kraskov(Shannon()), x)
h_vc = information(Vasicek(Shannon()), x)</code></pre><p>A normal distribution has a base-e Shannon differential entropy of <code>0.5*log(2π) + 0.5</code> nats.</p><pre><code class="language-julia hljs">est = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.
h = information(est, randn(2_000_000))
abs(h - 0.5*log(2π) - 0.5) # ≈ 0.0001</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/core/information_functions.jl#L227-L267">source</a></section><section><div><pre><code class="language-julia hljs">information(est::MultivariateInformationMeasureEstimator, x...)</code></pre><p>Estimate some <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a> on input data <code>x...</code>, using the given <a href="../information_multivariate_api/#Associations.MultivariateInformationMeasureEstimator"><code>MultivariateInformationMeasureEstimator</code></a>.</p><p>This is just a convenience wrapper around <a href="../../associations/#Associations.association"><code>association</code></a><code>(est, x...)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/8d1c2e4b4ef2effb00552366b0dad586a8553bbe/src/methods/information/core.jl#L158-L165">source</a></section></article><h2 id="Single-variable-information-measures"><a class="docs-heading-anchor" href="#Single-variable-information-measures">Single-variable information measures</a><a id="Single-variable-information-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Single-variable-information-measures" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Shannon" href="#ComplexityMeasures.Shannon"><code>ComplexityMeasures.Shannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Shannon &lt;: InformationMeasure
Shannon(; base = 2)</code></pre><p>The Shannon (<a href="../../references/#Shannon1948">Shannon, 1948</a>) entropy, used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$\log$</span> at the given <code>base</code>.</p><p>The maximum value of the Shannon entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/shannon.jl#L3-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Renyi" href="#ComplexityMeasures.Renyi"><code>ComplexityMeasures.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: InformationMeasure
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi generalized order-<code>q</code> entropy (<a href="../../references/#Rényi1961">Rényi, 1961</a>), used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <a href="../../references/#Shannon1948">Shannon (1948)</a>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>The maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/renyi.jl#L3-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Tsallis" href="#ComplexityMeasures.Tsallis"><code>ComplexityMeasures.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: InformationMeasure
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis generalized order-<code>q</code> entropy (<a href="../../references/#Tsallis1988">Tsallis, 1988</a>), used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>The maximum value of the Tsallis entropy is <span>$k(L^{1 - q} - 1)/(1 - q)$</span>, with <span>$L$</span> the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/tsallis.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Kaniadakis" href="#ComplexityMeasures.Kaniadakis"><code>ComplexityMeasures.Kaniadakis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kaniadakis &lt;: InformationMeasure
Kaniadakis(; κ = 1.0, base = 2.0)</code></pre><p>The Kaniadakis entropy (<a href="../../references/#Tsallis2009">Tsallis, 2009</a>), used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute</p><p class="math-container">\[H_K(p) = -\sum_{i=1}^N p_i f_\kappa(p_i),\]</p><p class="math-container">\[f_\kappa (x) = \dfrac{x^\kappa - x^{-\kappa}}{2\kappa},\]</p><p>where if <span>$\kappa = 0$</span>, regular logarithm to the given <code>base</code> is used, and 0 probabilities are skipped.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/kaniadakis.jl#L3-L18">source</a></section></article><h2 id="Discrete-information-estimators"><a class="docs-heading-anchor" href="#Discrete-information-estimators">Discrete information estimators</a><a id="Discrete-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-information-estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.DiscreteInfoEstimator" href="#ComplexityMeasures.DiscreteInfoEstimator"><code>ComplexityMeasures.DiscreteInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DiscreteInfoEstimator</code></pre><p>The supertype of all discrete information measure estimators, which are used in combination with a <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> as input to  <a href="#ComplexityMeasures.information"><code>information</code></a> or related functions.</p><p>The first argument to a discrete estimator is always an <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> (defaults to <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>).</p><p><strong>Description</strong></p><p>A discrete <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a> is a functional of a probability mass function. To estimate such a measure from data, we must first estimate a probability mass function using a <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> from the (encoded/discretized) input data, and then apply the estimator to the estimated probabilities. For example, the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically computed using the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimator to compute probabilities, which are then given to the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator. Many other estimators exist, not only for <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy, but other information measures as well.</p><p>We provide a library of both generic estimators such as <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> or <a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a> (which can be applied to any measure), as well as dedicated estimators such as <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>, which computes <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy using the Miller-Madow bias correction. The list below gives a complete overview.</p><p><strong>Implementations</strong></p><p>The following estimators are generic and can compute any <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><ul><li><a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a>. The default, generic plug-in estimator of any information measure.   It computes the measure exactly as stated in the definition, using the computed   probability mass function.</li><li><a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a>. Uses the a combination of the plug-in estimator and the jackknife   principle to estimate the information measure.</li></ul><p><strong><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators</strong></p><p>The following estimators are dedicated <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators, which provide improvements over the naive <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator.</p><ul><li><a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>.</li><li><a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a>.</li><li><a href="#ComplexityMeasures.Schuermann"><code>Schuermann</code></a>.</li><li><a href="#ComplexityMeasures.GeneralizedSchuermann"><code>GeneralizedSchuermann</code></a>.</li><li><a href="#ComplexityMeasures.ChaoShen"><code>ChaoShen</code></a>.</li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Any of the implemented <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s can be used in combination with <em>any</em> <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> as input to <a href="#ComplexityMeasures.information"><code>information</code></a>. What this means is that every estimator actually comes in many different variants - one for each <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically calculated with <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> probabilities. But here, you can use for example the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.BayesianRegularization"><code>BayesianRegularization</code></a> or the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.Shrinkage"><code>Shrinkage</code></a> probabilities estimators instead, i.e. <code>information(MillerMadow(), RelativeAmount(outcome_space), x)</code> and <code>information(MillerMadow(), BayesianRegularization(outcomes_space), x)</code> are distinct estimators. This holds for all <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s. Many of these estimators haven&#39;t been explored in the literature before, so feel free to explore, and please cite this software if you use it to explore some new estimator combination!</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/core/information_measures.jl#L100-L160">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.PlugIn" href="#ComplexityMeasures.PlugIn"><code>ComplexityMeasures.PlugIn</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PlugIn(e::InformationMeasure) &lt;: DiscreteInfoEstimatorGeneric</code></pre><p>The <code>PlugIn</code> estimator is also called the empirical/naive/&quot;maximum likelihood&quot; estimator, and is used with <a href="#ComplexityMeasures.information"><code>information</code></a> to any discrete <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p>It computes any quantity exactly as given by its formula. When computing an information measure, which here is defined as a probabilities functional, it computes the quantity directly from a probability mass function, which is derived from maximum-likelihood (<a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a> estimates of the probabilities.</p><p><strong>Bias of plug-in estimates</strong></p><p>The plugin-estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy underestimates the true entropy, with a bias that grows with the number of distinct <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a> (Arora et al., 2022)(<a href="../../references/#Arora2022">Arora <em>et al.</em>, 2022</a>),</p><p class="math-container">\[bias(H_S^{plugin}) = -\dfrac{K-1}{2N} + o(N^-1).\]</p><p>where <code>K</code> is the number of distinct outcomes, and <code>N</code> is the sample size. Many authors have tried to remedy this by proposing alternative Shannon entropy estimators. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator is a simple correction to the plug-in estimator that adds back the bias term above. Many other estimators exist; see <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s for an overview.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/plugin.jl#L4-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.MillerMadow" href="#ComplexityMeasures.MillerMadow"><code>ComplexityMeasures.MillerMadow</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MillerMadow &lt;: DiscreteInfoEstimatorShannon
MillerMadow(measure::Shannon = Shannon())</code></pre><p>The <code>MillerMadow</code> estimator is used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../../references/#Miller1955">Miller (1955)</a>.</p><p><strong>Description</strong></p><p>The Miller-Madow estimator of Shannon entropy is given by</p><p class="math-container">\[H_S^{MM} = H_S^{plugin} + \dfrac{m - 1}{2N},\]</p><p>where <span>$H_S^{plugin}$</span> is the Shannon entropy estimated using the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, <code>m</code> is the number of bins with nonzero probability (as defined in <a href="../../references/#Paninski2003">Paninski (2003)</a>), and <code>N</code> is the number of observations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/miller_madow.jl#L3-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Schuermann" href="#ComplexityMeasures.Schuermann"><code>ComplexityMeasures.Schuermann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Schuermann &lt;: DiscreteInfoEstimatorShannon
Schuermann(definition::Shannon; a = 1.0)</code></pre><p>The <code>Schuermann</code> estimator is used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in <a href="../../references/#Schurmann2004">Schuermann (2004)</a>.</p><p>See detailed description for <a href="#ComplexityMeasures.GeneralizedSchuermann"><code>GeneralizedSchuermann</code></a> for details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/schurmann.jl#L6-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.GeneralizedSchuermann" href="#ComplexityMeasures.GeneralizedSchuermann"><code>ComplexityMeasures.GeneralizedSchuermann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GeneralizedSchuermann &lt;: DiscreteInfoEstimatorShannon
GeneralizedSchuermann(definition = Shannon(); a = 1.0)</code></pre><p>The <code>GeneralizedSchuermann</code> estimator is used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in <a href="../../references/#Grassberger2022">Grassberger (2022)</a>.</p><p>The &quot;generalized&quot; part of the name, as opposed to the <a href="../../references/#Schurmann2004">Schuermann (2004)</a> estimator (<a href="#ComplexityMeasures.Schuermann"><code>Schuermann</code></a>), is due to the possibility of picking difference parameters <span>$a_i$</span> for different outcomes. If different parameters are assigned to the different outcomes, <code>a</code> must be a vector of parameters of length <code>length(outcomes)</code>, where the outcomes are obtained using <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. See <a href="../../references/#Grassberger2022">Grassberger (2022)</a> for more information. If <code>a</code> is a real number, then <span>$a_i = a \forall i$</span>, and the estimator reduces to the <a href="#ComplexityMeasures.Schuermann"><code>Schuermann</code></a> estimator.</p><p><strong>Description</strong></p><p>For a set of <span>$N$</span> observations over <span>$M$</span> outcomes, the estimator is given by</p><p class="math-container">\[H_S^{opt} = \varphi(N) - \dfrac{1}{N} \sum_{i=1}^M n_i G_{n_i}(a_i),\]</p><p>where <span>$n_i$</span> is the observed frequency of the i-th outcome,</p><p class="math-container">\[G_n(a) = \varphi(n) + (-1)^n \int_0^a \dfrac{x^{n - 1}}{x + 1} dx,\]</p><p><span>$G_n(1) = G_n$</span> and <span>$G_n(0) = \varphi(n)$</span>, and</p><p class="math-container">\[G_n = \varphi(n) + (-1)^n \int_0^1 \dfrac{x^{n - 1}}{x + 1} dx.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/schurmann_generalized.jl#L3-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Jackknife" href="#ComplexityMeasures.Jackknife"><code>ComplexityMeasures.Jackknife</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Jackknife &lt;: DiscreteInfoEstimatorGeneric
Jackknife(definition::InformationMeasure = Shannon())</code></pre><p>The <code>Jackknife</code> estimator is used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute any discrete <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p>The <code>Jackknife</code> estimator uses the generic jackknife principle to reduce bias. <a href="../../references/#Zahl1977">Zahl (1977)</a> was the first to apply the jaccknife technique in the context of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimation. Here, we&#39;ve generalized his estimator to work with any <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/information_measures/#ComplexityMeasures.InformationMeasure"><code>InformationMeasure</code></a>.</p><p><strong>Description</strong></p><p>As an example of the jackknife technique, here is the formula for a jackknife estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy</p><p class="math-container">\[H_S^{J} = N H_S^{plugin} - \dfrac{N-1}{N} \sum_{i=1}^N {H_S^{plugin}}^{-\{i\}},\]</p><p>where <span>$N$</span> is the sample size, <span>$H_S^{plugin}$</span> is the plugin estimate of Shannon entropy, and <span>${H_S^{plugin}}^{-\{i\}}$</span> is the plugin estimate, but computed with the <span>$i$</span>-th sample left out.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/jackknife.jl#L3-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.HorvitzThompson" href="#ComplexityMeasures.HorvitzThompson"><code>ComplexityMeasures.HorvitzThompson</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HorvitzThompson &lt;: DiscreteInfoEstimatorShannon
HorvitzThompson(measure::Shannon = Shannon())</code></pre><p>The <code>HorvitzThompson</code> estimator is used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../../references/#Horvitz1952">Horvitz and Thompson (1952)</a>.</p><p><strong>Description</strong></p><p>The Horvitz-Thompson estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is given by</p><p class="math-container">\[H_S^{HT} = -\sum_{i=1}^M \dfrac{p_i \log(p_i) }{1 - (1 - p_i)^N},\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. Given the true probability <span>$p_i$</span> of the <span>$i$</span>-th outcome, <span>$1 - (1 - p_i)^N$</span> is the probability that the outcome appears at least once in a sample of size <span>$N$</span> (<a href="../../references/#Arora2022">Arora <em>et al.</em>, 2022</a>). Dividing by this inclusion probability is a form of weighting, and compensates for situations where certain outcomes have so low probabilities that they are not often observed in a sample, for example in power-law distributions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/horvitz_thompson.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.ChaoShen" href="#ComplexityMeasures.ChaoShen"><code>ComplexityMeasures.ChaoShen</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChaoShen &lt;: DiscreteInfoEstimatorShannon
ChaoShen(definition::Shannon = Shannon())</code></pre><p>The <code>ChaoShen</code> estimator is used with <a href="#ComplexityMeasures.information"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../../references/#Chao2003">Chao and Shen (2003)</a>.</p><p><strong>Description</strong></p><p>This estimator is a modification of the <a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a> estimator that multiplies each plugin probability estimate by an estimate of sample coverage. If <span>$f_1$</span> is the number of singletons (outcomes that occur only once) in a sample of length <span>$N$</span>, then the sample coverage is <span>$C = 1 - \dfrac{f_1}{N}$</span>. The Chao-Shen estimator of Shannon entropy is then</p><p class="math-container">\[H_S^{CS} = -\sum_{i=1}^M \left( \dfrac{C p_i \log(C p_i)}{1 - (1 - C p_i)^N} \right),\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.outcomes"><code>outcomes</code></a>. If <span>$f_1 = N$</span>, then <span>$f_1$</span> is set to <span>$f_1 = N - 1$</span> to ensure positive entropy (<a href="../../references/#Arora2022">Arora <em>et al.</em>, 2022</a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/chao_shen.jl#L3-L25">source</a></section></article><h2 id="Differential-information-estimators"><a class="docs-heading-anchor" href="#Differential-information-estimators">Differential information estimators</a><a id="Differential-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-information-estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.DifferentialInfoEstimator" href="#ComplexityMeasures.DifferentialInfoEstimator"><code>ComplexityMeasures.DifferentialInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferentialInfoEstimator</code></pre><p>The supertype of all differential information measure estimators. These estimators compute an information measure in various ways that do not involve explicitly estimating a probability distribution.</p><p>Each <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>See <a href="#ComplexityMeasures.information"><code>information</code></a> for usage.</p><p><strong>Implementations</strong></p><ul><li><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</li><li><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>.</li><li><a href="#ComplexityMeasures.Goria"><code>Goria</code></a>.</li><li><a href="#ComplexityMeasures.Gao"><code>Gao</code></a>.</li><li><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></li><li><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a>.</li><li><a href="#ComplexityMeasures.Lord"><code>Lord</code></a>.</li><li><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>.</li><li><a href="#ComplexityMeasures.Correa"><code>Correa</code></a>.</li><li><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>.</li><li><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>.</li><li><a href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/core/information_measures.jl#L167-L194">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Kraskov" href="#ComplexityMeasures.Kraskov"><code>ComplexityMeasures.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: DifferentialInfoEstimator
Kraskov(definition = Shannon(); k::Int = 1, w::Int = 0)</code></pre><p>The <code>Kraskov</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a multi-dimensional <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> using the <code>k</code>-th nearest neighbor searches method from <a href="../../references/#Kraskov2004">Kraskov <em>et al.</em> (2004)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Kraskov</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Kraskov.jl#L3-L29">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.KozachenkoLeonenko" href="#ComplexityMeasures.KozachenkoLeonenko"><code>ComplexityMeasures.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: DifferentialInfoEstimator
KozachenkoLeonenko(definition = Shannon(); w::Int = 0)</code></pre><p>The <code>KozachenkoLeonenko</code> estimator (<a href="../../references/#KozachenkoLeonenko1987">Kozachenko and Leonenko, 1987</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a multi-dimensional <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>using the nearest neighbor method from <a href="../../references/#KozachenkoLeonenko1987">Kozachenko and Leonenko (1987)</a>, as described in <a href="../../references/#Charzyńska2015">Charzyńska and Gambin (2016)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/KozachenkoLeonenko.jl#L3-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Zhu" href="#ComplexityMeasures.Zhu"><code>ComplexityMeasures.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: DifferentialInfoEstimator
Zhu(; definition = Shannon(), k = 1, w = 0)</code></pre><p>The <code>Zhu</code> estimator (<a href="../../references/#Zhu2015">Zhu <em>et al.</em>, 2015</a>) is an extension to <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, and computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a multi-dimensional <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Zhu</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>by approximating densities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Zhu.jl#L3-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.ZhuSingh" href="#ComplexityMeasures.ZhuSingh"><code>ComplexityMeasures.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: DifferentialInfoEstimator
ZhuSingh(definition = Shannon(); k = 1, w = 0)</code></pre><p>The <code>ZhuSingh</code> estimator (<a href="../../references/#Zhu2015">Zhu <em>et al.</em>, 2015</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a multi-dimensional <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>ZhuSingh</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Like <a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in <a href="../../references/#Singh2003">Singh <em>et al.</em> (2003)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/ZhuSingh.jl#L8-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Gao" href="#ComplexityMeasures.Gao"><code>ComplexityMeasures.Gao</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Gao &lt;: DifferentialInfoEstimator
Gao(definition = Shannon(); k = 1, w = 0, corrected = true)</code></pre><p>The <code>Gao</code> estimator (<a href="../../references/#Gao2015">Gao <em>et al.</em>, 09–12 May 2015</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a>, using a <code>k</code>-th nearest-neighbor approach based on <a href="../../references/#Singh2003">Singh <em>et al.</em> (2003)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><a href="../../references/#Gao2015">Gao <em>et al.</em> (09–12 May 2015)</a> give two variants of this estimator. If <code>corrected == false</code>, then the uncorrected version is used. If <code>corrected == true</code>, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Gao.jl#L8-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Goria" href="#ComplexityMeasures.Goria"><code>ComplexityMeasures.Goria</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Goria &lt;: DifferentialInfoEstimator
Goria(measure = Shannon(); k = 1, w = 0)</code></pre><p>The <code>Goria</code> estimator (<a href="../../references/#Goria2005">Goria <em>et al.</em>, 2005</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a multi-dimensional <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Goria</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Specifically, let <span>$\bf{n}_1, \bf{n}_2, \ldots, \bf{n}_N$</span> be the distance of the samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> to their <code>k</code>-th nearest neighbors. Next, let the geometric mean of the distances be</p><p class="math-container">\[\hat{\rho}_k = \left( \prod_{i=1}^N \right)^{\dfrac{1}{N}}\]</p><p><a href="../../references/#Goria2005">Goria <em>et al.</em> (2005)</a>&#39;s estimate of Shannon differential entropy is then</p><p class="math-container">\[\hat{H} = m\hat{\rho}_k + \log(N - 1) - \psi(k) + \log c_1(m),\]</p><p>where <span>$c_1(m) = \dfrac{2\pi^\frac{m}{2}}{m \Gamma(m/2)}$</span> and <span>$\psi$</span> is the digamma function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Goria.jl#L8-L43">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Lord" href="#ComplexityMeasures.Lord"><code>ComplexityMeasures.Lord</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lord &lt;: DifferentialInfoEstimator
Lord(measure = Shannon(); k = 10, w = 0)</code></pre><p>The <code>Lord</code> estimator (<a href="../../references/#Lord2018">Lord <em>et al.</em>, 2018</a>) estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> using a nearest neighbor approach with a local nonuniformity correction (LNC), with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function <span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Lord</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))],\]</p><p>by using the resubstitution formula</p><p class="math-container">\[\hat{\bar{X}, k} = -\mathbb{E}[\log(f(X))]
\approx \sum_{i = 1}^N \log(\hat{f}(\bf{x}_i)),\]</p><p>where <span>$\hat{f}(\bf{x}_i)$</span> is an estimate of the density at <span>$\bf{x}_i$</span> constructed in a manner such that <span>$\hat{f}(\bf{x}_i) \propto \dfrac{k(x_i) / N}{V_i}$</span>, where <span>$k(x_i)$</span> is the number of points in the neighborhood of <span>$\bf{x}_i$</span>, and <span>$V_i$</span> is the volume of that neighborhood.</p><p>While most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities <span>$\hat{f}(\bf{x}_i)$</span>, the <code>Lord</code> estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point <code>xᵢ</code>, estimated using singular value decomposition (SVD) on the <code>k</code>-th nearest neighbors of <code>xᵢ</code>. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes <code>Lord</code> a well-suited entropy estimator for a wide range of systems.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Lord.jl#L25-L67">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.LeonenkoProzantoSavani" href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>ComplexityMeasures.LeonenkoProzantoSavani</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LeonenkoProzantoSavani &lt;: DifferentialInfoEstimator
LeonenkoProzantoSavani(definition = Shannon(); k = 1, w = 0)</code></pre><p>The <code>LeonenkoProzantoSavani</code> estimator (<a href="../../references/#LeonenkoProzantoSavani2008">Leonenko <em>et al.</em>, 2008</a>) computes the  <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>, <a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>, or <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a multi-dimensional <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>The estimator uses <code>k</code>-th nearest-neighbor searches.  <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>For details, see <a href="../../references/#LeonenkoProzantoSavani2008">Leonenko <em>et al.</em> (2008)</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/LeonenkoProzantoSavani.jl#L7-L23">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Vasicek" href="#ComplexityMeasures.Vasicek"><code>ComplexityMeasures.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: DifferentialInfoEstimator
Vasicek(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Vasicek</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a timeseries using the method from <a href="../../references/#Vasicek1976">Vasicek (1976)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Vasicek</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>, of which <a href="../../references/#Vasicek1976">Vasicek (1976)</a> was the first. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Vasicek</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Vasicek</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_V(\bar{X}, m) =
\dfrac{1}{n}
\sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/Vasicek.jl#L3-L53">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.AlizadehArghami" href="#ComplexityMeasures.AlizadehArghami"><code>ComplexityMeasures.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AlizadehArghami &lt;: DifferentialInfoEstimator
AlizadehArghami(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>AlizadehArghami</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a timeseries using the method from <a href="../../references/#Alizadeh2010">Alizadeh and Arghami (2010)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>AlizadehArghami</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>AlizadehArghami</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>:</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp.\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>AlizadehArghami</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then the the <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a> estimate <span>$\hat{H}_{V}(\bar{X}, m, n)$</span>, plus a correction factor</p><p class="math-container">\[\hat{H}_{A}(\bar{X}, m, n) = \hat{H}_{V}(\bar{X}, m, n) +
\dfrac{2}{n}\left(m \log(2) \right).\]</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/AlizadehArghami.jl#L3-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Ebrahimi" href="#ComplexityMeasures.Ebrahimi"><code>ComplexityMeasures.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: DifferentialInfoEstimator
Ebrahimi(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Ebrahimi</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> <a href="#ComplexityMeasures.information"><code>information</code></a> of a timeseries using the method from <a href="../../references/#Ebrahimi1994">Ebrahimi <em>et al.</em> (1994)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Ebrahimi</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Ebrahimi</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Ebrahimi</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_{E}(\bar{X}, m) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{n}{c_i m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \\
    2,                    &amp; m + 1 \geq i \geq n - m \\
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/Ebrahimi.jl#L3-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Correa" href="#ComplexityMeasures.Correa"><code>ComplexityMeasures.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: DifferentialInfoEstimator
Correa(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Correa</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.information"><code>information</code></a> of a timeseries using the method from <a href="../../references/#Correa1995">Correa (1995)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Correa</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Correa</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, <code>Correa</code> makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>, ensuring that end points are included. The <code>Correa</code> estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy is then</p><p class="math-container">\[H_C(\bar{X}, m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(\bar{X}_{(j)} -
\tilde{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (\bar{X}_{(j)} - \tilde{X}_{(i)})^2}
\right],\]</p><p>where</p><p class="math-container">\[\tilde{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p><p>See also: <a href="#ComplexityMeasures.information"><code>information</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/Correa.jl#L3-L55">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../counts_and_probabilities_api/">« Multivariate counts and probabilities API</a><a class="docs-footer-nextpage" href="../information_multivariate_api/">Multivariate information API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 1 August 2024 09:15">Thursday 1 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
