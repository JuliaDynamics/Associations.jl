<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Empirical dynamical modelling ¬∑ CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CausalityTools.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../surrogate/">Surrogate data</a></li><li><span class="tocitem">Distance based</span><ul><li><a class="tocitem" href="../joint_distance_distribution/">Joint distance distribution</a></li><li><a class="tocitem" href="../s_measure/">S-measure</a></li><li><a class="tocitem" href="../cross_mapping/">Cross mapping</a></li><li><a class="tocitem" href="../pairwise_asymmetric_inference/">Pairwise asymmetric inference</a></li></ul></li><li><span class="tocitem">Information/entropy based</span><ul><li><a class="tocitem" href="../mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../TransferEntropy/">Transfer entropy</a></li><li><a class="tocitem" href="../predictive_asymmetry/">Predictive asymmetry</a></li><li><a class="tocitem" href="../generalized_entropy/">Generalized entropy</a></li><li><a class="tocitem" href="../info_estimators/">Estimators</a></li></ul></li><li class="is-active"><a class="tocitem" href>Empirical dynamical modelling</a><ul class="internal"><li><a class="tocitem" href="#Simplex-projection"><span>Simplex projection</span></a></li><li><a class="tocitem" href="#S-map"><span>S-map</span></a></li></ul></li><li><a class="tocitem" href="../example_systems/">Example systems</a></li><li><span class="tocitem">Utilities</span><ul><li><a class="tocitem" href="../invariant_measure/">Invariant measures and transfer operators</a></li><li><a class="tocitem" href="../dataset/">Multivariate <code>Dataset</code>s</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Empirical dynamical modelling</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Empirical dynamical modelling</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/edm.md" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Empirical-dynamical-modelling"><a class="docs-heading-anchor" href="#Empirical-dynamical-modelling">Empirical dynamical modelling</a><a id="Empirical-dynamical-modelling-1"></a><a class="docs-heading-anchor-permalink" href="#Empirical-dynamical-modelling" title="Permalink"></a></h1><h2 id="Simplex-projection"><a class="docs-heading-anchor" href="#Simplex-projection">Simplex projection</a><a id="Simplex-projection-1"></a><a class="docs-heading-anchor-permalink" href="#Simplex-projection" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.EmpiricalDynamicalModelling.simplex_predictions" href="#CausalityTools.EmpiricalDynamicalModelling.simplex_predictions"><code>CausalityTools.EmpiricalDynamicalModelling.simplex_predictions</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">simplex_predictions(x, k, [, correspondence_measure]; 
    œÑ = 1, d = 2, p = 1, 
    training = 1:length(x) √∑ 2, prediction = length(x) √∑ 2 + 1:length(x)) ‚Üí Vector{Float64}, Vector{Float64}</code></pre><p>From an embedding <code>M‚Çì</code> computed from the time series <code>x</code>, first compute the  the <code>k</code>-step forward simplex projections for each predictee in <code>M‚Çì[prediction]</code>. This is done by locating, for each predictee, its <code>d+1</code> nearest neighbors in  <code>M‚Çì[training[1:end-p]]</code>, and projecting each neighbor <code>p</code> steps forward in time.  Then, for each scalar value <code>x[i]</code> where <code>i ‚àà prediction</code>, compute the prediction  <code>xÃÉ[i]</code> as an exponentially weighted average of the forward-projected neighbors  (there are <code>d+1</code> neighbors, so these form a simplex around the predictee).</p><p>If no third argument is given, then two vectors are returned: the observed values and  the predicted values. If a <code>correspondence_measure</code> is given as the third argument,  then return the correspondence between observed and predicted values.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">L = 2000
xs = 0.0:0.05:L
ts = sin.(xs) .+ rand(xs) ./ 0.2

# Compute the 3-step forward in time predictions and compute the correlation between
# observed and predicted values.
simplex_predictions(x, 1, Statistics.cor)</code></pre><p><strong>Details</strong></p><p>This is an implementation of the simplex projection method from Sugihara and May (1990)<sup class="footnote-reference"><a id="citeref-Sugihara1990" href="#footnote-Sugihara1990">[Sugihara1990]</a></sup>. The original  paper doesn not provide sufficiently detailed pseudocode for implementation, so the algorithm here  is based on Ye et al. (2015)<sup class="footnote-reference"><a id="citeref-Ye2015" href="#footnote-Ye2015">[Ye2015]</a></sup>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/81975d4c51df5d20e2558c1b62ed4861d0a076f1/src/EmpiricalDynamicalModelling/simplex_projection.jl#L5-L42">source</a></section></article><h3 id="Example:-reproducing-Sugihara-and-May-(1990)"><a class="docs-heading-anchor" href="#Example:-reproducing-Sugihara-and-May-(1990)">Example: reproducing Sugihara &amp; May (1990)</a><a id="Example:-reproducing-Sugihara-and-May-(1990)-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-reproducing-Sugihara-and-May-(1990)" title="Permalink"></a></h3><p>The simplex projection method was introduced in Sugihara &amp; May (1990). Let&#39;s try to reproduce their figure 1.</p><p>We start by defining the tent map and generating a time series for <code>Œº = 1.98</code>.</p><pre><code class="language-julia hljs">using CausalityTools, DynamicalSystems, Plots, Statistics, Distributions; gr()

function eom_tentmap(dx, x, p, n)
    x = x[1]
    Œº = p[1]
    dx[1] = x &lt; 0.5 ? Œº*x : Œº*(1 - x)

    return
end

function tentmap(u‚ÇÄ = rand(); Œº = 1.98)
    DiscreteDynamicalSystem(eom_tentmap, [u‚ÇÄ], [Œº])
end

npts = 2000
sys = tentmap(Œº = 1.98)
ts = diff(trajectory(sys, npts , Ttr = 1000)[:, 1])

p_ts = plot(xlabel = &quot;Time step&quot;, ylabel = &quot;Value&quot;, ylims = (-1.1, 1.1))
plot!(ts[1:100], label = &quot;&quot;)</code></pre><p><img src="../simplex_ts.svg" alt/></p><p>Next, let&#39;s compute the predicted and observed values for <code>k = 1</code> and <code>k = 7</code> using embedding dimension 3 and  embedding lag 1. We&#39;ll use the first 1000 points as our training set, and try to predict the next 500 points. </p><pre><code class="language-julia hljs">d, œÑ = 3, 1
training = 1:1000
prediction = 1001:1500
x_1, xÃÉ_1 = simplex_predictions(ts, 1, d = d, œÑ = œÑ, training = training, prediction = prediction)
x_7, xÃÉ_7 = simplex_predictions(ts, 7, d = d, œÑ = œÑ, training = training, prediction = prediction)

p_obs_vs_pred = plot(xlabel = &quot;Observed values&quot;, ylabel = &quot;Predicted values&quot;)
scatter!(x_1, xÃÉ_1, label = &quot;k = 1&quot;, shape = :circle)
scatter!(x_7, xÃÉ_7, label = &quot;k = 7&quot;, shape = :star5)</code></pre><p><img src="../simplex_correspondence.svg" alt/></p><p>There is high correlation between observed and predicted values when predicting only one time step (<code>k = 1</code>) into the future. As <code>k</code> increases, the performance drops off. Let&#39;s investigate this systematically.</p><pre><code class="language-julia hljs">kmax = 20
cors = zeros(kmax)
for k = 1:kmax
    X, XÃÉ = simplex_predictions(ts, k, d = d, œÑ = œÑ, training = training, prediction = prediction)
    cors[k] = cor(X, XÃÉ)
end

plot(legend = :bottomleft, ylabel = &quot;Correlation coefficient (œÅ)&quot;,
    xlabel = &quot;Prediction time (k)&quot;,
    ylims = (-1.1, 1.1))
hline!([0], ls = :dash, label = &quot;&quot;, color = :grey)
scatter!(1:kmax, cors, label = &quot;&quot;)</code></pre><p><img src="../simplex_correlation.svg" alt/></p><p>The correlation between observed and predicted values is near perfect until <code>k = 3</code>, and then rapidly  drops off as <code>k</code> increases. At <code>k = 8</code>, there is virtually no correlation between observed and predicted values. This means that, for this particular system, for this particular choice of embedding and choice of training/prediction sets, the predictability of the system is limited to about 4 or 5 time steps into the future (if you want good predictions). </p><p>The main point of Sugihara &amp; May&#39;s paper was that this drop-off of prediction accuracy with <code>k</code> is characteristic of chaotic systems, and can be used to distinguish chaos from regular behaviour in time series.</p><p>Let&#39;s demonstrate this by also investigating how the correlation between observed and predicted values behaves as a function of <code>k</code> for a regular, non-chaotic time series. We&#39;ll use a sine wave with additive noise.</p><pre><code class="language-julia hljs">ùí© = Uniform(-0.5, 0.5)
xs = 0.0:1.0:2000.0
r = sin.(0.5 .* xs) .+ rand(ùí©, length(xs))
plot(r[1:200])

cors_sine = zeros(kmax)
for k = 1:kmax
    X, XÃÉ = simplex_predictions(r, k, d = d, œÑ = œÑ, training = training, prediction = prediction)
    cors_sine[k] = cor(X, XÃÉ)
end

plot(legend = :bottomleft, ylabel = &quot;Correlation coefficient (œÅ)&quot;,
    xlabel = &quot;Prediction time (k)&quot;,
    ylims = (-1.1, 1.1))
hline!([0], ls = :dash, label = &quot;&quot;, color = :grey)
scatter!(1:kmax, cors, label = &quot;tent map&quot;, marker = :star)
scatter!(1:kmax, cors_sine, label = &quot;sine&quot;)</code></pre><p><img src="../simplex_correlation_sine_tent.svg" alt/></p><p>In contrast to the tent map, for which prediction accuracy drops off and stabilizes around zero for increasing <code>k</code>, the prediction accuracy is rather insensitive to the choice of <code>k</code> for the noisy sine time series. </p><h3 id="Example:-determining-optimal-embedding-dimension"><a class="docs-heading-anchor" href="#Example:-determining-optimal-embedding-dimension">Example: determining optimal embedding dimension</a><a id="Example:-determining-optimal-embedding-dimension-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-determining-optimal-embedding-dimension" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.EmpiricalDynamicalModelling.delay_simplex" href="#CausalityTools.EmpiricalDynamicalModelling.delay_simplex"><code>CausalityTools.EmpiricalDynamicalModelling.delay_simplex</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">delay_simplex(x, œÑ; ds = 2:10, ks = 1:10) ‚Üí œÅs</code></pre><p>Determine the optimal embedding dimension for <code>x</code> based on the simplex projection algorithm  from Sugihara &amp; May (1990)<sup class="footnote-reference"><a id="citeref-Sugihara1990" href="#footnote-Sugihara1990">[Sugihara1990]</a></sup>. </p><p>For each <code>d ‚àà ds</code>, we compute the correlation between observed and predicted values  for different prediction times <code>ks</code>, and average the correlation coefficients. The  embedding dimension for which the average correlation is highest is taken as the optimal  dimension. The embedding delay <code>œÑ</code> is given as a positive number.</p><p>Returns the prediction skills <code>œÅs</code> - one <code>œÅ</code> for each <code>d ‚àà ds</code>.</p><p>Note: the library/training and prediction sets are automatically taken as the first and  second halves of the data, respectively. This convenience method does not allow tuning  the libraries further.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/81975d4c51df5d20e2558c1b62ed4861d0a076f1/src/EmpiricalDynamicalModelling/delay_simplexprojection.jl#L5-L23">source</a></section></article><p>The simplex projection method can also be used to determine the optimal embedding dimension for a time series. Given an embedding lag <code>œÑ</code>, we can embed a time series <code>x</code> for a range of embedding dimensions <code>d ‚àà 2:dmax</code> and compute the average prediction power over multiple <code>ks</code> using the simplex projection method.</p><p>Here, we compute the average prediction skills from <code>k=1</code> up to <code>k=10</code> time steps into the future, for  embedding dimensions <code>d = 2:10</code>.</p><pre><code class="language-julia hljs">using CausalityTools, DynamicalSystems, Plots; gr()

sys = CausalityTools.ExampleSystems.lorenz_lorenz_bidir()
T, Œît = 150, 0.05
lorenz = trajectory(sys, T, Œît = Œît, Ttr = 100)[:, 1:3]
x1, x2, x3 = columns(lorenz)

# Determine the optimal embedding delay
œÑ = estimate_delay(x1, &quot;ac_zero&quot;)

# Compute average prediction skill for
ds, ks = 2:10, 1:10
œÅs = delay_simplex(x1, œÑ, ds = ds, ks = ks)

plot(xlabel = &quot;Embedding dimension&quot;, ylabel = &quot;œÅÃÑ(observed, predicted&quot;)
plot!(ds, œÅs, label = &quot;&quot;, c = :black, marker = :star)</code></pre><p><img src="../simplex_embedding.svg" alt/></p><p>Based on the predictability criterion, the optimal embedding dimension, for this particular realization of the first variable of the Lorenz system, seems to be 2.</p><h2 id="S-map"><a class="docs-heading-anchor" href="#S-map">S-map</a><a id="S-map-1"></a><a class="docs-heading-anchor-permalink" href="#S-map" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.EmpiricalDynamicalModelling.smap" href="#CausalityTools.EmpiricalDynamicalModelling.smap"><code>CausalityTools.EmpiricalDynamicalModelling.smap</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">smap(X::Dataset; Œ∏ = 1.0, k = 1, 
    training = 1:length(x) √∑ 2 - k, 
    predictees = length(x) √∑ 2 + 1:length(x)) ‚Üí Vector{Float64}, Vector{Float64}</code></pre><p>Sequential locally weighted global linear map (S-map; Sugihara, 1994)<sup class="footnote-reference"><a id="citeref-Sugihara1994" href="#footnote-Sugihara1994">[Sugihara1994]</a></sup>.</p><p>For each predictee <span>$x_i ‚àà X_{pred}$</span><code>, the algorithm uses points in the library/training set</code><code>X_{train} \setminus x_i</code><code>to fit a weighted linear model for predicting</code><code>x_{i+k}</code><code>(x·µ¢ projected</code>k` time steps into the future).</p><p>Returns two scalar vectors: <code>xÃÇs = [xÃÇ‚ÇÅ‚Çä‚Çñ, xÃÇ‚ÇÇ‚Çä‚Çñ, ..., xÃÇ‚Çô+k]</code>, which are the predicted values, and  <code>xÃÇs_truths = [x‚ÇÅ‚Çä‚Çñ, x‚ÇÇ‚Çä‚Çñ, ..., x‚Çô+k]</code>, which are the actual values <span>$x_{i+k} \in X_{pred}$</span>  for <span>$i = 1, \ldots, n$</span>, where <span>$n$</span> is the number of predictees.</p><p><strong>Input data</strong></p><p>The algorithm approximates a global map based on <em>embedding points</em> and predicts scalar values  in the <em>first</em> column of <code>X</code>. If your input data is a scalar time series, it must therefore  first be embedded using <code>DynamicalSystems.genembed</code> first.</p><p><strong>Training and prediction sets</strong></p><p>By default, the first half of the points of <code>x</code> are used as the library set  (<code>Xtrain = x[1:ntrain-k]</code>), and the remaining half (<code>Xpred = x[ntrain+1:end]</code>) is assigned  to the prediction set. Overlapping index ranges are not possible as of yet.</p><p>When <code>Œ∏ = 0.0</code>, all weights are identical, and the A reduces to a linear autoregressive A.  Nonlinearity is introduced when <code>Œ∏ &gt; 0</code>, so tuning this parameters can be used to distinguish  nonlinear dynamical systems from linear stochastic systems.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/81975d4c51df5d20e2558c1b62ed4861d0a076f1/src/EmpiricalDynamicalModelling/smap.jl#L5-L37">source</a></section></article><p>The s-map, or sequential locally weighted global map, was introduced in Sugihara (1994)<sup class="footnote-reference"><a id="citeref-Sugihara1994" href="#footnote-Sugihara1994">[Sugihara1994]</a></sup>. The s-map approximates the dynamics of a system as a locally weighted global map, with a tuning parameter <span>$\theta$</span> that controls the degree of nonlinearity in the model. For <span>$\theta = 0$</span>, the model is the maximum likelihood global linear solution (of eq. 2 in Sugihara, 1994), and for increasing <span>$\theta &gt; 0$</span>, the model becomes increasingly nonlinear and localized (Sugihara, 1996)<sup class="footnote-reference"><a id="citeref-Sugihara1996" href="#footnote-Sugihara1996">[Sugihara1996]</a></sup>.</p><p>When such a model has been constructed, it be used as prediction tool for out-of-sample points, and can be used to characterize nonlinearity in a time series (Sugihara, 1994).  Let&#39;s demonstrate with an example.</p><h3 id="Example:-prediction-power-for-the-Lorenz-system"><a class="docs-heading-anchor" href="#Example:-prediction-power-for-the-Lorenz-system">Example: prediction power for the Lorenz system</a><a id="Example:-prediction-power-for-the-Lorenz-system-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-prediction-power-for-the-Lorenz-system" title="Permalink"></a></h3><p>In our implementation of <code>smap</code>, the input is a multivariate dataset - which can be a <code>Dataset</code> of either the raw variables of a multivariate dynamical system, or a <code>Dataset</code> containing an embedding of a single time series.  Here, we&#39;ll show an example of the former.</p><p>Let&#39;s generate an example orbit from a bidirectionally coupled set of Lorenz systems. We&#39;ll use the built-in  <code>CausalityTools.ExampleSystems.lorenz_lorenz_bidir</code> system, and select the first three variables for analysis.</p><pre><code class="language-julia hljs">using CausalityTools, Plots, DynamicalSystems, Statistics; gr()

sys = CausalityTools.ExampleSystems.lorenz_lorenz_bidir()
T, Œît = 150, 0.05
lorenz = trajectory(sys, T, Œît = Œît, Ttr = 100)[:, 1:3]
p_orbit = plot(xlabel = &quot;x&quot;, ylabel = &quot;y&quot;, zlabel = &quot;z&quot;)
plot!(columns(lorenz)..., marker = :circle, label = &quot;&quot;, ms = 2, lŒ± = 0.5)</code></pre><p><img src="../lorenz_attractor.svg" alt/></p><p>Now, we compute the <code>k</code>-step forward predictions for <code>k</code> ranging from <code>1</code> to <code>15</code>. The tuning parameter <code>Œ∏</code>  varies from <code>0.0</code> (linear model) to <code>2.0</code> (strongly nonlinear model). Our goal is to see which model  yields the best predictions across multiple <code>k</code>.</p><p>We&#39;ll use the first 500 points of the orbit to train the model. Then, using that model, we try to predict the next 500 points (which are not part of the training set).  Finally, we compute the correlation between the predicted values and the observed values, which measures the model prediction skill. This procedure is repeated for each combination of <code>k</code> and <code>Œ∏</code>.</p><pre><code class="language-julia hljs">ks, Œ∏s = 1:15, 0.0:0.5:2.0
n_trainees, n_predictees = 500, 500;

# Compute correlations between predicted values `preds` and actual values `truths`
# for all parameter combinations
cors = zeros(length(ks), length(Œ∏s))
for (i, k) in enumerate(ks)
    for (j, Œ∏) in enumerate(Œ∏s)
        pred = n_trainees+1:n_trainees+n_predictees
        train = 1:n_trainees

        preds, truths = smap(lorenz, Œ∏ = Œ∏, k = k, trainees = train, predictees = pred)
        cors[i, j] = cor(preds, truths)
    end
end

p_Œ∏_k_sensitivity = plot(xlabel = &quot;Prediction time (k)&quot;, ylabel = &quot;cor(observed, predicted)&quot;,
    legend = :bottomleft, ylims = (-1.1, 1.1))
hline!([0], ls = :dash, c = :grey, label = &quot;&quot;)
markers = [:star :circle :square :star5 :hexagon :circle :star]
cols = [:black, :red, :blue, :green, :purple, :grey, :black]
labels = [&quot;Œ∏ = $Œ∏&quot; for Œ∏ in Œ∏s]
for i = 1:length(Œ∏s)
    plot!(ks, cors[:, i], marker = markers[i], c = cols[i], ms = 5, label = labels[i])
end

# Let&#39;s also plot the subset of points we&#39;re actually using.
p_orbit = plot(columns(lorenz[1:n_trainees+n_predictees])..., marker = :circle, label = &quot;&quot;, ms = 2, lŒ± = 0.5)

plot(p_orbit, p_Œ∏_k_sensitivity, layout = grid(1, 2), size = (700, 400))</code></pre><p><img src="../smap_sensitivity_lorenz.svg" alt/></p><p>The nonlinear models (colored lines and symbols) far outperform the linear model (black line + stars).</p><p>Because the predictions for our system improves with increasingly nonlinear models, it indicates that our  system has some inherent nonlinearity. This is, of course, correct, since our Lorenz system is chaotic.</p><p>A formal way to test the presence of nonlinearity is, for example, to define the null hypothesis  &quot;H0: predictions do not improve when using an equivalent nonlinear versus a linear model&quot; (equivalent in the sense  that the only parameter is <code>Œ∏</code>) or, equivalently, &quot;<code>H0: œÅ_linear = œÅ_nonlinear</code>&quot;. If predictions do in fact improve, we instead accept the alternative hypothesis that prediction <em>do</em> improve when using nonlinear models versus using linear models. This can be formally tested using a z-statistic <sup class="footnote-reference"><a id="citeref-Sugihara1994" href="#footnote-Sugihara1994">[Sugihara1994]</a></sup>.</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Sugihara1990"><a class="tag is-link" href="#citeref-Sugihara1990">Sugihara1990</a>Sugihara, George, and Robert M. May. &quot;Nonlinear forecasting as a way of distinguishing chaos from measurement error in time series.&quot; Nature 344.6268 (1990): 734-741.</li><li class="footnote" id="footnote-Ye2015"><a class="tag is-link" href="#citeref-Ye2015">Ye2015</a>Ye, H., Beamish, R. J., Glaser, S. M., Grant, S. C., Hsieh, C. H., Richards, L. J., ... &amp; Sugihara, G. (2015). Equation-free mechanistic ecosystem forecasting using empirical dynamic modeling. Proceedings of the National Academy of Sciences, 112(13), E1569-E1576.</li><li class="footnote" id="footnote-Sugihara1990"><a class="tag is-link" href="#citeref-Sugihara1990">Sugihara1990</a>Sugihara, George, and Robert M. May. &quot;Nonlinear forecasting as a way of distinguishing chaos from measurement error in time series.&quot; Nature 344.6268 (1990): 734-741.</li><li class="footnote" id="footnote-Sugihara1994"><a class="tag is-link" href="#citeref-Sugihara1994">Sugihara1994</a>Sugihara, G. (1994). Nonlinear forecasting for the classification of natural time series. Philosophical Transactions of the Royal Society of London. Series A: Physical and Engineering Sciences, 348(1688), 477-495.</li><li class="footnote" id="footnote-Sugihara1994"><a class="tag is-link" href="#citeref-Sugihara1994">Sugihara1994</a>Sugihara, G. (1994). Nonlinear forecasting for the classification of natural time series. Philosophical Transactions of the Royal Society of London. Series A: Physical and Engineering Sciences, 348(1688), 477-495.</li><li class="footnote" id="footnote-Sugihara1996"><a class="tag is-link" href="#citeref-Sugihara1996">Sugihara1996</a>Sugihara, George, et al. &quot;Nonlinear control of heart rate variability in human infants.&quot; Proceedings of the National Academy of Sciences 93.6 (1996): 2608-2613.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../info_estimators/">¬´ Estimators</a><a class="docs-footer-nextpage" href="../example_systems/">Example systems ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Wednesday 1 December 2021 03:15">Wednesday 1 December 2021</span>. Using Julia version 1.6.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
