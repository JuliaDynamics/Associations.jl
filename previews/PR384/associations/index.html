<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Association measures · Associations.jl</title><meta name="title" content="Association measures · Associations.jl"/><meta property="og:title" content="Association measures · Associations.jl"/><meta property="twitter:title" content="Association measures · Associations.jl"/><meta name="description" content="Documentation for Associations.jl."/><meta property="og:description" content="Documentation for Associations.jl."/><meta property="twitter:description" content="Documentation for Associations.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Associations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Associations.jl</a></li><li><span class="tocitem">Core API reference</span><ul><li class="is-active"><a class="tocitem" href>Association measures</a><ul class="internal"><li><a class="tocitem" href="#Association-API"><span>Association API</span></a></li><li><a class="tocitem" href="#information_api"><span>Information measures</span></a></li><li><a class="tocitem" href="#correlation_api"><span>Correlation measures</span></a></li><li><a class="tocitem" href="#cross_map_api"><span>Cross-map measures</span></a></li><li><a class="tocitem" href="#closeness_api"><span>Closeness measures</span></a></li><li><a class="tocitem" href="#Recurrence-measures"><span>Recurrence measures</span></a></li></ul></li><li><a class="tocitem" href="../independence/">Independence</a></li><li><a class="tocitem" href="../causal_graphs/">Network/graph inference</a></li></ul></li><li><span class="tocitem">Extended API reference</span><ul><li><a class="tocitem" href="../api/discretization_counts_probs_api/">Discretization API</a></li><li><a class="tocitem" href="../api/counts_and_probabilities_api/">Multivariate counts and probabilities API</a></li><li><a class="tocitem" href="../api/information_single_variable_api/">Single-variable information API</a></li><li><a class="tocitem" href="../api/information_multivariate_api/">Multivariate information API</a></li><li><a class="tocitem" href="../api/cross_map_api/">Cross-map API</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/examples_associations/">Associations</a></li><li><a class="tocitem" href="../examples/examples_independence/">Independence testing</a></li><li><a class="tocitem" href="../examples/examples_infer_graphs/">Causal graph inference</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Core API reference</a></li><li class="is-active"><a href>Association measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Association measures</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Associations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Associations.jl/blob/main/docs/src/associations.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="association_measures"><a class="docs-heading-anchor" href="#association_measures">Associations</a><a id="association_measures-1"></a><a class="docs-heading-anchor-permalink" href="#association_measures" title="Permalink"></a></h1><h2 id="Association-API"><a class="docs-heading-anchor" href="#Association-API">Association API</a><a id="Association-API-1"></a><a class="docs-heading-anchor-permalink" href="#Association-API" title="Permalink"></a></h2><p>The most basic components of Associations.jl are a collection of statistics that in some manner quantify the &quot;association&quot; between input datasets. Precisely what is meant by &quot;association&quot; depends on the measure, and precisely what is meant by &quot;quantify&quot; depends on the <em>estimator</em> of that measure. We formalize this notion below with the <a href="#Associations.association"><code>association</code></a> function, which dispatches on <a href="#Associations.AssociationMeasureEstimator"><code>AssociationMeasureEstimator</code></a> and <a href="#Associations.AssociationMeasure"><code>AssociationMeasure</code></a>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.association" href="#Associations.association"><code>Associations.association</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">association(estimator::AssociationMeasureEstimator, x, y, [z, ...]) → r
association(definition::AssociationMeasure, x, y, [z, ...]) → r</code></pre><p>Estimate the (conditional) association between input variables <code>x, y, z, …</code> using  the given <code>estimator</code> (an <a href="#Associations.AssociationMeasureEstimator"><code>AssociationMeasureEstimator</code></a>) or <code>definition</code> (an <a href="#Associations.AssociationMeasure"><code>AssociationMeasure</code></a>).  </p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>The type of the return value <code>r</code> depends on the <code>measure</code>/<code>estimator</code>. The <em>interpretation</em> of the returned value also depends on the specific measure and estimator used.</p></div></div><p><strong>Examples</strong></p><p>The <a href="../examples/examples_associations/#examples_associations">examples</a> section of the online documentation has numerous  using <code>association</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/core.jl#L132-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.AssociationMeasure" href="#Associations.AssociationMeasure"><code>Associations.AssociationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AssociationMeasure</code></pre><p>The supertype of all association measures. </p><p><strong>Abstract implementations</strong></p><p>Currently, the association measures are classified by abstract classes listed below. These abstract classes offer common functionality among association measures that are  conceptually similar. This makes maintenance and framework extension easier than  if each measure was implemented &quot;in isolation&quot;.</p><ul><li><a href="#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a></li><li><a href="#Associations.CrossmapMeasure"><code>CrossmapMeasure</code></a></li><li><a href="#Associations.ClosenessMeasure"><code>ClosenessMeasure</code></a></li><li><a href="#Associations.CorrelationMeasure"><code>CorrelationMeasure</code></a></li></ul><p><strong>Concrete implementations</strong></p><p>Concrete subtypes are given as input to <a href="#Associations.association"><code>association</code></a>. Many of these types require an <a href="#Associations.AssociationMeasureEstimator"><code>AssociationMeasureEstimator</code></a> to compute.</p><table><tr><th style="text-align: right">Type</th><th style="text-align: right"><a href="#Associations.AssociationMeasure"><code>AssociationMeasure</code></a></th><th style="text-align: center">Pairwise</th><th style="text-align: center">Conditional</th></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#Associations.PearsonCorrelation"><code>PearsonCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#Associations.PartialCorrelation"><code>PartialCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#Associations.DistanceCorrelation"><code>DistanceCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#Associations.ChatterjeeCorrelation"><code>ChatterjeeCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#Associations.SMeasure"><code>SMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#Associations.HMeasure"><code>HMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#Associations.MMeasure"><code>MMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness (ranks)</td><td style="text-align: right"><a href="#Associations.LMeasure"><code>LMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#Associations.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Cross-mapping</td><td style="text-align: right"><a href="#Associations.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Cross-mapping</td><td style="text-align: right"><a href="#Associations.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Conditional recurrence</td><td style="text-align: right"><a href="#Associations.MCR"><code>MCR</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Conditional recurrence</td><td style="text-align: right"><a href="#Associations.RMCD"><code>RMCD</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.MIShannon"><code>MIShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.MIRenyiJizba"><code>MIRenyiJizba</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.PartialCorrelation"><code>PartialCorrelation</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.CMIShannon"><code>CMIShannon</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#Associations.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Information transfer</td><td style="text-align: right"><a href="#Associations.TEShannon"><code>TEShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Information transfer</td><td style="text-align: right"><a href="#Associations.TERenyiJizba"><code>TERenyiJizba</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Partial mutual information</td><td style="text-align: right"><a href="#Associations.PartialMutualInformation"><code>PartialMutualInformation</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#Associations.JointEntropyShannon"><code>JointEntropyShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#Associations.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#Associations.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#Associations.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#Associations.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#Associations.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#Associations.HellingerDistance"><code>HellingerDistance</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#Associations.KLDivergence"><code>KLDivergence</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#Associations.RenyiDivergence"><code>RenyiDivergence</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#Associations.VariationDistance"><code>VariationDistance</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/core.jl#L13-L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.AssociationMeasureEstimator" href="#Associations.AssociationMeasureEstimator"><code>Associations.AssociationMeasureEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AssociationMeasureEstimator</code></pre><p>The supertype of all association measure estimators.</p><p>Concrete subtypes are given as input to <a href="#Associations.association"><code>association</code></a>.</p><p><strong>Abstract subtypes</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.MultivariateInformationMeasureEstimator"><code>MultivariateInformationMeasureEstimator</code></a></li><li><a href="../api/cross_map_api/#Associations.CrossmapEstimator"><code>CrossmapEstimator</code></a></li></ul><p><strong>Concrete implementations</strong></p><table><tr><th style="text-align: left">AssociationMeasure</th><th style="text-align: left">Estimators</th></tr><tr><td style="text-align: left"><a href="#Associations.PearsonCorrelation"><code>PearsonCorrelation</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.DistanceCorrelation"><code>DistanceCorrelation</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.PartialCorrelation"><code>PartialCorrelation</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.ChatterjeeCorrelation"><code>ChatterjeeCorrelation</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.SMeasure"><code>SMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.HMeasure"><code>HMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.MMeasure"><code>MMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.LMeasure"><code>LMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a></td><td style="text-align: left"><a href="../api/cross_map_api/#Associations.RandomVectors"><code>RandomVectors</code></a>, <a href="../api/cross_map_api/#Associations.RandomSegment"><code>RandomSegment</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a></td><td style="text-align: left"><a href="../api/cross_map_api/#Associations.RandomVectors"><code>RandomVectors</code></a>, <a href="../api/cross_map_api/#Associations.RandomSegment"><code>RandomSegment</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.MCR"><code>MCR</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.RMCD"><code>RMCD</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#Associations.MIShannon"><code>MIShannon</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>, <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a>, <a href="../api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a>, <a href="../api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a>, <a href="../api/information_multivariate_api/#Associations.GaoOhViswanath"><code>GaoOhViswanath</code></a>, <a href="../api/information_multivariate_api/#Associations.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a>, <a href="../api/information_multivariate_api/#Associations.GaussianMI"><code>GaussianMI</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.MIRenyiJizba"><code>MIRenyiJizba</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>, <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>, <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.MITsallisMartin"><code>MITsallisMartin</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>, <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.CMIShannon"><code>CMIShannon</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>, <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a>, <a href="../api/information_multivariate_api/#Associations.MIDecomposition"><code>MIDecomposition</code></a>, <a href="../api/information_multivariate_api/#Associations.GaussianCMI"><code>GaussianCMI</code></a>, <a href="../api/information_multivariate_api/#Associations.FPVP"><code>FPVP</code></a>, <a href="../api/information_multivariate_api/#Associations.MesnerShalizi"><code>MesnerShalizi</code></a>, <a href="../api/information_multivariate_api/#Associations.Rahimzamani"><code>Rahimzamani</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>, <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.TEShannon"><code>TEShannon</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>, <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a>, <a href="../api/information_multivariate_api/#Associations.Zhu1"><code>Zhu1</code></a>, <a href="../api/information_multivariate_api/#Associations.Lindner"><code>Lindner</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.TERenyiJizba"><code>TERenyiJizba</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.PartialMutualInformation"><code>PartialMutualInformation</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.JointEntropyShannon"><code>JointEntropyShannon</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.HellingerDistance"><code>HellingerDistance</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.KLDivergence"><code>KLDivergence</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.RenyiDivergence"><code>RenyiDivergence</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#Associations.VariationDistance"><code>VariationDistance</code></a></td><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/core.jl#L76-L129">source</a></section></article><p>Here are some examples of how to use <a href="#Associations.association"><code>association</code></a>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Associations</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x, y, z = rand(1000), rand(1000), rand(1000);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; association(LMeasure(), x, y)</code><code class="nohighlight hljs ansi" style="display:block;">-0.0027698381112035747</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; association(DistanceCorrelation(), x, y)</code><code class="nohighlight hljs ansi" style="display:block;">0.03361440104406626</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; association(JointProbabilities(JointEntropyShannon(), CodifyVariables(Dispersion(c = 3, m = 2))), x, y)</code><code class="nohighlight hljs ansi" style="display:block;">3.10382059703563</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; association(EntropyDecomposition(MIShannon(), PlugIn(Shannon()), CodifyVariables(OrdinalPatterns(m=3))), x, y)</code><code class="nohighlight hljs ansi" style="display:block;">0.01756523781257613</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; association(KSG2(MIShannon(base = 2)), x, y)</code><code class="nohighlight hljs ansi" style="display:block;">-0.05281460742703124</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; association(JointProbabilities(PartialMutualInformation(), CodifyVariables(OrdinalPatterns(m=3))), x, y, z)</code><code class="nohighlight hljs ansi" style="display:block;">0.2589380529239549</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; association(FPVP(CMIShannon(base = 2)), x, y, z)</code><code class="nohighlight hljs ansi" style="display:block;">-0.4214943568122166</code></pre><h2 id="information_api"><a class="docs-heading-anchor" href="#information_api">Information measures</a><a id="information_api-1"></a><a class="docs-heading-anchor-permalink" href="#information_api" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MultivariateInformationMeasure" href="#Associations.MultivariateInformationMeasure"><code>Associations.MultivariateInformationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MultivariateInformationMeasure &lt;: AssociationMeasure</code></pre><p>The supertype for all multivariate information-based measure definitions.</p><p><strong>Definition</strong></p><p>Following <a href="../references/#Datseris2024">Datseris and Haaga (2024)</a>, we define a multivariate information measure as <em>any functional  of a multidimensional probability mass functions (PMFs) or multidimensional probability density</em>.</p><p><strong>Implementations</strong></p><p><a href="#Associations.JointEntropy"><code>JointEntropy</code></a> definitions:</p><ul><li><a href="#Associations.JointEntropyShannon"><code>JointEntropyShannon</code></a></li><li><a href="#Associations.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></li><li><a href="#Associations.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></li></ul><p><a href="#Associations.ConditionalEntropy"><code>ConditionalEntropy</code></a> definitions:</p><ul><li><a href="#Associations.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></li><li><a href="#Associations.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></li><li><a href="#Associations.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></li></ul><p><a href="#Associations.DivergenceOrDistance"><code>DivergenceOrDistance</code></a> definitions:</p><ul><li><a href="#Associations.HellingerDistance"><code>HellingerDistance</code></a></li><li><a href="#Associations.KLDivergence"><code>KLDivergence</code></a></li><li><a href="#Associations.RenyiDivergence"><code>RenyiDivergence</code></a></li><li><a href="#Associations.VariationDistance"><code>VariationDistance</code></a></li></ul><p><a href="#Associations.MutualInformation"><code>MutualInformation</code></a> definitions:</p><ul><li><a href="#Associations.MIShannon"><code>MIShannon</code></a></li><li><a href="#Associations.MIRenyiJizba"><code>MIRenyiJizba</code></a></li><li><a href="#Associations.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></li><li><a href="#Associations.MITsallisMartin"><code>MITsallisMartin</code></a></li><li><a href="#Associations.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></li></ul><p><a href="#Associations.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> definitions:</p><ul><li><a href="#Associations.CMIShannon"><code>CMIShannon</code></a></li><li><a href="#Associations.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></li><li><a href="#Associations.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></li><li><a href="#Associations.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></li><li><a href="#Associations.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></li></ul><p><a href="#Associations.TransferEntropy"><code>TransferEntropy</code></a> definitions:</p><ul><li><a href="#Associations.TEShannon"><code>TEShannon</code></a></li><li><a href="#Associations.TERenyiJizba"><code>TERenyiJizba</code></a></li></ul><p>Other definitions:</p><ul><li><a href="#Associations.PartialMutualInformation"><code>PartialMutualInformation</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/core.jl#L82-L137">source</a></section></article><h3 id="conditional_entropies"><a class="docs-heading-anchor" href="#conditional_entropies">Conditional entropies</a><a id="conditional_entropies-1"></a><a class="docs-heading-anchor-permalink" href="#conditional_entropies" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ConditionalEntropy" href="#Associations.ConditionalEntropy"><code>Associations.ConditionalEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropy &lt;: MultivariateInformationMeasure</code></pre><p>The supertype for all conditional entropy measures.</p><p><strong>Concrete subtypes</strong></p><ul><li><a href="#Associations.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></li><li><a href="#Associations.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></li><li><a href="#Associations.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_entropies/conditional_entropies.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ConditionalEntropyShannon" href="#Associations.ConditionalEntropyShannon"><code>Associations.ConditionalEntropyShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropyShannon &lt;: ConditionalEntropy
ConditionalEntropyShannon(; base = 2)</code></pre><p>The <a href="../api/information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a> conditional entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the Shannon conditional entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Discrete definition</strong></p><p><strong>Sum formulation</strong></p><p>The conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H^{S}(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log(p(x | y)).\]</p><p>This is the definition used when calling <a href="#Associations.association"><code>association</code></a> with a <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator.</p><p><strong>Two-entropies formulation</strong></p><p>Equivalently, the following differenConditionalEntropy of entropies hold</p><p class="math-container">\[H^S(X | Y) = H^S(X, Y) - H^S(Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot | \cdot)$</span> are the <a href="../api/information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy and Shannon joint entropy, respectively. This is the definition used when calling <a href="#Associations.association"><code>association</code></a> with a <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Differential definition</strong></p><p>The differential conditional Shannon entropy is analogously defined as</p><p class="math-container">\[H^S(X | Y) = h^S(X, Y) - h^S(Y),\]</p><p>where <span>$h^S(\cdot)$</span> and <span>$h^S(\cdot | \cdot)$</span> are the <a href="../api/information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling <a href="#Associations.association"><code>association</code></a> with a <a href="../api/information_single_variable_api/#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConditionalEntropyShannon_analytical">Example 1</a>: Analytical example from Cover &amp; Thomas&#39;s book.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyShannon_JointProbabilities_CodifyVariables_UniqueElements">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator with<a href="../api/discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> discretization and    <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on categorical data.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyShannon_JointProbabilities_CodifyPoints_UniqueElementsEncoding">Example 3</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator with <a href="../api/discretization_counts_probs_api/#Associations.CodifyPoints"><code>CodifyPoints</code></a> discretization and <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElementsEncoding"><code>UniqueElementsEncoding</code></a>   encoding of points on numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_entropies/ConditionalEntropyShannon.jl#L6-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ConditionalEntropyTsallisFuruichi" href="#Associations.ConditionalEntropyTsallisFuruichi"><code>Associations.ConditionalEntropyTsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropyTsallisFuruichi &lt;: ConditionalEntropy
ConditionalEntropyTsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>Furuichi (2006)&#39;s discrete Tsallis conditional entropy definition.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the Tsallis-Furuichi conditional entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Furuichi&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}}
p(x, y)^q \log_q(p(x | y)),\]</p><p><span>$\ln_q(x) = \frac{x^{1-q} - 1}{1 - q}$</span> and <span>$q \neq 1$</span>. For <span>$q = 1$</span>, <span>$H_q^T(X | Y)$</span> reduces to the Shannon conditional entropy:</p><p class="math-container">\[H_{q=1}^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} =
p(x, y) \log(p(x | y))\]</p><p>If any of the entries of the marginal distribution for <code>Y</code> are zero, or the q-logarithm  is undefined for a particular value, then the measure is undefined and <code>NaN</code> is returned.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisFuruichi_JointProbabilities_CodifyVariables_UniqueElements">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator with<a href="../api/discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> discretization and    <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on categorical data.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisFuruichi_JointProbabilities_CodifyPoints_UniqueElementsEncoding">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator with <a href="../api/discretization_counts_probs_api/#Associations.CodifyPoints"><code>CodifyPoints</code></a> discretization and <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElementsEncoding"><code>UniqueElementsEncoding</code></a>   encoding of points on numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_entropies/ConditionalEntropyTsallisFuruichi.jl#L3-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ConditionalEntropyTsallisAbe" href="#Associations.ConditionalEntropyTsallisAbe"><code>Associations.ConditionalEntropyTsallisAbe</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropyTsallisAbe &lt;: ConditionalEntropy
ConditionalEntropyTsallisAbe(; base = 2, q = 1.5)</code></pre><p><a href="../references/#Abe2001">Abe and Rajagopal (2001)</a>&#39;s discrete Tsallis conditional entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the Tsallis-Abe conditional entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Abe &amp; Rajagopal&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^{T_A}(X | Y) = \dfrac{H_q^T(X, Y) - H_q^T(Y)}{1 + (1-q)H_q^T(Y)},\]</p><p>where <span>$H_q^T(\cdot)$</span> and <span>$H_q^T(\cdot, \cdot)$</span> is the <a href="../api/information_single_variable_api/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a> entropy and the joint Tsallis entropy.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisAbe_JointProbabilities_CodifyVariables_UniqueElements">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator with<a href="../api/discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> discretization and    <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on categorical data.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisAbe_JointProbabilities_CodifyPoints_UniqueElementsEncoding">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator with <a href="../api/discretization_counts_probs_api/#Associations.CodifyPoints"><code>CodifyPoints</code></a> discretization and <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElementsEncoding"><code>UniqueElementsEncoding</code></a>   encoding of points on numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_entropies/ConditionalEntropyTsallisAbe.jl#L5-L40">source</a></section></article><h3 id="divergences_and_distances"><a class="docs-heading-anchor" href="#divergences_and_distances">Divergences and distances</a><a id="divergences_and_distances-1"></a><a class="docs-heading-anchor-permalink" href="#divergences_and_distances" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.DivergenceOrDistance" href="#Associations.DivergenceOrDistance"><code>Associations.DivergenceOrDistance</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DivergenceOrDistance &lt;: BivariateInformationMeasure</code></pre><p>The supertype for bivariate information measures aiming to quantify some sort of divergence, distance or closeness between two probability distributions.</p><p>Some of these measures are proper metrics, while others are not, but they have in common that they aim to quantify how &quot;far from each other&quot; two probabilities distributions are.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#Associations.HellingerDistance"><code>HellingerDistance</code></a></li><li><a href="#Associations.KLDivergence"><code>KLDivergence</code></a></li><li><a href="#Associations.RenyiDivergence"><code>RenyiDivergence</code></a></li><li><a href="#Associations.VariationDistance"><code>VariationDistance</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/divergences_and_distances/divergences_and_distances.jl#L3-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.HellingerDistance" href="#Associations.HellingerDistance"><code>Associations.HellingerDistance</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HellingerDistance &lt;: DivergenceOrDistance</code></pre><p>The Hellinger distance.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the compute the Hellinger distance between two pre-computed   probability distributions, or from raw data using one of the estimators listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Description</strong></p><p>The Hellinger distance between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is <a href="https://en.wikipedia.org/wiki/Hellinger_distance">defined</a> as</p><p class="math-container">\[D_{H}(P_Y(\Omega) || P_Y(\Omega)) =
\dfrac{1}{\sqrt{2}} \sum_{\omega \in \Omega} (\sqrt{p_x(\omega)} - \sqrt{p_y(\omega)})^2\]</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_HellingerDistance_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_HellingerDistance_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/divergences_and_distances/HellingerDistance.jl#L3-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.KLDivergence" href="#Associations.KLDivergence"><code>Associations.KLDivergence</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KLDivergence &lt;: DivergenceOrDistance</code></pre><p>The Kullback-Leibler (KL) divergence.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the compute the KL-divergence between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#Associations.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></li></ul><p><strong>Estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>.</li></ul><p><strong>Description</strong></p><p>The KL-divergence between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is defined as</p><p class="math-container">\[D_{KL}(P_Y(\Omega) || P_Y(\Omega)) =
\sum_{\omega \in \Omega} p_x(\omega) \log\dfrac{p_x(\omega)}{p_y(\omega)}\]</p><p><strong>Implements</strong></p><ul><li><a href="#Associations.association"><code>association</code></a>. Used to compute the KL-divergence between two pre-computed   probability distributions. If used with <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a>, the KL divergence may   be undefined to due some outcomes having zero counts. Use some other   <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> like <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.BayesianRegularization"><code>BayesianRegularization</code></a> to ensure   all estimated probabilities are nonzero.</li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Distances.jl also defines <code>KLDivergence</code>. Quality it if you&#39;re loading both  packages, i.e. do <code>association(Associations.KLDivergence(), x, y)</code>.</p></div></div><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_KLDivergence_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_KLDivergence_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/divergences_and_distances/KLDivergence.jl#L3-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.RenyiDivergence" href="#Associations.RenyiDivergence"><code>Associations.RenyiDivergence</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RenyiDivergence &lt;: DivergenceOrDistance
RenyiDivergence(q; base = 2)</code></pre><p>The Rényi divergence of positive order <code>q</code>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the compute the Rényi divergence between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#Associations.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></li></ul><p><strong>Description</strong></p><p>The Rényi divergence between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is defined as <a href="../references/#vanErven2014">van Erven and Harremos (2014)</a>.</p><p class="math-container">\[D_{q}(P_Y(\Omega) || P_Y(\Omega)) =
\dfrac{1}{q - 1} \log \sum_{\omega \in \Omega}p_x(\omega)^{q}p_y(\omega)^{1-\alpha}\]</p><p><strong>Implements</strong></p><ul><li><a href="../api/information_single_variable_api/#ComplexityMeasures.information"><code>information</code></a>. Used to compute the Rényi divergence between two pre-computed   probability distributions. If used with <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.RelativeAmount"><code>RelativeAmount</code></a>, the KL divergence may   be undefined to due some outcomes having zero counts. Use some other   <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> like <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.BayesianRegularization"><code>BayesianRegularization</code></a> to ensure   all estimated probabilities are nonzero.</li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Distances.jl also defines <code>RenyiDivergence</code>. Quality it if you&#39;re loading both  packages, i.e. do <code>association(Associations.RenyiDivergence(), x, y)</code>.</p></div></div><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_RenyiDivergence_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_RenyiDivergence_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/divergences_and_distances/RenyiDivergence.jl#L3-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.VariationDistance" href="#Associations.VariationDistance"><code>Associations.VariationDistance</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">VariationDistance &lt;: DivergenceOrDistance</code></pre><p>The variation distance.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the compute the variation distance between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#Associations.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></li></ul><p><strong>Description</strong></p><p>The variation distance between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is <a href="https://en.wikipedia.org/wiki/Variation_distance">defined</a> as</p><p class="math-container">\[D_{V}(P_Y(\Omega) || P_Y(\Omega)) =
\dfrac{1}{2} \sum_{\omega \in \Omega} | p_x(\omega) - p_y(\omega) |\]</p><p><strong>Examples</strong></p><ul><li><a href="../examples/examples_associations/#example_VariationDistance_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_VariationDistance_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/divergences_and_distances/VariationDistance.jl#L3-L36">source</a></section></article><h3 id="joint_entropies"><a class="docs-heading-anchor" href="#joint_entropies">Joint entropies</a><a id="joint_entropies-1"></a><a class="docs-heading-anchor-permalink" href="#joint_entropies" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.JointEntropy" href="#Associations.JointEntropy"><code>Associations.JointEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropy &lt;: BivariateInformationMeasure</code></pre><p>The supertype for all joint entropy measures.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#Associations.JointEntropyShannon"><code>JointEntropyShannon</code></a></li><li><a href="#Associations.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></li><li><a href="#Associations.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/joint_entropies/joint_entropies.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.JointEntropyShannon" href="#Associations.JointEntropyShannon"><code>Associations.JointEntropyShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropyShannon &lt;: JointEntropy
JointEntropyShannon(; base = 2)</code></pre><p>The Shannon joint entropy measure (<a href="../references/#CoverThomas1999">Cover, 1999</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the Shannon joint entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Given two two discrete random variables <span>$X$</span> and <span>$Y$</span> with ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{X}$</span>, <a href="../references/#CoverThomas1999">Cover (1999)</a> defines the Shannon joint entropy as</p><p class="math-container">\[H^S(X, Y) = -\sum_{x\in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y),\]</p><p>where we define <span>$log(p(x, y)) := 0$</span> if <span>$p(x, y) = 0$</span>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_JointEntropyShannon_Dispersion">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/joint_entropies/JointEntropyShannon.jl#L5-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.JointEntropyTsallis" href="#Associations.JointEntropyTsallis"><code>Associations.JointEntropyTsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropyTsallis &lt;: JointEntropy
JointEntropyTsallis(; base = 2, q = 1.5)</code></pre><p>The Tsallis joint entropy definition from <a href="../references/#Furuichi2006">Furuichi (2006)</a>. </p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the Furuichi-Tsallis joint entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Given two two discrete random variables <span>$X$</span> and <span>$Y$</span> with ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{X}$</span>, <a href="../references/#Furuichi2006">Furuichi (2006)</a> defines the Tsallis joint entropy as</p><p class="math-container">\[H_q^T(X, Y) = -\sum_{x\in \mathcal{X}, y \in \mathcal{Y}} p(x, y)^q \log_q p(x, y),\]</p><p>where <span>$log_q(x, q) = \dfrac{x^{1-q} - 1}{1-q}$</span> is the q-logarithm, and  we define <span>$log_q(x, q) := 0$</span> if <span>$q = 0$</span>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_JointEntropyTsallis_OrdinalPatterns">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/joint_entropies/JointEntropyTsallis.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.JointEntropyRenyi" href="#Associations.JointEntropyRenyi"><code>Associations.JointEntropyRenyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropyRenyi &lt;: JointEntropy
JointEntropyRenyi(; base = 2, q = 1.5)</code></pre><p>The Rényi joint entropy measure (<a href="../references/#Golshani2009">Golshani <em>et al.</em>, 2009</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the Golshani-Rényi joint entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Given two two discrete random variables <span>$X$</span> and <span>$Y$</span> with ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{X}$</span>, <a href="../references/#Golshani2009">Golshani <em>et al.</em> (2009)</a> defines the Rényi joint entropy as</p><p class="math-container">\[H_q^R(X, Y) = \dfrac{1}{1-\alpha} \log \sum_{i = 1}^N p_i^q,\]</p><p>where <span>$q &gt; 0$</span> and <span>$q != 1$</span>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_JointEntropyRenyi_ValueBinning">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/joint_entropies/JointEntropyRenyi.jl#L5-L35">source</a></section></article><h3 id="Mutual-informations"><a class="docs-heading-anchor" href="#Mutual-informations">Mutual informations</a><a id="Mutual-informations-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-informations" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MutualInformation" href="#Associations.MutualInformation"><code>Associations.MutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MutualInformation</code></pre><p>Abstract type for all mutual information measures.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#Associations.MIShannon"><code>MIShannon</code></a></li><li><a href="#Associations.MITsallisMartin"><code>MITsallisMartin</code></a></li><li><a href="#Associations.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></li><li><a href="#Associations.MIRenyiJizba"><code>MIRenyiJizba</code></a></li><li><a href="#Associations.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></li></ul><p>See also: <a href="../api/information_multivariate_api/#Associations.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/mutual_informations/mutual_informations.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MIShannon" href="#Associations.MIShannon"><code>Associations.MIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIShannon &lt;: BivariateInformationMeasure
MIShannon(; base = 2)</code></pre><p>The Shannon mutual information <span>$I_S(X; Y)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Shannon mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Shannon mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> (generic)</li><li><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> (generic)</li><li><a href="../api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a></li><li><a href="../api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a></li><li><a href="../api/information_multivariate_api/#Associations.GaoOhViswanath"><code>GaoOhViswanath</code></a></li><li><a href="../api/information_multivariate_api/#Associations.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></li><li><a href="../api/information_multivariate_api/#Associations.GaussianMI"><code>GaussianMI</code></a></li></ul><p><strong>Discrete definition</strong></p><p>There are many equivalent formulations of discrete Shannon mutual information, meaning that  it can be estimated in several ways, either using <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>  (double-sum formulation), <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> (three-entropies decomposition), or some dedicated estimator.</p><p><strong>Double sum formulation</strong></p><p>Assume we observe samples <span>$\bar{\bf{X}}_{1:N_y} = \{\bar{\bf{X}}_1, \ldots, \bar{\bf{X}}_n \}$</span> and <span>$\bar{\bf{Y}}_{1:N_x} = \{\bar{\bf{Y}}_1, \ldots, \bar{\bf{Y}}_n \}$</span> from two discrete random variables <span>$X$</span> and <span>$Y$</span> with finite supports <span>$\mathcal{X} = \{ x_1, x_2, \ldots, x_{M_x} \}$</span> and <span>$\mathcal{Y} = y_1, y_2, \ldots, x_{M_y}$</span>. The double-sum estimate is obtained by replacing the double sum</p><p class="math-container">\[\hat{I}_{DS}(X; Y) =
 \sum_{x_i \in \mathcal{X}, y_i \in \mathcal{Y}} p(x_i, y_j) \log \left( \dfrac{p(x_i, y_i)}{p(x_i)p(y_j)} \right)\]</p><p>where  <span>$\hat{p}(x_i) = \frac{n(x_i)}{N_x}$</span>, <span>$\hat{p}(y_i) = \frac{n(y_j)}{N_y}$</span>, and <span>$\hat{p}(x_i, x_j) = \frac{n(x_i)}{N}$</span>, and <span>$N = N_x N_y$</span>. This definition is used by <a href="#Associations.association"><code>association</code></a> when called with a <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator.</p><p><strong>Three-entropies formulation</strong></p><p>An equivalent formulation of discrete Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = H^S(X) + H_q^S(Y) - H^S(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint discrete Shannon entropies. This definition is used by <a href="#Associations.association"><code>association</code></a> when called with a <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> estimator and a discretization.</p><p><strong>Differential mutual information</strong></p><p>One possible formulation of differential Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = h^S(X) + h_q^S(Y) - h^S(X, Y),\]</p><p>where <span>$h^S(\cdot)$</span> and <span>$h^S(\cdot, \cdot)$</span> are the marginal and joint differential Shannon entropies. This definition is used by <a href="#Associations.association"><code>association</code></a> when called with <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> estimator and a <a href="../api/information_single_variable_api/#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MIShannon_JointProbabilities_ValueBinning">Example 1</a>: <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MIShannon_JointProbabilities_UniqueElements">Example 2</a>: <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on string data.</li><li><a href="../examples/examples_associations/#example_MIShannon_GaussianMI">Example 3</a>: Dedicated <a href="../api/information_multivariate_api/#Associations.GaussianMI"><code>GaussianMI</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_KSG1">Example 4</a>: Dedicated <a href="../api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_KSG2">Example 5</a>: Dedicated <a href="../api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_GaoKannanOhViswanath">Example 6</a>: Dedicated <a href="../api/information_multivariate_api/#Associations.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_EntropyDecomposition_Kraskov">Example 7</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/information_single_variable_api/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_EntropyDecomposition_BubbleSortSwaps">Example 8</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.BubbleSortSwaps"><code>BubbleSortSwaps</code></a>.</li><li><a href="../examples/examples_associations/#example_MIShannon_EntropyDecomposition_Jackknife_ValueBinning">Example 9</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/information_single_variable_api/#ComplexityMeasures.Jackknife"><code>Jackknife</code></a> estimator and <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MIShannon_reproducing_Kraskov">Example 10</a>: Reproducing Kraskov et al. (2004).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/mutual_informations/MIShannon.jl#L5-L90">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MITsallisFuruichi" href="#Associations.MITsallisFuruichi"><code>Associations.MITsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisFuruichi &lt;: BivariateInformationMeasure
MITsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from Furuichi (2006)(<a href="../references/#Furuichi2006">Furuichi, 2006</a>), which in that paper is called the <em>mutual entropy</em>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Tsallis-Furuichi mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Tsallis-Furuichi mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></li></ul><p><strong>Description</strong></p><p>Furuichi&#39;s Tsallis mutual entropy between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_q^T(X; Y) = H_q^T(X) - H_q^T(X | Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X, Y),\]</p><p>where <span>$H^T(\cdot)$</span> and <span>$H^T(\cdot, \cdot)$</span> are the marginal and joint Tsallis entropies, and <code>q</code> is the <a href="../api/information_single_variable_api/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MITsallisFuruichi_JointProbabilities_UniqueElements">Example 1</a>: <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MITsallisFuruichi_EntropyDecomposition_LeonenkoProzantoSavani">Example 2</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/information_single_variable_api/#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MITsallisFuruichi_EntropyDecomposition_Dispersion">Example 3</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/mutual_informations/MITsallisFuruichi.jl#L4-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MITsallisMartin" href="#Associations.MITsallisMartin"><code>Associations.MITsallisMartin</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisMartin &lt;: BivariateInformationMeasure
MITsallisMartin(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from <a href="../references/#Martin2004">Martin <em>et al.</em> (2004)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Tsallis-Martin mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Tsallis-Martin mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></li></ul><p><strong>Description</strong></p><p>Martin et al.&#39;s Tsallis mutual information between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_{\text{Martin}}^T(X, Y, q) := H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint Shannon entropies, and <code>q</code> is the <a href="../api/information_single_variable_api/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MITsallisMartin_JointProbabilities_UniqueElements">Example 1</a>: <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MITsallisMartin_EntropyDecomposition_LeonenkoProzantoSavani">Example 2</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/information_single_variable_api/#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MITsallisMartin_EntropyDecomposition_OrdinalPatterns">Example 3</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/mutual_informations/MITsallisMartin.jl#L5-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MIRenyiJizba" href="#Associations.MIRenyiJizba"><code>Associations.MIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiJizba &lt;: &lt;: BivariateInformationMeasure
MIRenyiJizba(; q = 1.5, base = 2)</code></pre><p>The Rényi mutual information <span>$I_q^{R_{J}}(X; Y)$</span> defined in (<a href="../references/#Jizba2012">Jizba <em>et al.</em>, 2012</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Rényi-Jizba mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Rényi-Jizba mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>.</li><li><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a>.</li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y) = H_q^{R}(X) + H_q^{R}(Y) - H_q^{R}(X, Y),\]</p><p>where <span>$H_q^{R}(\cdot)$</span> is the <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/examples/#R%C3%A9nyi-entropy"><code>Rényi</code></a> entropy.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MIRenyiJizba_JointProbabilities_UniqueElements">Example 1</a>: <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MIRenyiJizba_JointProbabilities_LeonenkoProzantoSavani">Example 2</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/information_single_variable_api/#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a>.</li><li><a href="../examples/examples_associations/#example_MIRenyiJizba_EntropyDecomposition_ValueBinning">Example 3</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/mutual_informations/MIRenyiJizba.jl#L5-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MIRenyiSarbu" href="#Associations.MIRenyiSarbu"><code>Associations.MIRenyiSarbu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiSarbu &lt;: BivariateInformationMeasure
MIRenyiSarbu(; base = 2, q = 1.5)</code></pre><p>The discrete Rényi mutual information from <a href="../references/#Sarbu2014">Sarbu (2014)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Rényi-Sarbu mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Rényi-Sarbu mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a>.</li></ul><p><strong>Description</strong></p><p>Sarbu (2014) defines discrete Rényi mutual information as the Rényi <span>$\alpha$</span>-divergence between the conditional joint probability mass function <span>$p(x, y)$</span> and the product of the conditional marginals, <span>$p(x) \cdot p(y)$</span>:</p><p class="math-container">\[I(X, Y)^R_q =
\dfrac{1}{q-1}
\log \left(
    \sum_{x \in X, y \in Y}
    \dfrac{p(x, y)^q}{\left( p(x)\cdot p(y) \right)^{q-1}}
\right)\]</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MIRenyiSarbu_JointProbabilities_UniqueElements">Example 1</a>: <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> for categorical data.</li><li><a href="../examples/examples_associations/#example_MIRenyiSarbu_JointProbabilities_CosineSimilarityBinning">Example 2</a>: <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.CosineSimilarityBinning"><code>CosineSimilarityBinning</code></a> for numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/mutual_informations/MIRenyiSarbu.jl#L5-L41">source</a></section></article><h3 id="Conditional-mutual-informations"><a class="docs-heading-anchor" href="#Conditional-mutual-informations">Conditional mutual informations</a><a id="Conditional-mutual-informations-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-informations" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ConditionalMutualInformation" href="#Associations.ConditionalMutualInformation"><code>Associations.ConditionalMutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CondiitionalMutualInformation</code></pre><p>Abstract type for all mutual information measures.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#Associations.CMIShannon"><code>CMIShannon</code></a></li><li><a href="#Associations.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></li><li><a href="#Associations.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></li><li><a href="#Associations.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></li><li><a href="#Associations.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></li></ul><p>See also: <a href="../api/information_multivariate_api/#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_mutual_informations/conditional_mutual_informations.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CMIShannon" href="#Associations.CMIShannon"><code>Associations.CMIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIShannon &lt;: ConditionalMutualInformation
CMIShannon(; base = 2)</code></pre><p>The Shannon conditional mutual information (CMI) <span>$I^S(X; Y | Z)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Shannon conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Shannon conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></li><li><a href="../api/information_multivariate_api/#Associations.MIDecomposition"><code>MIDecomposition</code></a></li><li><a href="../api/information_multivariate_api/#Associations.FPVP"><code>FPVP</code></a></li><li><a href="../api/information_multivariate_api/#Associations.MesnerShalizi"><code>MesnerShalizi</code></a></li><li><a href="../api/information_multivariate_api/#Associations.Rahimzamani"><code>Rahimzamani</code></a></li><li><a href="../api/information_multivariate_api/#Associations.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li><li><a href="../api/information_multivariate_api/#Associations.GaussianCMI"><code>GaussianCMI</code></a></li></ul><p><strong>Supported definitions</strong></p><p>Consider random variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span>, given <span>$Z \in \mathbb{R}^{d_Z}$</span>. The Shannon conditional mutual information is defined as</p><p class="math-container">\[\begin{align*}
I(X; Y | Z)
&amp;= H^S(X, Z) + H^S(Y, z) - H^S(X, Y, Z) - H^S(Z) \\
&amp;= I^S(X; Y, Z) + I^S(X; Y)
\end{align*},\]</p><p>where <span>$I^S(\cdot; \cdot)$</span> is the Shannon mutual information <a href="#Associations.MIShannon"><code>MIShannon</code></a>, and <span>$H^S(\cdot)$</span> is the <a href="../api/information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>Differential Shannon CMI is obtained by replacing the entropies by differential entropies.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_CMIShannon_EntropyDecomposition_Kraskov">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/information_single_variable_api/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_CMIShannon_EntropyDecomposition_ValueBinning">Example 2</a>:   <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_CMIShannon_MIDecomposition_KSG1">Example 3</a>:    <a href="../api/information_multivariate_api/#Associations.MIDecomposition"><code>MIDecomposition</code></a> with <a href="../api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> estimator.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_mutual_informations/CMIShannon.jl#L6-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CMIRenyiSarbu" href="#Associations.CMIRenyiSarbu"><code>Associations.CMIRenyiSarbu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiSarbu &lt;: ConditionalMutualInformation
CMIRenyiSarbu(; base = 2, q = 1.5)</code></pre><p>The Rényi conditional mutual information from <a href="../references/#Sarbu2014">Sarbu (2014)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw  Rényi-Sarbu conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Sarbu conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Discrete description</strong></p><p>Assume we observe three discrete random variables <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span>. Sarbu (2014) defines discrete conditional Rényi mutual information as the conditional Rényi <span>$\alpha$</span>-divergence between the conditional joint probability mass function <span>$p(x, y | z)$</span> and the product of the conditional marginals, <span>$p(x |z) \cdot p(y|z)$</span>:</p><p class="math-container">\[I(X, Y; Z)^R_q =
\dfrac{1}{q-1} \sum_{z \in Z} p(Z = z)
\log \left(
    \sum_{x \in X}\sum_{y \in Y}
    \dfrac{p(x, y|z)^q}{\left( p(x|z)\cdot p(y|z) \right)^{q-1}}
\right)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_mutual_informations/CMIRenyiSarbu.jl#L6-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CMIRenyiJizba" href="#Associations.CMIRenyiJizba"><code>Associations.CMIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiJizba &lt;: ConditionalMutualInformation
CMIRenyiJizba(; base = 2, q = 1.5)</code></pre><p>The Rényi conditional mutual information <span>$I_q^{R_{J}}(X; Y | Z)$</span> defined in <a href="../references/#Jizba2012">Jizba <em>et al.</em> (2012)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw  Rényi-Jizba conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Jizba conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y | Z) = I_q^{R_{J}}(X; Y, Z) - I_q^{R_{J}}(X; Z),\]</p><p>where <span>$I_q^{R_{J}}(X; Z)$</span> is the <a href="#Associations.MIRenyiJizba"><code>MIRenyiJizba</code></a> mutual information.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_CMIRenyiJizba_JointProbabilities_BubbleSortSwaps">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.BubbleSortSwaps"><code>BubbleSortSwaps</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_CMIRenyiJizba_EntropyDecomposition_OrdinalPatterns">Example 2</a>:    <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_CMIRenyiJizba_EntropyDecomposition_LeonenkoProzantoSavani">Example 3</a>:    <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with differential entropy estimator <a href="../api/information_single_variable_api/#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_mutual_informations/CMIRenyiJizba.jl#L5-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CMIRenyiPoczos" href="#Associations.CMIRenyiPoczos"><code>Associations.CMIRenyiPoczos</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiPoczos &lt;: ConditionalMutualInformation
CMIRenyiPoczos(; base = 2, q = 1.5)</code></pre><p>The differential Rényi conditional mutual information <span>$I_q^{R_{P}}(X; Y | Z)$</span> defined in <a href="../references/#Poczos2012">Póczos and Schneider (2012)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Rényi-Poczos conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Poczos conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li></ul><p><strong>Definition</strong></p><p class="math-container">\[\begin{align*}
I_q^{R_{P}}(X; Y | Z) &amp;= \dfrac{1}{q-1}
\int \int \int \dfrac{p_Z(z) p_{X, Y | Z}^q}{( p_{X|Z}(x|z) p_{Y|Z}(y|z) )^{q-1}} \\
&amp;= \mathbb{E}_{X, Y, Z} \sim p_{X, Y, Z}
\left[ \dfrac{p_{X, Z}^{1-q}(X, Z) p_{Y, Z}^{1-q}(Y, Z) }{p_{X, Y, Z}^{1-q}(X, Y, Z) p_Z^{1-q}(Z)} \right]
\end{align*}\]</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#CMIRenyiPoczos_PoczosSchneiderCMI">Example 1</a>: Dedicated <a href="../api/information_multivariate_api/#Associations.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a> estimator.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_mutual_informations/CMIRenyiPoczos.jl#L5-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CMITsallisPapapetrou" href="#Associations.CMITsallisPapapetrou"><code>Associations.CMITsallisPapapetrou</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMITsallisPapapetrou &lt;: ConditionalMutualInformation
CMITsallisPapapetrou(; base = 2, q = 1.5)</code></pre><p>The Tsallis-Papapetrou conditional mutual information (<a href="../references/#Papapetrou2020">Papapetrou and Kugiumtzis, 2020</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Tsallis-Papapetrou conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Tsallis-Papapetrou conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/information_multivariate_api/#Associations.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Tsallis-Papapetrou conditional mutual information is defined as </p><p class="math-container">\[I_T^q(X, Y \mid Z) = \frac{1}{1 - q} \left( 1 - \sum_{XYZ} \frac{p(x, y, z)^q}{p(x \mid z)^{q-1} p(y \mid z)^{q-1} p(z)^{q-1}} \right).\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/conditional_mutual_informations/CMITsallisPapapetrou.jl#L5-L29">source</a></section></article><h3 id="Transfer-entropy"><a class="docs-heading-anchor" href="#Transfer-entropy">Transfer entropy</a><a id="Transfer-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.TransferEntropy" href="#Associations.TransferEntropy"><code>Associations.TransferEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TransferEntropy &lt;: AssociationMeasure</code></pre><p>The supertype of all transfer entropy measures. Concrete subtypes are</p><ul><li><a href="#Associations.TEShannon"><code>TEShannon</code></a></li><li><a href="#Associations.TERenyiJizba"><code>TERenyiJizba</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/transferentropy/transfer_entropies.jl#L3-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.TEShannon" href="#Associations.TEShannon"><code>Associations.TEShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TEShannon &lt;: TransferEntropy
TEShannon(; base = 2; embedding = EmbeddingTE()) &lt;: TransferEntropy</code></pre><p>The Shannon-type transfer entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw transfer entropy.</li><li>Use with an <a href="../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I^S(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I^S(T^+; S^- | T^-, C^-)
\end{align*}\]</p><p>where <span>$I(T^+; S^- | T^-)$</span> is the Shannon conditional mutual information (<a href="#Associations.CMIShannon"><code>CMIShannon</code></a>). The <code>-</code> and <code>+</code> subscripts on the marginal variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> indicate that the embedding vectors for that marginal are constructed using present/past values and future values, respectively.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_TEShannon_EntropyDecomposition_TransferOperator">Example 1</a>:    <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.TransferOperator"><code>TransferOperator</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_TEShannon_SymbolicTransferEntropy">Example 2</a>: Estimation using the   <a href="../api/information_multivariate_api/#Associations.SymbolicTransferEntropy"><code>SymbolicTransferEntropy</code></a> estimator.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/transferentropy/TEShannon.jl#L3-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.TERenyiJizba" href="#Associations.TERenyiJizba"><code>Associations.TERenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TERenyiJizba() &lt;: TransferEntropy</code></pre><p>The Rényi transfer entropy from <a href="../references/#Jizba2012">Jizba <em>et al.</em> (2012)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw transfer entropy.</li><li>Use with an <a href="../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I_q^{R_J}(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I_q^{R_J}(T^+; S^- | T^-, C^-),
\end{align*},\]</p><p>where <span>$I_q^{R_J}(T^+; S^- | T^-)$</span> is Jizba et al. (2012)&#39;s definition of conditional mutual information (<a href="#Associations.CMIRenyiJizba"><code>CMIRenyiJizba</code></a>). The <code>-</code> and <code>+</code> subscripts on the marginal variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> indicate that the embedding vectors for that marginal are constructed using present/past values and future values, respectively.</p><p><strong>Estimation</strong></p><p>Estimating Jizba&#39;s Rényi transfer entropy is a bit complicated, since it doesn&#39;t have  a dedicated estimator. Instead, we re-write the Rényi transfer entropy as a  Rényi conditional mutual information, and estimate it using an  <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with a suitable discrete/differential Rényi entropy estimator from the list below as its input.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Sub-estimator</th><th style="text-align: left">Principle</th></tr><tr><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../api/information_single_variable_api/#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../api/discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../api/discretization_counts_probs_api/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.TransferOperator"><code>TransferOperator</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr></table><p>Any of these estimators must be given as input to a <a href="../api/information_multivariate_api/#Associations.CMIDecomposition">`CMIDecomposition</a> estimator.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_TERenyiJizba_EntropyDecomposition_TransferOperator">Example 1</a>: <a href="../api/information_multivariate_api/#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.TransferOperator"><code>TransferOperator</code></a> outcome space.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/transferentropy/TERenyiJizba.jl#L3-L54">source</a></section></article><p>The following utility functions and types are also useful for transfer entropy estimation.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.optimize_marginals_te" href="#Associations.optimize_marginals_te"><code>Associations.optimize_marginals_te</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">optimize_marginals_te([scheme = OptimiseTraditional()], s, t, [c]) → EmbeddingTE</code></pre><p>Optimize marginal embeddings for transfer entropy computation from source time series <code>s</code> to target time series <code>t</code>, conditioned on <code>c</code> if <code>c</code> is given, using the provided optimization <code>scheme</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/transferentropy/utils/OptimiseTraditional.jl#L23-L29">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.EmbeddingTE" href="#Associations.EmbeddingTE"><code>Associations.EmbeddingTE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EmbeddingTE(; dS = 1, dT = 1, dTf = 1, dC = 1, τS = -1, τT = -1, ηTf = 1, τC = -1)
EmbeddingTE(opt::OptimiseTraditional, s, t, [c])</code></pre><p><code>EmbeddingTE</code> provide embedding parameters for transfer entropy analysis using either <a href="#Associations.TEShannon"><code>TEShannon</code></a>, <a href="#Associations.TERenyiJizba"><code>TERenyiJizba</code></a>, or in general any subtype of <a href="#Associations.TransferEntropy"><code>TransferEntropy</code></a>.</p><p>The second method finds parameters using the <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/traditional/">&quot;traditional&quot;</a> optimised embedding techniques from DynamicalSystems.jl</p><p><strong>Convention for generalized delay reconstruction</strong></p><p>We use the following convention. Let <span>$s(i)$</span> be time series for the source variable, <span>$t(i)$</span> be the time series for the target variable and <span>$c(i)$</span> the time series for the conditional variable. To compute transfer entropy, we need the following marginals:</p><p class="math-container">\[\begin{aligned}
T^{+} &amp;= \{t(i+\eta^1), t(i+\eta^2), \ldots, (t(i+\eta^{d_{T^{+}}}) \} \\
T^{-} &amp;= \{ (t(i+\tau^0_{T}), t(i+\tau^1_{T}), t(i+\tau^2_{T}), \ldots, t(t + \tau^{d_{T} - 1}_{T})) \} \\
S^{-} &amp;= \{ (s(i+\tau^0_{S}), s(i+\tau^1_{S}), s(i+\tau^2_{S}), \ldots, s(t + \tau^{d_{S} - 1}_{S})) \} \\
C^{-} &amp;= \{ (c(i+\tau^0_{C}), c(i+\tau^1_{C}), c(i+\tau^2_{C}), \ldots, c(t + \tau^{d_{C} - 1}_{C})) \}
\end{aligned}\]</p><p>Depending on the application, the delay reconstruction lags <span>$\tau^k_{T} \leq 0$</span>, <span>$\tau^k_{S} \leq 0$</span>, and <span>$\tau^k_{C} \leq 0$</span> may be equally spaced, or non-equally spaced. The same applied to the prediction lag(s), but typically only a only a single predictions lag <span>$\eta^k$</span> is used (so that <span>$d_{T^{+}} = 1$</span>).</p><p>For transfer entropy, traditionally at least one <span>$\tau^k_{T}$</span>, one <span>$\tau^k_{S}$</span> and one <span>$\tau^k_{C}$</span> equals zero. This way, the <span>$T^{-}$</span>, <span>$S^{-}$</span> and <span>$C^{-}$</span> marginals always contains present/past states, while the <span>$\mathcal T$</span> marginal contain future states relative to the other marginals. However, this is not a strict requirement, and modern approaches that searches for optimal embeddings can return embeddings without the intantaneous lag.</p><p>Combined, we get the generalized delay reconstruction <span>$\mathbb{E} = (T^{+}_{(d_{T^{+}})}, T^{-}_{(d_{T})}, S^{-}_{(d_{S})}, C^{-}_{(d_{C})})$</span>. Transfer entropy is then computed as</p><p class="math-container">\[\begin{aligned}
TE_{S \rightarrow T | C} = \int_{\mathbb{E}} P(T^{+}, T^-, S^-, C^-)
\log_{b}{\left(\frac{P(T^{+} | T^-, S^-, C^-)}{P(T^{+} | T^-, C^-)}\right)},
\end{aligned}\]</p><p>or, if conditionals are not relevant,</p><p class="math-container">\[\begin{aligned}
TE_{S \rightarrow T} = \int_{\mathbb{E}} P(T^{+}, T^-, S^-)
\log_{b}{\left(\frac{P(T^{+} | T^-, S^-)}{P(T^{+} | T^-)}\right)},
\end{aligned}\]</p><p>Here,</p><ul><li><span>$T^{+}$</span> denotes the <span>$d_{T^{+}}$</span>-dimensional set of vectors furnishing the future   states of <span>$T$</span> (almost always equal to 1 in practical applications),</li><li><span>$T^{-}$</span> denotes the <span>$d_{T}$</span>-dimensional set of vectors furnishing the past and   present states of <span>$T$</span>,</li><li><span>$S^{-}$</span> denotes the <span>$d_{S}$</span>-dimensional set of vectors furnishing the past and   present of <span>$S$</span>, and</li><li><span>$C^{-}$</span> denotes the <span>$d_{C}$</span>-dimensional set of vectors furnishing the past and   present of <span>$C$</span>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>dS</code>, <code>dT</code>, <code>dC</code>, <code>dTf</code> (<code>f</code> for <em>future</em>) are the dimensions of the <span>$S^{-}$</span>,   <span>$T^{-}$</span>, <span>$C^{-}$</span> and <span>$T^{+}$</span> marginals. The parameters <code>dS</code>, <code>dT</code>, <code>dC</code> and <code>dTf</code>   must each be a <em>positive</em> integer number.</li><li><code>τS</code>, <code>τT</code>, <code>τC</code> are the embedding lags for <span>$S^{-}$</span>, <span>$T^{-}$</span>, <span>$C^{-}$</span>.   Each parameter are integers <code>∈ 𝒩⁰⁻</code>, or a vector of integers <code>∈ 𝒩⁰⁻</code>, so   that <span>$S^{-}$</span>, <span>$T^{-}$</span>, <span>$C^{-}$</span> always represents present/past values.   If e.g. <code>τT</code> is an integer, then for the <span>$T^-$</span> marginal is constructed using   lags <span>$\tau_{T} = \{0, \tau, 2\tau, \ldots, (d_{T}- 1)\tau_T \}$</span>.   If is a vector, e.g. <code>τΤ = [-1, -5, -7]</code>, then the dimension <code>dT</code> must match the lags,   and precisely those lags are used: <span>$\tau_{T} = \{-1, -5, -7 \}$</span>.</li><li>The prediction lag(s) <code>ηTf</code> is a positive integer. Combined with the requirement   that the other delay parameters are zero or negative, this ensures that we&#39;re   always predicting from past/present to future. In typical applications,   <code>ηTf = 1</code> is used for transfer entropy.</li></ul><p><strong>Examples</strong></p><p>Say we wanted to compute the Shannon transfer entropy <span>$TE^S(S \to T) = I^S(T^+; S^- | T^-)$</span>. Using some modern procedure for determining optimal embedding parameters using <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/unified/">methods from DynamicalSystems.jl</a>, we find that the optimal embedding of <span>$T^{-}$</span> is three-dimensional and is given by the lags <code>[0, -5, -8]</code>. Using the same procedure, we find that the optimal embedding of <span>$S^{-}$</span> is two-dimensional with lags <span>$[-1, -8]$</span>. We want to predicting a univariate version of the target variable one time step into the future (<code>ηTf = 1</code>). The total embedding is then the set of embedding vectors</p><p><span>$E_{TE} = \{ (T(i+1), S(i-1), S(i-8), T(i), T(i-5), T(i-8)) \}$</span>. Translating this to code, we get:</p><pre><code class="language-julia-repl hljs">using Associations
julia&gt; EmbeddingTE(dT=3, τT=[0, -5, -8], dS=2, τS=[-1, -4], ηTf=1)

# output
EmbeddingTE(dS=2, dT=3, dC=1, dTf=1, τS=[-1, -4], τT=[0, -5, -8], τC=-1, ηTf=1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/transferentropy/embedding.jl#L3-L113">source</a></section></article><h3 id="Partial-mutual-information"><a class="docs-heading-anchor" href="#Partial-mutual-information">Partial mutual information</a><a id="Partial-mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Partial-mutual-information" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.PartialMutualInformation" href="#Associations.PartialMutualInformation"><code>Associations.PartialMutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PartialMutualInformation &lt;: MultivariateInformationMeasure
PartialMutualInformation(; base = 2)</code></pre><p>The partial mutual information (PMI) measure of conditional association (<a href="../references/#Zhao2016">Zhao <em>et al.</em>, 2016</a>).</p><p><strong>Definition</strong></p><p>PMI is defined as for variables <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span> as</p><p class="math-container">\[PMI(X; Y | Z) = D(p(x, y, z) || p^{*}(x|z) p^{*}(y|z) p(z)),\]</p><p>where <span>$p(x, y, z)$</span> is the joint distribution for <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span>, and <span>$D(\cdot, \cdot)$</span> is the extended Kullback-Leibler divergence from <span>$p(x, y, z)$</span> to <span>$p^{*}(x|z) p^{*}(y|z) p(z)$</span>. See <a href="../references/#Zhao2016">Zhao <em>et al.</em> (2016)</a> for details.</p><p><strong>Estimation</strong></p><p>The PMI is estimated by first estimating a 3D probability mass function using  <a href="../api/counts_and_probabilities_api/#ComplexityMeasures.probabilities-Tuple{OutcomeSpace}"><code>probabilities</code></a>, then computing <span>$PMI(X; Y | Z)$</span> from those probaiblities.</p><p><strong>Properties</strong></p><p>For the discrete case, the following identities hold in theory (when estimating PMI, they may not).</p><ul><li><code>PMI(X, Y, Z) &gt;= CMI(X, Y, Z)</code> (where CMI is the Shannon CMI). Holds in theory, but   when estimating PMI, the identity may not hold.</li><li><code>PMI(X, Y, Z) &gt;= 0</code>. Holds both in theory and when estimating using discrete estimators.</li><li><code>X ⫫ Y | Z =&gt; PMI(X, Y, Z) = CMI(X, Y, Z) = 0</code> (in theory, but not necessarily for   estimation).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/information/definitions/partial_mutual_information/partial_mutual_information.jl#L3-L36">source</a></section></article><h2 id="correlation_api"><a class="docs-heading-anchor" href="#correlation_api">Correlation measures</a><a id="correlation_api-1"></a><a class="docs-heading-anchor-permalink" href="#correlation_api" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CorrelationMeasure" href="#Associations.CorrelationMeasure"><code>Associations.CorrelationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CorrelationMeasure &lt;: AssociationMeasure end</code></pre><p>The supertype for correlation measures.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#Associations.PearsonCorrelation"><code>PearsonCorrelation</code></a></li><li><a href="#Associations.PartialCorrelation"><code>PartialCorrelation</code></a></li><li><a href="#Associations.DistanceCorrelation"><code>DistanceCorrelation</code></a></li><li><a href="#Associations.ChatterjeeCorrelation"><code>ChatterjeeCorrelation</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/correlation/correlation.jl#L3-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.PearsonCorrelation" href="#Associations.PearsonCorrelation"><code>Associations.PearsonCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PearsonCorrelation</code></pre><p>The Pearson correlation of two variables.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Pearson correlation coefficient.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence   using the Pearson correlation coefficient.</li></ul><p><strong>Description</strong></p><p>The sample <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation coefficient</a> for real-valued random variables <span>$X$</span> and <span>$Y$</span> with associated samples <span>$\{x_i\}_{i=1}^N$</span> and <span>$\{y_i\}_{i=1}^N$</span> is defined as</p><p class="math-container">\[\rho_{xy} = \dfrac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }{\sqrt{\sum_{i=1}^N (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^N (y_i - \bar{y})^2}},\]</p><p>where <span>$\bar{x}$</span> and <span>$\bar{y}$</span> are the means of the observations <span>$x_k$</span> and <span>$y_k$</span>, respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/correlation/pearson_correlation.jl#L3-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.PartialCorrelation" href="#Associations.PartialCorrelation"><code>Associations.PartialCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PartialCorrelation &lt;: AssociationMeasure</code></pre><p>The correlation of two variables, with the effect of a set of conditioning variables removed.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw partial correlation coefficient.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for   correlated-based conditional independence.</li></ul><p><strong>Description</strong></p><p>There are several ways of estimating the partial correlation. We follow the <a href="https://en.wikipedia.org/wiki/Partial_correlation">matrix inversion method</a>, because for <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s, we can very efficiently compute the required joint covariance matrix <span>$\Sigma$</span> for the random variables.</p><p>Formally, let <span>$X_1, X_2, \ldots, X_n$</span> be a set of <span>$n$</span> real-valued random variables. Consider the joint precision matrix,<span>$P = (p_{ij}) = \Sigma^-1$</span>. The partial correlation of any pair of variables <span>$(X_i, X_j)$</span>, given the remaining variables <span>$\bf{Z} = \{X_k\}_{i=1, i \neq i, j}^n$</span>, is defined as</p><p class="math-container">\[\rho_{X_i X_j | \bf{Z}} = -\dfrac{p_ij}{\sqrt{ p_{ii} p_{jj} }}\]</p><p>In practice, we compute the estimate</p><p class="math-container">\[\hat{\rho}_{X_i X_j | \bf{Z}} =
-\dfrac{\hat{p}_ij}{\sqrt{ \hat{p}_{ii} \hat{p}_{jj} }},\]</p><p>where <span>$\hat{P} = \hat{\Sigma}^{-1}$</span> is the sample precision matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/correlation/partial_correlation.jl#L3-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.DistanceCorrelation" href="#Associations.DistanceCorrelation"><code>Associations.DistanceCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DistanceCorrelation</code></pre><p>The distance correlation (<a href="../references/#Szekely2007">Székely <em>et al.</em>, 2007</a>) measure quantifies potentially nonlinear associations between pairs of variables. If applied to three variables, the partial distance correlation (<a href="../references/#Szekely2014">Székely and Rizzo, 2014</a>) is computed.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw (partial) distance correlation   coefficient.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for   pairwise dependence.</li></ul><p><strong>Description</strong></p><p>The distance correlation can be used to compute the association between two variables, or the conditional association between three variables, like so:</p><pre><code class="nohighlight hljs">association(DistanceCorrelation(), x, y) → dcor ∈ [0, 1]
association(DistanceCorrelation(), x, y, z) → pdcor</code></pre><p>With two variable, we comptue <code>dcor</code>, which is called the empirical/sample distance  correlation (<a href="../references/#Szekely2007">Székely <em>et al.</em>, 2007</a>). With three variables, the  partial distance correlation <code>pdcor</code> is computed (<a href="../references/#Szekely2014">Székely and Rizzo, 2014</a>).</p><div class="admonition is-category-warn"><header class="admonition-header">Warn</header><div class="admonition-body"><p>A partial distance correlation <code>distance_correlation(X, Y, Z) = 0</code> doesn&#39;t always guarantee conditional independence <code>X ⫫ Y | Z</code>. <a href="../references/#Szekely2014">Székely and Rizzo (2014)</a> for an in-depth discussion.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/correlation/distance_correlation.jl#L8-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ChatterjeeCorrelation" href="#Associations.ChatterjeeCorrelation"><code>Associations.ChatterjeeCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChatterjeeCorrelation &lt;: CorrelationMeasure
ChatterjeeCorrelation(; handle_ties = true, rng = Random.default_rng())</code></pre><p>The Chatterjee correlation measure (<a href="../references/#Chatterjee2021">Chatterjee, 2021</a>) is an asymmetric measure of dependence between two variables. </p><div class="admonition is-info"><header class="admonition-header">Speeding up computations</header><div class="admonition-body"><p>If <code>handle_ties == true</code>, then the first formula below is used. If you know for sure that there are no ties in your data, then set <code>handle_ties == false</code>,  which will use the second (faster) formula below.</p></div></div><div class="admonition is-info"><header class="admonition-header">Randomization and reproducibility</header><div class="admonition-body"><p>When rearranging the input datasets, the second variable <code>y</code> is sorted  according to a sorting of the first variable <code>x</code>. If <code>x</code> has ties, then  these ties are broken randomly and uniformly. For complete reproducibility in  this step, you can specify <code>rng</code>. If <code>x</code> has no ties, then no randomization  is performed.</p></div></div><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw Chatterjee correlation coefficient.</li><li>Use with <a href="../independence/#Associations.SurrogateAssociationTest"><code>SurrogateAssociationTest</code></a> to perform a surrogate test for significance    of a Chatterjee-type association (<a href="../examples/examples_independence/#example_SurrogateAssociationTest_ChatterjeeCorrelation">example</a>).    When using a surrogate test for significance, the <em>first</em> input variable is shuffled    according to the given surrogate method.</li></ul><p><strong>Description</strong></p><p>The correlation statistic is defined as</p><p class="math-container">\[\epsilon_n(X, Y) = 
1 - \dfrac{n\sum_{i=1}^{n-1} |r_{i+1} - r_i|}{2\sum_{i=1}^n }.\]</p><p>When there are no ties among the <span>$Y_1, Y_2, \ldots, Y_n$</span>, the  measure is </p><p class="math-container">\[\epsilon_n(X, Y) = 
1 - \dfrac{3\sum_{i=1}^{n-1} |r_{i+1} - r_i|}{n^2 - 1}.\]</p><p>This statistic estimates a quantity proposed by <a href="../references/#Dette2013">Dette <em>et al.</em> (2013)</a>, as indicated in  <a href="../references/#Shi2022">Shi <em>et al.</em> (2022)</a>. It can therefore also be called the Chatterjee-Dette-Siburg-Stoimenov correlation coefficient.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ChatterjeeCorrelation">Example 1</a>. Estimating the Chatterjee correlation coefficient   for independent and for dependent variables. </li><li><a href="../examples/examples_independence/#example_SurrogateAssociationTest_ChatterjeeCorrelation">Example 2</a>. Testing the significance   of a Chatterjee-type association using a surrogate test.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/correlation/chatterjee.jl#L7-L61">source</a></section></article><h2 id="cross_map_api"><a class="docs-heading-anchor" href="#cross_map_api">Cross-map measures</a><a id="cross_map_api-1"></a><a class="docs-heading-anchor-permalink" href="#cross_map_api" title="Permalink"></a></h2><p>The cross-map measures define different ways of quantifying association based on the  concept of &quot;cross mapping&quot;, which has appeared in many contexts in the literature, and gained huge popularity with  <a href="../references/#Sugihara2012">Sugihara <em>et al.</em> (2012)</a>&#39;s on <em>convergent cross mapping</em>.</p><p>Since their paper, several cross mapping methods and frameworks have emerged in the literature. In Associations.jl, we provide a unified interface for using these cross mapping methods.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CrossmapMeasure" href="#Associations.CrossmapMeasure"><code>Associations.CrossmapMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CrossmapMeasure &lt;: AssociationMeasure</code></pre><p>The supertype for all cross-map measures. Concrete subtypes are</p><ul><li><a href="#Associations.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a>, or <code>CCM</code> for short.</li><li><a href="#Associations.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a>, or <code>PAI</code> for short.</li></ul><p>See also: <a href="../api/cross_map_api/#Associations.CrossmapEstimator"><code>CrossmapEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/crossmappings/crossmappings.jl#L12-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ConvergentCrossMapping" href="#Associations.ConvergentCrossMapping"><code>Associations.ConvergentCrossMapping</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConvergentCrossMapping &lt;: CrossmapMeasure
ConvergentCrossMapping(; d::Int = 2, τ::Int = -1, w::Int = 0,
    f = Statistics.cor, embed_warn = true)</code></pre><p>The convergent cross mapping measure (<a href="../references/#Sugihara2012">Sugihara <em>et al.</em>, 2012</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> together with a <a href="../api/cross_map_api/#Associations.CrossmapEstimator"><code>CrossmapEstimator</code></a> to compute the    cross-map correlation between input variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/cross_map_api/#Associations.RandomSegment"><code>RandomSegment</code></a></li><li><a href="../api/cross_map_api/#Associations.RandomVectors"><code>RandomVectors</code></a></li><li><a href="../api/cross_map_api/#Associations.ExpandingSegment"><code>ExpandingSegment</code></a></li></ul><p><strong>Description</strong></p><p>The Theiler window <code>w</code> controls how many temporal neighbors are excluded during neighbor  searches (<code>w = 0</code> means that only the point itself is excluded). <code>f</code> is a function that computes the agreement between observations and predictions (the default, <code>f = Statistics.cor</code>, gives the Pearson correlation coefficient).</p><p><strong>Embedding</strong></p><p>Let <code>S(i)</code> be the source time series variable and <code>T(i)</code> be the target time series variable. This version produces regular embeddings with fixed dimension <code>d</code> and embedding lag <code>τ</code> as follows:</p><p class="math-container">\[( S(i), S(i+\tau), S(i+2\tau), \ldots, S(i+(d-1)\tau, T(i))_{i=1}^{N-(d-1)\tau}.\]</p><p>In this joint embedding, neighbor searches are performed in the subspace spanned by the first <code>D-1</code> variables, while the last (<code>D</code>-th) variable is to be predicted.</p><p>With this convention, <code>τ &lt; 0</code> implies &quot;past/present values of source used to predict target&quot;, and <code>τ &gt; 0</code> implies &quot;future/present values of source used to predict target&quot;. The latter case may not be meaningful for many applications, so by default, a warning will be given if <code>τ &gt; 0</code> (<code>embed_warn = false</code> turns off warnings).</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConvergentCrossMapping_RandomVectors">Example 1</a>.    Estimation with <a href="../api/cross_map_api/#Associations.RandomVectors"><code>RandomVectors</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_ConvergentCrossMapping_RandomSegment">Example 2</a>.    Estimation with <a href="../api/cross_map_api/#Associations.RandomSegment"><code>RandomSegment</code></a> estimator.</li><li><a href="../api/cross_map_api/#example_ConvergentCrossMapping_reproducing_sugihara">Example 3</a>: Reproducing    figures from <a href="../references/#Sugihara2012">Sugihara <em>et al.</em> (2012)</a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/crossmappings/ccm-like/ConvergentCrossMapping.jl#L6-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.PairwiseAsymmetricInference" href="#Associations.PairwiseAsymmetricInference"><code>Associations.PairwiseAsymmetricInference</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PairwiseAsymmetricInference &lt;: CrossmapMeasure
PairwiseAsymmetricInference(; d::Int = 2, τ::Int = -1, w::Int = 0,
    f = Statistics.cor, embed_warn = true)</code></pre><p>The pairwise asymmetric inference (PAI) measure (<a href="../references/#McCracken2014">McCracken and Weigel, 2014</a>) is a version of <a href="#Associations.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a> that searches for neighbors in <em>mixed</em> embeddings (i.e. both source and target variables included); otherwise, the algorithms are identical.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the pairwise asymmetric inference measure    between variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="../api/cross_map_api/#Associations.RandomSegment"><code>RandomSegment</code></a></li><li><a href="../api/cross_map_api/#Associations.RandomVectors"><code>RandomVectors</code></a></li><li><a href="../api/cross_map_api/#Associations.ExpandingSegment"><code>ExpandingSegment</code></a></li></ul><p><strong>Description</strong></p><p>The Theiler window <code>w</code> controls how many temporal neighbors are excluded during neighbor  searches (<code>w = 0</code> means that only the point itself is excluded). <code>f</code> is a function that computes the agreement between observations and predictions (the default, <code>f = Statistics.cor</code>, gives the Pearson correlation coefficient).</p><p><strong>Embedding</strong></p><p>There are many possible ways of defining the embedding for PAI. Currently, we only implement the <em>&quot;add one non-lagged source timeseries to an embedding of the target&quot;</em> approach, which is used as an example in McCracken &amp; Weigel&#39;s paper. Specifically: Let <code>S(i)</code> be the source time series variable and <code>T(i)</code> be the target time series variable. <code>PairwiseAsymmetricInference</code> produces regular embeddings with fixed dimension <code>d</code> and embedding lag <code>τ</code> as follows:</p><p class="math-container">\[(S(i), T(i+(d-1)\tau, \ldots, T(i+2\tau), T(i+\tau), T(i)))_{i=1}^{N-(d-1)\tau}.\]</p><p>In this joint embedding, neighbor searches are performed in the subspace spanned by the first <code>D</code> variables, while the last variable is to be predicted.</p><p>With this convention, <code>τ &lt; 0</code> implies &quot;past/present values of source used to predict target&quot;, and <code>τ &gt; 0</code> implies &quot;future/present values of source used to predict target&quot;. The latter case may not be meaningful for many applications, so by default, a warning will be given if <code>τ &gt; 0</code> (<code>embed_warn = false</code> turns off warnings).</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_PairwiseAsymmetricInference_RandomVectors">Example 1</a>.    Estimation with <a href="../api/cross_map_api/#Associations.RandomVectors"><code>RandomVectors</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_PairwiseAsymmetricInference_RandomSegment">Example 2</a>.    Estimation with <a href="../api/cross_map_api/#Associations.RandomSegment"><code>RandomSegment</code></a> estimator.</li><li><a href="../api/cross_map_api/#example_PairwiseAsymmetricInference_reproduce_mccracken">Example 3</a>. Reproducing    McCracken &amp; Weigel&#39;s results from the original paper.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/crossmappings/ccm-like/PairwiseAsymmetricInference.jl#L6-L64">source</a></section></article><h2 id="closeness_api"><a class="docs-heading-anchor" href="#closeness_api">Closeness measures</a><a id="closeness_api-1"></a><a class="docs-heading-anchor-permalink" href="#closeness_api" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ClosenessMeasure" href="#Associations.ClosenessMeasure"><code>Associations.ClosenessMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClosenessMeasure &lt;: AssociationMeasure</code></pre><p>The supertype for all multivariate information-based measure definitions.</p><p><strong>Implementations</strong></p><ul><li><a href="#Associations.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></li><li><a href="#Associations.SMeasure"><code>SMeasure</code></a></li><li><a href="#Associations.HMeasure"><code>HMeasure</code></a></li><li><a href="#Associations.MMeasure"><code>MMeasure</code></a></li><li><a href="#Associations.LMeasure"><code>LMeasure</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/closeness/closeness.jl#L2-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.JointDistanceDistribution" href="#Associations.JointDistanceDistribution"><code>Associations.JointDistanceDistribution</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointDistanceDistribution &lt;: AssociationMeasure end
JointDistanceDistribution(; metric = Euclidean(), B = 10, D = 2, τ = -1, μ = 0.0)</code></pre><p>The joint distance distribution (JDD) measure (<a href="../references/#Amigo2018">Amigó and Hirata, 2018</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the joint distance distribution measure <code>Δ</code> from   <a href="../references/#Amigo2018">Amigó and Hirata (2018)</a>.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for directional   dependence.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><strong><code>distance_metric::Metric</code></strong>: An instance of a valid distance metric from <code>Distances.jl</code>.   Defaults to <code>Euclidean()</code>.</li><li><strong><code>B::Int</code></strong>: The number of equidistant subintervals to divide the interval <code>[0, 1]</code> into   when comparing the normalised distances.</li><li><strong><code>D::Int</code></strong>: Embedding dimension.</li><li><strong><code>τ::Int</code></strong>: Embedding delay. By convention, <code>τ</code> is negative.</li><li><strong><code>μ</code></strong>: The hypothetical mean value of the joint distance distribution if there   is no coupling between <code>x</code> and <code>y</code> (default is <code>μ = 0.0</code>).</li></ul><p><strong>Description</strong></p><p>From input time series <span>$x(t)$</span> and <span>$y(t)$</span>, we first construct the delay embeddings (note the positive sign in the embedding lags; therefore the input parameter <code>τ</code> is by convention negative).</p><p class="math-container">\[\begin{align*}
\{\bf{x}_i \} &amp;= \{(x_i, x_{i+\tau}, \ldots, x_{i+(d_x - 1)\tau}) \} \\
\{\bf{y}_i \} &amp;= \{(y_i, y_{i+\tau}, \ldots, y_{i+(d_y - 1)\tau}) \} \\
\end{align*}\]</p><p>The algorithm then proceeds to analyze the distribution of distances between points of these embeddings, as described in <a href="../references/#Amigo2018">Amigó and Hirata (2018)</a>.</p><p><strong>Examples</strong></p><ul><li><a href="../examples/examples_independence/#examples_independence_JointDistanceDistributionTest">Independence testing using JDD</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/closeness/JointDistanceDistribution.jl#L18-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.SMeasure" href="#Associations.SMeasure"><code>Associations.SMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SMeasure &lt; ClosenessMeasure
SMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p><code>SMeasure</code> is a bivariate association measure from <a href="../references/#Arnhold1999">Arnhold <em>et al.</em> (1999)</a> and <a href="../references/#Quiroga2000">Quiroga <em>et al.</em> (2000)</a> that measure directional dependence between two input (potentially multivariate) time series.</p><p>Note that <code>τx</code> and <code>τy</code> are negative; see explanation below.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw s-measure statistic.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p>The steps of the algorithm are:</p><ol><li>From input time series <span>$x(t)$</span> and <span>$y(t)$</span>, construct the delay embeddings (note  the positive sign in the embedding lags; therefore inputs parameters  <code>τx</code> and <code>τy</code> are by convention negative).</li></ol><p class="math-container">\[\begin{align*}
\{\bf{x}_i \} &amp;= \{(x_i, x_{i+\tau_x}, \ldots, x_{i+(d_x - 1)\tau_x}) \} \\
\{\bf{y}_i \} &amp;= \{(y_i, y_{i+\tau_y}, \ldots, y_{i+(d_y - 1)\tau_y}) \} \\
\end{align*}\]</p><ol><li><p>Let <span>$r_{i,j}$</span> and <span>$s_{i,j}$</span> be the indices of the <code>K</code>-th nearest neighbors  of <span>$\bf{x}_i$</span> and <span>$\bf{y}_i$</span>, respectively. Neighbors closed than <code>w</code> time indices  are excluded during searches (i.e. <code>w</code> is the Theiler window).</p></li><li><p>Compute the the mean squared Euclidean distance to the <span>$K$</span> nearest neighbors  for each <span>$x_i$</span>, using the indices <span>$r_{i, j}$</span>.</p></li></ol><p class="math-container">\[R_i^{(k)}(x) = \dfrac{1}{k} \sum_{i=1}^{k}(\bf{x}_i, \bf{x}_{r_{i,j}})^2\]</p><ul><li>Compute the y-conditioned mean squared Euclidean distance to the <span>$K$</span> nearest   neighbors for each <span>$x_i$</span>, now using the indices <span>$s_{i,j}$</span>.</li></ul><p class="math-container">\[R_i^{(k)}(x|y) = \dfrac{1}{k} \sum_{i=1}^{k}(\bf{x}_i, \bf{x}_{s_{i,j}})^2\]</p><ul><li>Define the following measure of independence, where <span>$0 \leq S \leq 1$</span>, and   low values indicate independence and values close to one occur for   synchronized signals.</li></ul><p class="math-container">\[S^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N} \dfrac{R_i^{(k)}(x)}{R_i^{(k)}(x|y)}\]</p><p><strong>Input data</strong></p><p>The algorithm is slightly modified from (<a href="../references/#Arnhold1999">Arnhold <em>et al.</em>, 1999</a>) to allow univariate timeseries as input.</p><ul><li>If <code>x</code> and <code>y</code> are <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s then use <code>x</code> and <code>y</code> as is and ignore the parameters   <code>dx</code>/<code>τx</code> and <code>dy</code>/<code>τy</code>.</li><li>If <code>x</code> and <code>y</code> are scalar time series, then create <code>dx</code> and <code>dy</code> dimensional embeddings,   respectively, of both <code>x</code> and <code>y</code>, resulting in <code>N</code> different <code>m</code>-dimensional embedding points   <span>$X = \{x_1, x_2, \ldots, x_N \}$</span> and <span>$Y = \{y_1, y_2, \ldots, y_N \}$</span>.   <code>τx</code> and <code>τy</code> control the embedding lags for <code>x</code> and <code>y</code>.</li><li>If <code>x</code> is a scalar-valued vector and <code>y</code> is a <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, or vice versa,   then create an embedding of the scalar timeseries using parameters <code>dx</code>/<code>τx</code> or <code>dy</code>/<code>τy</code>.</li></ul><p>In all three cases, input StateSpaceSets are length-matched by eliminating points at the end of the longest StateSpaceSet (after the embedding step, if relevant) before analysis.</p><p>See also: <a href="#Associations.ClosenessMeasure"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/closeness/SMeasure.jl#L9-L82">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.HMeasure" href="#Associations.HMeasure"><code>Associations.HMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HMeasure &lt;: AssociationMeasure
HMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>HMeasure</code> (<a href="../references/#Arnhold1999">Arnhold <em>et al.</em>, 1999</a>) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#Associations.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw h-measure statistic.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p>The <code>HMeasure</code> (<a href="../references/#Arnhold1999">Arnhold <em>et al.</em>, 1999</a>) is similar to the <a href="#Associations.SMeasure"><code>SMeasure</code></a>, but the numerator of the formula is replaced by <span>$R_i(x)$</span>, the mean squared Euclidean distance to <em>all other points</em>, and there is a <span>$\log$</span>-term inside the sum:</p><p class="math-container">\[H^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{R_i(x)}{R_i^{(k)}(x|y)} \right).\]</p><p>Parameters are the same and <span>$R_i^{(k)}(x|y)$</span> is computed as for <a href="#Associations.SMeasure"><code>SMeasure</code></a>.</p><p>See also: <a href="#Associations.ClosenessMeasure"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/closeness/HMeasure.jl#L9-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MMeasure" href="#Associations.MMeasure"><code>Associations.MMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MMeasure &lt;: ClosenessMeasure
MMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>MMeasure</code> (<a href="../references/#Andrzejak2003">Andrzejak <em>et al.</em>, 2003</a>) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#Associations.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw m-measure statistic.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p>The <code>MMeasure</code> is based on <a href="#Associations.SMeasure"><code>SMeasure</code></a> and <a href="#Associations.HMeasure"><code>HMeasure</code></a>. It is given by</p><p class="math-container">\[M^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{R_i(x) - R_i^{(k)}(x|y)}{R_i(x) - R_i^k(x)} \right),\]</p><p>where <span>$R_i(x)$</span> is computed as for <a href="#Associations.HMeasure"><code>HMeasure</code></a>, while <span>$R_i^k(x)$</span> and <span>$R_i^{(k)}(x|y)$</span> is computed as for <a href="#Associations.SMeasure"><code>SMeasure</code></a>. Parameters also have the same meaning as for <a href="#Associations.SMeasure"><code>SMeasure</code></a>/<a href="#Associations.HMeasure"><code>HMeasure</code></a>.</p><p>See also: <a href="#Associations.ClosenessMeasure"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/closeness/MMeasure.jl#L9-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.LMeasure" href="#Associations.LMeasure"><code>Associations.LMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LMeasure &lt;: ClosenessMeasure
LMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>LMeasure</code> (<a href="../references/#Chicharro2009">Chicharro and Andrzejak, 2009</a>) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#Associations.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw L-measure statistic.</li><li>Use with <a href="../independence/#Associations.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p><code>LMeasure</code> is similar to <a href="#Associations.MMeasure"><code>MMeasure</code></a>, but uses distance ranks instead of the raw distances.</p><p>Let <span>$\bf{x_i}$</span> be an embedding vector, and let <span>$g_{i,j}$</span> denote the rank that the distance between <span>$\bf{x_i}$</span> and some other vector <span>$\bf{x_j}$</span> in a sorted ascending list of distances between <span>$\bf{x_i}$</span> and <span>$\bf{x_{i \neq j}}$</span> In other words, <span>$g_{i,j}$</span> this is just the <span>$N-1$</span> nearest neighbor distances sorted )</p><p><code>LMeasure</code> is then defined as</p><p class="math-container">\[L^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{G_i(x) - G_i^{(k)}(x|y)}{G_i(x) - G_i^k(x)} \right),\]</p><p>where <span>$G_i(x) = \frac{N}{2}$</span> and <span>$G_i^K(x) = \frac{k+1}{2}$</span> are the mean and minimal rank, respectively.</p><p>The <span>$y$</span>-conditioned mean rank is defined as</p><p class="math-container">\[G_i^{(k)}(x|y) = \dfrac{1}{K}\sum_{j=1}^{K} g_{i,w_{i, j}},\]</p><p>where <span>$w_{i,j}$</span> is the index of the <span>$j$</span>-th nearest neighbor of <span>$\bf{y_i}$</span>.</p><p>See also: <a href="#Associations.ClosenessMeasure"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/closeness/LMeasure.jl#L9-L54">source</a></section></article><h2 id="Recurrence-measures"><a class="docs-heading-anchor" href="#Recurrence-measures">Recurrence measures</a><a id="Recurrence-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrence-measures" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MCR" href="#Associations.MCR"><code>Associations.MCR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MCR &lt;: AssociationMeasure
MCR(; r, metric = Euclidean())</code></pre><p>An association measure based on mean conditional probabilities of recurrence (MCR) introduced by <a href="../references/#Romano2007">Romano <em>et al.</em> (2007)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw MCR for pairwise or conditional association.</li><li>Use with <a href="../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to perform a formal hypothesis test for pairwise   or conditional association.</li></ul><p><strong>Description</strong></p><p><code>r</code> is  mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of <code>AbstractRecurrenceType</code> from <a href="https://juliadynamics.github.io/RecurrenceAnalysis.jl/stable/">RecurrenceAnalysis.jl</a>. To use any <code>r</code> that is not a real number, you have to do <code>using RecurrenceAnalysis</code> first. The <code>metric</code> is any valid metric from <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a>.</p><p>For input variables <code>X</code> and <code>Y</code>, the conditional probability of recurrence is defined as</p><p class="math-container">\[M(X | Y) = \dfrac{1}{N} \sum_{i=1}^N p(\bf{y_i} | \bf{x_i}) =
\dfrac{1}{N} \sum_{i=1}^N \dfrac{\sum_{i=1}^N J_{R_{i, j}}^{X, Y}}{\sum_{i=1}^N R_{i, j}^X},\]</p><p>where <span>$R_{i, j}^X$</span> is the recurrence matrix and <span>$J_{R_{i, j}}^{X, Y}$</span> is the joint recurrence matrix, constructed using the given <code>metric</code>. The measure <span>$M(Y | X)$</span> is defined analogously.</p><p><a href="../references/#Romano2007">Romano <em>et al.</em> (2007)</a>&#39;s interpretation of this quantity is that if <code>X</code> drives <code>Y</code>, then <code>M(X|Y) &gt; M(Y|X)</code>, if <code>Y</code> drives <code>X</code>, then <code>M(Y|X) &gt; M(X|Y)</code>, and if coupling is symmetric,  then <code>M(Y|X) = M(X|Y)</code>.</p><p><strong>Input data</strong></p><p><code>X</code> and <code>Y</code> can be either both univariate timeseries, or both multivariate <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MCR">Example 1</a>. Pairwise versus conditional MCR.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/recurrence/MCR.jl#L7-L56">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.RMCD" href="#Associations.RMCD"><code>Associations.RMCD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMCD &lt;: AssociationMeasure
RMCD(; r, metric = Euclidean(), base = 2)</code></pre><p>The recurrence measure of conditional dependence, or RMCD (<a href="../references/#Ramos2017">Ramos <em>et al.</em>, 2017</a>), is a recurrence-based measure that mimics the conditional mutual information, but uses recurrence probabilities.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#Associations.association"><code>association</code></a> to compute the raw RMCD for pairwise    or conditional association.</li><li>Use with <a href="../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to perform a formal hypothesis test for pairwise   or conditional association.</li></ul><p><strong>Description</strong></p><p><code>r</code> is a mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of <code>AbstractRecurrenceType</code> from <a href="https://juliadynamics.github.io/RecurrenceAnalysis.jl/stable/">RecurrenceAnalysis.jl</a>. To use any <code>r</code> that is not a real number, you have to do <code>using RecurrenceAnalysis</code> first. The <code>metric</code> is any valid metric from <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a>.</p><p>Both the pairwise and conditional RMCD is non-negative, but due to round-off error, negative values may occur. If that happens, an RMCD value of <code>0.0</code> is returned.</p><p><strong>Description</strong></p><p>The RMCD measure is defined by</p><p class="math-container">\[I_{RMCD}(X; Y | Z) = \dfrac{1}{N}
\sum_{i} \left[
\dfrac{1}{N} \sum_{j} R_{ij}^{X, Y, Z}
\log \left(
    \dfrac{\sum_{j} R_{ij}^{X, Y, Z} \sum_{j} R_{ij}^{Z} }{\sum_{j} \sum_{j} R_{ij}^{X, Z} \sum_{j} \sum_{j} R_{ij}^{Y, Z}}
    \right)
\right],\]</p><p>where  <code>base</code> controls the base of the logarithm. <span>$I_{RMCD}(X; Y | Z)$</span> is zero when <span>$Z = X$</span>, <span>$Z = Y$</span> or when <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span> are mutually independent.</p><p>Our implementation allows dropping the third/last argument, in which case the following mutual information-like quantitity is computed (not discussed in <a href="../references/#Ramos2017">Ramos <em>et al.</em> (2017)</a>.</p><p class="math-container">\[I_{RMCD}(X; Y) = \dfrac{1}{N}
\sum_{i} \left[
\dfrac{1}{N} \sum_{j} R_{ij}^{X, Y}
\log \left(
    \dfrac{\sum_{j} R_{ij}^{X}  R_{ij}^{Y} }{\sum_{j} R_{ij}^{X, Y}}
    \right)
\right]\]</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_RMCD">Example 1</a>. Pairwise versus conditional RMCD.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/cc2765e3f2cd50c44f32ac40d54b730d589eb9ac/src/methods/recurrence/RMCD.jl#L5-L68">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Associations.jl</a><a class="docs-footer-nextpage" href="../independence/">Independence »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 1 August 2024 08:55">Thursday 1 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
