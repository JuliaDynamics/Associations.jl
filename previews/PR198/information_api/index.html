<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Information API · CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="CausalityTools.jl logo"/></a><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../independence/">Independence testing</a></li><li><a class="tocitem" href="../correlation_measures/">Correlation measures</a></li><li><a class="tocitem" href="../closeness_measures/">Closeness measures</a></li><li><a class="tocitem" href="../information_measures/">Information measures</a></li><li><span class="tocitem">APIs and estimators</span><ul><li class="is-active"><a class="tocitem" href>Information API</a><ul class="internal"><li><a class="tocitem" href="#information_measures"><span>API &amp; design</span></a></li><li><a class="tocitem" href="#Mutual-information"><span>Mutual information</span></a></li><li><a class="tocitem" href="#Conditional-mutual-information-(CMI)"><span>Conditional mutual information (CMI)</span></a></li><li><a class="tocitem" href="#Transfer-entropy"><span>Transfer entropy</span></a></li><li><a class="tocitem" href="#Probability-mass-functions-(pmf)"><span>Probability mass functions (pmf)</span></a></li><li><a class="tocitem" href="#entropies"><span>Entropies</span></a></li><li><a class="tocitem" href="#Conditional-entropy"><span>Conditional entropy</span></a></li></ul></li><li><a class="tocitem" href="../crossmap_api/">Cross mapping API</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-7-1" type="checkbox"/><label class="tocitem" for="menuitem-7-1"><span class="docs-label">Quickstart</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../quickstart/quickstart_mi/">Mutual information</a></li><li><a class="tocitem" href="../quickstart/quickstart_jdd/">Joint distance distribution</a></li><li><a class="tocitem" href="../quickstart/quickstart_independence/">Independence testing</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-2" type="checkbox"/><label class="tocitem" for="menuitem-7-2"><span class="docs-label">Longer examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../examples/examples_entropy/">Entropy</a></li><li><a class="tocitem" href="../examples/examples_conditional_entropy/">Conditional entropy</a></li><li><a class="tocitem" href="../examples/examples_mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../examples/examples_transferentropy/">Transfer entropy</a></li><li><a class="tocitem" href="../examples/examples_cross_mappings/">Cross mappings</a></li><li><a class="tocitem" href="../examples/examples_independence/">Independence testing</a></li></ul></li></ul></li><li><a class="tocitem" href="../experimental/">Experimental</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">APIs and estimators</a></li><li class="is-active"><a href>Information API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Information API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/information_api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="information_api"><a class="docs-heading-anchor" href="#information_api">Information API</a><a id="information_api-1"></a><a class="docs-heading-anchor-permalink" href="#information_api" title="Permalink"></a></h1><p>This page outlines the information API. It contains a lot of information, so for convenience, we list all concrete implementation of pairwise and conditional association measures <a href="@ref information_measures">here</a>.</p><h2 id="information_measures"><a class="docs-heading-anchor" href="#information_measures">API &amp; design</a><a id="information_measures-1"></a><a class="docs-heading-anchor-permalink" href="#information_measures" title="Permalink"></a></h2><h3 id="Modularity"><a class="docs-heading-anchor" href="#Modularity">Modularity</a><a id="Modularity-1"></a><a class="docs-heading-anchor-permalink" href="#Modularity" title="Permalink"></a></h3><p>We have taken great care to make sure that information estimators are reusable and modular. The power of this design really shines when computing things like conditional mutual information, which can be estimated in more than 20 different ways, using both discrete, mixed and continuous estimators. Functions have the the general form</p><pre><code class="language-julia hljs">f([measure], estimator, input_data...)

# Some examples
mutualinfo(MIShannon(base = ℯ), Kraskov(k = 1), x, y)
mutualinfo(MITsallisFuruichi(base = ℯ), KozachenkoLeonenko(k = 3), x, y)
condmutualinfo(CMIShannon(base = 2), ValueHistogram(3), x, y, z)
condmutualinfo(CMIRenyiJizba(base = 2), KSG2(k = 5), x, y, z)
condmutualinfo(CMIRenyiPoczos(base = 2), PoczosSchneiderCMI(k = 10), x, y, z)</code></pre><p>This modular design really shines when it comes to independence testing and causal graph inference. You can essentially test the performance of <em>any</em> independence <code>measure</code> with <em>any</em> <code>estimator</code>, as long as their combination is implemented (and if it&#39;s not, please submit a PR or issue!). We hope that this will both ease reproduction of existing literature results, and spawn new research. Please let us know if you use the package for something useful, or publish something based on it!</p><h3 id="API"><a class="docs-heading-anchor" href="#API">API</a><a id="API-1"></a><a class="docs-heading-anchor-permalink" href="#API" title="Permalink"></a></h3><p>Information measures are build on <a href="@ref probabilities_header">probabilities</a>/densities and <a href="@ref entropy_header">entropies</a>. We implement estimators of these quantities in <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl">ComplexityMeasures.jl</a>. ComplexityMeasures.jl was built with modularity in mind, and provides a plethora of estimators of probabilities and generalized entropies, both discrete and continuous. These estimators are used frequently throughout CausalityTools.jl, relyin on the fact that any &quot;high-level&quot; information measure, in some way or another, can be expressed in terms of probabilities or entropies.</p><ul><li>Information measures are computed in their discrete form by using   <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</li><li>Information measures are computed in their differential/continuous   form by using <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s. Many measures also   have dedicated estimators (like <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> for   <a href="#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>, some of which are designed to compute continuous quantities.</li></ul><h3 id="Naming-convention:-The-same-name-for-different-things"><a class="docs-heading-anchor" href="#Naming-convention:-The-same-name-for-different-things">Naming convention: The same name for different things</a><a id="Naming-convention:-The-same-name-for-different-things-1"></a><a class="docs-heading-anchor-permalink" href="#Naming-convention:-The-same-name-for-different-things" title="Permalink"></a></h3><p>In contrast to generalized entropies, which each have <em>one</em> definition, it gets a bit more complicated when it comes to the &quot;higher-level&quot; measures we provide here.</p><p>Upon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use <em>the same name for different concepts</em>. For novices, and experienced practitioners too, this can be confusing. We designed this package to alleviate any confusion regarding the names of information theoretic quantities and their estimation.</p><p>Our API clearly distinguishes between methods that are conceptually the same but named differently in the literature due to differing <em>estimation</em> strategies, from methods that actually have different definitions.</p><ul><li>Multiple, equivalent definitions occur for example for the Shannon mutual   information (MI; <a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a>), which has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral   over a joint probability mass function (pmf), as a sum of three entropy terms, and as   a Kullback-Leibler divergence between the joint pmf and the product of the marginal   distributions. Since these are all equivalent, we only need once type (<code>[</code>MIShannon`](@ref)) to represent them.</li><li>But Shannon MI is not the  only type of mutual information! &quot;Tsallis mutual information&quot;   has been proposed in different variants by various authors. Despite sharing the   same name, these are actually <em>nonequivalent definitions</em>. Naming ambiguities like   these are likely to cause confusion. We&#39;ve thus assigned   them entirely different measure names (e.g. <a href="../information_measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a> and   <a href="../information_measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a>).</li></ul><p>Every measure starts with an abbrevation of the quantity it measures, followed by the name of the measure:</p><ul><li><a href="@ref"><code>CERenyi</code></a> measures conditional Rényi entropy, and <a href="#CausalityTools.CEShannon"><code>CEShannon</code></a>   measures conditional Shannon entropy.</li></ul><p>If there are multiple definitions for the same name, the author name is appended to the type:</p><ul><li><a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a> has many <em>equivalent</em> definitions that all share the same name.</li><li><a href="../information_measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a> and <a href="../information_measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a> are separate measures,   because they are defined by <em>nonequivalent</em> mathematical formulas</li></ul><p>To estimate some information measure, an instance of the measure type (e.g. <a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a>) is combined with an <em>estimator</em>, which control <em>how</em> the quantity is computed, given som input data. The most basic estimators are <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s for discrete measures, and <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s for continuous/differential measures. Some measures have dedicated estimators, that may be discrete, continuous or try to estimate a mixture of discrete and continuous data.</p><h4 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h4><p><a href="@ref example_mi_quickstart">Here</a>)&#39;s an example of computing Shannon mutual information using various estimators on various kinds of data.</p><p>Other measure like <a href="#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a> also have multiple estimation routes. To compute your favorite measure, simply find a suitable estimator in one of the overview tables, and apply it to some input data! Follow one of the examples for inspiration.</p><h4 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h4><p>With this modular API, one could in principle estimate <em>any</em> information measure using <em>any</em> estimator. Although the current interface doesn&#39;t allow <em>every</em> combination of measure and estimator (and it&#39;s probably not theoretically meaningful to do so), you can already do a lot!</p><p>If you&#39;re interested in a deeper understanding, we try to give mathematical formulas and implementation details as best we can in the docstrings of the various measures and definitions.</p><h2 id="Mutual-information"><a class="docs-heading-anchor" href="#Mutual-information">Mutual information</a><a id="Mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information" title="Permalink"></a></h2><h3 id="api_mutualinfo"><a class="docs-heading-anchor" href="#api_mutualinfo">Mutual information API</a><a id="api_mutualinfo-1"></a><a class="docs-heading-anchor-permalink" href="#api_mutualinfo" title="Permalink"></a></h3><p>The mutual information API is defined by</p><ul><li><a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a>,</li><li><a href="#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>,</li><li><a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>.</li></ul><p>We provide a suite of estimators of various mutual information quantities. Many more variants exist in the literature. Pull requests are welcome!</p><h4 id="Definitions"><a class="docs-heading-anchor" href="#Definitions">Definitions</a><a id="Definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Definitions" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MutualInformation" href="#CausalityTools.MutualInformation"><code>CausalityTools.MutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The supertype of all mutual information measures </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L6">source</a></section></article><h4 id="Dedicated-estimators"><a class="docs-heading-anchor" href="#Dedicated-estimators">Dedicated estimators</a><a id="Dedicated-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Dedicated-estimators" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}" href="#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>CausalityTools.mutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mutualinfo([measure::MutualInformation], est::MutualInformationEstimator, x, y)</code></pre><p>Estimate the mutual information <code>measure</code> between <code>x</code> and <code>y</code> using the dedicated <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> <code>est</code>, which can be either discrete, continuous, or a mixture of both, and typically involve some bias correction. If <code>measure</code> is not given, then the default is <code>MIShannon()</code>.</p><p>See the <a href="#dedicated_estimators_mi">online documentation</a> for a list of compatible measures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L99-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MutualInformationEstimator" href="#CausalityTools.MutualInformationEstimator"><code>CausalityTools.MutualInformationEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MutualInformationEstimator</code></pre><p>The supertype of all dedicated mutual information estimators.</p><p><a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s can be either mixed, discrete or a combination of both. Each estimator uses a specialized technique to approximate relevant densities/integrals and/or probabilities, and is typically tailored to a specific type of <a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a> (mostly <a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L9-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.KraskovStögbauerGrassberger1" href="#CausalityTools.KraskovStögbauerGrassberger1"><code>CausalityTools.KraskovStögbauerGrassberger1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger1</code> mutual information estimator (you can use <code>KSG1</code> for short) is the <span>$I^{(1)}$</span> <code>k</code>-th nearest neighbor estimator from Kraskov et al. (2004)<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Description</strong></p><p>Let the joint dataset <span>$X := \{\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m \}$</span> be defined by the concatenation of the marginal datasets <span>$\{ \bf{X}_k \}_{k=1}^m$</span>, where each <span>$\bf{X}_k$</span> is potentially multivariate. Let <span>$\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N$</span> be the points in the joint space <span>$X$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/estimators/KSG1.jl#L9-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.KraskovStögbauerGrassberger2" href="#CausalityTools.KraskovStögbauerGrassberger2"><code>CausalityTools.KraskovStögbauerGrassberger2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger2</code> mutual information estimator (you can use <code>KSG2</code> for short) is the <span>$I^{(2)}$</span> <code>k</code>-th nearest neighbor estimator from Kraskov et al. (2004)<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Description</strong></p><p>Let the joint dataset <span>$X := \{\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m \}$</span> be defined by the concatenation of the marginal datasets <span>$\{ \bf{X}_k \}_{k=1}^m$</span>, where each <span>$\bf{X}_k$</span> is potentially multivariate. Let <span>$\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N$</span> be the points in the joint space <span>$X$</span>.</p><p>The <code>KraskovStögbauerGrassberger2</code> estimator first locates, for each <span>$\bf{x}_i \in X$</span>, the point <span>$\bf{n}_i \in X$</span>, the <code>k</code>-th nearest neighbor to <span>$\bf{x}_i$</span>, according to the maximum norm (<code>Chebyshev</code> metric). Let <span>$\epsilon_i$</span> be the distance <span>$d(\bf{x}_i, \bf{n}_i)$</span>.</p><p>Consider <span>$x_i^m \in \bf{X}_m$</span>, the <span>$i$</span>-th point in the marginal space <span>$\bf{X}_m$</span>. For each <span>$\bf{x}_i^m$</span>, we determine <span>$\theta_i^m$</span> := the number of points <span>$\bf{x}_k^m \in \bf{X}_m$</span> that are a distance less than <span>$\epsilon_i$</span> away from <span>$\bf{x}_i^m$</span>. That is, we use the distance from a query point <span>$\bf{x}_i \in X$</span> (in the <em>joint</em> space) to count neighbors of <span>$x_i^m \in \bf{X}_m$</span> (in the marginal space).</p><p>Mutual information between the variables <span>$\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m$</span> is then estimated as</p><p class="math-container">\[\hat{I}_{KSG2}(\bf{X}) =
    \psi{(k)} -
    \dfrac{m - 1}{k} +
    (m - 1)\psi{(N)} -
    \dfrac{1}{N} \sum_{i = 1}^N \sum_{j = 1}^m \psi{(\theta_i^j + 1)}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/estimators/KSG2.jl#L8-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.GaoKannanOhViswanath" href="#CausalityTools.GaoKannanOhViswanath"><code>CausalityTools.GaoKannanOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoKannanOhViswanath &lt;: MutualInformationEstimator
GaoKannanOhViswanath(; k = 1, w = 0)</code></pre><p>The <code>GaoKannanOhViswanath</code> (Shannon) estimator is designed for estimating mutual information between variables that may be either discrete, continuous or a mixture of both (Gao et al., 2017).</p><div class="admonition is-info"><header class="admonition-header">Explicitly convert your discrete data to floats</header><div class="admonition-body"><p>Even though the <code>GaoKannanOhViswanath</code> estimator is designed to handle discrete data, our implementation demands that all input data are <code>Dataset</code>s whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to <a href="#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>.</p></div></div><p><strong>Description</strong></p><p>The estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using <code>k</code>-nearest neighbor distances from empirical samples.</p><p>The estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator <em>&quot;strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.&quot;</em></p><div class="admonition is-category-warn"><header class="admonition-header">Implementation note</header><div class="admonition-body"><p>In Gao et al., (2017), they claim (roughly speaking) that the estimator reduces to the <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> estimator for continuous-valued data. However, <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> uses the digamma function, while <code>GaoKannanOhViswanath</code> uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.</p><p>Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the <code>KSG1</code> approach. The <code>KSG1</code> estimator uses <code>k</code>-th neighbor distances in the <em>joint</em> space, while the <code>GaoKannanOhViswanath</code> algorithm selects the maximum <code>k</code>-th nearest distances among the two marginal spaces, which are in general not the same as the <code>k</code>-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in <code>GaoKannanOhViswanath</code>. We have modified it in a way that mimics <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> for continous data. Note that because of using the <code>log</code> function instead of <code>digamma</code>, there will be slight differences between the methods. See the source code for more details.</p></div></div><p>See also: <a href="#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/estimators/GaoKannanOhViswanath.jl#L5-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.GaoOhViswanath" href="#CausalityTools.GaoOhViswanath"><code>CausalityTools.GaoOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoOhViswanath &lt;: MutualInformationEstimator</code></pre><p>The <code>GaoOhViswanath</code> mutual information estimator, also called the bias-improved-KSG estimator, or BI-KSG, by Gao et al. (2018)<sup class="footnote-reference"><a id="citeref-Gao2018" href="#footnote-Gao2018">[Gao2018]</a></sup>, is given by</p><p class="math-container">\[\begin{align*}
\hat{H}_{GAO}(X, Y)
&amp;= \hat{H}_{KSG}(X) + \hat{H}_{KSG}(Y) - \hat{H}_{KZL}(X, Y) \\
&amp;= \psi{(k)} +
    \log{(N)} +
    \log{
        \left(
            \dfrac{c_{d_{x}, 2} c_{d_{y}, 2}}{c_{d_{x} + d_{y}, 2}}
        \right)
    } - \\
    &amp; \dfrac{1}{N} \sum_{i=1}^N \left( \log{(n_{x, i, 2})} + \log{(n_{y, i, 2})} \right)
\end{align*},\]</p><p>where <span>$c_{d, 2} = \dfrac{\pi^{\frac{d}{2}}}{\Gamma{(\dfrac{d}{2} + 1)}}$</span> is the volume of a <span>$d$</span>-dimensional unit <span>$\mathcal{l}_2$</span>-ball.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/estimators/GaoOhViswanath.jl#L4-L31">source</a></section></article><h5 id="dedicated_estimators_mi"><a class="docs-heading-anchor" href="#dedicated_estimators_mi">Table of dedicated estimators</a><a id="dedicated_estimators_mi-1"></a><a class="docs-heading-anchor-permalink" href="#dedicated_estimators_mi" title="Permalink"></a></h5><table><tr><th style="text-align: right">Estimator</th><th style="text-align: center">Type</th><th style="text-align: center">Principle</th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></th></tr><tr><td style="text-align: right"><a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></td><td style="text-align: center">Mixed</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><h4 id="Discrete-mutual-information"><a class="docs-heading-anchor" href="#Discrete-mutual-information">Discrete mutual information</a><a id="Discrete-mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-mutual-information" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.mutualinfo-Tuple{ProbabilitiesEstimator, Any, Any}" href="#CausalityTools.mutualinfo-Tuple{ProbabilitiesEstimator, Any, Any}"><code>CausalityTools.mutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mutualinfo([measure::MutualInformation], est::ProbabilitiesEstimator, x, y)</code></pre><p>Estimate the mutual information <code>measure</code> between <code>x</code> and <code>y</code> by a sum of three entropy terms, without any bias correction, using the provided <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> <code>est</code>. If <code>measure</code> is not given, then the default is <code>MIShannon()</code>.</p><p>Joint and marginal probabilities are computed by jointly discretizing <code>x</code> and <code>y</code> using the approach given by <code>est</code>, and obtaining marginal distributions from the joint distribution.</p><p>This only works for estimators that have an implementation for <a href="#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>. See the <a href="@ref dedicated_probabilities_estimators_mi">online documentation</a> for a list of compatible measures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L67-L83">source</a></section></article><h5 id="@id-dedicated_probabilities_estimators_mi"><a class="docs-heading-anchor" href="#@id-dedicated_probabilities_estimators_mi">Table of discrete mutual information estimators</a><a id="@id-dedicated_probabilities_estimators_mi-1"></a><a class="docs-heading-anchor-permalink" href="#@id-dedicated_probabilities_estimators_mi" title="Permalink"></a></h5><p>Here, we list the <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s that can be used to compute discrete <a href="@ref"><code>mutualinformation</code></a>.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></th></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>SymbolicPermuation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr></table><h5 id="contingency_matrix_mi"><a class="docs-heading-anchor" href="#contingency_matrix_mi">Contingency matrix</a><a id="contingency_matrix_mi-1"></a><a class="docs-heading-anchor-permalink" href="#contingency_matrix_mi" title="Permalink"></a></h5><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.mutualinfo-Tuple{MutualInformation, ContingencyMatrix}" href="#CausalityTools.mutualinfo-Tuple{MutualInformation, ContingencyMatrix}"><code>CausalityTools.mutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mutualinfo(measure::MutualInformation, est::MutualInformationEstimator, x, y)
mutualinfo(measure::MutualInformation, est::DifferentialEntropyEstimator, x, y)
mutualinfo(measure::MutualInformation, est::ProbabilitiesEstimator, x, y)
mutualinfo(measure::MutualInformation, c::ContingencyMatrix)</code></pre><p>Estimate the mutual information <code>measure</code> (either <a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a> or <a href="@ref"><code>MITsallis</code></a>, ) between <code>x</code> and <code>y</code> using the provided estimator <code>est</code>. Alternatively, compute mutual information from a pre-computed <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>.</p><p>Compatible measures/definitions and estimators are listed in the <a href="#mutualinfo_overview">online documentation</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L30-L42">source</a></section></article><p>Discrete mutual information can be computed directly from its double-sum definition by using the probabilities from a <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>. This estimation method works for    both numerical and categorical data, and the following <a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a>s are supported.</p><table><tr><th style="text-align: right"></th><th style="text-align: center"><a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a></th></tr><tr><td style="text-align: right"><a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../information_measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../information_measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../information_measures/#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../information_measures/#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></td><td style="text-align: center">✓</td></tr></table><h4 id="Differential/continuous-mutual-information"><a class="docs-heading-anchor" href="#Differential/continuous-mutual-information">Differential/continuous mutual information</a><a id="Differential/continuous-mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Differential/continuous-mutual-information" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.mutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any}" href="#CausalityTools.mutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any}"><code>CausalityTools.mutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">mutualinfo([measure::MutualInformation], est::DifferentialEntropyEstimator, x, y)</code></pre><p>Estimate the mutual information <code>measure</code> between <code>x</code> and <code>y</code> by a sum of three entropy terms, without any bias correction, using any <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a> compatible with multivariate data. If <code>measure</code> is not given, then the default is <code>MIShannon()</code>.</p><p>See the <a href="#dedicated_diffentropy_estimators_mi">online documentation</a> for a list of compatible measures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L86-L96">source</a></section></article><h5 id="dedicated_diffentropy_estimators_mi"><a class="docs-heading-anchor" href="#dedicated_diffentropy_estimators_mi">Table of differential mutual information estimators</a><a id="dedicated_diffentropy_estimators_mi-1"></a><a class="docs-heading-anchor-permalink" href="#dedicated_diffentropy_estimators_mi" title="Permalink"></a></h5><p>In addition to the dedicated differential mutual information estimators listed above, continuous/differential mutual information may also be estimated using any of our <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a> that support multivariate input data. When using these estimators, mutual information is computed as a sum of entropy terms (with different dimensions), and no bias correction is applied.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></th><th style="text-align: center"><a href="@ref"><code>MIRenyiSurbu</code></a></th></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>Lord</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><h2 id="Conditional-mutual-information-(CMI)"><a class="docs-heading-anchor" href="#Conditional-mutual-information-(CMI)">Conditional mutual information (CMI)</a><a id="Conditional-mutual-information-(CMI)-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-information-(CMI)" title="Permalink"></a></h2><h3 id="CMI-API"><a class="docs-heading-anchor" href="#CMI-API">CMI API</a><a id="CMI-API-1"></a><a class="docs-heading-anchor-permalink" href="#CMI-API" title="Permalink"></a></h3><p>The condition mutual information API is defined by</p><ul><li><a href="#CausalityTools.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a>,</li><li><a href="#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>,</li><li><a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>.</li></ul><h3 id="CMI-definitions"><a class="docs-heading-anchor" href="#CMI-definitions">CMI definitions</a><a id="CMI-definitions-1"></a><a class="docs-heading-anchor-permalink" href="#CMI-definitions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.ConditionalMutualInformation" href="#CausalityTools.ConditionalMutualInformation"><code>CausalityTools.ConditionalMutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalMutualInformation &lt;: InformationMeasure
CMI # alias</code></pre><p>The supertype of all conditional mutual informations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/condmutualinfo.jl#L6-L11">source</a></section></article><h3 id="Dedicated-CMI-estimators"><a class="docs-heading-anchor" href="#Dedicated-CMI-estimators">Dedicated CMI estimators</a><a id="Dedicated-CMI-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Dedicated-CMI-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}" href="#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>CausalityTools.condmutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">condmutualinfo([measure::CMI], est::CMIEstimator, x, y, z) → cmi::Real</code></pre><p>Estimate a conditional mutual information (CMI) of some kind (specified by <code>measure</code>), between <code>x</code> and <code>y</code>, given <code>z</code>, using the given dedicated <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>, which may be discrete, continuous or mixed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/condmutualinfo.jl#L31-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.ConditionalMutualInformationEstimator" href="#CausalityTools.ConditionalMutualInformationEstimator"><code>CausalityTools.ConditionalMutualInformationEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalMutualInformationEstimator &lt;: InformationEstimator
CMIEstimator # alias</code></pre><p>The supertype of all conditional mutual information estimators.</p><p><strong>Subtypes</strong></p><ul><li><a href="#CausalityTools.FPVP"><code>FPVP</code></a>.</li><li><a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a>.</li><li><a href="#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a>.</li><li><a href="#CausalityTools.MesnerShalisi"><code>MesnerShalisi</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/condmutualinfo.jl#L15-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.FPVP" href="#CausalityTools.FPVP"><code>CausalityTools.FPVP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FPVP &lt;: ConditionalMutualInformationEstimator
FPVP(k = 1, w = 0)</code></pre><p>The Frenzel-Pompe-Vejmelka-Paluš (or <code>FPVP</code> for short) estimator is used to estimate the differential conditional mutual information using a <code>k</code>-th nearest neighbor approach that is analogous to that of the <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> mutual information estimator (Frenzel &amp; Pompe, 2007<sup class="footnote-reference"><a id="citeref-Frenzel2007" href="#footnote-Frenzel2007">[Frenzel2007]</a></sup>; Vejmelka &amp; Paluš, 2008<sup class="footnote-reference"><a id="citeref-Vejmelka2008" href="#footnote-Vejmelka2008">[Vejmelka2008]</a></sup>).</p><p><code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/estimators/FPVP.jl#L8-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MesnerShalisi" href="#CausalityTools.MesnerShalisi"><code>CausalityTools.MesnerShalisi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MesnerShalisi &lt;: ConditionalMutualInformationEstimator
MesnerShalisi(k = 1, w = 0)</code></pre><p>The <code>MesnerShalisi</code> estimator is an estimator for conditional mutual information for data that can be mixtures of discrete and continuous data (Mesner &amp; Shalisi et al., 2020)<sup class="footnote-reference"><a id="citeref-MesnerShalisi2020" href="#footnote-MesnerShalisi2020">[MesnerShalisi2020]</a></sup>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/estimators/MesnerShalisi.jl#L3-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.PoczosSchneiderCMI" href="#CausalityTools.PoczosSchneiderCMI"><code>CausalityTools.PoczosSchneiderCMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PoczosSchneiderCMI &lt;: ConditionalMutualInformationEstimator
PoczosSchneiderCMI(k = 1, w = 0)</code></pre><p>The <code>PoczosSchneiderCMI</code> estimator computes various (differential) conditional mutual informations, using a <code>k</code>-th nearest neighbor approach (Póczos &amp; Schneider, 2012)<sup class="footnote-reference"><a id="citeref-Póczos2012" href="#footnote-Póczos2012">[Póczos2012]</a></sup>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/estimators/PoczosSchneiderCMI.jl#L5-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.Rahimzamani" href="#CausalityTools.Rahimzamani"><code>CausalityTools.Rahimzamani</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Rahimzamani &lt;: ConditionalMutualInformationEstimator
Rahimzamani(k = 1, w = 0)</code></pre><p>The <code>Rahimzamani</code> estimator, short for Rahimzamani-Asnani-Viswanath-Kannan, is an estimator for Shannon conditional mutual information for data that can be mixtures of discrete and continuous data (Rahimzamani et al., 2018)<sup class="footnote-reference"><a id="citeref-Rahimzamani2018" href="#footnote-Rahimzamani2018">[Rahimzamani2018]</a></sup>.</p><p>This is very similar to the <a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a> mutual information estimator, but has been expanded to the conditional case.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/estimators/Rahimzamani.jl#L3-L18">source</a></section></article><h4 id="condmutualinfo_dedicated_estimators"><a class="docs-heading-anchor" href="#condmutualinfo_dedicated_estimators">Table of dedicated CMI estimators</a><a id="condmutualinfo_dedicated_estimators-1"></a><a class="docs-heading-anchor-permalink" href="#condmutualinfo_dedicated_estimators" title="Permalink"></a></h4><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="../information_measures/#CausalityTools.CMIShannon"><code>CMIShannon</code></a></th><th style="text-align: center"><a href="../information_measures/#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></th></tr><tr><td style="text-align: right"><a href="#CausalityTools.FPVP"><code>FPVP</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.MesnerShalisi"><code>MesnerShalisi</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">x</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>GaussianCMI</code></a></td><td style="text-align: right">Parametric</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr></table><h3 id="Estimation-through-mutual-information"><a class="docs-heading-anchor" href="#Estimation-through-mutual-information">Estimation through mutual information</a><a id="Estimation-through-mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Estimation-through-mutual-information" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.condmutualinfo-Tuple{MutualInformationEstimator, Any, Any, Any}" href="#CausalityTools.condmutualinfo-Tuple{MutualInformationEstimator, Any, Any, Any}"><code>CausalityTools.condmutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">condmutualinfo([measure::CMI], est::MutualInformationEstimator, x, y, z) → cmi::Real</code></pre><p>Estimate the conditional mutual information (CMI) <code>measure</code> between <code>x</code> and <code>y</code> using a difference of mutual information terms, without any bias correction, using the provided <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> <code>est</code>, which may be continuous/differential, discrete or mixed. If <code>measure</code> is not given, then the default is <code>CMIShannon()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/condmutualinfo.jl#L79-L87">source</a></section></article><table><tr><th style="text-align: right">Estimator</th><th style="text-align: center">Type</th><th style="text-align: center">Principle</th><th style="text-align: center"><a href="../information_measures/#CausalityTools.CMIShannon"><code>CMIShannon</code></a></th></tr><tr><td style="text-align: right"><a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></td><td style="text-align: center">Mixed</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">Nearest neighbors</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>GaussianMI</code></a></td><td style="text-align: center"></td><td style="text-align: center">Parametric</td><td style="text-align: center">✓</td></tr></table><h3 id="Discrete-CMI"><a class="docs-heading-anchor" href="#Discrete-CMI">Discrete CMI</a><a id="Discrete-CMI-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-CMI" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.condmutualinfo-Tuple{ProbabilitiesEstimator, Any, Any, Any}" href="#CausalityTools.condmutualinfo-Tuple{ProbabilitiesEstimator, Any, Any, Any}"><code>CausalityTools.condmutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">condmutualinfo([measure::CMI], est::ProbabilitiesEstimator, x, y, z) → cmi::Real ∈ [0, a)</code></pre><p>Estimate the conditional mutual information (CMI) <code>measure</code> between <code>x</code> and <code>y</code> given <code>z</code> using a sum of entropy terms, without any bias correction, using the provided <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> <code>est</code>. If <code>measure</code> is not given, then the default is <code>CMIShannon()</code>.</p><p>With a <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>, the returned <code>cmi</code> is guaranteed to be non-negative.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/condmutualinfo.jl#L52-L62">source</a></section></article><h4 id="mutualinfo_overview"><a class="docs-heading-anchor" href="#mutualinfo_overview">Table of discrete mutual information estimators</a><a id="mutualinfo_overview-1"></a><a class="docs-heading-anchor-permalink" href="#mutualinfo_overview" title="Permalink"></a></h4><p>Here, we list the <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s that are compatible with <a href="#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a>, and which definitions they are valid for.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="../information_measures/#CausalityTools.CMIShannon"><code>CMIShannon</code></a></th><th style="text-align: center"><a href="@ref"><code>CMIRenyiSarbu</code></a></th></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>SymbolicPermuation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr></table><h3 id="Differential-CMI"><a class="docs-heading-anchor" href="#Differential-CMI">Differential CMI</a><a id="Differential-CMI-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-CMI" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.condmutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any, Any}" href="#CausalityTools.condmutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any, Any}"><code>CausalityTools.condmutualinfo</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">condmutualinfo([measure::CMI], est::DifferentialEntropyEstimator, x, y, z) → cmi</code></pre><p>Estimate the conditional mutual information (CMI) <code>measure</code> between <code>x</code> and <code>y</code> using a sum of entropy terms, without any bias correction, using the provided <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a> <code>est</code> (which must support multivariate data). If <code>measure</code> is not given, then the default is <code>CMIShannon()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/condmutualinfo.jl#L67-L74">source</a></section></article><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: right">Input data</th><th style="text-align: center"><a href="../information_measures/#CausalityTools.CMIShannon"><code>CMIShannon</code></a></th></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>Lord</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: right"><code>Dataset</code></td><td style="text-align: center">✓</td></tr></table><h2 id="Transfer-entropy"><a class="docs-heading-anchor" href="#Transfer-entropy">Transfer entropy</a><a id="Transfer-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy" title="Permalink"></a></h2><p>The transfer entropy API is made up of the following functions and types, which are listed below:</p><ul><li><a href="#CausalityTools.transferentropy"><code>transferentropy</code></a>.</li><li><a href="#CausalityTools.TransferEntropy"><code>TransferEntropy</code></a>, and its subtypes.</li><li><a href="#CausalityTools.EmbeddingTE"><code>EmbeddingTE</code></a>, which exists to provide embedding instructions to   subtypes of <a href="#CausalityTools.TransferEntropy"><code>TransferEntropy</code></a>.</li><li><a href="#CausalityTools.TransferEntropyEstimator"><code>TransferEntropyEstimator</code></a>, and its subtypes.</li></ul><h3 id="Transfer-entropy-API"><a class="docs-heading-anchor" href="#Transfer-entropy-API">Transfer entropy API</a><a id="Transfer-entropy-API-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy-API" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.transferentropy" href="#CausalityTools.transferentropy"><code>CausalityTools.transferentropy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">transferentropy([measure::TEShannon], est, s, t, [c])
transferentropy(measure::TERenyiJizba, est, s, t, [c])</code></pre><p>Estimate the transfer entropy <span>$TE^*(S \to T)$</span> or <span>$TE^*(S \to T | C)$</span> if <code>c</code> is given, using the provided estimator <code>est</code>, where <span>$*$</span> indicates the given <code>measure</code>. If <code>measure</code> is not given, then <code>TEShannon(; base = 2)</code> is the default.</p><p><strong>Arguments</strong></p><ul><li><strong><code>measure</code></strong>: The transfer entropy measure, e.g. <a href="../information_measures/#CausalityTools.TEShannon"><code>TEShannon</code></a> or   <a href="@ref"><code>TERenyi</code></a>, which dictates which formula is computed.   Embedding parameters are stored in <code>measure.embedding</code>, and   is represented by an <a href="#CausalityTools.EmbeddingTE"><code>EmbeddingTE</code></a> instance. If calling <code>transferentropy</code>   without giving <code>measure</code>, then the embedding is optimized by finding   suitable delay embedding parameters using the <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/traditional/">&quot;traditional&quot;</a>   approach from DynamicalSystems.jl.</li><li><strong><code>s</code></strong>: The source timeseries.</li><li><strong><code>t</code></strong>: The target timeseries.</li><li><strong><code>c</code></strong>: Optional. A conditional timeseries.</li></ul><p><strong>Description</strong></p><p>The Shannon transfer entropy is defined as <span>$TE^S(S \to T | C) := I^S(T^+; S^- | T^-, C^-)$</span>, where <span>$I^S(T^+; S^- | T^-, C^-)$</span> is <a href="../information_measures/#CausalityTools.CMIShannon"><code>CMIShannon</code></a>, and marginals for the CMI are constructed as described in <a href="#CausalityTools.EmbeddingTE"><code>EmbeddingTE</code></a>. The definition is analogous for <a href="../information_measures/#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a>.</p><p>If <code>s</code>, <code>t</code>, and <code>c</code> are univariate timeseries, then the the marginal embedding variables <span>$T^+$</span> (target future), <span>$T^-$</span> (target present/past), <span>$S^-$</span> (source present/past) and <span>$C^-$</span> (present/past of conditioning variables) are constructed by first jointly embedding  <code>s</code>, <code>t</code> and <code>c</code> with relevant delay embedding parameters, then subsetting relevant columns of the embedding.</p><p>Since estimates of <span>$TE^*(S \to T)$</span> and <span>$TE^*(S \to T | C)$</span> are just a special cases of conditional mutual information where input data are marginals of a particular form of <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/reconstruction/">delay embedding</a>, <em>any</em> combination of variables, e.g. <span>$S = (A, B)$</span>, <span>$T = (C, D)$</span>, <span>$C = (D, E, F)$</span> are valid inputs (given as <code>Dataset</code>s). In practice, however, <code>s</code>, <code>t</code> and <code>c</code> are most often timeseries, and if  <code>s</code>, <code>t</code> and <code>c</code> are <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a>s, it is assumed that the data are pre-embedded and the embedding step is skipped.</p><p><strong>Compatible estimators</strong></p><p><code>transferentropy</code> is just a simple wrapper around <a href="#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a> that constructs an appropriate delay embedding from the input data before CMI is estimated. Consequently, any estimator that can be used for <a href="#CausalityTools.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> is, in principle, also a valid transfer entropy estimator. Documentation strings for <a href="../information_measures/#CausalityTools.TEShannon"><code>TEShannon</code></a> and <a href="../information_measures/#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a> list compatible estimators, and an overview table can be found in the online documentation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/transferentropy.jl#L21-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.EmbeddingTE" href="#CausalityTools.EmbeddingTE"><code>CausalityTools.EmbeddingTE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EmbeddingTE(; dS = 1, dT = 1, dTf = 1, dC = 1, τS = -1, τT = -1, ηTf = 1, τC = -1)
EmbeddingTE(opt::OptimiseTraditional, s, t, [c])</code></pre><p><code>EmbeddingTE</code> provide embedding parameters for transfer entropy analysis using either <a href="../information_measures/#CausalityTools.TEShannon"><code>TEShannon</code></a>, <a href="@ref"><code>TERenyi</code></a>, or in general any subtype of <a href="#CausalityTools.TransferEntropy"><code>TransferEntropy</code></a>, which in turns dictates the embedding used with <a href="#CausalityTools.transferentropy"><code>transferentropy</code></a>.</p><p>The second method finds parameters using the <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/traditional/">&quot;traditional&quot;</a> optimised embedding techniques from DynamicalSystems.jl</p><p><strong>Convention for generalized delay reconstruction</strong></p><p>We use the following convention. Let <span>$s(i)$</span> be time series for the source variable, <span>$t(i)$</span> be the time series for the target variable and <span>$c(i)$</span> the time series for the conditional variable. To compute transfer entropy, we need the following marginals:</p><p class="math-container">\[\begin{aligned}
T^{+} &amp;= \{t(i+\eta^1), t(i+\eta^2), \ldots, (t(i+\eta^{d_{T^{+}}}) \} \\
T^{-} &amp;= \{ (t(i+\tau^0_{T}), t(i+\tau^1_{T}), t(i+\tau^2_{T}), \ldots, t(t + \tau^{d_{T} - 1}_{T})) \} \\
S^{-} &amp;= \{ (s(i+\tau^0_{S}), s(i+\tau^1_{S}), s(i+\tau^2_{S}), \ldots, s(t + \tau^{d_{S} - 1}_{S})) \} \\
C^{-} &amp;= \{ (c(i+\tau^0_{C}), c(i+\tau^1_{C}), c(i+\tau^2_{C}), \ldots, c(t + \tau^{d_{C} - 1}_{C})) \}
\end{aligned}\]</p><p>Depending on the application, the delay reconstruction lags <span>$\tau^k_{T} \leq 0$</span>, <span>$\tau^k_{S} \leq 0$</span>, and <span>$\tau^k_{C} \leq 0$</span> may be equally spaced, or non-equally spaced. The same applied to the prediction lag(s), but typically only a only a single predictions lag <span>$\eta^k$</span> is used (so that <span>$d_{T^{+}} = 1$</span>).</p><p>For transfer entropy, traditionally at least one <span>$\tau^k_{T}$</span>, one <span>$\tau^k_{S}$</span> and one <span>$\tau^k_{C}$</span> equals zero. This way, the <span>$T^{-}$</span>, <span>$S^{-}$</span> and <span>$C^{-}$</span> marginals always contains present/past states, while the <span>$\mathcal T$</span> marginal contain future states relative to the other marginals. However, this is not a strict requirement, and modern approaches that searches for optimal embeddings can return embeddings without the intantaneous lag.</p><p>Combined, we get the generalized delay reconstruction <span>$\mathbb{E} = (T^{+}_{(d_{T^{+}})}, T^{-}_{(d_{T})}, S^{-}_{(d_{S})}, C^{-}_{(d_{C})})$</span>. Transfer entropy is then computed as</p><p class="math-container">\[\begin{aligned}
TE_{S \rightarrow T | C} = \int_{\mathbb{E}} P(T^{+}, T^-, S^-, C^-)
\log_{b}{\left(\frac{P(T^{+} | T^-, S^-, C^-)}{P(T^{+} | T^-, C^-)}\right)},
\end{aligned}\]</p><p>or, if conditionals are not relevant,</p><p class="math-container">\[\begin{aligned}
TE_{S \rightarrow T} = \int_{\mathbb{E}} P(T^{+}, T^-, S^-)
\log_{b}{\left(\frac{P(T^{+} | T^-, S^-)}{P(T^{+} | T^-)}\right)},
\end{aligned}\]</p><p>Here,</p><ul><li><span>$T^{+}$</span> denotes the <span>$d_{T^{+}}$</span>-dimensional set of vectors furnishing the future   states of <span>$T$</span> (almost always equal to 1 in practical applications),</li><li><span>$T^{-}$</span> denotes the <span>$d_{T}$</span>-dimensional set of vectors furnishing the past and   present states of <span>$T$</span>,</li><li><span>$S^{-}$</span> denotes the <span>$d_{S}$</span>-dimensional set of vectors furnishing the past and   present of <span>$S$</span>, and</li><li><span>$C^{-}$</span> denotes the <span>$d_{C}$</span>-dimensional set of vectors furnishing the past and   present of <span>$C$</span>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>dS</code>, <code>dT</code>, <code>dC</code>, <code>dTf</code> (<code>f</code> for <em>future</em>) are the dimensions of the <span>$S^{-}$</span>,   <span>$T^{-}$</span>, <span>$C^{-}$</span> and <span>$T^{+}$</span> marginals. The parameters <code>dS</code>, <code>dT</code>, <code>dC</code> and <code>dTf</code>   must each be a <em>positive</em> integer number.</li><li><code>τS</code>, <code>τT</code>, <code>τC</code> are the embedding lags for <span>$S^{-}$</span>, <span>$T^{-}$</span>, <span>$C^{-}$</span>.   Each parameter are integers <code>∈ 𝒩⁰⁻</code>, or a vector of integers <code>∈ 𝒩⁰⁻</code>, so   that <span>$S^{-}$</span>, <span>$T^{-}$</span>, <span>$C^{-}$</span> always represents present/past values.   If e.g. <code>τT</code> is an integer, then for the <span>$T^-$</span> marginal is constructed using   lags <span>$\tau_{T} = \{0, \tau, 2\tau, \ldots, (d_{T}- 1)\tau_T \}$</span>.   If is a vector, e.g. <code>τΤ = [-1, -5, -7]</code>, then the dimension <code>dT</code> must match the lags,   and precisely those lags are used: <span>$\tau_{T} = \{-1, -5, -7 \}$</span>.</li><li>The prediction lag(s) <code>ηTf</code> is a positive integer. Combined with the requirement   that the other delay parameters are zero or negative, this ensures that we&#39;re   always predicting from past/present to future. In typical applications,   <code>ηTf = 1</code> is used for transfer entropy.</li></ul><p><strong>Examples</strong></p><p>Say we wanted to compute the Shannon transfer entropy <span>$TE^S(S \to T) = I^S(T^+; S^- | T^-)$</span>. Using some modern procedure for determining optimal embedding parameters using <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/unified/">methods from DynamicalSystems.jl</a>, we find that the optimal embedding of <span>$T^{-}$</span> is three-dimensional and is given by the lags <code>[0, -5, -8]</code>. Using the same procedure, we find that the optimal embedding of <span>$S^{-}$</span> is two-dimensional with lags <span>$[-1, -8]$</span>. We want to predicting a univariate version of the target variable one time step into the future (<code>ηTf = 1</code>). The total embedding is then the set of embedding vectors</p><p><span>$E_{TE} = \{ (T(i+1), S(i-1), S(i-8), T(i), T(i-5), T(i-8)) \}$</span>. Translating this to code, we get:</p><pre><code class="language-julia-repl hljs">using CausalityTools
julia&gt; EmbeddingTE(dT=3, τT=[0, -5, -8], dS=2, τS=[-1, -4], ηTf=1)

# output
EmbeddingTE(dS=2, dT=3, dC=1, dTf=1, τS=[-1, -4], τT=[0, -5, -8], τC=-1, ηTf=1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/embedding.jl#L3-L114">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.optimize_marginals_te" href="#CausalityTools.optimize_marginals_te"><code>CausalityTools.optimize_marginals_te</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">optimize_marginals_te([scheme = OptimiseTraditional()], s, t, [c]) → EmbeddingTE</code></pre><p>Optimize marginal embeddings for transfer entropy computation from source time series <code>s</code> to target time series <code>t</code>, conditioned on <code>c</code> if <code>c</code> is given, using the provided optimization <code>scheme</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/optimization/traditional_optimal_embedding.jl#L22-L28">source</a></section></article><h4 id="Transfer-entropy-definitions"><a class="docs-heading-anchor" href="#Transfer-entropy-definitions">Transfer entropy definitions</a><a id="Transfer-entropy-definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy-definitions" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.TransferEntropy" href="#CausalityTools.TransferEntropy"><code>CausalityTools.TransferEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The supertype of all transfer entropy measures.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/transferentropy.jl#L11-L13">source</a></section></article><h4 id="Transfer-entropy-estimators"><a class="docs-heading-anchor" href="#Transfer-entropy-estimators">Transfer entropy estimators</a><a id="Transfer-entropy-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy-estimators" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.TransferEntropyEstimator" href="#CausalityTools.TransferEntropyEstimator"><code>CausalityTools.TransferEntropyEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The supertype of all dedicated transfer entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/transferentropy.jl#L16-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.Zhu1" href="#CausalityTools.Zhu1"><code>CausalityTools.Zhu1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu1 &lt;: DifferentialEntropyEstimator
Zhu1(k = 1, w = 0, base = MathConstants.e)</code></pre><p>The <code>Zhu1</code> transfer entropy estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup>.</p><p>Assumes that the input data have been normalized as described in (Zhu et al., 2015).</p><p>This estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/estimators/Zhu1.jl#L10-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.Lindner" href="#CausalityTools.Lindner"><code>CausalityTools.Lindner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lindner &lt;: TransferEntropyEstimator
Lindner(k = 1, w = 0, base = 2)</code></pre><p>The <code>Lindner</code> transfer entropy estimator (Lindner et al., 2011)<sup class="footnote-reference"><a id="citeref-Lindner2011" href="#footnote-Lindner2011">[Lindner2011]</a></sup>, which is also used in the Trentool MATLAB toolbox, and is based on nearest neighbor searches.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>For a given points in the joint embedding space <code>jᵢ</code>, this estimator first computes the distance <code>dᵢ</code> from <code>jᵢ</code> to its <code>k</code>-th nearest neighbor. Then, for each point <code>mₖ[i]</code> in the <code>k</code>-th marginal space, it counts the number of points within radius <code>dᵢ</code>.</p><p>The transfer entropy is then computed as</p><p class="math-container">\[TE(X \to Y) =
\psi(k) + \dfrac{1}{N} \sum_{i}^n
\left[
    \sum_{k=1}^3 \left( \psi(m_k[i] + 1) \right)
\right],\]</p><p>where the index <code>k</code> references the three marginal subspaces <code>T</code>, <code>TTf</code> and <code>ST</code> for which neighbor searches are performed.</p><p>[Lindner2011]: Lindner, M., Vicente, R., Priesemann, V., &amp; Wibral, M. (2011). TRENTOOL:     A Matlab open source toolbox to analyse information flow in time series data with     transfer entropy. BMC neuroscience, 12(1), 1-22.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/estimators/Lindner.jl#L7-L40">source</a></section></article><h4 id="Convenience"><a class="docs-heading-anchor" href="#Convenience">Convenience</a><a id="Convenience-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.SymbolicTransferEntropy" href="#CausalityTools.SymbolicTransferEntropy"><code>CausalityTools.SymbolicTransferEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicTransferEntropy &lt;: TransferEntropyEstimator
SymbolicTransferEntropy(; m = 3, τ = 1, lt = ComplexityMeasures.isless_rand</code></pre><p>A convenience estimator for symbolic transfer entropy (Stanieck &amp; Lenertz, 2008)<sup class="footnote-reference"><a id="citeref-Stanieck2008" href="#footnote-Stanieck2008">[Stanieck2008]</a></sup>.</p><p><strong>Description</strong></p><p><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.100.158101">Symbolic transfer entropy</a> consists of two simple steps. First, the input time series are embedded with embedding lag <code>m</code> and delay <code>τ</code>. The ordinal patterns of the embedding vectors are then encoded using <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a> with <a href="#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>. This transforms the input time series into integer time series using <a href="#ComplexityMeasures.OrdinalPatternEncoding"><code>OrdinalPatternEncoding</code></a>.</p><p>Transfer entropy is then estimated as usual on the encoded timeseries with <a href="#CausalityTools.transferentropy"><code>transferentropy</code></a> and the <a href="#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a> naive frequency estimator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/convenience/SymbolicTransferEntropy.jl#L3-L24">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.Hilbert" href="#CausalityTools.Hilbert"><code>CausalityTools.Hilbert</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Hilbert(est;
    source::InstantaneousSignalProperty = Phase(),
    target::InstantaneousSignalProperty = Phase(),
    cond::InstantaneousSignalProperty = Phase())
) &lt;: TransferDifferentialEntropyEstimator</code></pre><p>Compute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are obtained by first applying the Hilbert transform to each signal, then extracting the phases/amplitudes of the resulting complex numbers<sup class="footnote-reference"><a id="citeref-Palus2014" href="#footnote-Palus2014">[Palus2014]</a></sup>. Original time series are thus transformed to instantaneous phase/amplitude time series. Transfer entropy is then estimated using the provided <code>est</code> on those phases/amplitudes (use e.g. <a href="@ref"><code>VisitationFrequency</code></a>, or <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a>).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Details on estimation of the transfer entropy (conditional mutual information) following the phase/amplitude extraction step is not given in Palus (2014). Here, after instantaneous phases/amplitudes have been obtained, these are treated as regular time series, from which transfer entropy is then computed as usual.</p></div></div><p>See also: <a href="#CausalityTools.Phase"><code>Phase</code></a>, <a href="#CausalityTools.Amplitude"><code>Amplitude</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/convenience/Hilbert.jl#L20-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.Phase" href="#CausalityTools.Phase"><code>CausalityTools.Phase</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Phase &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous phases of a signal should be used. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/convenience/Hilbert.jl#L14-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.Amplitude" href="#CausalityTools.Amplitude"><code>CausalityTools.Amplitude</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Amplitude &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous amplitudes of a signal should be used. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/convenience/Hilbert.jl#L8-L11">source</a></section></article><h2 id="Probability-mass-functions-(pmf)"><a class="docs-heading-anchor" href="#Probability-mass-functions-(pmf)">Probability mass functions (pmf)</a><a id="Probability-mass-functions-(pmf)-1"></a><a class="docs-heading-anchor-permalink" href="#Probability-mass-functions-(pmf)" title="Permalink"></a></h2><h3 id="Probabilities-API"><a class="docs-heading-anchor" href="#Probabilities-API">Probabilities API</a><a id="Probabilities-API-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilities-API" title="Permalink"></a></h3><p>The probabilities API is defined by</p><ul><li><a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></li><li><a href="#ComplexityMeasures.probabilities"><code>probabilities</code></a></li><li><a href="#ComplexityMeasures.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a></li><li><a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a></li><li><a href="#CausalityTools.contingency_matrix"><code>contingency_matrix</code></a></li></ul><p>and related functions that you will find in the following documentation blocks:</p><h4 id="Probabilitities"><a class="docs-heading-anchor" href="#Probabilitities">Probabilitities</a><a id="Probabilitities-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilitities" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ProbabilitiesEstimator" href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ComplexityMeasures.ProbabilitiesEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ProbabilitiesEstimator</code></pre><p>The supertype for all probabilities estimators.</p><p>In ComplexityMeasures.jl, probability distributions are estimated from data by defining a set of possible outcomes <span>$\Omega = \{\omega_1, \omega_2, \ldots, \omega_L \}$</span>, and assigning to each outcome <span>$\omega_i$</span> a probability <span>$p(\omega_i)$</span>, such that <span>$\sum_{i=1}^N p(\omega_i) = 1$</span>. It is the role of a <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> to</p><ol><li>Define <span>$\Omega$</span>, the &quot;outcome space&quot;, which is the set of all possible outcomes over  which probabilities are estimated. The cardinality of this set can be obtained using  <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</li><li>Define how probabilities <span>$p_i = p(\omega_i)$</span> are assigned to outcomes <span>$\omega_i$</span>.</li></ol><p>In practice, probability estimation is done by calling <a href="#ComplexityMeasures.probabilities"><code>probabilities</code></a> with some input data and one of the following probabilities estimators. The result is a <a href="#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> <code>p</code> (<code>Vector</code>-like), where each element <code>p[i]</code> is the probability of the outcome <code>ω[i]</code>. Use <a href="#ComplexityMeasures.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a> if you need both the probabilities and the outcomes, and use <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a> to obtain <span>$\Omega$</span> alone. The element type of <span>$\Omega$</span> varies between estimators, but it is guaranteed to be <em>hashable</em>. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).</p><p>Some estimators can deduce <span>$\Omega$</span> without knowledge of the input, such as <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a>. For others, knowledge of input is necessary for concretely specifying <span>$\Omega$</span>, such as <a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a> with <a href="#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a>. This only matters for the functions <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a> and <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p><p>All currently implemented probability estimators are listed in a nice table in the <a href="#probabilities_estimators">probabilities estimators</a> section of the online documentation.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.probabilities" href="#ComplexityMeasures.probabilities"><code>ComplexityMeasures.probabilities</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) → p::Probabilities</code></pre><p>Compute a probability distribution over the set of possible outcomes defined by the probabilities estimator <code>est</code>, given input data <code>x</code>, which is typically an <code>Array</code> or a <code>Dataset</code>; see <a href="@ref">Input data for ComplexityMeasures.jl</a>. Configuration options are always given as arguments to the chosen estimator.</p><p>To obtain the outcomes corresponding to these probabilities, use <a href="#ComplexityMeasures.outcomes"><code>outcomes</code></a>.</p><p>Due to performance optimizations, whether the returned probablities contain <code>0</code>s as entries or not depends on the estimator. E.g., in <a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a> <code>0</code>s are skipped, while in <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a> <code>0</code> are not, because we get them for free.</p><pre><code class="nohighlight hljs">probabilities(x::Vector_or_Dataset) → p::Probabilities</code></pre><p>Estimate probabilities by directly counting the elements of <code>x</code>, assuming that <code>Ω = sort(unique(x))</code>, i.e. that the outcome space is the unique elements of <code>x</code>. This is mostly useful when <code>x</code> contains categorical data.</p><p>See also: <a href="#ComplexityMeasures.Probabilities"><code>Probabilities</code></a>, <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.probabilities!" href="#ComplexityMeasures.probabilities!"><code>ComplexityMeasures.probabilities!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities!(s, args...)</code></pre><p>Similar to <code>probabilities(args...)</code>, but allows pre-allocation of temporarily used containers <code>s</code>.</p><p>Only works for certain estimators. See for example <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Probabilities" href="#ComplexityMeasures.Probabilities"><code>ComplexityMeasures.Probabilities</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Probabilities &lt;: AbstractArray
Probabilities(x) → p</code></pre><p><code>Probabilities</code> is a simple wrapper around <code>x::AbstractArray{&lt;:Real, N}</code> that ensures its values sum to 1, so that <code>p</code> can be interpreted as <code>N</code>-dimensional probability mass function. In most use cases, <code>p</code> will be a vector.</p></div></section></article><h4 id="Outcomes"><a class="docs-heading-anchor" href="#Outcomes">Outcomes</a><a id="Outcomes-1"></a><a class="docs-heading-anchor-permalink" href="#Outcomes" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.probabilities_and_outcomes" href="#ComplexityMeasures.probabilities_and_outcomes"><code>ComplexityMeasures.probabilities_and_outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">probabilities_and_outcomes(est, x)</code></pre><p>Return <code>probs, outs</code>, where <code>probs = probabilities(x, est)</code> and <code>outs[i]</code> is the outcome with probability <code>probs[i]</code>. The element type of <code>outs</code> depends on the estimator. <code>outs</code> is a subset of the <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a> of <code>est</code>.</p><p>See also <a href="#ComplexityMeasures.outcomes"><code>outcomes</code></a>, <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.outcomes" href="#ComplexityMeasures.outcomes"><code>ComplexityMeasures.outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">outcomes(est::ProbabilitiesEstimator, x)</code></pre><p>Return all (unique) outcomes contained in <code>x</code> according to the given estimator. Equivalent with <code>probabilities_and_outcomes(x, est)[2]</code>, but for some estimators it may be explicitly extended for better performance.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.outcome_space" href="#ComplexityMeasures.outcome_space"><code>ComplexityMeasures.outcome_space</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">outcome_space(est::ProbabilitiesEstimator, x) → Ω</code></pre><p>Return a container containing all <em>possible</em> outcomes of <code>est</code> for input <code>x</code>.</p><p>For some estimators the concrete outcome space is known without knowledge of input <code>x</code>, in which case the function dispatches to <code>outcome_space(est)</code>. In general it is recommended to use the 2-argument version irrespectively of estimator.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.total_outcomes" href="#ComplexityMeasures.total_outcomes"><code>ComplexityMeasures.total_outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">total_outcomes(est::ProbabilitiesEstimator, x)</code></pre><p>Return the length (cardinality) of the outcome space <span>$\Omega$</span> of <code>est</code>.</p><p>For some estimators the concrete outcome space is known without knowledge of input <code>x</code>, in which case the function dispatches to <code>total_outcomes(est)</code>. In general it is recommended to use the 2-argument version irrespectively of estimator.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.missing_outcomes" href="#ComplexityMeasures.missing_outcomes"><code>ComplexityMeasures.missing_outcomes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">missing_outcomes(est::ProbabilitiesEstimator, x) → n_missing::Int</code></pre><p>Estimate a probability distribution for <code>x</code> using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.</p><p>See also: <a href="@ref"><code>MissingDispersionPatterns</code></a>.</p></div></section></article><h3 id="Estimators"><a class="docs-heading-anchor" href="#Estimators">Estimators</a><a id="Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Estimators" title="Permalink"></a></h3><h4 id="probabilities_estimators"><a class="docs-heading-anchor" href="#probabilities_estimators">Overview of probabilities estimators</a><a id="probabilities_estimators-1"></a><a class="docs-heading-anchor-permalink" href="#probabilities_estimators" title="Permalink"></a></h4><p>Any of the following estimators can be used with <a href="#ComplexityMeasures.probabilities"><code>probabilities</code></a> (in the column &quot;input data&quot;  it is assumed that the <code>eltype</code> of the input is <code>&lt;: Real</code>). Some estimators can also be used with <a href="#CausalityTools.contingency_matrix"><code>contingency_matrix</code></a> to estimate multivariate pmfs.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Principle</th><th style="text-align: left">Input data</th></tr><tr><td style="text-align: left"><a href="#CausalityTools.Contingency"><code>Contingency</code></a></td><td style="text-align: left">Count frequencies, optionally discretize first</td><td style="text-align: left"><code>Any</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: left">Count of unique elements</td><td style="text-align: left"><code>Any</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: left">Binning (histogram)</td><td style="text-align: left"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.TransferOperator"><code>TransferOperator</code></a></td><td style="text-align: left">Binning (transfer operator)</td><td style="text-align: left"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.NaiveKernel"><code>NaiveKernel</code></a></td><td style="text-align: left">Kernel density estimation</td><td style="text-align: left"><code>Dataset</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a></td><td style="text-align: left">Ordinal patterns</td><td style="text-align: left"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a></td><td style="text-align: left">Ordinal patterns</td><td style="text-align: left"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a></td><td style="text-align: left">Ordinal patterns</td><td style="text-align: left"><code>Vector</code>, <code>Dataset</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a></td><td style="text-align: left">Ordinal patterns in space</td><td style="text-align: left"><code>Array</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: left">Dispersion patterns</td><td style="text-align: left"><code>Vector</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.SpatialDispersion"><code>SpatialDispersion</code></a></td><td style="text-align: left">Dispersion patterns in space</td><td style="text-align: left"><code>Array</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Diversity"><code>Diversity</code></a></td><td style="text-align: left">Cosine similarity</td><td style="text-align: left"><code>Vector</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.WaveletOverlap"><code>WaveletOverlap</code></a></td><td style="text-align: left">Wavelet transform</td><td style="text-align: left"><code>Vector</code></td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.PowerSpectrum"><code>PowerSpectrum</code></a></td><td style="text-align: left">Fourier transform</td><td style="text-align: left"><code>Vector</code></td></tr></table><h3 id="Contingency"><a class="docs-heading-anchor" href="#Contingency">Contingency</a><a id="Contingency-1"></a><a class="docs-heading-anchor-permalink" href="#Contingency" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.Contingency" href="#CausalityTools.Contingency"><code>CausalityTools.Contingency</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Contingency &lt;: ProbabilitiesEstimator
Contingency(est::Union{ProbabilitiesEstimator, Nothing} = nothing)</code></pre><p><code>Contingency</code> is a probabilities estimator that transforms input data to a multidimensional probability mass function (internally represented as <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>.</p><p>It works directly on raw discrete/categorical data. Alternatively, if a <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> <code>est</code> for which <a href="#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a> is implemented is given, then input data are first discretized before creating the contingency matrix.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><code>Contingency</code> estimator differs from other <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s in that it&#39;s not compatible with <a href="#ComplexityMeasures.probabilities"><code>probabilities</code></a> and other methods. Instead, it is used to construct <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>, from which probabilities can be computed.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/various/probabilities/Contingency.jl#L3-L19">source</a></section></article><h3 id="Count-occurrences"><a class="docs-heading-anchor" href="#Count-occurrences">Count occurrences</a><a id="Count-occurrences-1"></a><a class="docs-heading-anchor-permalink" href="#Count-occurrences" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.CountOccurrences" href="#ComplexityMeasures.CountOccurrences"><code>ComplexityMeasures.CountOccurrences</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CountOccurrences()</code></pre><p>A probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to <a href="#ComplexityMeasures.probabilities"><code>probabilities</code></a>.</p><p><strong>Outcome space</strong></p><p>The outcome space is the unique sorted values of the input. Hence, input <code>x</code> is needed for a well-defined <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>.</p></div></section></article><h3 id="Histograms"><a class="docs-heading-anchor" href="#Histograms">Histograms</a><a id="Histograms-1"></a><a class="docs-heading-anchor-permalink" href="#Histograms" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ValueHistogram" href="#ComplexityMeasures.ValueHistogram"><code>ComplexityMeasures.ValueHistogram</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ValueHistogram(b::AbstractBinning) &lt;: ProbabilitiesEstimator</code></pre><p>A probability estimator based on binning the values of the data as dictated by the binning scheme <code>b</code> and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is <code>VisitationFrequency</code>. Available binnings are:</p><ul><li><a href="#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a></li><li><a href="#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a></li></ul><p>The <code>ValueHistogram</code> estimator has a linearithmic time complexity (<code>n log(n)</code> for <code>n = length(x)</code>) and a linear space complexity (<code>l</code> for <code>l = dimension(x)</code>). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes <code>ε</code> without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.</p><pre><code class="nohighlight hljs">ValueHistogram(ϵ::Union{Real,Vector})</code></pre><p>A convenience method that accepts same input as <a href="#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a> and initializes this binning directly.</p><p><strong>Outcomes</strong></p><p>The outcome space for <code>ValueHistogram</code> is the unique bins constructed from <code>b</code>. Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals <code>[a, b)</code>. The bins are in data units, not integer (cartesian indices units), and are returned as <code>SVector</code>s, i.e., same type as input data.</p><p>For convenience, <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a> returns the outcomes in the same array format as the underlying binning (e.g., <code>Matrix</code> for 2D input).</p><p>For <a href="#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> the <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a> is well-defined from the binning, but for <a href="#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a> input <code>x</code> is needed as well.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.RectangularBinning" href="#ComplexityMeasures.RectangularBinning"><code>ComplexityMeasures.RectangularBinning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RectangularBinning(ϵ, precise = false) &lt;: AbstractBinning</code></pre><p>Rectangular box partition of state space using the scheme <code>ϵ</code>, deducing the histogram extent and bin width from the input data.</p><p><code>RectangularBinning</code> is a convenience struct. It is re-cast into <a href="#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> once the data are provided, so see that docstring for info on the bin calculation and the meaning of <code>precise</code>.</p><p>Binning instructions are deduced from the type of <code>ϵ</code> as follows:</p><ol><li><code>ϵ::Int</code> divides each coordinate axis into <code>ϵ</code> equal-length intervals  that cover all data.</li><li><code>ϵ::Float64</code> divides each coordinate axis into intervals of fixed size <code>ϵ</code>, starting  from the axis minima until the data is completely covered by boxes.</li><li><code>ϵ::Vector{Int}</code> divides the i-th coordinate axis into <code>ϵ[i]</code> equal-length  intervals that cover all data.</li><li><code>ϵ::Vector{Float64}</code> divides the i-th coordinate axis into intervals of fixed size  <code>ϵ[i]</code>, starting from the axis minima until the data is completely covered by boxes.</li></ol><p><code>RectangularBinning</code> ensures all input data are covered by extending the created ranges if need be.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.FixedRectangularBinning" href="#ComplexityMeasures.FixedRectangularBinning"><code>ComplexityMeasures.FixedRectangularBinning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FixedRectangularBinning &lt;: AbstractBinning
FixedRectangularBinning(ranges::Tuple{&lt;:AbstractRange...}, precise = false)</code></pre><p>Rectangular box partition of state space where the partition along each dimension is explicitly given by each range <code>ranges</code>, which is a tuple of <code>AbstractRange</code> subtypes. Typically, each range is the output of the <code>range</code> Base function, e.g., <code>ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33))</code>. All ranges must be sorted.</p><p>The optional second argument <code>precise</code> dictates whether Julia Base&#39;s <code>TwicePrecision</code> is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is <code>false</code>.</p><p>Points falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open: <code>[a, b)</code>. <strong>This means that the last value of each of the ranges dictates the last right-closing value.</strong> This value does <em>not</em> belong to the histogram! E.g., if given a range <code>r = range(0, 1; length = 11)</code>, with <code>r[end] = 1</code>, the value <code>1</code> is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here <code>[0.9, 1)</code>)!</p><p><strong>Equivalently, the size of the histogram is <code>histsize = map(r -&gt; length(r)-1, ranges)</code>!</strong></p><p><code>FixedRectangularBinning</code> leads to a well-defined outcome space without knowledge of input data, see <a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a>.</p></div></section></article><h3 id="Symbolic-permutations"><a class="docs-heading-anchor" href="#Symbolic-permutations">Symbolic permutations</a><a id="Symbolic-permutations-1"></a><a class="docs-heading-anchor-permalink" href="#Symbolic-permutations" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.SymbolicPermutation" href="#ComplexityMeasures.SymbolicPermutation"><code>ComplexityMeasures.SymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicPermutation &lt;: ProbabilitiesEstimator
SymbolicPermutation(; m = 3, τ = 1, lt::Function = ComplexityMeasures.isless_rand)</code></pre><p>A probabilities estimator based on ordinal permutation patterns.</p><p>When passed to <a href="#ComplexityMeasures.probabilities"><code>probabilities</code></a> the output depends on the input data type:</p><ul><li><strong>Univariate data</strong>. If applied to a univariate timeseries (<code>AbstractVector</code>), then the timeseries   is first embedded using embedding delay <code>τ</code> and dimension <code>m</code>, resulting in embedding   vectors <span>$\{ \bf{x}_i \}_{i=1}^{N-(m-1)\tau}$</span>. Then, for each <span>$\bf{x}_i$</span>,   we find its permutation pattern <span>$\pi_{i}$</span>. Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using <a href="#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a>. When giving the resulting probabilities to   <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, the original permutation entropy is computed <sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup>.</li><li><strong>Multivariate data</strong>. If applied to a an <code>D</code>-dimensional <code>Dataset</code>,   then no embedding is constructed, <code>m</code> must be equal to <code>D</code> and <code>τ</code> is ignored.   Each vector <span>$\bf{x}_i$</span> of the dataset is mapped   directly to its permutation pattern <span>$\pi_{i}$</span> by comparing the   relative magnitudes of the elements of <span>$\bf{x}_i$</span>.   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy<sup class="footnote-reference"><a id="citeref-He2016" href="#footnote-He2016">[He2016]</a></sup>, although here we don&#39;t perform any further subdivision   of the permutation patterns (as in Figure 3 of<sup class="footnote-reference"><a id="citeref-He2016" href="#footnote-He2016">[He2016]</a></sup>).</li></ul><p>Internally, <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a> uses the <a href="#ComplexityMeasures.OrdinalPatternEncoding"><code>OrdinalPatternEncoding</code></a> to represent ordinal patterns as integers for efficient computations.</p><p>See <a href="#ComplexityMeasures.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a> and <a href="#ComplexityMeasures.SymbolicAmplitudeAwarePermutation"><code>SymbolicAmplitudeAwarePermutation</code></a> for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see <a href="#ComplexityMeasures.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>.</p><div class="admonition is-info"><header class="admonition-header">Handling equal values in ordinal patterns</header><div class="admonition-body"><p>In Bandt &amp; Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution <sup class="footnote-reference"><a id="citeref-Zunino2017" href="#footnote-Zunino2017">[Zunino2017]</a></sup>. Here, by default, if two values are equal, then one of the is randomly assigned as &quot;the largest&quot;, using <code>lt = ComplexityMeasures.isless_rand</code>. To get the behaviour from Bandt and Pompe (2002), use <code>lt = Base.isless</code>.</p></div></div><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>SymbolicPermutation</code> is the set of length-<code>m</code> ordinal patterns (i.e. permutations) that can be formed by the integers <code>1, 2, …, m</code>. There are <code>factorial(m)</code> such patterns.</p><p>For example, the outcome <code>[2, 3, 1]</code> corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also [<code>OrdinalPatternEncoding</code>(@ref).</p><p><strong>In-place symbolization</strong></p><p><code>SymbolicPermutation</code> also implements the in-place <a href="#ComplexityMeasures.probabilities!"><code>probabilities!</code></a> for <code>Dataset</code> input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example</p><pre><code class="language-julia hljs">using ComplexityMeasures
m, N = 2, 100
est = SymbolicPermutation(; m, τ)
x = Dataset(rand(N, m)) # some input dataset
πs_ts = zeros(Int, N) # length must match length of `x`
p = probabilities!(πs_ts, est, x)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.SymbolicWeightedPermutation" href="#ComplexityMeasures.SymbolicWeightedPermutation"><code>ComplexityMeasures.SymbolicWeightedPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicWeightedPermutation &lt;: ProbabilitiesEstimator
SymbolicWeightedPermutation(; τ = 1, m = 3, lt::Function = ComplexityMeasures.isless_rand)</code></pre><p>A variant of <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a> that also incorporates amplitude information, based on the weighted permutation entropy<sup class="footnote-reference"><a id="citeref-Fadlallah2013" href="#footnote-Fadlallah2013">[Fadlallah2013]</a></sup>. The outcome space and keywords are the same as in <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p><p><strong>Description</strong></p><p>For each ordinal pattern extracted from each state (or delay) vector, a weight is attached to it which is the variance of the vector. Probabilities are then estimated by summing the weights corresponding to the same pattern, instead of just counting the occurrence of the same pattern.</p><div class="admonition is-info"><header class="admonition-header">An implementation note</header><div class="admonition-body"><p><em>Note: in equation 7, section III, of the original paper, the authors write</em></p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j-(k-1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</p><p>*But given the formula they give for the arithmetic mean, this is <strong>not</strong> the variance of the delay vector <span>$\mathbf{x}_i$</span>, because the indices are mixed: <span>$x_{j+(k-1)\tau}$</span> in the weights formula, vs. <span>$x_{j+(k+1)\tau}$</span> in the arithmetic mean formula. Here, delay embedding and computation of the patterns and their weights are completely separated processes, ensuring that we compute the arithmetic mean correctly for each vector of the input dataset (which may be a delay-embedded timeseries).</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.SymbolicAmplitudeAwarePermutation" href="#ComplexityMeasures.SymbolicAmplitudeAwarePermutation"><code>ComplexityMeasures.SymbolicAmplitudeAwarePermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicAmplitudeAwarePermutation &lt;: ProbabilitiesEstimator
SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = ComplexityMeasures.isless_rand)</code></pre><p>A variant of <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a> that also incorporates amplitude information, based on the amplitude-aware permutation entropy<sup class="footnote-reference"><a id="citeref-Azami2016" href="#footnote-Azami2016">[Azami2016]</a></sup>. The outcome space and keywords are the same as in <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a>.</p><p><strong>Description</strong></p><p>Similarly to <a href="#ComplexityMeasures.SymbolicWeightedPermutation"><code>SymbolicWeightedPermutation</code></a>, a weight <span>$w_i$</span> is attached to each ordinal pattern extracted from each state (or delay) vector <span>$\mathbf{x}_i = (x_1^i, x_2^i, \ldots, x_m^i)$</span> as</p><p class="math-container">\[w_i = \dfrac{A}{m} \sum_{k=1}^m |x_k^i | + \dfrac{1-A}{d-1}
\sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\]</p><p>with <span>$0 \leq A \leq 1$</span>. When <span>$A=0$</span> , only internal differences between the elements of <span>$\mathbf{x}_i$</span> are weighted. Only mean amplitude of the state vector elements are weighted when <span>$A=1$</span>. With, <span>$0&lt;A&lt;1$</span>, a combined weighting is used.</p></div></section></article><h3 id="Dispersion-patterns"><a class="docs-heading-anchor" href="#Dispersion-patterns">Dispersion patterns</a><a id="Dispersion-patterns-1"></a><a class="docs-heading-anchor-permalink" href="#Dispersion-patterns" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Dispersion" href="#ComplexityMeasures.Dispersion"><code>ComplexityMeasures.Dispersion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)</code></pre><p>A probability estimator based on dispersion patterns, originally used by Rostaghi &amp; Azami, 2016<sup class="footnote-reference"><a id="citeref-Rostaghi2016" href="#footnote-Rostaghi2016">[Rostaghi2016]</a></sup> to compute the &quot;dispersion entropy&quot;, which characterizes the complexity and irregularity of a time series.</p><p>Recommended parameter values<sup class="footnote-reference"><a id="citeref-Li2018" href="#footnote-Li2018">[Li2018]</a></sup> are <code>m ∈ [2, 3]</code>, <code>τ = 1</code> for the embedding, and <code>c ∈ [3, 4, …, 8]</code> categories for the Gaussian symbol mapping.</p><p><strong>Description</strong></p><p>Assume we have a univariate time series <span>$X = \{x_i\}_{i=1}^N$</span>. First, this time series is encoded into a symbol timeseries <span>$S$</span> using the Gaussian encoding <a href="#ComplexityMeasures.GaussianCDFEncoding"><code>GaussianCDFEncoding</code></a> with empirical mean <code>μ</code> and empirical standard deviation <code>σ</code> (both determined from <span>$X$</span>), and <code>c</code> as given to <code>Dispersion</code>.</p><p>Then, <span>$S$</span> is embedded into an <span>$m$</span>-dimensional time series, using an embedding lag of <span>$\tau$</span>, which yields a total of <span>$N - (m - 1)\tau$</span> delay vectors <span>$z_i$</span>, or &quot;dispersion patterns&quot;. Since each element of <span>$z_i$</span> can take on <code>c</code> different values, and each delay vector has <code>m</code> entries, there are <code>c^m</code> possible dispersion patterns. This number is used for normalization when computing dispersion entropy.</p><p>The returned probabilities are simply the frequencies of the unique dispersion patterns present in <span>$S$</span> (i.e., the <a href="@ref"><code>CountOccurences</code></a> of <span>$S$</span>).</p><p><strong>Outcome space</strong></p><p>The outcome space for <code>Dispersion</code> is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of <span>$S$</span>.</p><p><strong>Data requirements and parameters</strong></p><p>The input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that <code>x</code> has at least 1000 data points.</p><p>If <code>check_unique == true</code> (default), then it is checked that the input has more than one unique value. If <code>check_unique == false</code> and the input only has one unique element, then a <code>InexactError</code> is thrown when trying to compute probabilities.</p><div class="admonition is-info"><header class="admonition-header">Why &#39;dispersion patterns&#39;?</header><div class="admonition-body"><p>Each embedding vector is called a &quot;dispersion pattern&quot;. Why? Let&#39;s consider the case when <span>$m = 5$</span> and <span>$c = 3$</span>, and use some very imprecise terminology for illustration:</p><p>When <span>$c = 3$</span>, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector <span>$[2, 2, 2, 2, 2]$</span> consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector <span>$[1, 1, 2, 3, 3]$</span>, however, represents numbers that are much more spread out (more dispersed), because the categories representing &quot;outliers&quot; both above and below the mean are represented, not only values close to the mean.</p></div></div><p>For a version of this estimator that can be used on high-dimensional arrays, see <a href="#ComplexityMeasures.SpatialDispersion"><code>SpatialDispersion</code></a>.</p></div></section></article><h3 id="Transfer-operator-(binning)"><a class="docs-heading-anchor" href="#Transfer-operator-(binning)">Transfer operator (binning)</a><a id="Transfer-operator-(binning)-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-operator-(binning)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.TransferOperator" href="#ComplexityMeasures.TransferOperator"><code>ComplexityMeasures.TransferOperator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TransferOperator &lt;: ProbabilitiesEstimator
TransferOperator(b::RectangularBinning)</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by the given binning scheme <code>b</code>, then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).</p><p>This implementation follows the grid estimator approach in Diego et al. (2019)<sup class="footnote-reference"><a id="citeref-Diego2019" href="#footnote-Diego2019">[Diego2019]</a></sup>.</p><p><strong>Outcome space</strong></p><p>The outcome space for <code>TransferOperator</code> is the set of unique bins constructed from <code>b</code>. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as <code>SVector</code>s.</p><p><strong>Bin ordering</strong></p><p>Bins returned by <a href="#ComplexityMeasures.probabilities_and_outcomes"><code>probabilities_and_outcomes</code></a> are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if</p><pre><code class="language-julia hljs">b = RectangularBinning(4)
est = TransferOperator(b)
probs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries</code></pre><p>then <code>probs[i]</code> is the invariant measure (probability) of the bin <code>outcomes[i]</code>, which is the <code>i</code>-th bin visited by the timeseries with nonzero measure.</p><p><strong>Description</strong></p><p>The transfer operator <span>$P^{N}$</span>is computed as an <code>N</code>-by-<code>N</code> matrix of transition probabilities between the states defined by the partition elements, where <code>N</code> is the number of boxes in the partition that is visited by the orbit/points.</p><p>If  <span>$\{x_t^{(D)} \}_{n=1}^L$</span> are the <span>$L$</span> different <span>$D$</span>-dimensional points over which the transfer operator is approximated, <span>$\{ C_{k=1}^N \}$</span> are the <span>$N$</span> different partition elements (as dictated by <code>ϵ</code>) that gets visited by the points, and  <span>$\phi(x_t) = x_{t+1}$</span>, then</p><p class="math-container">\[P_{ij} = \dfrac
{\#\{ x_n | \phi(x_n) \in C_j \cap x_n \in C_i \}}
{\#\{ x_m | x_m \in C_i \}},\]</p><p>where <span>$\#$</span> denotes the cardinal. The element <span>$P_{ij}$</span> thus indicates how many points that are initially in box <span>$C_i$</span> end up in box <span>$C_j$</span> when the points in <span>$C_i$</span> are projected one step forward in time. Thus, the row <span>$P_{ik}^N$</span> where <span>$k \in \{1, 2, \ldots, N \}$</span> gives the probability of jumping from the state defined by box <span>$C_i$</span> to any of the other <span>$N$</span> states. It follows that <span>$\sum_{k=1}^{N} P_{ik} = 1$</span> for all <span>$i$</span>. Thus, <span>$P^N$</span> is a row/right stochastic matrix.</p><p><strong>Invariant measure estimation from transfer operator</strong></p><p>The left invariant distribution <span>$\mathbf{\rho}^N$</span> is a row vector, where <span>$\mathbf{\rho}^N P^{N} = \mathbf{\rho}^N$</span>. Hence, <span>$\mathbf{\rho}^N$</span> is a row eigenvector of the transfer matrix <span>$P^{N}$</span> associated with eigenvalue 1. The distribution <span>$\mathbf{\rho}^N$</span> approximates the invariant density of the system subject to <code>binning</code>, and can be taken as a probability distribution over the partition elements.</p><p>In practice, the invariant measure <span>$\mathbf{\rho}^N$</span> is computed using <a href="#ComplexityMeasures.invariantmeasure"><code>invariantmeasure</code></a>, which also approximates the transfer matrix. The invariant distribution is initialized as a length-<code>N</code> random distribution which is then applied to <span>$P^{N}$</span>. The resulting length-<code>N</code> distribution is then applied to <span>$P^{N}$</span> again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.</p><p>See also: <a href="#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a>, <a href="#ComplexityMeasures.invariantmeasure"><code>invariantmeasure</code></a>.</p></div></section></article><p>For explicit estimation of the transfer operator, see <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl">ComplexityMeasures.jl</a>.</p><h4 id="Utility-methods/types"><a class="docs-heading-anchor" href="#Utility-methods/types">Utility methods/types</a><a id="Utility-methods/types-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-methods/types" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.InvariantMeasure" href="#ComplexityMeasures.InvariantMeasure"><code>ComplexityMeasures.InvariantMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InvariantMeasure(to, ρ)</code></pre><p>Minimal return struct for <a href="#ComplexityMeasures.invariantmeasure"><code>invariantmeasure</code></a> that contains the estimated invariant measure <code>ρ</code>, as well as the transfer operator <code>to</code> from which it is computed (including bin information).</p><p>See also: <a href="#ComplexityMeasures.invariantmeasure"><code>invariantmeasure</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.invariantmeasure" href="#ComplexityMeasures.invariantmeasure"><code>ComplexityMeasures.invariantmeasure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">invariantmeasure(x::AbstractDataset, binning::RectangularBinning) → iv::InvariantMeasure</code></pre><p>Estimate an invariant measure over the points in <code>x</code> based on binning the data into rectangular boxes dictated by the <code>binning</code>, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.</p><p>Details on the estimation procedure is found the <a href="#ComplexityMeasures.TransferOperator"><code>TransferOperator</code></a> docstring.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using DynamicalSystems, Plots, ComplexityMeasures
D = 4
ds = Systems.lorenz96(D; F = 32.0)
N, dt = 20000, 0.1
orbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)

# Estimate the invariant measure over some coarse graining of the orbit.
iv = invariantmeasure(orbit, RectangularBinning(15))

# Get the probabilities and bins
invariantmeasure(iv)</code></pre><p><strong>Probabilities and bin information</strong></p><pre><code class="nohighlight hljs">invariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{&lt;:SVector})</code></pre><p>From a pre-computed invariant measure, return the probabilities and associated bins. The element <code>ρ[i]</code> is the probability of visitation to the box <code>bins[i]</code>. Analogous to <a href="@ref"><code>binhist</code></a>.</p><div class="admonition is-category-hint"><header class="admonition-header">Transfer operator approach vs. naive histogram approach</header><div class="admonition-body"><p>Why bother with the transfer operator instead of using regular histograms to obtain probabilities?</p><p>In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as <span>$n \to \intfy$</span>), which is guaranteed by the ergodic theorem. There is a crucial difference, however:</p><p>The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the <em>transition probabilities</em> between states (see <a href="#ComplexityMeasures.transfermatrix"><code>transfermatrix</code></a>).</p></div></div><p>See also: <a href="#ComplexityMeasures.InvariantMeasure"><code>InvariantMeasure</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.transfermatrix" href="#ComplexityMeasures.transfermatrix"><code>ComplexityMeasures.transfermatrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{&lt;:Real, 2}, bins::Vector{&lt;:SVector})</code></pre><p>Return the transfer matrix/operator and corresponding bins. Here, <code>bins[i]</code> corresponds to the i-th row/column of the transfer matrix. Thus, the entry <code>M[i, j]</code> is the probability of jumping from the state defined by <code>bins[i]</code> to the state defined by <code>bins[j]</code>.</p><p>See also: <a href="#ComplexityMeasures.TransferOperator"><code>TransferOperator</code></a>.</p></div></section></article><h3 id="Kernel-density"><a class="docs-heading-anchor" href="#Kernel-density">Kernel density</a><a id="Kernel-density-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-density" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.NaiveKernel" href="#ComplexityMeasures.NaiveKernel"><code>ComplexityMeasures.NaiveKernel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NaiveKernel(ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) &lt;: ProbabilitiesEstimator</code></pre><p>Estimate probabilities/entropy using a &quot;naive&quot; kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) <sup class="footnote-reference"><a id="citeref-PrichardTheiler1995" href="#footnote-PrichardTheiler1995">[PrichardTheiler1995]</a></sup>.</p><p>Probabilities <span>$P(\mathbf{x}, \epsilon)$</span> are assigned to every point <span>$\mathbf{x}$</span> by counting how many other points occupy the space spanned by a hypersphere of radius <code>ϵ</code> around <span>$\mathbf{x}$</span>, according to:</p><p class="math-container">\[P_i( X, \epsilon) \approx \dfrac{1}{N} \sum_{s} B(||X_i - X_j|| &lt; \epsilon),\]</p><p>where <span>$B$</span> gives 1 if the argument is <code>true</code>. Probabilities are then normalized.</p><p><strong>Keyword arguments</strong></p><ul><li><code>method = KDTree</code>: the search structure supported by Neighborhood.jl. Specifically, use <code>KDTree</code> to use a tree-based neighbor search, or <code>BruteForce</code> for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.</li><li><code>w = 0</code>: the Theiler window, which excludes indices <span>$s$</span> that are within <span>$|i - s| ≤ w$</span> from the given point <span>$x_i$</span>.</li><li><code>metric = Euclidean()</code>: the distance metric.</li></ul><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>NaiveKernel</code> are the indices of the input data, <code>eachindex(x)</code>. Hence, input <code>x</code> is needed for a well-defined <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>. The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors).</p></div></section></article><h3 id="Local-likelihood"><a class="docs-heading-anchor" href="#Local-likelihood">Local likelihood</a><a id="Local-likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Local-likelihood" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.LocalLikelihood" href="#CausalityTools.LocalLikelihood"><code>CausalityTools.LocalLikelihood</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LocalLikelihood &lt;: ProbabilitiesEstimator
LocalLikelihood(k = 5, w = 0, metric = Euclidean())</code></pre><p>The <code>LocalLikelihood</code> estimator estimates the density around a given query point by a Gaussian kernel informed by the local mean and covariance.</p><p>To form probabilities from the pointwise density estimates, the densities are simply sum-normalized to 1.</p><p><strong>Outcome space</strong></p><p>The <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a> for <code>LocalLikelihood</code> is the set of input points.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/various/probabilities/LocalLikelihood.jl#L12-L25">source</a></section></article><h3 id="Timescales"><a class="docs-heading-anchor" href="#Timescales">Timescales</a><a id="Timescales-1"></a><a class="docs-heading-anchor-permalink" href="#Timescales" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.WaveletOverlap" href="#ComplexityMeasures.WaveletOverlap"><code>ComplexityMeasures.WaveletOverlap</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WaveletOverlap([wavelet]) &lt;: ProbabilitiesEstimator</code></pre><p>Apply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)<sup class="footnote-reference"><a id="citeref-Rosso2001" href="#footnote-Rosso2001">[Rosso2001]</a></sup>. Input timeseries <code>x</code> is needed for a well-defined outcome space.</p><p>By default the wavelet <code>Wavelets.WT.Daubechies{12}()</code> is used. Otherwise, you may choose a wavelet from the <code>Wavelets</code> package (it must subtype <code>OrthoWaveletClass</code>).</p><p><strong>Outcome space</strong></p><p>The outcome space for <code>WaveletOverlap</code> are the integers <code>1, 2, …, N</code> enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can <a href=" https:/github.com/kahaaga/waveletentropy_example/blob/main/wavelet_entropy_example.ipynb">view online</a>. As such, this estimator only works for timeseries input and input <code>x</code> is needed for a well-defined <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.PowerSpectrum" href="#ComplexityMeasures.PowerSpectrum"><code>ComplexityMeasures.PowerSpectrum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PowerSpectrum() &lt;: ProbabilitiesEstimator</code></pre><p>Calculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as <em>spectral entropy</em>, e.g. <sup class="footnote-reference"><a id="citeref-Llanos2016" href="#footnote-Llanos2016">[Llanos2016]</a></sup>,<sup class="footnote-reference"><a id="citeref-Tian2017" href="#footnote-Tian2017">[Tian2017]</a></sup>.</p><p>The closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can&#39;t compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.</p><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>PowerSpectrum</code> is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be <code>1</code>. Input <code>x</code> is needed for a well-defined <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>.</p></div></section></article><h3 id="Diversity"><a class="docs-heading-anchor" href="#Diversity">Diversity</a><a id="Diversity-1"></a><a class="docs-heading-anchor-permalink" href="#Diversity" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Diversity" href="#ComplexityMeasures.Diversity"><code>ComplexityMeasures.Diversity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Diversity(; m::Int, τ::Int, nbins::Int)</code></pre><p>A <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> based on the cosine similarity. It can be used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute the diversity entropy of an input timeseries<sup class="footnote-reference"><a id="citeref-Wang2020" href="#footnote-Wang2020">[Wang2020]</a></sup>.</p><p>The implementation here allows for <code>τ != 1</code>, which was not considered in the original paper.</p><p><strong>Description</strong></p><p>Diversity probabilities are computed as follows.</p><ol><li>From the input time series <code>x</code>, using embedding lag <code>τ</code> and embedding dimension <code>m</code>,  construct the embedding  <span>$Y = \{\bf x_i \} = \{(x_{i}, x_{i+\tau}, x_{i+2\tau}, \ldots, x_{i+m\tau - 1}\}_{i = 1}^{N-mτ}$</span>.</li><li>Compute <span>$D = \{d(\bf x_t, \bf x_{t+1}) \}_{t=1}^{N-mτ-1}$</span>,  where <span>$d(\cdot, \cdot)$</span> is the cosine similarity between two <code>m</code>-dimensional  vectors in the embedding.</li><li>Divide the interval <code>[-1, 1]</code> into <code>nbins</code> equally sized subintervals (including the value <code>+1</code>).</li><li>Construct a histogram of cosine similarities <span>$d \in D$</span> over those subintervals.</li><li>Sum-normalize the histogram to obtain probabilities.</li></ol><p><strong>Outcome space</strong></p><p>The outcome space for <code>Diversity</code> is the bins of the <code>[-1, 1]</code> interval, and the return configuration is the same as in <a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a> (left bin edge).</p></div></section></article><h3 id="Spatial-estimators"><a class="docs-heading-anchor" href="#Spatial-estimators">Spatial estimators</a><a id="Spatial-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Spatial-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.SpatialSymbolicPermutation" href="#ComplexityMeasures.SpatialSymbolicPermutation"><code>ComplexityMeasures.SpatialSymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SpatialSymbolicPermutation &lt;: ProbabilitiesEstimator
SpatialSymbolicPermutation(stencil, x; periodic = true)</code></pre><p>A symbolic, permutation-based probabilities estimator for spatiotemporal systems that generalises <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a> to high-dimensional arrays. The order <code>m</code> of the permutation pattern is extracted from the <code>stencil</code>, see below.</p><p><code>SpatialSymbolicPermutation</code> is based on the 2D and 3D <em>spatiotemporal permutation entropy</em> estimators by by Ribeiro et al. (2012)<sup class="footnote-reference"><a id="citeref-Ribeiro2012" href="#footnote-Ribeiro2012">[Ribeiro2012]</a></sup> and Schlemmer et al. (2018)<sup class="footnote-reference"><a id="citeref-Schlemmer2018" href="#footnote-Schlemmer2018">[Schlemmer2018]</a></sup>), respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for <code>D</code>-dimensional input array <code>x</code>, with arbitrary regions (stencils) to get patterns form and (possibly) periodic boundary conditions.</p><p>See below for ways to specify the <code>stencil</code>. If <code>periodic = true</code>, then the stencil wraps around at the ends of the array. If <code>false</code>, then collected regions with indices which exceed the array bounds are skipped.</p><p>In combination with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> and <a href="#ComplexityMeasures.entropy_normalized"><code>entropy_normalized</code></a>, this probabilities estimator can be used to compute generalized spatiotemporal permutation <a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a> of any type.</p><p><strong>Outcome space</strong></p><p>The outcome space <code>Ω</code> for <code>SpatialSymbolicPermutation</code> is the set of length-<code>m</code> ordinal patterns (i.e. permutations) that can be formed by the integers <code>1, 2, …, m</code>, ordered lexicographically. There are <code>factorial(m)</code> such patterns. Here <code>m</code> refers to the number of points included in <code>stencil</code>.</p><p><strong>Stencils</strong></p><p>The <code>stencil</code> defines what local area to use to group hypervoxels. Each grouping of hypervoxels is mapped to an order-<code>m</code> permutation pattern, which is then mapped to an integer as in <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a>. The <code>stencil</code> is moved around the input array, in a sense &quot;scanning&quot; the input array, to collect all possible groupings allowed by the boundary condition (periodic or not).</p><p>Stencils are passed in one of the following three ways:</p><ol><li>As vectors of <code>CartesianIndex</code> which encode the offset of indices to include in the  stencil, with respect to the current array index when scanning over the array.  For example <code>stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)])</code>.  Don&#39;t forget to include the zero offset index if you want to include the hypervoxel  itself, which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of <code>CartesianIndex</code>, <code>m = length(stencil)</code>.</li><li>As a <code>D</code>-dimensional array (where <code>D</code> matches the dimensionality of the input data)  containing <code>0</code>s and <code>1</code>s, where if <code>stencil[index] == 1</code>, the corresponding pixel is  included, and if <code>stencil[index] == 0</code>, it is not included.  To generate the same estimator as in 1., use <code>stencil = [1 1; 1 1]</code>.  When passing a stencil as a <code>D</code>-dimensional array, <code>m = sum(stencil)</code></li><li>As a <code>Tuple</code> containing two <code>Tuple</code>s, both of length <code>D</code>, for <code>D</code>-dimensional data.  The first tuple specifies the <code>extent</code> of the stencil, where <code>extent[i]</code>  dictates the number of hypervoxels to be included along the <code>i</code>th axis and <code>lag[i]</code>  the separation of hypervoxels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here <code>stencil = ((2, 2), (1, 1))</code>.  When passing a stencil using <code>extent</code> and <code>lag</code>, <code>m = prod(extent)</code>.</li></ol></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.SpatialDispersion" href="#ComplexityMeasures.SpatialDispersion"><code>ComplexityMeasures.SpatialDispersion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SpatialDispersion &lt;: ProbabilitiesEstimator
SpatialDispersion(stencil, x::AbstractArray;
    periodic = true,
    c = 5,
    skip_encoding = false,
    L = nothing,
)</code></pre><p>A dispersion-based probabilities estimator that generalises <a href="#ComplexityMeasures.Dispersion"><code>Dispersion</code></a> for input data that are high-dimensional arrays.</p><p><code>SpatialDispersion</code> is based on Azami et al. (2019)<sup class="footnote-reference"><a id="citeref-Azami2019" href="#footnote-Azami2019">[Azami2019]</a></sup>&#39;s 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for <code>N</code>-dimensional input data <code>x</code>, with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions.</p><p>In combination with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> and <a href="#ComplexityMeasures.entropy_normalized"><code>entropy_normalized</code></a>, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion <a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a> of any type.</p><p><strong>Arguments</strong></p><ul><li><code>stencil</code>. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   <a href="#ComplexityMeasures.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>. See <a href="#ComplexityMeasures.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a> for   more information about stencils.</li><li><code>x::AbstractArray</code>. The input data. Must be provided because we need to know its size   for optimization and bound checking.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>periodic::Bool</code>. If <code>periodic == true</code>, then the stencil should wrap around at the   end of the array. If <code>periodic = false</code>, then pixels whose stencil exceeds the array   bounds are skipped.</li><li><code>c::Int</code>. Determines how many discrete categories to use for the Gaussian encoding.</li><li><code>skip_encoding</code>. If <code>skip_encoding == true</code>, <code>encoding</code> is ignored, and dispersion   patterns are computed directly from <code>x</code>, under the assumption that <code>L</code> is the alphabet   length for <code>x</code> (useful for categorical or integer data). Thus, if   <code>skip_encoding == true</code>, then <code>L</code> must also be specified. This is useful for   categorical or integer-valued data.</li><li><code>L</code>. If <code>L == nothing</code> (default), then the number of total outcomes is inferred from   <code>stencil</code> and <code>encoding</code>. If <code>L</code> is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to <code>L</code>.</li></ul><p><strong>Outcome space</strong></p><p>The outcome space for <code>SpatialDispersion</code> is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all <code>m</code>-dimensional delay vectors whose elements are all possible values in <code>1:c</code>. There are <code>c^m</code> such vectors.</p><p><strong>Description</strong></p><p>Estimating probabilities/entropies from higher-dimensional data is conceptually simple.</p><ol><li>Discretize each value (hypervoxel) in <code>x</code> relative to all other values <code>xᵢ ∈ x</code> using the  provided <code>encoding</code> scheme.</li><li>Use <code>stencil</code> to extract relevant (discretized) points around each hypervoxel.</li><li>Construct a symbol these points.</li><li>Take the sum-normalized histogram of the symbol as a probability distribution.</li><li>Optionally, compute <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> or <a href="#ComplexityMeasures.entropy_normalized"><code>entropy_normalized</code></a> from this  probability distribution.</li></ol><p><strong>Usage</strong></p><p>Here&#39;s how to compute spatial dispersion entropy using the three different ways of specifying stencils.</p><pre><code class="language-julia hljs">x = rand(50, 50) # first &quot;time slice&quot; of a spatial system evolution

# Cartesian stencil
stencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])
est = SpatialDispersion(stencil_cartesian, x)
entropy_normalized(est, x)

# Extent/lag stencil
extent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)
est = SpatialDispersion(stencil_ext_lag, x)
entropy_normalized(est, x)

# Matrix stencil
stencil_matrix = [1 1; 1 1]
est = SpatialDispersion(stencil_matrix, x)
entropy_normalized(est, x)</code></pre><p>To apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.:</p><pre><code class="language-julia hljs">imgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds
stencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)
est = SpatialDispersion(stencil, first(imgs))
h_vs_t = entropy_normalized.(Ref(est), imgs)</code></pre><p>Computing generalized spatiotemporal dispersion entropy is trivial, e.g. with <a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>:</p><pre><code class="language-julia hljs">x = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)
est = SpatialDispersion(stencil, x)
entropy(Renyi(q = 2), est, x)</code></pre><p>See also: <a href="#ComplexityMeasures.SpatialSymbolicPermutation"><code>SpatialSymbolicPermutation</code></a>, <a href="#ComplexityMeasures.GaussianCDFEncoding"><code>GaussianCDFEncoding</code></a>, <a href="@ref"><code>symbolize</code></a>.</p></div></section></article><h3 id="Encodings"><a class="docs-heading-anchor" href="#Encodings">Encodings</a><a id="Encodings-1"></a><a class="docs-heading-anchor-permalink" href="#Encodings" title="Permalink"></a></h3><h4 id="Encodings-API"><a class="docs-heading-anchor" href="#Encodings-API">Encodings API</a><a id="Encodings-API-1"></a><a class="docs-heading-anchor-permalink" href="#Encodings-API" title="Permalink"></a></h4><p>Some probability estimators first &quot;encode&quot; input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an &quot;encoding&quot;.</p><p>The encodings API is defined by:</p><ul><li><a href="#ComplexityMeasures.Encoding"><code>Encoding</code></a></li><li><a href="#ComplexityMeasures.encode"><code>encode</code></a></li><li><a href="#ComplexityMeasures.decode"><code>decode</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Encoding" href="#ComplexityMeasures.Encoding"><code>ComplexityMeasures.Encoding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Encoding</code></pre><p>The supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions <a href="#ComplexityMeasures.encode"><code>encode</code></a> and <a href="#ComplexityMeasures.decode"><code>decode</code></a>. Some probability estimators utilize encodings internally.</p><p>Current available encodings are:</p><ul><li><a href="#ComplexityMeasures.OrdinalPatternEncoding"><code>OrdinalPatternEncoding</code></a>.</li><li><a href="#ComplexityMeasures.GaussianCDFEncoding"><code>GaussianCDFEncoding</code></a>.</li><li><a href="#ComplexityMeasures.RectangularBinEncoding"><code>RectangularBinEncoding</code></a>.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.encode" href="#ComplexityMeasures.encode"><code>ComplexityMeasures.encode</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">encode(c::Encoding, χ) -&gt; i::Int</code></pre><p>Encode an element <code>χ ∈ x</code> of input data <code>x</code> (those given to <a href="#ComplexityMeasures.probabilities"><code>probabilities</code></a>) using encoding <code>c</code>.</p><p>The special value of <code>-1</code> is reserved as a return value for inappropriate elements <code>χ</code> that cannot be encoded according to <code>c</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.decode" href="#ComplexityMeasures.decode"><code>ComplexityMeasures.decode</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">decode(c::Encoding, i::Int) -&gt; ω</code></pre><p>Decode an encoded element <code>i</code> into the outcome <code>ω ∈ Ω</code> it corresponds to.</p><p><code>Ω</code> is the <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a> of a probabilities estimator that uses encoding <code>c</code>.</p></div></section></article><h4 id="Available-encodings"><a class="docs-heading-anchor" href="#Available-encodings">Available encodings</a><a id="Available-encodings-1"></a><a class="docs-heading-anchor-permalink" href="#Available-encodings" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.OrdinalPatternEncoding" href="#ComplexityMeasures.OrdinalPatternEncoding"><code>ComplexityMeasures.OrdinalPatternEncoding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OrdinalPatternEncoding &lt;: Encoding
OrdinalPatternEncoding(m::Int, lt = ComplexityMeasures.isless_rand)</code></pre><p>An encoding scheme that <a href="#ComplexityMeasures.encode"><code>encode</code></a>s length-<code>m</code> vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by <a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a> and similar estimators, see that for a description of the outcome space.</p><p>The ordinal/permutation pattern of a vector <code>χ</code> is simply <code>sortperm(χ)</code>, which gives the indices that would sort <code>χ</code> in ascending order.</p><p><strong>Description</strong></p><p>The Lehmer code, as implemented here, is a bijection between the set of <code>factorial(m)</code> possible permutations for a length-<code>m</code> sequence, and the integers <code>1, 2, …, factorial(m)</code>. The encoding step uses algorithm 1 in Berger et al. (2019)<sup class="footnote-reference"><a id="citeref-Berger2019" href="#footnote-Berger2019">[Berger2019]</a></sup>, which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!).</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using ComplexityMeasures

julia&gt; χ = [4.0, 1.0, 9.0];

julia&gt; c = OrdinalPatternEncoding(3);

julia&gt; i = encode(c, χ)
3

julia&gt; decode(c, i)
3-element SVector{3, Int64} with indices SOneTo(3):
 2
 1
 3</code></pre><p>If you want to encode something that is already a permutation pattern, then you can use the non-exported <code>permutation_to_integer</code> function.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.GaussianCDFEncoding" href="#ComplexityMeasures.GaussianCDFEncoding"><code>ComplexityMeasures.GaussianCDFEncoding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianCDFEncoding &lt;: Encoding
GaussianCDFEncoding(; μ, σ, c::Int = 3)</code></pre><p>An encoding scheme that <a href="#ComplexityMeasures.encode"><code>encode</code></a>s a scalar value into one of the integers <code>sᵢ ∈ [1, 2, …, c]</code> based on the normal cumulative distribution function (NCDF), and <a href="#ComplexityMeasures.decode"><code>decode</code></a>s the <code>sᵢ</code> into subintervals of <code>[0, 1]</code> (with some loss of information).</p><p>Notice that the decoding step does not yield an element of any outcome space of the estimators that use <code>GaussianCDFEncoding</code> internally, such as <a href="#ComplexityMeasures.Dispersion"><code>Dispersion</code></a>. That is because these estimators additionally delay embed the encoded data.</p><p><strong>Description</strong></p><p><code>GaussianCDFEncoding</code> first maps an input point <span>$x$</span>  (scalar) to a new real number <span>$y_ \in [0, 1]$</span> by using the normal cumulative distribution function (CDF) with the given mean <code>μ</code> and standard deviation <code>σ</code>, according to the map</p><p class="math-container">\[x \to y : y = \dfrac{1}{ \sigma
    \sqrt{2 \pi}} \int_{-\infty}^{x} e^{(-(x - \mu)^2)/(2 \sigma^2)} dx.\]</p><p>Next, the interval <code>[0, 1]</code> is equidistantly binned and enumerated <span>$1, 2, \ldots, c$</span>,  and <span>$y$</span> is linearly mapped to one of these integers using the linear map  <span>$y \to z : z = \text{floor}(y(c-1)) + 1$</span>.</p><p>Because of the floor operation, some information is lost, so when used with <a href="#ComplexityMeasures.decode"><code>decode</code></a>, each decoded <code>sᵢ</code> is mapped to a <em>subinterval</em> of <code>[0, 1]</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using ComplexityMeasures, Statistics

julia&gt; x = [0.1, 0.4, 0.7, -2.1, 8.0];

julia&gt; μ, σ = mean(x), std(x); encoding = GaussianCDFEncoding(; μ, σ, c = 5)

julia&gt; es = encode.(Ref(encoding), x)
5-element Vector{Int64}:
 2
 2
 3
 1
 5

julia&gt; decode(encoding, 3)
2-element SVector{2, Float64} with indices SOneTo(2):
 0.4
 0.6</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.RectangularBinEncoding" href="#ComplexityMeasures.RectangularBinEncoding"><code>ComplexityMeasures.RectangularBinEncoding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RectangularBinEncoding &lt;: Encoding
RectangularBinEncoding(binning::RectangularBinning, x)
RectangularBinEncoding(binning::FixedRectangularBinning)</code></pre><p>An encoding scheme that <a href="#ComplexityMeasures.encode"><code>encode</code></a>s points <code>χ ∈ x</code> into their histogram bins.</p><p>The first call signature simply initializes a <a href="#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> and then calls the second call signature.</p><p>See <a href="#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> for info on mapping points to bins.</p></div></section></article><h3 id="Contingency-tables"><a class="docs-heading-anchor" href="#Contingency-tables">Contingency tables</a><a id="Contingency-tables-1"></a><a class="docs-heading-anchor-permalink" href="#Contingency-tables" title="Permalink"></a></h3><p>To estimate discrete information theoretic quantities that are functions of more than one variable, we must estimate empirical joint probability mass functions (pmf). The function <a href="#CausalityTools.contingency_matrix"><code>contingency_matrix</code></a> accepts an arbitrary number of equal-length input data and returns the corresponding multidimensional contingency table as a <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>. From this table, we can extract the necessary joint and marginal pmfs for computing any discrete function of multivariate discrete probability distributions. This is essentially the multivariate analogue of <a href="#ComplexityMeasures.Probabilities"><code>Probabilities</code></a>.</p><p>But why would I use a <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a> instead of some other indirect estimation method, you may ask. The answer is that <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a> allows you to compute <em>any</em> of the information theoretic quantities offered in this package for <em>any</em> type of input data. You input data can literally be any hashable type, for example <code>String</code>, <code>Tuple{Int, String, Int}</code>, or <code>YourCustomHashableDataType</code>.</p><p>In the case of numeric data, using a <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a> is typically a bit slower than other dedicated estimation procedures. For example, quantities like discrete Shannon-type <a href="#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a> are faster to estimate using a formulation based on sums of four entropies (the H4-principle). This is faster because we can both utilize the blazingly fast <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> structure directly, and we can avoid <em>explicitly</em> estimating the entire joint pmf, which demands many extra calculation steps. Whatever you use in practice depends on your use case and available estimation methods, but you can always fall back to contingency matrices for any discrete measure.</p><h4 id="Contingency-matrix-API"><a class="docs-heading-anchor" href="#Contingency-matrix-API">Contingency matrix API</a><a id="Contingency-matrix-API-1"></a><a class="docs-heading-anchor-permalink" href="#Contingency-matrix-API" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.ContingencyMatrix" href="#CausalityTools.ContingencyMatrix"><code>CausalityTools.ContingencyMatrix</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ContingencyMatrix{T, N} &lt;: Probabilities{T, N}
ContingencyMatrix(frequencies::AbstractArray{Int, N})</code></pre><p>A contingency matrix is essentially a multivariate analogue of <a href="#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> that also keep track of raw frequencies.</p><p>The contingency matrix can be constructed directyly from an <code>N</code>-dimensional <code>frequencies</code> array. Alternatively, the <a href="#CausalityTools.contingency_matrix"><code>contingency_matrix</code></a> function performs counting for you; this works on both raw categorical data, or by first discretizing data using a a <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Description</strong></p><p>A <code>ContingencyMatrix</code> <code>c</code> is just a simple wrapper around around <code>AbstractArray{T, N}</code>. Indexing <code>c</code> with multiple indices <code>i, j, …</code> returns the <code>(i, j, …)</code>th element of the empirical probability mass function (pmf). The following convencience methods are defined:</p><ul><li><code>frequencies(c; dims)</code> returns the multivariate raw counts along the given `dims   (default to all available dimensions).</li><li><code>probabilities(c; dims)</code> returns a multidimensional empirical   probability mass function (pmf) along the given <code>dims</code> (defaults to all available   dimensions), i.e. the normalized counts.</li><li><code>probabilities(c, i::Int)</code> returns the marginal probabilities for the <code>i</code>-th dimension.</li><li><code>outcomes(c, i::Int)</code> returns the marginal outcomes for the <code>i</code>-th dimension.</li></ul><p><strong>Ordering</strong></p><p>The ordering of outcomes are internally consistent, but we make no promise on the ordering of outcomes relative to the input data. This means that if your input data are <code>x = rand([&quot;yes&quot;, &quot;no&quot;], 100); y = rand([&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;], 100)</code>, you&#39;ll get a 2-by-3 contingency matrix, but there currently no easy way to determine which outcome the i-j-th row/column of this matrix corresponds to.</p><p>Since <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a> is intended for use in information theoretic methods that don&#39;t care about ordering, as long as the ordering is internally consistent, this is not an issue for practical applications in this package. This may change in future releases.</p><p><strong>Usage</strong></p><p>Contingency matrices is used in the computation of discrete versions of the following quantities:</p><ul><li><a href="@ref"><code>entropy_joint</code></a>.</li><li><a href="#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>.</li><li><a href="#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/contingency_matrices.jl#L17-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.contingency_matrix" href="#CausalityTools.contingency_matrix"><code>CausalityTools.contingency_matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">contingency_matrix(x, y, [z, ...]) → c::ContingencyMatrix
contingency_matrix(est::ProbabilitiesEstimator, x, y, [z, ...]) → c::ContingencyMatrix</code></pre><p>Estimate a multidimensional contingency matrix <code>c</code> from input data <code>x, y, …</code>, where the input data can be of any and different types, as long as <code>length(x) == length(y) == …</code>.</p><p>For already discretized data, use the first method. For continuous data, you want to discretize the data before computing the contingency table. You can do this manually and then use the first method. Alternatively, you can provide a <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> as the first argument to the constructor. Then the input variables <code>x, y, …</code> are discretized <em>separately</em> according to <code>est</code> (<em>enforcing the same outcome space for all variables</em>), by calling <a href="#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/contingency_matrices.jl#L97-L111">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.marginal_encodings" href="#CausalityTools.marginal_encodings"><code>CausalityTools.marginal_encodings</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">marginal_encodings(est::ProbabilitiesEstimator, x::VectorOrDataset...)</code></pre><p>Encode/discretize each input vector <code>xᵢ ∈ x</code> according to a procedure determined by <code>est</code>. Any <code>xᵢ ∈ X</code> that are multidimensional (<a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a>s) will be encoded column-wise, i.e. each column of <code>xᵢ</code> is treated as a timeseries and is encoded separately.</p><p>This is useful for computing any discrete information theoretic quantity, and is used internally by <a href="#CausalityTools.contingency_matrix"><code>contingency_matrix</code></a>.</p><p><strong>Supported estimators</strong></p><ul><li><a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a>. Bin visitation frequencies are counted in the joint space <code>XY</code>,   then marginal visitations are obtained from the joint bin visits.   This behaviour is the same for both <a href="#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> and   <a href="#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a> (which adapts the grid to the data).   When using <a href="#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a>, the range along the first dimension   is used as a template for all other dimensions.</li><li><a href="#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a>. Each timeseries is separately <a href="#ComplexityMeasures.encode"><code>encode</code></a>d according   to its ordinal pattern.</li><li><a href="#ComplexityMeasures.Dispersion"><code>Dispersion</code></a>. Each timeseries is separately <a href="#ComplexityMeasures.encode"><code>encode</code></a>d according to its   dispersion pattern.</li></ul><p>Many more implementations are possible. Each new implementation gives one new way of estimating the <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/marginal_encodings.jl#L4-L29">source</a></section></article><h2 id="entropies"><a class="docs-heading-anchor" href="#entropies">Entropies</a><a id="entropies-1"></a><a class="docs-heading-anchor-permalink" href="#entropies" title="Permalink"></a></h2><h3 id="Entropies-API"><a class="docs-heading-anchor" href="#Entropies-API">Entropies API</a><a id="Entropies-API-1"></a><a class="docs-heading-anchor-permalink" href="#Entropies-API" title="Permalink"></a></h3><p>The entropies API is defined by</p><ul><li><a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a></li><li><a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a></li><li><a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></li></ul><p>Please be sure you have read the <a href="@ref">Terminology</a> section before going through the API here, to have a good idea of the different &quot;flavors&quot; of entropies and how they all come together over the common interface of the <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> function.</p><h3 id="Entropy-definitions"><a class="docs-heading-anchor" href="#Entropy-definitions">Entropy definitions</a><a id="Entropy-definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Entropy-definitions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.EntropyDefinition" href="#ComplexityMeasures.EntropyDefinition"><code>ComplexityMeasures.EntropyDefinition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EntropyDefinition</code></pre><p><code>EntropyDefinition</code> is the supertype of all types that encapsulate definitions of (generalized) entropies. These also serve as estimators of discrete entropies, see description below.</p><p>Currently implemented entropy definitions are:</p><ul><li><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>.</li><li><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>.</li><li><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li><li><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a>.</li><li><a href="#ComplexityMeasures.Curado"><code>Curado</code></a>.</li><li><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a>.</li></ul><p>These types can be given as inputs to <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> or <a href="#ComplexityMeasures.entropy_normalized"><code>entropy_normalized</code></a>.</p><p><strong>Description</strong></p><p>Mathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amigó et al.&#39;s<sup class="footnote-reference"><a id="citeref-Amigó2018" href="#footnote-Amigó2018">[Amigó2018]</a></sup> summary paper gives a nice overview.</p><p>However, for a software implementation computing entropies <em>in practice</em>, definitions is not really what matters; <strong>estimators matter</strong>. Because in the practical sense, one needs to estimate a definition from finite data, and different ways of estimating a quantity come with their own pros and cons.</p><p>That is why the type <a href="@ref"><code>DiscreteEntropyEstimator</code></a> exists, which is what is actually given to <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>. Some ways to estimate a discrete entropy only apply to a specific entropy definition. For estimators that can be applied to various entropy definitions, this is specified by providing an instance of <code>EntropyDefinition</code> to the estimator.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Shannon" href="#ComplexityMeasures.Shannon"><code>ComplexityMeasures.Shannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Shannon &lt;: EntropyDefinition
Shannon(; base = 2)</code></pre><p>The Shannon<sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup> entropy, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$\log$</span> at the given <code>base</code>.</p><p>The maximum value of the Shannon entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Renyi" href="#ComplexityMeasures.Renyi"><code>ComplexityMeasures.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: EntropyDefinition
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>The maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Tsallis" href="#ComplexityMeasures.Tsallis"><code>ComplexityMeasures.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: EntropyDefinition
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to Shannon entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>The maximum value of the Tsallis entropy is ``<span>$k(L^{1 - q} - 1)/(1 - q)$</span>, with <span>$L$</span> the <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kaniadakis" href="#ComplexityMeasures.Kaniadakis"><code>ComplexityMeasures.Kaniadakis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kaniadakis &lt;: EntropyDefinition
Kaniadakis(; κ = 1.0, base = 2.0)</code></pre><p>The Kaniadakis entropy (Tsallis, 2009)<sup class="footnote-reference"><a id="citeref-Tsallis2009" href="#footnote-Tsallis2009">[Tsallis2009]</a></sup>, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute</p><p class="math-container">\[H_K(p) = -\sum_{i=1}^N p_i f_\kappa(p_i),\]</p><p class="math-container">\[f_\kappa (x) = \dfrac{x^\kappa - x^{-\kappa}}{2\kappa},\]</p><p>where if <span>$\kappa = 0$</span>, regular logarithm to the given <code>base</code> is used, and 0 probabilities are skipped.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Curado" href="#ComplexityMeasures.Curado"><code>ComplexityMeasures.Curado</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Curado &lt;: EntropyDefinition
Curado(; b = 1.0)</code></pre><p>The Curado entropy (Curado &amp; Nobre, 2004)<sup class="footnote-reference"><a id="citeref-Curado2004" href="#footnote-Curado2004">[Curado2004]</a></sup>, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute</p><p class="math-container">\[H_C(p) = \left( \sum_{i=1}^N e^{-b p_i} \right) + e^{-b} - 1,\]</p><p>with <code>b ∈ ℛ, b &gt; 0</code>, where the terms outside the sum ensures that <span>$H_C(0) = H_C(1) = 0$</span>.</p><p>The maximum entropy for Curado is <span>$L(1 - \exp(-b/L)) + \exp(-b) - 1$</span> with <span>$L$</span> the <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.StretchedExponential" href="#ComplexityMeasures.StretchedExponential"><code>ComplexityMeasures.StretchedExponential</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StretchedExponential &lt;: EntropyDefinition
StretchedExponential(; η = 2.0, base = 2)</code></pre><p>The stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo &amp; Plastino, 1999<sup class="footnote-reference"><a id="citeref-Anteneodo1999" href="#footnote-Anteneodo1999">[Anteneodo1999]</a></sup>), used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute</p><p class="math-container">\[S_{\eta}(p) = \sum_{i = 1}^N
\Gamma \left( \dfrac{\eta + 1}{\eta}, - \log_{base}(p_i) \right) -
p_i \Gamma \left( \dfrac{\eta + 1}{\eta} \right),\]</p><p>where <span>$\eta \geq 0$</span>, <span>$\Gamma(\cdot, \cdot)$</span> is the upper incomplete Gamma function, and <span>$\Gamma(\cdot) = \Gamma(\cdot, 0)$</span> is the Gamma function. Reduces to <a href="@ref">Shannon</a> entropy for <code>η = 1.0</code>.</p><p>The maximum entropy for <code>StrechedExponential</code> is a rather complicated expression involving incomplete Gamma functions (see source code).</p></div></section></article><h3 id="Discrete"><a class="docs-heading-anchor" href="#Discrete">Discrete</a><a id="Discrete-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}" href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>ComplexityMeasures.entropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">entropy([e::DiscreteEntropyEstimator,] probs::Probabilities)
entropy([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x)</code></pre><p>Compute the <strong>discrete entropy</strong> <code>h::Real ∈ [0, ∞)</code>, using the estimator <code>e</code>, in one of two ways:</p><ol><li>Directly from existing <a href="#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> <code>probs</code>.</li><li>From input data <code>x</code>, by first estimating a probability mass function using the provided <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>, and then computing the entropy from that mass fuction using the provided <a href="@ref"><code>DiscreteEntropyEstimator</code></a>.</li></ol><p>Instead of providing a <a href="@ref"><code>DiscreteEntropyEstimator</code></a>, an <a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a> can be given directly, in which case <a href="@ref"><code>MLEntropy</code></a> is used as the estimator. If <code>e</code> is not provided, <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a><code>()</code> is used by default.</p><p><strong>Maximum entropy and normalized entropy</strong></p><p>All discrete entropies have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the <a href="#ComplexityMeasures.entropy_maximum"><code>entropy_maximum</code></a>. Or, one can use <a href="#ComplexityMeasures.entropy_normalized"><code>entropy_normalized</code></a> to obtain the normalized form of the entropy (divided by the maximum).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = entropy(ps) # gives 1, about 1 bit by definition
h = entropy(Shannon(), ps) # syntactically equivalent to above
h = entropy(Shannon(), CountOccurrences(x), x) # syntactically equivalent to above
h = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition
h = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy_maximum" href="#ComplexityMeasures.entropy_maximum"><code>ComplexityMeasures.entropy_maximum</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_maximum(e::EntropyDefinition, est::ProbabilitiesEstimator, x)</code></pre><p>Return the maximum value of a discrete entropy with the given probabilities estimator and input data <code>x</code>. Like in <a href="#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>, for some estimators the concrete outcome space is known without knowledge of input <code>x</code>, in which case the function dispatches to <code>entropy_maximum(e, est)</code>.</p><pre><code class="nohighlight hljs">entropy_maximum(e::EntropyDefinition, L::Int)</code></pre><p>Same as above, but computed directly from the number of total outcomes <code>L</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy_normalized" href="#ComplexityMeasures.entropy_normalized"><code>ComplexityMeasures.entropy_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_normalized([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) → h̃</code></pre><p>Return <code>h̃ ∈ [0, 1]</code>, the normalized discrete entropy of <code>x</code>, i.e. the value of <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> divided by the maximum value for <code>e</code>, according to the given probabilities estimator.</p><p>Instead of a discrete entropy estimator, an <a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a> can be given as first argument. If <code>e</code> is not given, it defaults to <code>Shannon()</code>.</p><p>Notice that there is no method <code>entropy_normalized(e::DiscreteEntropyEstimator, probs::Probabilities)</code>, because there is no way to know the amount of <em>possible</em> events (i.e., the <a href="#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>) from <code>probs</code>.</p></div></section></article><h3 id="Differential/continuous"><a class="docs-heading-anchor" href="#Differential/continuous">Differential/continuous</a><a id="Differential/continuous-1"></a><a class="docs-heading-anchor-permalink" href="#Differential/continuous" title="Permalink"></a></h3><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>entropy(::EntropyDefinition, ::DifferentialEntropyEstimator, ::Any)</code>. Check Documenter&#39;s build log for details.</p></div></div><h4 id="Table-of-differential-entropy-estimators"><a class="docs-heading-anchor" href="#Table-of-differential-entropy-estimators">Table of differential entropy estimators</a><a id="Table-of-differential-entropy-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Table-of-differential-entropy-estimators" title="Permalink"></a></h4><p>The following estimators are <em>differential</em> entropy estimators, and can also be used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>.</p><p>Each <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Principle</th><th style="text-align: left">Input data</th><th style="text-align: center"><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Curado"><code>Curado</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a></th></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="@ref"><code>Lord</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Correa"><code>Correa</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.DifferentialEntropyEstimator" href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>ComplexityMeasures.DifferentialEntropyEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferentialEntropyEstimator
DiffEntropyEst # alias</code></pre><p>The supertype of all differential entropy estimators. These estimators compute an entropy value in various ways that do not involve explicitly estimating a probability distribution.</p><p>See the <a href="@ref table_diff_ent_est">table of differential entropy estimators</a> in the docs for all differential entropy estimators.</p><p>See <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> for usage.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kraskov" href="#ComplexityMeasures.Kraskov"><code>ComplexityMeasures.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: DiffEntropyEst
Kraskov(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>The <code>Kraskov</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> using the <code>k</code>-th nearest neighbor searches method from <sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup> at the given <code>base</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Kraskov</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.KozachenkoLeonenko" href="#ComplexityMeasures.KozachenkoLeonenko"><code>ComplexityMeasures.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: DiffEntropyEst
KozachenkoLeonenko(; w::Int = 0, base = 2)</code></pre><p>The <code>KozachenkoLeonenko</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>using the nearest neighbor method from Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Zhu" href="#ComplexityMeasures.Zhu"><code>ComplexityMeasures.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: DiffEntropyEst
Zhu(; k = 1, w = 0, base = 2)</code></pre><p>The <code>Zhu</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> is an extension to <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, and computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Zhu</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>by approximating densities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ZhuSingh" href="#ComplexityMeasures.ZhuSingh"><code>ComplexityMeasures.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: DiffEntropyEst
ZhuSingh(; k = 1, w = 0, base = 2)</code></pre><p>The <code>ZhuSingh</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>ZhuSingh</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Like <a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Gao" href="#ComplexityMeasures.Gao"><code>ComplexityMeasures.Gao</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Gao &lt;: DifferentialEntropyEstimator
Gao(; k = 1, w = 0, base = 2, corrected = true)</code></pre><p>The <code>Gao</code> estimator (Gao et al., 2015) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, using a <code>k</code>-th nearest-neighbor approach based on Singh et al. (2003)<sup class="footnote-reference"><a id="citeref-Singh2003" href="#footnote-Singh2003">[Singh2003]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>Gao et al., 2015 give two variants of this estimator. If <code>corrected == false</code>, then the uncorrected version is used. If <code>corrected == true</code>, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Goria" href="#ComplexityMeasures.Goria"><code>ComplexityMeasures.Goria</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Goria &lt;: DifferentialEntropyEstimator
Goria(; k = 1, w = 0, base = 2)</code></pre><p>The <code>Goria</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Goria</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Specifically, let <span>$\bf{n}_1, \bf{n}_2, \ldots, \bf{n}_N$</span> be the distance of the samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> to their <code>k</code>-th nearest neighbors. Next, let the geometric mean of the distances be</p><p class="math-container">\[\hat{\rho}_k = \left( \prod_{i=1}^N \right)^{\dfrac{1}{N}}\]</p><p>Goria et al. (2005)<sup class="footnote-reference"><a id="citeref-Goria2005" href="#footnote-Goria2005">[Goria2005]</a></sup>&#39;s estimate of Shannon differential entropy is then</p><p class="math-container">\[\hat{H} = m\hat{\rho}_k + \log(N - 1) - \psi(k) + \log c_1(m),\]</p><p>where <span>$c_1(m) = \dfrac{2\pi^\frac{m}{2}}{m \Gamma(m/2)}$</span> and <span>$\psi$</span> is the digamma function.</p></div></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>Lord</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Vasicek" href="#ComplexityMeasures.Vasicek"><code>ComplexityMeasures.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: DiffEntropyEst
Vasicek(; m::Int = 1, base = 2)</code></pre><p>The <code>Vasicek</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given <code>base</code>) of a timeseries using the method from Vasicek (1976)<sup class="footnote-reference"><a id="citeref-Vasicek1976" href="#footnote-Vasicek1976">[Vasicek1976]</a></sup>.</p><p>The <code>Vasicek</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>, of which Vasicek (1976) was the first. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Vasicek</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Vasicek</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_V(\bar{X}, m) =
\dfrac{1}{n}
\sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.AlizadehArghami" href="#ComplexityMeasures.AlizadehArghami"><code>ComplexityMeasures.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AlizadehArghami &lt;: DiffEntropyEst
AlizadehArghami(; m::Int = 1, base = 2)</code></pre><p>The <code>AlizadehArghami</code>estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given <code>base</code>) of a timeseries using the method from Alizadeh &amp; Arghami (2010)<sup class="footnote-reference"><a id="citeref-Alizadeh2010" href="#footnote-Alizadeh2010">[Alizadeh2010]</a></sup>.</p><p>The <code>AlizadehArghami</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>AlizadehArghami</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>:</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp.\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>AlizadehArghami</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then the the <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a> estimate <span>$\hat{H}_{V}(\bar{X}, m, n)$</span>, plus a correction factor</p><p class="math-container">\[\hat{H}_{A}(\bar{X}, m, n) = \hat{H}_{V}(\bar{X}, m, n) +
\dfrac{2}{n}\left(m \log(2) \right).\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Ebrahimi" href="#ComplexityMeasures.Ebrahimi"><code>ComplexityMeasures.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: DiffEntropyEst
Ebrahimi(; m::Int = 1, base = 2)</code></pre><p>The <code>Ebrahimi</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given <code>base</code>) of a timeseries using the method from Ebrahimi (1994)<sup class="footnote-reference"><a id="citeref-Ebrahimi1994" href="#footnote-Ebrahimi1994">[Ebrahimi1994]</a></sup>.</p><p>The <code>Ebrahimi</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Ebrahimi</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Ebrahimi</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_{E}(\bar{X}, m) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{n}{c_i m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \\
    2,                    &amp; m + 1 \geq i \geq n - m \\
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Correa" href="#ComplexityMeasures.Correa"><code>ComplexityMeasures.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: DiffEntropyEst
Correa(; m::Int = 1, base = 2)</code></pre><p>The <code>Correa</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given `base) of a timeseries using the method from Correa (1995)<sup class="footnote-reference"><a id="citeref-Correa1995" href="#footnote-Correa1995">[Correa1995]</a></sup>.</p><p>The <code>Correa</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Correa</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, <code>Correa</code> makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>, ensuring that end points are included. The <code>Correa</code> estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy is then</p><p class="math-container">\[H_C(\bar{X}, m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(\bar{X}_{(j)} -
\tilde{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (\bar{X}_{(j)} - \tilde{X}_{(i)})^2}
\right],\]</p><p>where</p><p class="math-container">\[\tilde{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><h2 id="Conditional-entropy"><a class="docs-heading-anchor" href="#Conditional-entropy">Conditional entropy</a><a id="Conditional-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropy" title="Permalink"></a></h2><h3 id="Conditional-entropy-API"><a class="docs-heading-anchor" href="#Conditional-entropy-API">Conditional entropy API</a><a id="Conditional-entropy-API-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropy-API" title="Permalink"></a></h3><p>The conditional entropy API is defined by</p><ul><li><a href="#CausalityTools.ConditionalEntropy"><code>ConditionalEntropy</code></a>,</li><li><a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a>,</li></ul><h3 id="Conditional-entropy-definitions"><a class="docs-heading-anchor" href="#Conditional-entropy-definitions">Conditional entropy definitions</a><a id="Conditional-entropy-definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropy-definitions" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.ConditionalEntropy" href="#CausalityTools.ConditionalEntropy"><code>CausalityTools.ConditionalEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The supertype for all conditional entropies.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/entropy_conditional/entropy_conditional.jl#L5-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CEShannon" href="#CausalityTools.CEShannon"><code>CausalityTools.CEShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CEShannon &lt;: ConditionalEntropy
CEShannon(; base = 2,)</code></pre><p>The<a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> conditional entropy measure.</p><p><strong>Discrete definition</strong></p><p><strong>Sum formulation</strong></p><p>The conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H^{S}(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} = p(x, y) \log(p(x | y)).\]</p><p>This is the definition used when calling <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a> with a <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>.</p><p><strong>Two-entropies formulation</strong></p><p>Equivalently, the following difference of entropies hold</p><p class="math-container">\[H^S(X | Y) = H^S(X, Y) - H^S(Y),\]</p><p>where <span>$H^S(\cdot$</span> and <span>$H^S(\cdot | \cdot)$</span> are the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy and Shannon joint entropy, respectively. This is the definition used when calling <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a> with a <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Differential definition</strong></p><p>The differential conditional Shannon entropy is analogously defined as</p><p class="math-container">\[H^S(X | Y) = h^S(X, Y) - h^S(Y),\]</p><p>where <span>$h^S(\cdot$</span> and <span>$h^S(\cdot | \cdot)$</span> are the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a> with a <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/entropy_conditional/CEShannon.jl#L3-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CETsallisFuruichi" href="#CausalityTools.CETsallisFuruichi"><code>CausalityTools.CETsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CETsallisFuruichi &lt;: ConditionalEntropy
CETsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>Furuichi (2006)&#39;s discrete Tsallis conditional entropy measure.</p><p><strong>Definition</strong></p><p>Furuichi&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}}
p(x, y)^q \log_q(p(x | y)),\]</p><p>when <span>$q \neq 1$</span>. For <span>$q = 1$</span>, <span>$H_q^T(X | Y)$</span> reduces to the Shannon conditional entropy:</p><p class="math-container">\[H_{q=1}^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} =
p(x, y) \log(p(x | y))\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/entropy_conditional/CETsallisFuruichi.jl#L3-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CETsallisAbe" href="#CausalityTools.CETsallisAbe"><code>CausalityTools.CETsallisAbe</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CETsallisAbe &lt;: ConditionalEntropy
CETsallisAbe(; base = 2, q = 1.5)</code></pre><p>Abe &amp; Rajagopal (2001)&#39;s discrete Tsallis conditional entropy measure.</p><p><strong>Definition</strong></p><p>Abe &amp; Rajagopal&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^{T_A}(X | Y) = \dfrac{H_q^T(X, Y) - H_q^T(Y)}{1 + (1-q)H_q^T(Y)},\]</p><p>where <span>$H_q^T(\cdot)$</span> and <span>$H_q^T(\cdot, \cdot)$</span> is the <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a> entropy and the joint Tsallis entropy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/entropy_conditional/CETsallisAbe.jl#L3-L25">source</a></section></article><p>More variants exist in the literature. Pull requests are welcome!</p><h3 id="Discrete-conditional-entropy"><a class="docs-heading-anchor" href="#Discrete-conditional-entropy">Discrete conditional entropy</a><a id="Discrete-conditional-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-conditional-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}" href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>CausalityTools.entropy_conditional</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">entropy_conditional(measure::ConditionalEntropy, c::ContingencyMatrix{T, 2}) where T</code></pre><p>Estimate the discrete version of the given <a href="#CausalityTools.ConditionalEntropy"><code>ConditionalEntropy</code></a> <code>measure</code> from its direct (sum) definition, using the probabilities from a pre-computed <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>, constructed from two input variables <code>x</code> and <code>y</code>.</p><p>The convention is to compute the entropy of the variable in the <em>first</em> column of <code>c</code> conditioned on the variable in the <em>second</em> column of <code>c</code>. To do the opposite, call this function with a new contingency matrix where the order of the variables is reversed.</p><p>If <code>measure</code> is not given, then the default is <code>CEShannon()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/entropy_conditional/entropy_conditional.jl#L24-L36">source</a></section></article><h4 id="contingency_matrix_ce"><a class="docs-heading-anchor" href="#contingency_matrix_ce">Contingency matrix</a><a id="contingency_matrix_ce-1"></a><a class="docs-heading-anchor-permalink" href="#contingency_matrix_ce" title="Permalink"></a></h4><p>Discrete conditional entropy can be computed directly from its sum-definition by using the probabilities from a <a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>. This estimation method works for  both numerical and categorical data, and the following <a href="#CausalityTools.ConditionalEntropy"><code>ConditionalEntropy</code></a> definitions are supported.</p><table><tr><th style="text-align: right"></th><th style="text-align: center"><a href="#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a></th></tr><tr><td style="text-align: right"><a href="#CausalityTools.CEShannon"><code>CEShannon</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.CETsallisFuruichi"><code>CETsallisFuruichi</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.CETsallisAbe"><code>CETsallisAbe</code></a></td><td style="text-align: center">✓</td></tr></table><h4 id="probabilities_estimators_ce"><a class="docs-heading-anchor" href="#probabilities_estimators_ce">Table of discrete conditional entropy estimators</a><a id="probabilities_estimators_ce-1"></a><a class="docs-heading-anchor-permalink" href="#probabilities_estimators_ce" title="Permalink"></a></h4><p>Here, we list the <a href="#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s that are compatible with <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a>, and which definitions they are valid for.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="#CausalityTools.CEShannon"><code>CEShannon</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisAbe"><code>CETsallisAbe</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisFuruichi"><code>CETsallisFuruichi</code></a></th></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>SymbolicPermuation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr></table><h3 id="Differential/continuous-conditional-entropy"><a class="docs-heading-anchor" href="#Differential/continuous-conditional-entropy">Differential/continuous conditional entropy</a><a id="Differential/continuous-conditional-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Differential/continuous-conditional-entropy" title="Permalink"></a></h3><h4 id="diffentropy_estimators_ce"><a class="docs-heading-anchor" href="#diffentropy_estimators_ce">Table of differential conditional entropy estimators</a><a id="diffentropy_estimators_ce-1"></a><a class="docs-heading-anchor-permalink" href="#diffentropy_estimators_ce" title="Permalink"></a></h4><p>Continuous/differential mutual information may be estimated using any of our <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s that support multivariate input data.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="#CausalityTools.CEShannon"><code>CEShannon</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisAbe"><code>CETsallisAbe</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisFuruichi"><code>CETsallisFuruichi</code></a></th></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>Lord</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-GaoKannanOhViswanath2017"><a class="tag is-link" href="#citeref-GaoKannanOhViswanath2017">GaoKannanOhViswanath2017</a>Gao, W., Kannan, S., Oh, S., &amp; Viswanath, P. (2017). Estimating mutual information for discrete-continuous mixtures. Advances in neural information processing systems, 30.</li><li class="footnote" id="footnote-Gao2018"><a class="tag is-link" href="#citeref-Gao2018">Gao2018</a>Gao, W., Oh, S., &amp; Viswanath, P. (2018). Demystifying fixed k-nearest neighbor information estimators. IEEE Transactions on Information Theory, 64(8), 5629-5661.</li><li class="footnote" id="footnote-Frenzel2007"><a class="tag is-link" href="#citeref-Frenzel2007">Frenzel2007</a>Frenzel, S., &amp; Pompe, B. (2007). Partial mutual information for coupling analysis of multivariate time series. Physical review letters, 99(20), 204101. <code>w</code> is the Theiler window.</li><li class="footnote" id="footnote-Vejmelka2008"><a class="tag is-link" href="#citeref-Vejmelka2008">Vejmelka2008</a>Vejmelka, M., &amp; Paluš, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E, 77(2), 026214.</li><li class="footnote" id="footnote-MesnerShalisi2020"><a class="tag is-link" href="#citeref-MesnerShalisi2020">MesnerShalisi2020</a>Mesner, O. C., &amp; Shalizi, C. R. (2020). Conditional mutual information estimation for mixed, discrete and continuous data. IEEE Transactions on Information Theory, 67(1), 464-484.</li><li class="footnote" id="footnote-Póczos2012"><a class="tag is-link" href="#citeref-Póczos2012">Póczos2012</a>Póczos, B., &amp; Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.</li><li class="footnote" id="footnote-Rahimzamani2018"><a class="tag is-link" href="#citeref-Rahimzamani2018">Rahimzamani2018</a>Rahimzamani, A., Asnani, H., Viswanath, P., &amp; Kannan, S. (2018). Estimators for multivariate information measures in general probability spaces. Advances in Neural Information Processing Systems, 31.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Stanieck2008"><a class="tag is-link" href="#citeref-Stanieck2008">Stanieck2008</a>Staniek, M., &amp; Lehnertz, K. (2008). Symbolic transfer entropy. Physical review letters, 100(15), 158101.</li><li class="footnote" id="footnote-Palus2014"><a class="tag is-link" href="#citeref-Palus2014">Palus2014</a>Paluš, M. (2014). Cross-scale interactions and information transfer. Entropy, 16(10), 5263-5289.</li><li class="footnote" id="footnote-BandtPompe2002"><a class="tag is-link" href="#citeref-BandtPompe2002">BandtPompe2002</a>Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a natural complexity measure for timeseries.&quot; Physical review letters 88.17 (2002): 174102.</li><li class="footnote" id="footnote-Zunino2017"><a class="tag is-link" href="#citeref-Zunino2017">Zunino2017</a>Zunino, L., Olivares, F., Scholkmann, F., &amp; Rosso, O. A. (2017). Permutation entropy based timeseries analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.</li><li class="footnote" id="footnote-He2016"><a class="tag is-link" href="#citeref-He2016">He2016</a>He, S., Sun, K., &amp; Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823.</li><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, et al. &quot;Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.&quot; Physical Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Azami2016"><a class="tag is-link" href="#citeref-Azami2016">Azami2016</a>Azami, H., &amp; Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.</li><li class="footnote" id="footnote-Rostaghi2016"><a class="tag is-link" href="#citeref-Rostaghi2016">Rostaghi2016</a>Rostaghi, M., &amp; Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.</li><li class="footnote" id="footnote-Li2018"><a class="tag is-link" href="#citeref-Li2018">Li2018</a>Li, G., Guan, Q., &amp; Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. EntropyDefinition, 21(1), 11.</li><li class="footnote" id="footnote-Diego2019"><a class="tag is-link" href="#citeref-Diego2019">Diego2019</a>Diego, D., Haaga, K. A., &amp; Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.</li><li class="footnote" id="footnote-PrichardTheiler1995"><a class="tag is-link" href="#citeref-PrichardTheiler1995">PrichardTheiler1995</a>Prichard, D., &amp; Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.</li><li class="footnote" id="footnote-Rosso2001"><a class="tag is-link" href="#citeref-Rosso2001">Rosso2001</a>Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.</li><li class="footnote" id="footnote-Llanos2016"><a class="tag is-link" href="#citeref-Llanos2016">Llanos2016</a>Llanos et al., <em>Power spectral entropy as an information-theoretic correlate of manner of articulation in American English</em>, <a href="https://doi.org/10.1121/1.4976109">The Journal of the Acoustical Society of America 141, EL127 (2017)</a></li><li class="footnote" id="footnote-Tian2017"><a class="tag is-link" href="#citeref-Tian2017">Tian2017</a>Tian et al, <em>Spectral EntropyDefinition Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task</em>, <a href="https://doi.org/10.3389/fnhum.2017.00437">Front. Hum. Neurosci.</a></li><li class="footnote" id="footnote-Wang2020"><a class="tag is-link" href="#citeref-Wang2020">Wang2020</a>Wang, X., Si, S., &amp; Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.</li><li class="footnote" id="footnote-Ribeiro2012"><a class="tag is-link" href="#citeref-Ribeiro2012">Ribeiro2012</a>Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689</li><li class="footnote" id="footnote-Schlemmer2018"><a class="tag is-link" href="#citeref-Schlemmer2018">Schlemmer2018</a>Schlemmer et al. (2018). Spatiotemporal Permutation EntropyDefinition as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039</li><li class="footnote" id="footnote-Azami2019"><a class="tag is-link" href="#citeref-Azami2019">Azami2019</a>Azami, H., da Silva, L. E. V., Omoto, A. C. M., &amp; Humeau-Heurtier, A. (2019). Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images. Signal Processing: Image Communication, 75, 178-187.</li><li class="footnote" id="footnote-Berger2019"><a class="tag is-link" href="#citeref-Berger2019">Berger2019</a>Berger et al. &quot;Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.&quot; Entropy 21.10 (2019): 1023.</li><li class="footnote" id="footnote-Amigó2018"><a class="tag is-link" href="#citeref-Amigó2018">Amigó2018</a>Amigó, J. M., Balogh, S. G., &amp; Hernández, S. (2018). A brief review of generalized entropies. <a href="https://www.mdpi.com/1099-4300/20/11/813">Entropy, 20(11), 813.</a></li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Tsallis2009"><a class="tag is-link" href="#citeref-Tsallis2009">Tsallis2009</a>Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1.</li><li class="footnote" id="footnote-Curado2004"><a class="tag is-link" href="#citeref-Curado2004">Curado2004</a>Curado, E. M., &amp; Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.</li><li class="footnote" id="footnote-Anteneodo1999"><a class="tag is-link" href="#citeref-Anteneodo1999">Anteneodo1999</a>Anteneodo, C., &amp; Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. EntropyDefinition, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Gao2015"><a class="tag is-link" href="#citeref-Gao2015">Gao2015</a>Gao, S., Ver Steeg, G., &amp; Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PMLR.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Goria2005"><a class="tag is-link" href="#citeref-Goria2005">Goria2005</a>Goria, M. N., Leonenko, N. N., Mergel, V. V., &amp; Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297.</li><li class="footnote" id="footnote-Vasicek1976"><a class="tag is-link" href="#citeref-Vasicek1976">Vasicek1976</a>Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.</li><li class="footnote" id="footnote-Alizadeh2010"><a class="tag is-link" href="#citeref-Alizadeh2010">Alizadeh2010</a>Alizadeh, N. H., &amp; Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).</li><li class="footnote" id="footnote-Ebrahimi1994"><a class="tag is-link" href="#citeref-Ebrahimi1994">Ebrahimi1994</a>Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.</li><li class="footnote" id="footnote-Correa1995"><a class="tag is-link" href="#citeref-Correa1995">Correa1995</a>Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.</li><li class="footnote" id="footnote-Abe2001"><a class="tag is-link" href="#citeref-Abe2001">Abe2001</a>Abe, S., &amp; Rajagopal, A. K. (2001). Nonadditive conditional entropy and its significance for local realism. Physica A: Statistical Mechanics and its Applications, 289(1-2), 157-164.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../information_measures/">« Information measures</a><a class="docs-footer-nextpage" href="../crossmap_api/">Cross mapping API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 6 February 2023 22:44">Monday 6 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
