<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Information measures · CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="CausalityTools.jl logo"/></a><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../independence/">Independence testing</a></li><li><a class="tocitem" href="../correlation_measures/">Correlation measures</a></li><li><a class="tocitem" href="../closeness_measures/">Closeness measures</a></li><li class="is-active"><a class="tocitem" href>Information measures</a><ul class="internal"><li><a class="tocitem" href="#[MIShannon](@ref)"><span><code>MIShannon</code></span></a></li><li><a class="tocitem" href="#[MITsallisFuruichi](@ref)"><span><code>MITsallisFuruichi</code></span></a></li><li><a class="tocitem" href="#[MITsallisMartin](@ref)"><span><code>MITsallisMartin</code></span></a></li><li><a class="tocitem" href="#[MIRenyiSarbu](@ref)"><span><code>MIRenyiSarbu</code></span></a></li><li><a class="tocitem" href="#[MIRenyiJizba](@ref)"><span><code>MIRenyiJizba</code></span></a></li><li><a class="tocitem" href="#[CMIShannon](@ref)"><span><code>CMIShannon</code></span></a></li><li><a class="tocitem" href="#[CMIRenyiJizba](@ref)"><span><code>CMIRenyiJizba</code></span></a></li><li><a class="tocitem" href="#[CMIRenyiPoczos](@ref)"><span><code>CMIRenyiPoczos</code></span></a></li><li><a class="tocitem" href="#[TEShannon](@ref)"><span><code>TEShannon</code></span></a></li><li><a class="tocitem" href="#[TERenyiJizba](@ref)"><span><code>TERenyiJizba</code></span></a></li></ul></li><li><span class="tocitem">APIs and estimators</span><ul><li><a class="tocitem" href="../information_api/">Information API</a></li><li><a class="tocitem" href="../crossmap_api/">Cross mapping API</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-7-1" type="checkbox"/><label class="tocitem" for="menuitem-7-1"><span class="docs-label">Quickstart</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../quickstart/quickstart_mi/">Mutual information</a></li><li><a class="tocitem" href="../quickstart/quickstart_jdd/">Joint distance distribution</a></li><li><a class="tocitem" href="../quickstart/quickstart_independence/">Independence testing</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-2" type="checkbox"/><label class="tocitem" for="menuitem-7-2"><span class="docs-label">Longer examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../examples/examples_entropy/">Entropy</a></li><li><a class="tocitem" href="../examples/examples_conditional_entropy/">Conditional entropy</a></li><li><a class="tocitem" href="../examples/examples_mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../examples/examples_transferentropy/">Transfer entropy</a></li><li><a class="tocitem" href="../examples/examples_cross_mappings/">Cross mappings</a></li><li><a class="tocitem" href="../examples/examples_independence/">Independence testing</a></li></ul></li></ul></li><li><a class="tocitem" href="../experimental/">Experimental</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Information measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Information measures</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/information_measures.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="information_measures"><a class="docs-heading-anchor" href="#information_measures">Information measures</a><a id="information_measures-1"></a><a class="docs-heading-anchor-permalink" href="#information_measures" title="Permalink"></a></h1><p>Association measures that are information-based are listed here. Available estimators are listed in the <a href="../information_api/#information_api">information API</a>.</p><h2 id="[MIShannon](@ref)"><a class="docs-heading-anchor" href="#[MIShannon](@ref)"><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></a><a id="[MIShannon](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MIShannon](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MIShannon" href="#CausalityTools.MIShannon"><code>CausalityTools.MIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIShannon &lt;: MutualInformation
MIShannon(; base = 2)</code></pre><p>The Shannon mutual information <span>$I^S(X; Y)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> to compute the raw mutual information. </li></ul><p><strong>Discrete definition</strong></p><p>There are many equivalent formulations of discrete Shannon mutual information. In this package, we currently use the double-sum and the three-entropies formulations.</p><p><strong>Double sum formulation</strong></p><p>Assume we observe samples <span>$\bar{\bf{X}}_{1:N_y} = \{\bar{\bf{X}}_1, \ldots, \bar{\bf{X}}_n \}$</span> and <span>$\bar{\bf{Y}}_{1:N_x} = \{\bar{\bf{Y}}_1, \ldots, \bar{\bf{Y}}_n \}$</span> from two discrete random variables <span>$X$</span> and <span>$Y$</span> with finite supports <span>$\mathcal{X} = \{ x_1, x_2, \ldots, x_{M_x} \}$</span> and <span>$\mathcal{Y} = y_1, y_2, \ldots, x_{M_y}$</span>. The double-sum estimate is obtained by replacing the double sum</p><p class="math-container">\[\hat{I}_{DS}(X; Y) =
 \sum_{x_i \in \mathcal{X}, y_i \in \mathcal{Y}} p(x_i, y_j) \log \left( \dfrac{p(x_i, y_i)}{p(x_i)p(y_j)} \right)\]</p><p>where  <span>$\hat{p}(x_i) = \frac{n(x_i)}{N_x}$</span>, <span>$\hat{p}(y_i) = \frac{n(y_j)}{N_y}$</span>, and <span>$\hat{p}(x_i, x_j) = \frac{n(x_i)}{N}$</span>, and <span>$N = N_x N_y$</span>. This definition is used by <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> when called with a <a href="../information_api/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>.</p><p><strong>Three-entropies formulation</strong></p><p>An equivalent formulation of discrete Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = H^S(X) + H_q^S(Y) - H^S(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint discrete Shannon entropies. This definition is used by <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> when called with a <a href="../information_api/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Differential mutual information</strong></p><p>One possible formulation of differential Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = h^S(X) + h_q^S(Y) - h^S(X, Y),\]</p><p>where <span>$h^S(\cdot)$</span> and <span>$h^S(\cdot, \cdot)$</span> are the marginal and joint differential Shannon entropies. This definition is used by <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> when called with a <a href="../information_api/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p><p>See also: <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/MIShannon.jl#L4-L66">source</a></section></article><h2 id="[MITsallisFuruichi](@ref)"><a class="docs-heading-anchor" href="#[MITsallisFuruichi](@ref)"><a href="#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></a><a id="[MITsallisFuruichi](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MITsallisFuruichi](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MITsallisFuruichi" href="#CausalityTools.MITsallisFuruichi"><code>CausalityTools.MITsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisFuruichi &lt;: MutualInformation
MITsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from Furuichi (2006)<sup class="footnote-reference"><a id="citeref-Furuichi2006" href="#footnote-Furuichi2006">[Furuichi2006]</a></sup>, which in that paper is called the <em>mutual entropy</em>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> to compute the raw mutual information. </li></ul><p><strong>Description</strong></p><p>Furuichi&#39;s Tsallis mutual entropy between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_q^T(X; Y) = H_q^T(X) - H_q^T(X | Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X, Y),\]</p><p>where <span>$H^T(\cdot)$</span> and <span>$H^T(\cdot, \cdot)$</span> are the marginal and joint Tsallis entropies, and <code>q</code> is the <a href="../information_api/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter. ```</p><p>See also: <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/MITsallisFuruichi.jl#L3-L34">source</a></section></article><h2 id="[MITsallisMartin](@ref)"><a class="docs-heading-anchor" href="#[MITsallisMartin](@ref)"><a href="#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></a><a id="[MITsallisMartin](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MITsallisMartin](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MITsallisMartin" href="#CausalityTools.MITsallisMartin"><code>CausalityTools.MITsallisMartin</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisMartin &lt;: MutualInformation
MITsallisMartin(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from Martin et al. (2005)<sup class="footnote-reference"><a id="citeref-Martin2004" href="#footnote-Martin2004">[Martin2004]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> to compute the raw mutual information. </li></ul><p><strong>Description</strong></p><p>Martin et al.&#39;s Tsallis mutual information between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_{\text{Martin}}^T(X, Y, q) := H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint Shannon entropies, and <code>q</code> is the <a href="../information_api/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter.</p><p>See also: <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/MITsallisMartin.jl#L3-L32">source</a></section></article><h2 id="[MIRenyiSarbu](@ref)"><a class="docs-heading-anchor" href="#[MIRenyiSarbu](@ref)"><a href="#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></a><a id="[MIRenyiSarbu](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MIRenyiSarbu](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MIRenyiSarbu" href="#CausalityTools.MIRenyiSarbu"><code>CausalityTools.MIRenyiSarbu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiSarbu &lt;: MutualInformation
MIRenyiSarbu(; base = 2, q = 1.5)</code></pre><p>The discrete Rényi mutual information from Sarbu (2014)<sup class="footnote-reference"><a id="citeref-Sarbu2014" href="#footnote-Sarbu2014">[Sarbu2014]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> to compute the raw mutual information. </li></ul><p><strong>Description</strong></p><p>Sarbu (2014) defines discrete Rényi mutual information as the Rényi <span>$\alpha$</span>-divergence between the conditional joint probability mass function <span>$p(x, y)$</span> and the product of the conditional marginals, <span>$p(x) \cdot p(y)$</span>:</p><p class="math-container">\[I(X, Y)^R_q =
\dfrac{1}{q-1}
\log \left(
    \sum_{x \in X, y \in Y}
    \dfrac{p(x, y)^q}{\left( p(x)\cdot p(y) \right)^{q-1}}
\right)\]</p><p>See also: <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/MIRenyiSarbu.jl#L3-L34">source</a></section></article><h2 id="[MIRenyiJizba](@ref)"><a class="docs-heading-anchor" href="#[MIRenyiJizba](@ref)"><a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></a><a id="[MIRenyiJizba](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[MIRenyiJizba](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MIRenyiJizba" href="#CausalityTools.MIRenyiJizba"><code>CausalityTools.MIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiJizba &lt;: MutualInformation</code></pre><p>The Rényi mutual information <span>$I_q^{R_{J}}(X; Y)$</span> defined in Jizba et al. (2012)<sup class="footnote-reference"><a id="citeref-Jizba2012" href="#footnote-Jizba2012">[Jizba2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a> to compute the raw mutual information. </li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y) = S_q^{R}(X) + S_q^{R}(Y) - S_q^{R}(X, Y),\]</p><p>where <span>$S_q^{R}(\cdot)$</span> and <span>$S_q^{R}(\cdot, \cdot)$</span> the <a href="@ref"><code>Rényi</code></a> entropy and the joint Rényi entropy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/mutualinfo/MIRenyiJizba.jl#L3-L27">source</a></section></article><h2 id="[CMIShannon](@ref)"><a class="docs-heading-anchor" href="#[CMIShannon](@ref)"><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></a><a id="[CMIShannon](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[CMIShannon](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CMIShannon" href="#CausalityTools.CMIShannon"><code>CausalityTools.CMIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIShannon &lt;: ConditionalMutualInformation
CMIShannon(; base = 2)</code></pre><p>The Shannon conditional mutual information (CMI) <span>$I^S(X; Y | Z)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a> to compute the raw conditional mutual information. </li></ul><p><strong>Supported definitions</strong></p><p>Consider random variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span>, given <span>$Z \in \mathbb{R}^{d_Z}$</span>. The Shannon conditional mutual information is defined as</p><p class="math-container">\[\begin{align*}
I(X; Y | Z)
&amp;= H^S(X, Z) + H^S(Y, z) - H^S(X, Y, Z) - H^S(Z) \\
&amp;= I^S(X; Y, Z) + I^S(X; Y)
\end{align*},\]</p><p>where <span>$I^S(\cdot; \cdot)$</span> is the Shannon mutual information <a href="#CausalityTools.MIShannon"><code>MIShannon</code></a>, and <span>$H^S(\cdot)$</span> is the <a href="../information_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>Differential Shannon CMI is obtained by replacing the entropies by differential entropies.</p><p>See also: <a href="../information_api/#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/CMIShannon.jl#L4-L36">source</a></section></article><h2 id="[CMIRenyiJizba](@ref)"><a class="docs-heading-anchor" href="#[CMIRenyiJizba](@ref)"><a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></a><a id="[CMIRenyiJizba](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[CMIRenyiJizba](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CMIRenyiJizba" href="#CausalityTools.CMIRenyiJizba"><code>CausalityTools.CMIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiJizba &lt;: ConditionalMutualInformation</code></pre><p>The Rényi conditional mutual information <span>$I_q^{R_{J}}(X; Y | Z$</span> defined in Jizba et al. (2012)<sup class="footnote-reference"><a id="citeref-Jizba2012" href="#footnote-Jizba2012">[Jizba2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a> to compute the raw conditional mutual information. </li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y | Z) = I_q^{R_{J}}(X; Y, Z) - I_q^{R_{J}}(X; Z),\]</p><p>where <span>$I_q^{R_{J}}(X; Z)$</span> is the <a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a> mutual information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/CMIRenyiJizba.jl#L2-L25">source</a></section></article><h2 id="[CMIRenyiPoczos](@ref)"><a class="docs-heading-anchor" href="#[CMIRenyiPoczos](@ref)"><a href="#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></a><a id="[CMIRenyiPoczos](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[CMIRenyiPoczos](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CMIRenyiPoczos" href="#CausalityTools.CMIRenyiPoczos"><code>CausalityTools.CMIRenyiPoczos</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiPoczos &lt;: ConditionalMutualInformation</code></pre><p>The differential Rényi conditional mutual information <span>$I_q^{R_{P}}(X; Y | Z)$</span> defined in (Póczos &amp; Schneider, 2012)<sup class="footnote-reference"><a id="citeref-Póczos2012" href="#footnote-Póczos2012">[Póczos2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence.</li><li>Use with <a href="../information_api/#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a> to compute the raw conditional mutual information. </li></ul><p><strong>Definition</strong></p><p class="math-container">\[\begin{align*}
I_q^{R_{P}}(X; Y | Z) = \dfrac{1}{q-1}
\int \int \int \dfrac{p_Z(z) p_{X, Y | Z}^q}{( p_{X|Z}(x|z) p_{Y|Z}(y|z) )^{q-1}} \\
\mathbb{E}_{X, Y, Z} \sim p_{X, Y, Z}
\left[ \dfrac{p_{X, Z}^{1-q}(X, Z) p_{Y, Z}^{1-q}(Y, Z) }{p_{X, Y, Z}^{1-q}(X, Y, Z) p_Z^{1-q}(Z)} \right]
\end{align*}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/condmutualinfo/CMIRenyiPoczos.jl#L3-L29">source</a></section></article><h2 id="[TEShannon](@ref)"><a class="docs-heading-anchor" href="#[TEShannon](@ref)"><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></a><a id="[TEShannon](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[TEShannon](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.TEShannon" href="#CausalityTools.TEShannon"><code>CausalityTools.TEShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TEShannon &lt;: TransferEntropy
TEShannon(; base = 2; embedding = EmbeddingTE()) &lt;: TransferEntropy</code></pre><p>The Shannon-type transfer entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li><li>Use with <a href="../information_api/#CausalityTools.transferentropy"><code>transferentropy</code></a> to compute the raw transfer entropy.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I^S(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I^S(T^+; S^- | T^-, C^-)
\end{align*}\]</p><p>where <span>$I(T^+; S^- | T^-)$</span> is the Shannon conditional mutual information (<a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a>). The variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> are described in the docstring for <a href="../information_api/#CausalityTools.transferentropy"><code>transferentropy</code></a>.</p><p><strong>Compatible estimators</strong></p><ul><li><strong><a href="../information_api/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></strong>: Any probabilities estimator that accepts   multivariate input data or has an implementation for <a href="../information_api/#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>.   Transfer entropy is computed a sum of marginal (discrete) entropy estimates.   Example: <a href="../information_api/#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a>.</li><li><strong><a href="../information_api/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></strong>. Any differential entropy   estimator that accepts multivariate input data.   Transfer entropy is computed a sum of marginal differential entropy estimates.   Example: <a href="../information_api/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>.</li><li><strong><a href="../information_api/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></strong>. Any mutual information estimator.   Formulates the transfer entropy as a sum of mutual information terms, which are   estimated separately using <a href="../information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>. Example: <a href="../information_api/#CausalityTools.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a>.</li><li><strong><a href="../information_api/#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></strong>. Dedicated CMI estimators.   Example: <a href="../information_api/#CausalityTools.FPVP"><code>FPVP</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/TEShannon.jl#L4-L47">source</a></section></article><h2 id="[TERenyiJizba](@ref)"><a class="docs-heading-anchor" href="#[TERenyiJizba](@ref)"><a href="#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a></a><a id="[TERenyiJizba](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[TERenyiJizba](@ref)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.TERenyiJizba" href="#CausalityTools.TERenyiJizba"><code>CausalityTools.TERenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TERenyiJizba() &lt;: TransferEntropy</code></pre><p>The Rényi transfer entropy from Jizba et al. (2012)<sup class="footnote-reference"><a id="citeref-Jizba2012" href="#footnote-Jizba2012">[Jizba2012]</a></sup>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li><li>Use with <a href="../information_api/#CausalityTools.transferentropy"><code>transferentropy</code></a> to compute the raw transfer entropy.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I_q^{R_J}(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I_q^{R_J}(T^+; S^- | T^-, C^-),
\end{align*},\]</p><p>where <span>$I_q^{R_J}(T^+; S^- | T^-)$</span> is Jizba et al. (2012)&#39;s definition of conditional mutual information (<a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a>). The variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> are described in the docstring for <a href="../information_api/#CausalityTools.transferentropy"><code>transferentropy</code></a>.</p><p><strong>Compatible estimators</strong></p><ul><li><strong><a href="../information_api/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a></strong>: Any probabilities estimator that accepts   multivariate input data or has an implementation for <a href="../information_api/#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>.   Transfer entropy is computed a sum of marginal (discrete) entropy estimates.   Example: <a href="../information_api/#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a>.</li><li><strong><a href="../information_api/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></strong>. Any differential entropy   estimator that accepts multivariate input data.   Transfer entropy is computed a sum of marginal differential entropy estimates.   Example: <a href="../information_api/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/c4bc84a03c35ee5f778037813df3f2d3602b71f1/src/methods/infomeasures/transferentropy/TERenyiJizba.jl#L3-L45">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Furuichi2006"><a class="tag is-link" href="#citeref-Furuichi2006">Furuichi2006</a>Furuichi, S. (2006). Information theoretical properties of Tsallis entropies. Journal of Mathematical Physics, 47(2), 023302.</li><li class="footnote" id="footnote-Martin2004"><a class="tag is-link" href="#citeref-Martin2004">Martin2004</a>Martin, S., Morison, G., Nailon, W., &amp; Durrani, T. (2004). Fast and accurate image registration using Tsallis entropy and simultaneous perturbation stochastic approximation. Electronics Letters, 40(10), 1.</li><li class="footnote" id="footnote-Sarbu2014"><a class="tag is-link" href="#citeref-Sarbu2014">Sarbu2014</a>Sarbu, S. (2014, May). Rényi information transfer: Partial Rényi transfer entropy and partial Rényi mutual information. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5666-5670). IEEE.</li><li class="footnote" id="footnote-Jizba2012"><a class="tag is-link" href="#citeref-Jizba2012">Jizba2012</a>Jizba, P., Kleinert, H., &amp; Shefaat, M. (2012). Rényi&#39;s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.</li><li class="footnote" id="footnote-Jizba2012"><a class="tag is-link" href="#citeref-Jizba2012">Jizba2012</a>Jizba, P., Kleinert, H., &amp; Shefaat, M. (2012). Rényi’s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.</li><li class="footnote" id="footnote-Póczos2012"><a class="tag is-link" href="#citeref-Póczos2012">Póczos2012</a>Póczos, B., &amp; Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.</li><li class="footnote" id="footnote-Jizba2012"><a class="tag is-link" href="#citeref-Jizba2012">Jizba2012</a>Jizba, P., Kleinert, H., &amp; Shefaat, M. (2012). Rényi’s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../closeness_measures/">« Closeness measures</a><a class="docs-footer-nextpage" href="../information_api/">Information API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 6 February 2023 22:44">Monday 6 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
