var documenterSearchIndex = {"docs":
[{"location":"quickstart/quickstart_smeasure/#quickstart_smeasure","page":"S-measure","title":"S-measure","text":"","category":"section"},{"location":"quickstart/quickstart_smeasure/","page":"S-measure","title":"S-measure","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = SMeasure(dx = 3, dy = 3)\ns = s_measure(measure, x, y)","category":"page"},{"location":"quickstart/quickstart_smeasure/","page":"S-measure","title":"S-measure","text":"The s statistic is larger when there is stronger coupling and smaller when there is weaker coupling. To check whether s is significant (i.e. large enough to claim directional dependence), we can use a SurrogateTest, like here.","category":"page"},{"location":"quickstart/quickstart_smeasure/","page":"S-measure","title":"S-measure","text":"test = SurrogateTest(measure)\nindependence(test, x, y)","category":"page"},{"location":"quickstart/quickstart_smeasure/","page":"S-measure","title":"S-measure","text":"The p-value is high, and we can't reject the null at any reasonable significance level.","category":"page"},{"location":"quickstart/quickstart_smeasure/","page":"S-measure","title":"S-measure","text":"","category":"page"},{"location":"examples/examples_transferentropy/#examples_transferentropy","page":"Transfer entropy","title":"Transfer entropy","text":"","category":"section"},{"location":"examples/examples_transferentropy/#Schreiber's-original-example","page":"Transfer entropy","title":"Schreiber's original example","text":"","category":"section"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Let's try to reproduce the results from Schreiber's original paper[Schreiber2000] where he introduced the transfer entropy. We'll use the ValueHistogram estimator, which is visitation frequency based and computes entropies by counting visits of the system's orbit to discrete portions of its reconstructed state space.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"using CausalityTools\nusing DynamicalSystemsBase\nusing CairoMakie\nusing Statistics\nusing Random; Random.seed!(12234);\n\nfunction ulam_system(dx, x, p, t)\n    f(x) = 2 - x^2\n    Œµ = p[1]\n    dx[1] = f(Œµ*x[length(dx)] + (1-Œµ)*x[1])\n    for i in 2:length(dx)\n        dx[i] = f(Œµ*x[i-1] + (1-Œµ)*x[i])\n    end\nend\n\nds = DiscreteDynamicalSystem(ulam_system, rand(100) .- 0.5, [0.04])\ntrajectory(ds, 1000; Ttr = 1000);\n\nŒµs = 0.02:0.02:1.0\nbase = 2\nte_x1x2 = zeros(length(Œµs)); te_x2x1 = zeros(length(Œµs))\n# Guess an appropriate bin width of 0.2 for the histogram\nest = ValueHistogram(0.2)\n\nfor (i, Œµ) in enumerate(Œµs)\n    set_parameter!(ds, 1, Œµ)\n    tr = trajectory(ds, 2000; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = transferentropy(TEShannon(; base), est, X1, X2)\n    te_x2x1[i] = transferentropy(TEShannon(; base), est, X2, X1)\nend\n\nfig = with_theme(theme_minimal(), markersize = 2) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\n    scatterlines!(ax, Œµs, te_x1x2, label = \"X1 to X2\", color = :black, lw = 1.5)\n    scatterlines!(ax, Œµs, te_x2x1, label = \"X2 to X1\", color = :red, lw = 1.5)\n    axislegend(ax, position = :lt)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"As expected, transfer entropy from X1 to X2 is higher than from X2 to X1 across parameter values for Œµ. But, by our definition of the ulam system, dynamical coupling only occurs from X1 to X2. The results, however, show nonzero transfer entropy in both directions. What does this mean?","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Computing transfer entropy from finite time series introduces bias, and so does any particular choice of entropy estimator used to calculate it. To determine whether a transfer entropy estimate should be trusted, we can employ surrogate testing. We'll generate surrogate using TimeseriesSurrogates.jl.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"In the example below, we continue with the same time series generated above. However, at each value of Œµ, we also compute transfer entropy for nsurr = 50 different randomly shuffled (permuted) versions of the source process. If the original transfer entropy exceeds that of some percentile the transfer entropy estimates of the surrogate ensemble, we will take that as \"significant\" transfer entropy.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"nsurr = 25 # in real applications, you should use more surrogates\nbase = 2\nte_x1x2 = zeros(length(Œµs)); te_x2x1 = zeros(length(Œµs))\nte_x1x2_surr = zeros(length(Œµs), nsurr); te_x2x1_surr = zeros(length(Œµs), nsurr)\nest = ValueHistogram(0.2) # use same bin-width as before\n\nfor (i, Œµ) in enumerate(Œµs)\n    set_parameter!(ds, 1, Œµ)\n    tr = trajectory(ds, 500; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = transferentropy(TEShannon(; base), est, X1, X2)\n    te_x2x1[i] = transferentropy(TEShannon(; base), est, X2, X1)\n    s1 = surrogenerator(X1, RandomShuffle()); s2 = surrogenerator(X2, RandomShuffle())\n\n    for j = 1:nsurr\n        te_x1x2_surr[i, j] =  transferentropy(TEShannon(; base), est, s1(), X2)\n        te_x2x1_surr[i, j] =  transferentropy(TEShannon(; base), est, s2(), X1)\n    end\nend\n\n# Compute 95th percentiles of the surrogates for each Œµ\nqs_x1x2 = [quantile(te_x1x2_surr[i, :], 0.95) for i = 1:length(Œµs)]\nqs_x2x1 = [quantile(te_x2x1_surr[i, :], 0.95) for i = 1:length(Œµs)]\n\nfig = with_theme(theme_minimal(), markersize = 2) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\n    scatterlines!(ax, Œµs, te_x1x2, label = \"X1 to X2\", color = :black, lw = 1.5)\n    scatterlines!(ax, Œµs, qs_x1x2, color = :black, linestyle = :dot, lw = 1.5)\n    scatterlines!(ax, Œµs, te_x2x1, label = \"X2 to X1\", color = :red)\n    scatterlines!(ax, Œµs, qs_x2x1, color = :red, linestyle = :dot)\n    axislegend(ax, position = :lt)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The plot above shows the original transfer entropies (solid lines) and the 95th percentile transfer entropies of the surrogate ensembles (dotted lines). As expected, using the surrogate test, the transfer entropies from X1 to X2 are mostly significant (solid black line is above dashed black line). The transfer entropies from X2 to X1, on the other hand, are mostly not significant (red solid line is below red dotted line).","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"[Schreiber2000]: Schreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.","category":"page"},{"location":"examples/examples_transferentropy/#Estimator-comparison","page":"Transfer entropy","title":"Estimator comparison","text":"","category":"section"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Let's reproduce Figure 4 from Zhu et al (2015)[Zhu2015], where they test some dedicated transfer entropy estimators on a bivariate autoregressive system. We will test","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The Lindner and Zhu1 dedicated transfer entropy estimators,   which try to eliminate bias.\nThe KSG1 estimator, which computes TE naively as a sum of mutual information   terms (without guaranteed cancellation of biases for the total sum).\nThe Kraskov estimator, which computes TE naively as a sum of entropy    terms (without guaranteed cancellation of biases for the total sum).","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeann√®s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"using CausalityTools\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\n\nfunction model2(n::Int)\n    ùí©x = Normal(0, 0.1)\n    ùí©y = Normal(0, 0.1)\n    x = zeros(n+2)\n    y = zeros(n+2)\n    x[1] = rand(ùí©x)\n    x[2] = rand(ùí©x)\n    y[1] = rand(ùí©y)\n    y[2] = rand(ùí©y)\n\n    for i = 3:n+2\n        x[i] = 0.45*sqrt(2)*x[i-1] - 0.9*x[i-2] - 0.6*y[i-2] + rand(ùí©x)\n        y[i] = 0.6*x[i-2] - 0.175*sqrt(2)*y[i-1] + 0.55*sqrt(2)*y[i-2] + rand(ùí©y)\n    end\n    return x[3:end], y[3:end]\nend\nte_true = 0.42 # eyeball the theoretical value from their Figure 4.\n\nm = TEShannon(embedding = EmbeddingTE(dT = 2, dS = 2), base = ‚ÑØ)\nestimators = [Zhu1(k = 8), Lindner(k = 8), KSG1(k = 8), Kraskov(k = 8)]\nLs = [floor(Int, 2^i) for i in 8.0:0.5:11]\nnreps = 8\ntes_xy = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\ntes_yx = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\nfor (k, est) in enumerate(estimators)\n    for (i, L) in enumerate(Ls)\n        for j = 1:nreps\n            x, y = model2(L);\n            tes_xy[k][i][j] = transferentropy(m, est, x, y)\n            tes_yx[k][i][j] = transferentropy(m, est, y, x)\n        end\n    end\nend\n\nymin = minimum(map(x -> minimum(Iterators.flatten(Iterators.flatten(x))), (tes_xy, tes_yx)))\nestimator_names = [\"Zhu1\", \"Lindner\", \"KSG1\", \"Kraskov\"]\nls = [:dash, :dot, :dash, :dot]\nmr = [:rect, :hexagon, :xcross, :pentagon]\n\nfig = Figure(resolution = (800, 350))\nax_xy = Axis(fig[1,1], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"x ‚Üí y\")\nax_yx = Axis(fig[1,2], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"y ‚Üí x\")\nfor (k, e) in enumerate(estimators)\n    label = estimator_names[k]\n    marker = mr[k]\n    scatterlines!(ax_xy, Ls, mean.(tes_xy[k]); label, marker)\n    scatterlines!(ax_yx, Ls, mean.(tes_yx[k]); label, marker)\n    hlines!(ax_xy, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black) \n    hlines!(ax_yx, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black)\n    linkaxes!(ax_xy, ax_yx)\nend\naxislegend(ax_xy, position = :rb)\n\nfig","category":"page"},{"location":"quickstart/quickstart_jdd/#quickstart_jdd","page":"Joint distance distribution","title":"Joint distance distribution","text":"","category":"section"},{"location":"quickstart/quickstart_jdd/","page":"Joint distance distribution","title":"Joint distance distribution","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = JointDistanceDistribution(D = 3, B = 5)\nŒî = jdd(measure, x, y)","category":"page"},{"location":"quickstart/quickstart_jdd/","page":"Joint distance distribution","title":"Joint distance distribution","text":"The joint distance distribution measure indicates directional coupling between x and y if Œî is skewed towards positive values. We can use a JointDistanceDistributionTest to formally check this.","category":"page"},{"location":"quickstart/quickstart_jdd/","page":"Joint distance distribution","title":"Joint distance distribution","text":"test = JointDistanceDistributionTest(measure)\nindependence(test, x, y)","category":"page"},{"location":"quickstart/quickstart_jdd/","page":"Joint distance distribution","title":"Joint distance distribution","text":"The p-value is fairly low, and depending on the significance level 1 - Œ±, we cannot reject the null hypothesis that Œî is not skewed towards positive values, and hence we cannot reject that the variables are independent.","category":"page"},{"location":"quickstart/quickstart_mi/#quickstart_mutualinfo","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"quickstart/quickstart_mi/#[MIShannon](@ref)-(differential)","page":"Mutual information","title":"MIShannon (differential)","text":"","category":"section"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"The differential Shannon mutual information (MIShannon) can be estimated using a dedicated mutual information estimator like KraskovSt√∂gbauerGrassberger2. These estimators typically apply some form of bias correction.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(KSG2(k = 5), x, y)","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"We can also estimate MIShannon by naively applying a DifferentialEntropyEstimator, which doesn't apply any bias correction.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(Kraskov(k = 3), x, y)","category":"page"},{"location":"quickstart/quickstart_mi/#[MIShannon](@ref)-(discrete,-numerical)","page":"Mutual information","title":"MIShannon (discrete, numerical)","text":"","category":"section"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"A ValueHistogram estimator can be used to bin the data and compute discrete Shannon mutual information.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\n\n# Use the H3-estimation method with a discrete visitation frequency based \n# probabilities estimator over a fixed grid covering the range of the data,\n# which is on [0, 1].\nest = ValueHistogram(FixedRectangularBinning(0, 1, 5))\nmutualinfo(est, x, y)","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"If you need access to the estimated joint probability mass function, use a ContingencyMatrix. This is slower, but convenient if you need to investigate the probabilities manually.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nc = contingency_matrix(est, x, y)\nmutualinfo(c)","category":"page"},{"location":"quickstart/quickstart_mi/#[MIShannon](@ref)-(discrete,-categorical)","page":"Mutual information","title":"MIShannon (discrete, categorical)","text":"","category":"section"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"The ContingencyMatrix approach can also be used with categorical data. For example, let's compare the Shannon mutual information between the preferences of a population sample with regards to different foods.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nn = 1000\npreferences = rand([\"neutral\", \"like it\", \"hate it\"], n);\nrandom_foods = rand([\"water\", \"flour\", \"bananas\", \"booze\", \"potatoes\", \"beans\", \"soup\"], n)\nbiased_foods = map(preferences) do preference\n    if cmp(preference, \"neutral\") == 1\n        return rand([\"water\", \"flour\"])\n    elseif cmp(preference, \"like it\") == 1\n        return rand([\"bananas\", \"booze\"])\n    else\n        return rand([\"potatoes\", \"beans\", \"soup\"])\n    end\nend\n\nc_biased = contingency_matrix(preferences, biased_foods) \nc_random = contingency_matrix(preferences, random_foods) \nmutualinfo(c_biased), mutualinfo(c_random)","category":"page"},{"location":"crossmap_api/#cross_mapping_api","page":"Cross mapping API","title":"Cross mapping API","text":"","category":"section"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"This page outlines the cross-mapping API. For concrete implementations of cross-map based association measures, see .","category":"page"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"Several cross mapping methods have emerged in the literature Following Sugihara et al. (2012)'s paper on the convergent cross mapping. In CausalityTools.jl, we provide a unified interface for using these cross mapping methods. We indicate the different types of cross mappings by passing an CrossmapMeasure instance as the first argument to crossmap or predict.","category":"page"},{"location":"crossmap_api/#API","page":"Cross mapping API","title":"API","text":"","category":"section"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"The cross mapping API consists of the following functions.","category":"page"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"predict\ncrossmap","category":"page"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"These functions can dispatch on a CrossmapMeasure, and we currently implement","category":"page"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"ConvergentCrossMapping.\nPairwiseAsymmetricEmbedding.","category":"page"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"crossmap\npredict","category":"page"},{"location":"crossmap_api/#CausalityTools.crossmap","page":"Cross mapping API","title":"CausalityTools.crossmap","text":"crossmap(measure::CrossmapMeasure, tÃÑ::AbstractVector, SÃÑ::AbstractDataset) ‚Üí œÅ\ncrossmap(measure::CrossmapMeasure, target::AbstractVector, source::AbstractVector) ‚Üí œÅ\n\nCompute the cross map estimates between time-aligned time series tÃÑ and source embedding SÃÑ, or between raw time series t and s.\n\nThis is just a wrapper around predict that simply returns the correspondence measure between the source and the target.\n\n\n\n\n\n","category":"function"},{"location":"crossmap_api/#CausalityTools.predict","page":"Cross mapping API","title":"CausalityTools.predict","text":"predict(measure::CrossmapMeasure, tÃÑ::AbstractVector, SÃÑ::AbstractDataset) ‚Üí tÃÇ‚Çõ\npredict(measure::CrossmapMeasure, target::AbstractVector, source::AbstractVector) ‚Üí tÃÇ‚Çõ, tÃÑ, œÅ\n\nPerform point-wise cross mappings between source embeddings and target time series according to the algorithm specified by the given cross-map measure (e.g. ConvergentCrossMapping or PairwiseAsymmetricInference).\n\nFirst method: Returns a vector of predictions tÃÇ‚Çõ (tÃÇ‚Çõ := \"predictions of tÃÑ based   on source embedding SÃÑ\"), where tÃÇ‚Çõ[i] is the prediction for tÃÑ[i]. It assumes   pre-embedded data which have been correctly time-aligned using a joint embedding   (see embed), i.e. such that tÃÑ[i] and SÃÑ[i] correspond to the same time   index.\nSecond method: Jointly embeds the target and source time series (according to   measure) to obtain time-index aligned target timeseries tÃÑ and source embedding   SÃÑ (which is now a Dataset).   Then calls predict(measure, tÃÑ, SÃÑ) (the first method), and returns both the   predictions tÃÇ‚Çõ, observations tÃÑ and their correspondence œÅ according to measure.\n\nDescription\n\nFor each i ‚àà {1, 2, ‚Ä¶, N} where N = length(t) == length(s), we make the prediction tÃÇ[i] (an estimate of t[i]) based on a linear combination of D + 1 other points in t, where the selection of points and weights for the linear combination are determined by the D+1 nearest neighbors of the point SÃÑ[i]. The details of point selection and weights depend on measure.\n\nNote: Some CrossmapMeasures may define more general mapping procedures. If so, the algorithm is described in their docstring.\n\n\n\n\n\n","category":"function"},{"location":"crossmap_api/#Measures","page":"Cross mapping API","title":"Measures","text":"","category":"section"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"CrossmapMeasure","category":"page"},{"location":"crossmap_api/#CausalityTools.CrossmapMeasure","page":"Cross mapping API","title":"CausalityTools.CrossmapMeasure","text":"The supertype for all cross-map measures\n\nCurrently implemented measures are:\n\nConvergentCrossMapping, or CCM for short.\nPairwiseAsymmetricInference, or PAI for short.\n\n\n\n\n\n","category":"type"},{"location":"crossmap_api/#Estimators","page":"Cross mapping API","title":"Estimators","text":"","category":"section"},{"location":"crossmap_api/","page":"Cross mapping API","title":"Cross mapping API","text":"CrossmapEstimator\nRandomVectors\nRandomSegment\nExpandingSegment","category":"page"},{"location":"crossmap_api/#CausalityTools.CrossmapEstimator","page":"Cross mapping API","title":"CausalityTools.CrossmapEstimator","text":"CrossmapEstimator{LIBSIZES, RNG}\n\nA parametric supertype for all cross-map estimators, which are used with predict and crossmap.\n\nBecause the type of the library may differ between estimators, and because RNGs from different packages may be used, subtypes must implement the LIBSIZES and RNG type parameters.\n\nFor efficiency purposes, subtypes may contain mutable containers that can be re-used for ensemble analysis (see Ensemble).\n\nLibraries\n\nA cross-map estimator uses the concept of \"libraries\". A library is essentially just a reference to a set of points, and usually, a library refers to indices of points, not the actual points themselves.\n\nFor example, for timeseries, RandomVectors(libsizes = 50:25:100) produces three separate libraries, where the first contains 50 randomly selected time indices, the second contains 75 randomly selected time indices, and the third contains 100 randomly selected time indices. This of course assumes that all quantities involved can be indexed using the same time indices, meaning that the concept of \"library\" only makes sense after relevant quantities have been jointly embedded, so that they can be jointly indexed. For non-instantaneous prediction, the maximum possible library size shrinks with the magnitude of the index/time-offset for the prediction.\n\nFor spatial analyses (not yet implemented), indices could be more complex and involve multi-indices.\n\n\n\n\n\n","category":"type"},{"location":"crossmap_api/#CausalityTools.RandomVectors","page":"Cross mapping API","title":"CausalityTools.RandomVectors","text":"RandomVectors <: CrossmapEstimator\nRandomVectors(; libsizes, replace = false, rng = Random.default_rng())\n\nCross-map over N different libraries, where N = length(libsizes), and the i-th library has cardinality k = libsizes[i]. Points within each library are randomly selected, independently of other libraries, and replace controls whether or not to sample with replacement. A user-specified rng may be specified for reproducibility.\n\nThis is method 3 from Luo et al. (2015)[Luo2015].\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou.\n\nSee also: CrossmapEstimator.\n\n\n\n\n\n","category":"type"},{"location":"crossmap_api/#CausalityTools.RandomSegment","page":"Cross mapping API","title":"CausalityTools.RandomSegment","text":"RandomSegment <: CrossmapEstimator\nRandomSegment(; libsizes::Int, rng = Random.default_rng())\n\nIndicatates that cross mapping is performed on contiguous time series segments/windows of length L with a randomly selected starting point.\n\nThis is method 2 from Luo et al. (2015)[Luo2015].\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou.\n\n\n\n\n\n","category":"type"},{"location":"crossmap_api/#CausalityTools.ExpandingSegment","page":"Cross mapping API","title":"CausalityTools.ExpandingSegment","text":"ExpandingSegment <: CrossmapEstimator\nExpandingSegment(; libsizes::Int, rng = Random.default_rng())\n\nIndicatates that cross mapping is performed on a contiguous time series segment/window, starting from the first available data point up to the Lth data point.\n\nIf used in an ensemble setting, the estimator is applied to time indices Lmin:step:Lmax of the joint embedding.\n\n\n\n\n\n","category":"type"},{"location":"independence/#independence_testing","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"independence/#Independence-testing-API","page":"Independence testing","title":"Independence testing API","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"The independence test API is defined by","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"independence\nIndependenceTest","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"independence\nIndependenceTest","category":"page"},{"location":"independence/#CausalityTools.independence","page":"Independence testing","title":"CausalityTools.independence","text":"independence(test::IndependenceTest, x, y, [z]) ‚Üí summary\n\nPerform the given IndependenceTest test on data x, y and z. If only x and y are given, test must provide a bivariate association measure. If z is given too, then test must provide a conditional association measure.\n\nReturns a test summary, whose type depends on test.\n\n\n\n\n\n","category":"function"},{"location":"independence/#CausalityTools.IndependenceTest","page":"Independence testing","title":"CausalityTools.IndependenceTest","text":"IndependenceTest <: IndependenceTest\n\nThe supertype for all independence tests.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[SurrogateTest](@ref)","page":"Independence testing","title":"SurrogateTest","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"SurrogateTest","category":"page"},{"location":"independence/#CausalityTools.SurrogateTest","page":"Independence testing","title":"CausalityTools.SurrogateTest","text":"SurrogateTest <: IndependenceTest\nSurrogateTest(measure, [est];\n    nshuffles::Int = 100,\n    surrogate = RandomShuffle(),\n    rng = Random.default_rng(),\n)\n\nA generic (conditional) independence test for assessing whether two variables X and Y are independendent, potentially conditioned on a third variable Z, based on surrogate data. Used with independence.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether x and y are independent (given z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between the input variables.\n\nThere are different ways of shuffling, dictated by surrogate, each representing a distinct null hypothesis. For each shuffle, the provided measure is computed (using est, if relevant). This procedure is repeated nshuffles times, and a test summary is returned.\n\nFor bivariate measures, the default is to shuffle both input variables. For conditional measures accepting three input variables, the default is to shuffle the first input. Exceptions are:\n\nIf TransferEntropy measure such as TEShannon,   then the source variable is always shuffled, and the target and conditional   variable are left unshuffled.\n\nExamples\n\nQuickstart examples.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[LocalPermutationTest](@ref)","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"LocalPermutationTest","category":"page"},{"location":"independence/#CausalityTools.LocalPermutationTest","page":"Independence testing","title":"CausalityTools.LocalPermutationTest","text":"LocalPermutationTest <: IndependenceTest\nLocalPermutationTest(measure, [est];\n    kperm::Int = 5,\n    nshuffles::Int = 100,\n    rng = Random.default_rng())\n\nLocalPermutationTest is a generic conditional independence test (Runge, 2018)[Runge2018] for assessing whether two variables X and Y are conditionally independendent given a third variable Z (all of which may be multivariate).\n\nAny association measure (with a compatible estimator est, if relevant) with ordering hatM(X Y  Z) (conditional variable is the third) can be used. To obtain the nearest-neighbor approach in Runge, 2018, use the CMIShannon measure with the FPVP estimator.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether x and y are independent (given z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between the input variables.\n\nFor each shuffle, the provided measure is computed (using est, if relevant) while keeping Y and Z fixed, but permuting X, i.e. hatM(hatX Y  Z). Each shuffle of X is done conditional on Z, such that x·µ¢ is replaced with x‚±º only if z·µ¢ ‚âà z‚±º, i.e. z·µ¢ and z‚±º are close. Closeness is determined by a kperm-th nearest neighbor search among the points in Z, and permuted points are constructed as (x_i^* y_i z_i)_i=1^N, where the goal is that x_i^* are drawn without replacement, and x_i is replaced by x_j only if z_i approx z_j. This procedure is repeated nshuffles times, and a test summary is returned.\n\nExamples\n\nSee quickstart examples.\n\n[Runge2018]: Runge, J. (2018, March). Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information. In International Conference on Artificial Intelligence and Statistics (pp. 938-947). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[JointDistanceDistributionTest](@ref)","page":"Independence testing","title":"JointDistanceDistributionTest","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"JointDistanceDistributionTest","category":"page"},{"location":"independence/#CausalityTools.JointDistanceDistributionTest","page":"Independence testing","title":"CausalityTools.JointDistanceDistributionTest","text":"JointDistanceDistributionTest <: IndependenceTest\nJointDistanceDistributionTest(measure::JointDistanceDistribution; rng = Random.default_rng())\n\nAn independence test for two variables based on the JointDistanceDistribution (Amig√≥ & Hirata, 2018)[Amigo2018].\n\nDescription\n\nThe joint distance distribution (labelled Œî in their paper) is used by Amig√≥ & Hirata (2018) to detect directional couplings of the form X to Y or Y to X. JointDistanceDistributionTest formulates their method as an independence test.\n\nFormally, we test the hypothesis H_0 (the variables are independent) against H_1 (there is directional coupling between the variables). To do so, we use a right-sided/upper-tailed t-test to check mean of Œî is skewed towards positive value, i.e.\n\nH_0 = mu(Delta) = 0\nH_1 = mu(Delta)  0.\n\nWhen used with independence, a JDDTestResult is returned, which contains the joint distance distribution and a p-value. If you only need Œî, use jdd directly.\n\nExamples\n\nThis example shows how the JointDistanceDistributionTest can be used in practice.\n\n[Amigo2018]: Amig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#information_api","page":"Information API","title":"Information API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"This page outlines the information API. It contains a lot of information, so for convenience, we list all concrete implementation of pairwise and conditional association measures here.","category":"page"},{"location":"information_api/#information_measures_design","page":"Information API","title":"Design","text":"","category":"section"},{"location":"information_api/#Modularity","page":"Information API","title":"Modularity","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"We have taken great care to make sure that information estimators are reusable and modular. Functions have the following general form.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"f([measure], estimator, input_data...)\n\n# Some examples\nmutualinfo(MIShannon(base = ‚ÑØ), Kraskov(k = 1), x, y)\nmutualinfo(MITsallisFuruichi(base = ‚ÑØ), KozachenkoLeonenko(k = 3), x, y)\ncondmutualinfo(CMIShannon(base = 2), ValueHistogram(3), x, y, z)\ncondmutualinfo(CMIRenyiJizba(base = 2), KSG2(k = 5), x, y, z)\ncondmutualinfo(CMIRenyiPoczos(base = 2), PoczosSchneiderCMI(k = 10), x, y, z)","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"This modular design really shines when it comes to independence testing and causal graph inference. You can essentially test the performance of any independence measure with any estimator, as long as their combination is implemented (and if it's not, please submit a PR or issue!). We hope that this will both ease reproduction of existing literature results, and spawn new research. Please let us know if you use the package for something useful, or publish something based on it!","category":"page"},{"location":"information_api/#Estimators","page":"Information API","title":"Estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Information measures are either estimated using one of the following basic estimator types,","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ProbabilitiesEstimators,\nDifferentialEntropyEstimators,","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"or using measure-specific estimators:","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"MutualInformationEstimators are used with mutualinfo\nConditionalMutualInformationEstimators are used with condmutualinfo\nTransferEntropyEstimators are used with transferentropy","category":"page"},{"location":"information_api/#Naming-convention:-The-same-name-for-different-things","page":"Information API","title":"Naming convention: The same name for different things","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Upon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use the same name for different concepts. For novices, and experienced practitioners too, this can be confusing. Our API clearly distinguishes between methods that are conceptually the same but named differently in the literature due to differing estimation strategies, from methods that actually have different definitions.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Multiple, equivalent definitions occur for example for the Shannon mutual   information (MI; MIShannon), which has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral   over a joint probability mass function (pmf), as a sum of three entropy terms, and as   a Kullback-Leibler divergence between the joint pmf and the product of the marginal   distributions. Since these definitions are all equivalent, we only need once type   (MIShannon) to represent them.\nBut Shannon MI is not the  only type of mutual information! For example, \"Tsallis mutual information\"   has been proposed in different variants by various authors. Despite sharing the   same name, these are actually nonequivalent definitions. We've thus assigned   them entirely different measure names (e.g. MITsallisFuruichi and   MITsallisMartin), with the author name at the end.","category":"page"},{"location":"information_api/#Probability-mass-functions-(pmf)","page":"Information API","title":"Probability mass functions (pmf)","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Discrete information theoretic association measures and other quantities are estimated using ProbabilitiesEstimators. Here, we list probabilities estimators that are compatible with this package.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle Input data\nContingency Count frequencies, optionally discretize first Any\nCountOccurrences Count of unique elements Any\nValueHistogram Binning (histogram) Vector, Dataset\nTransferOperator Binning (transfer operator) Vector, Dataset\nNaiveKernel Kernel density estimation Dataset\nSymbolicPermutation Ordinal patterns Vector, Dataset\nDispersion Dispersion patterns Vector","category":"page"},{"location":"information_api/#Contingency","page":"Information API","title":"Contingency","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Contingency","category":"page"},{"location":"information_api/#CausalityTools.Contingency","page":"Information API","title":"CausalityTools.Contingency","text":"Contingency <: ProbabilitiesEstimator\nContingency(est::Union{ProbabilitiesEstimator, Nothing} = nothing)\n\nContingency is a probabilities estimator that transforms input data to a multidimensional probability mass function (internally represented as ContingencyMatrix.\n\nIt works directly on raw discrete/categorical data. Alternatively, if a ProbabilitiesEstimator est for which marginal_encodings is implemented is given, then input data are first discretized before creating the contingency matrix.\n\nnote: Note\nContingency estimator differs from other ProbabilitiesEstimators in that it's not compatible with probabilities and other methods. Instead, it is used to construct ContingencyMatrix, from which probabilities can be computed.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Count-occurrences","page":"Information API","title":"Count occurrences","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"CountOccurrences","category":"page"},{"location":"information_api/#ComplexityMeasures.CountOccurrences","page":"Information API","title":"ComplexityMeasures.CountOccurrences","text":"CountOccurrences()\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input. Hence, input x is needed for a well-defined outcome_space.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Histograms-(binning)","page":"Information API","title":"Histograms (binning)","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ValueHistogram\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"information_api/#ComplexityMeasures.ValueHistogram","page":"Information API","title":"ComplexityMeasures.ValueHistogram","text":"ValueHistogram(b::AbstractBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are:\n\nRectangularBinning\nFixedRectangularBinning\n\nThe ValueHistogram estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes Œµ without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueHistogram(œµ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueHistogram is the unique bins constructed from b. Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals [a, b). The bins are in data units, not integer (cartesian indices units), and are returned as SVectors, i.e., same type as input data.\n\nFor convenience, outcome_space returns the outcomes in the same array format as the underlying binning (e.g., Matrix for 2D input).\n\nFor FixedRectangularBinning the outcome_space is well-defined from the binning, but for RectangularBinning input x is needed as well.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.RectangularBinning","page":"Information API","title":"ComplexityMeasures.RectangularBinning","text":"RectangularBinning(œµ, precise = false) <: AbstractBinning\n\nRectangular box partition of state space using the scheme œµ, deducing the histogram extent and bin width from the input data.\n\nRectangularBinning is a convenience struct. It is re-cast into FixedRectangularBinning once the data are provided, so see that docstring for info on the bin calculation and the meaning of precise.\n\nBinning instructions are deduced from the type of œµ as follows:\n\nœµ::Int divides each coordinate axis into œµ equal-length intervals  that cover all data.\nœµ::Float64 divides each coordinate axis into intervals of fixed size œµ, starting  from the axis minima until the data is completely covered by boxes.\nœµ::Vector{Int} divides the i-th coordinate axis into œµ[i] equal-length  intervals that cover all data.\nœµ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  œµ[i], starting from the axis minima until the data is completely covered by boxes.\n\nRectangularBinning ensures all input data are covered by extending the created ranges if need be.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.FixedRectangularBinning","page":"Information API","title":"ComplexityMeasures.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false)\n\nRectangular box partition of state space where the partition along each dimension is explicitly given by each range ranges, which is a tuple of AbstractRange subtypes. Typically, each range is the output of the range Base function, e.g., ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)). All ranges must be sorted.\n\nThe optional second argument precise dictates whether Julia Base's TwicePrecision is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is false.\n\nPoints falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open: [a, b). This means that the last value of each of the ranges dictates the last right-closing value. This value does not belong to the histogram! E.g., if given a range r = range(0, 1; length = 11), with r[end] = 1, the value 1 is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here [0.9, 1))!\n\nEquivalently, the size of the histogram is histsize = map(r -> length(r)-1, ranges)!\n\nFixedRectangularBinning leads to a well-defined outcome space without knowledge of input data, see ValueHistogram.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Transfer-operator-(binning)","page":"Information API","title":"Transfer operator (binning)","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"TransferOperator","category":"page"},{"location":"information_api/#ComplexityMeasures.TransferOperator","page":"Information API","title":"ComplexityMeasures.TransferOperator","text":"TransferOperator <: ProbabilitiesEstimator\nTransferOperator(b::RectangularBinning)\n\nA probability estimator based on binning data into rectangular boxes dictated by the given binning scheme b, then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nOutcome space\n\nThe outcome space for TransferOperator is the set of unique bins constructed from b. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as SVectors.\n\nBin ordering\n\nBins returned by probabilities_and_outcomes are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if\n\nb = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries\n\nthen probs[i] is the invariant measure (probability) of the bin outcomes[i], which is the i-th bin visited by the timeseries with nonzero measure.\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by œµ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to binning, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Utility-methods/types","page":"Information API","title":"Utility methods/types","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"For explicit estimation of the transfer operator, see ComplexityMeasures.jl.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"information_api/#ComplexityMeasures.InvariantMeasure","page":"Information API","title":"ComplexityMeasures.InvariantMeasure","text":"InvariantMeasure(to, œÅ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure œÅ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.invariantmeasure","page":"Information API","title":"ComplexityMeasures.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, binning::RectangularBinning) ‚Üí iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, ComplexityMeasures\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) ‚Üí (œÅ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element œÅ[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.transfermatrix","page":"Information API","title":"ComplexityMeasures.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) ‚Üí (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#Symbolic-permutations","page":"Information API","title":"Symbolic permutations","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"SymbolicPermutation","category":"page"},{"location":"information_api/#ComplexityMeasures.SymbolicPermutation","page":"Information API","title":"ComplexityMeasures.SymbolicPermutation","text":"SymbolicPermutation <: ProbabilitiesEstimator\nSymbolicPermutation(; m = 3, œÑ = 1, lt::Function = ComplexityMeasures.isless_rand)\n\nA probabilities estimator based on ordinal permutation patterns.\n\nWhen passed to probabilities the output depends on the input data type:\n\nUnivariate data. If applied to a univariate timeseries (AbstractVector), then the timeseries   is first embedded using embedding delay œÑ and dimension m, resulting in embedding   vectors  bfx_i _i=1^N-(m-1)tau. Then, for each bfx_i,   we find its permutation pattern pi_i. Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using CountOccurrences. When giving the resulting probabilities to   entropy, the original permutation entropy is computed [BandtPompe2002].\nMultivariate data. If applied to a an D-dimensional Dataset,   then no embedding is constructed, m must be equal to D and œÑ is ignored.   Each vector bfx_i of the dataset is mapped   directly to its permutation pattern pi_i by comparing the   relative magnitudes of the elements of bfx_i.   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy[He2016], although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of[He2016]).\n\nInternally, SymbolicPermutation uses the OrdinalPatternEncoding to represent ordinal patterns as integers for efficient computations.\n\nSee SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see SpatialSymbolicPermutation.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution [Zunino2017]. Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = ComplexityMeasures.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless.\n\nOutcome space\n\nThe outcome space Œ© for SymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, ‚Ä¶, m. There are factorial(m) such patterns.\n\nFor example, the outcome [2, 3, 1] corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also [OrdinalPatternEncoding(@ref).\n\nIn-place symbolization\n\nSymbolicPermutation also implements the in-place probabilities! for Dataset input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example\n\nusing ComplexityMeasures\nm, N = 2, 100\nest = SymbolicPermutation(; m, œÑ)\nx = Dataset(rand(N, m)) # some input dataset\nœÄs_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(œÄs_ts, est, x)\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for timeseries.\" Physical review letters 88.17 (2002): 174102.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based timeseries analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n[He2016]: He, S., Sun, K., & Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Dispersion-patterns","page":"Information API","title":"Dispersion patterns","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Dispersion","category":"page"},{"location":"information_api/#ComplexityMeasures.Dispersion","page":"Information API","title":"ComplexityMeasures.Dispersion","text":"Dispersion(; c = 5, m = 2, œÑ = 1, check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values[Li2018] are m ‚àà [2, 3], œÑ = 1 for the embedding, and c ‚àà [3, 4, ‚Ä¶, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is encoded into a symbol timeseries S using the Gaussian encoding GaussianCDFEncoding with empirical mean Œº and empirical standard deviation œÉ (both determined from X), and c as given to Dispersion.\n\nThen, S is embedded into an m-dimensional time series, using an embedding lag of tau, which yields a total of N - (m - 1)tau delay vectors z_i, or \"dispersion patterns\". Since each element of z_i can take on c different values, and each delay vector has m entries, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nThe returned probabilities are simply the frequencies of the unique dispersion patterns present in S (i.e., the CountOccurences of S).\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of S.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\nFor a version of this estimator that can be used on high-dimensional arrays, see SpatialDispersion.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. EntropyDefinition, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Kernel-density","page":"Information API","title":"Kernel density","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"NaiveKernel","category":"page"},{"location":"information_api/#ComplexityMeasures.NaiveKernel","page":"Information API","title":"ComplexityMeasures.NaiveKernel","text":"NaiveKernel(œµ::Real; method = KDTree, w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius œµ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nKeyword arguments\n\nmethod = KDTree: the search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\nw = 0: the Theiler window, which excludes indices s that are within i - s  w from the given point x_i.\nmetric = Euclidean(): the distance metric.\n\nOutcome space\n\nThe outcome space Œ© for NaiveKernel are the indices of the input data, eachindex(x). Hence, input x is needed for a well-defined outcome_space. The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors).\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Timescales","page":"Information API","title":"Timescales","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"information_api/#ComplexityMeasures.WaveletOverlap","page":"Information API","title":"ComplexityMeasures.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001]. Input timeseries x is needed for a well-defined outcome space.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\nOutcome space\n\nThe outcome space for WaveletOverlap are the integers 1, 2, ‚Ä¶, N enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input and input x is needed for a well-defined outcome_space.\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.PowerSpectrum","page":"Information API","title":"ComplexityMeasures.PowerSpectrum","text":"PowerSpectrum() <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nOutcome space\n\nThe outcome space Œ© for PowerSpectrum is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1. Input x is needed for a well-defined outcome_space.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral EntropyDefinition Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Diversity","page":"Information API","title":"Diversity","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Diversity","category":"page"},{"location":"information_api/#ComplexityMeasures.Diversity","page":"Information API","title":"ComplexityMeasures.Diversity","text":"Diversity(; m::Int, œÑ::Int, nbins::Int)\n\nA ProbabilitiesEstimator based on the cosine similarity. It can be used with entropy to compute the diversity entropy of an input timeseries[Wang2020].\n\nThe implementation here allows for œÑ != 1, which was not considered in the original paper.\n\nDescription\n\nDiversity probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag œÑ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mœÑ.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mœÑ-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals (including the value +1).\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for Diversity is the bins of the [-1, 1] interval, and the return configuration is the same as in ValueHistogram (left bin edge).\n\n[Wang2020]: Wang, X., Si, S., & Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Probabilities-API","page":"Information API","title":"Probabilities API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The probabilities API is defined by","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities_and_outcomes\nContingencyMatrix\ncontingency_matrix","category":"page"},{"location":"information_api/#Probabilities-and-outcomes","page":"Information API","title":"Probabilities and outcomes","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities!\nProbabilities\nprobabilities_and_outcomes\noutcomes\noutcome_space\ntotal_outcomes\nmissing_outcomes","category":"page"},{"location":"information_api/#ComplexityMeasures.ProbabilitiesEstimator","page":"Information API","title":"ComplexityMeasures.ProbabilitiesEstimator","text":"ProbabilitiesEstimator\n\nThe supertype for all probabilities estimators.\n\nIn ComplexityMeasures.jl, probability distributions are estimated from data by defining a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L , and assigning to each outcome omega_i a probability p(omega_i), such that sum_i=1^N p(omega_i) = 1. It is the role of a ProbabilitiesEstimator to\n\nDefine Omega, the \"outcome space\", which is the set of all possible outcomes over  which probabilities are estimated. The cardinality of this set can be obtained using  total_outcomes.\nDefine how probabilities p_i = p(omega_i) are assigned to outcomes omega_i.\n\nIn practice, probability estimation is done by calling probabilities with some input data and one of the following probabilities estimators. The result is a Probabilities p (Vector-like), where each element p[i] is the probability of the outcome œâ[i]. Use probabilities_and_outcomes if you need both the probabilities and the outcomes, and use outcome_space to obtain Omega alone. The element type of Omega varies between estimators, but it is guaranteed to be hashable. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\nSome estimators can deduce Omega without knowledge of the input, such as SymbolicPermutation. For others, knowledge of input is necessary for concretely specifying Omega, such as ValueHistogram with RectangularBinning. This only matters for the functions outcome_space and total_outcomes.\n\nAll currently implemented probability estimators are listed in a nice table in the probabilities estimators section of the online documentation.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.probabilities","page":"Information API","title":"ComplexityMeasures.probabilities","text":"probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) ‚Üí p::Probabilities\n\nCompute a probability distribution over the set of possible outcomes defined by the probabilities estimator est, given input data x, which is typically an Array or a Dataset; see Input data for ComplexityMeasures.jl. Configuration options are always given as arguments to the chosen estimator.\n\nTo obtain the outcomes corresponding to these probabilities, use outcomes.\n\nDue to performance optimizations, whether the returned probablities contain 0s as entries or not depends on the estimator. E.g., in ValueHistogram 0s are skipped, while in SymbolicPermutation 0 are not, because we get them for free.\n\nprobabilities(x::Vector_or_Dataset) ‚Üí p::Probabilities\n\nEstimate probabilities by directly counting the elements of x, assuming that Œ© = sort(unique(x)), i.e. that the outcome space is the unique elements of x. This is mostly useful when x contains categorical data.\n\nSee also: Probabilities, ProbabilitiesEstimator.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.probabilities!","page":"Information API","title":"ComplexityMeasures.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.Probabilities","page":"Information API","title":"ComplexityMeasures.Probabilities","text":"Probabilities <: AbstractArray\nProbabilities(x) ‚Üí p\n\nProbabilities is a simple wrapper around x::AbstractArray{<:Real, N} that ensures its values sum to 1, so that p can be interpreted as N-dimensional probability mass function. In most use cases, p will be a vector.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.probabilities_and_outcomes","page":"Information API","title":"ComplexityMeasures.probabilities_and_outcomes","text":"probabilities_and_outcomes(est, x)\n\nReturn probs, outs, where probs = probabilities(x, est) and outs[i] is the outcome with probability probs[i]. The element type of outs depends on the estimator. outs is a subset of the outcome_space of est.\n\nSee also outcomes, total_outcomes.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.outcomes","page":"Information API","title":"ComplexityMeasures.outcomes","text":"outcomes(est::ProbabilitiesEstimator, x)\n\nReturn all (unique) outcomes contained in x according to the given estimator. Equivalent with probabilities_and_outcomes(x, est)[2], but for some estimators it may be explicitly extended for better performance.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.outcome_space","page":"Information API","title":"ComplexityMeasures.outcome_space","text":"outcome_space(est::ProbabilitiesEstimator, x) ‚Üí Œ©\n\nReturn a container containing all possible outcomes of est for input x.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to outcome_space(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.total_outcomes","page":"Information API","title":"ComplexityMeasures.total_outcomes","text":"total_outcomes(est::ProbabilitiesEstimator, x)\n\nReturn the length (cardinality) of the outcome space Omega of est.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to total_outcomes(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.missing_outcomes","page":"Information API","title":"ComplexityMeasures.missing_outcomes","text":"missing_outcomes(est::ProbabilitiesEstimator, x) ‚Üí n_missing::Int\n\nEstimate a probability distribution for x using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.\n\nSee also: MissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#Encodings","page":"Information API","title":"Encodings","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Some probability estimators first \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\".","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The encodings API is defined by:","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Encoding\nencode\ndecode","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Encoding\nencode\ndecode","category":"page"},{"location":"information_api/#ComplexityMeasures.Encoding","page":"Information API","title":"ComplexityMeasures.Encoding","text":"Encoding\n\nThe supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions encode and decode. Some probability estimators utilize encodings internally.\n\nCurrent available encodings are:\n\nOrdinalPatternEncoding.\nGaussianCDFEncoding.\nRectangularBinEncoding.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.encode","page":"Information API","title":"ComplexityMeasures.encode","text":"encode(c::Encoding, œá) -> i::Int\n\nEncode an element œá ‚àà x of input data x (those given to probabilities) using encoding c.\n\nThe special value of -1 is reserved as a return value for inappropriate elements œá that cannot be encoded according to c.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.decode","page":"Information API","title":"ComplexityMeasures.decode","text":"decode(c::Encoding, i::Int) -> œâ\n\nDecode an encoded element i into the outcome œâ ‚àà Œ© it corresponds to.\n\nŒ© is the outcome_space of a probabilities estimator that uses encoding c.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#Available-encodings","page":"Information API","title":"Available encodings","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"OrdinalPatternEncoding\nGaussianCDFEncoding\nRectangularBinEncoding","category":"page"},{"location":"information_api/#ComplexityMeasures.OrdinalPatternEncoding","page":"Information API","title":"ComplexityMeasures.OrdinalPatternEncoding","text":"OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding(m::Int, lt = ComplexityMeasures.isless_rand)\n\nAn encoding scheme that encodes length-m vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by SymbolicPermutation and similar estimators, see that for a description of the outcome space.\n\nThe ordinal/permutation pattern of a vector œá is simply sortperm(œá), which gives the indices that would sort œá in ascending order.\n\nDescription\n\nThe Lehmer code, as implemented here, is a bijection between the set of factorial(m) possible permutations for a length-m sequence, and the integers 1, 2, ‚Ä¶, factorial(m). The encoding step uses algorithm 1 in Berger et al. (2019)[Berger2019], which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!).\n\nExample\n\njulia> using ComplexityMeasures\n\njulia> œá = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, œá)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3\n\nIf you want to encode something that is already a permutation pattern, then you can use the non-exported permutation_to_integer function.\n\n[Berger2019]: Berger et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.GaussianCDFEncoding","page":"Information API","title":"ComplexityMeasures.GaussianCDFEncoding","text":"GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding(; Œº, œÉ, c::Int = 3)\n\nAn encoding scheme that encodes a scalar value into one of the integers s·µ¢ ‚àà [1, 2, ‚Ä¶, c] based on the normal cumulative distribution function (NCDF), and decodes the s·µ¢ into subintervals of [0, 1] (with some loss of information).\n\nNotice that the decoding step does not yield an element of any outcome space of the estimators that use GaussianCDFEncoding internally, such as Dispersion. That is because these estimators additionally delay embed the encoded data.\n\nDescription\n\nGaussianCDFEncoding first maps an input point x  (scalar) to a new real number y_ in 0 1 by using the normal cumulative distribution function (CDF) with the given mean Œº and standard deviation œÉ, according to the map\n\nx to y  y = dfrac1 sigma\n    sqrt2 pi int_-infty^x e^(-(x - mu)^2)(2 sigma^2) dx\n\nNext, the interval [0, 1] is equidistantly binned and enumerated 1 2 ldots c,  and y is linearly mapped to one of these integers using the linear map  y to z  z = textfloor(y(c-1)) + 1.\n\nBecause of the floor operation, some information is lost, so when used with decode, each decoded s·µ¢ is mapped to a subinterval of [0, 1].\n\nExamples\n\njulia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> Œº, œÉ = mean(x), std(x); encoding = GaussianCDFEncoding(; Œº, œÉ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.RectangularBinEncoding","page":"Information API","title":"ComplexityMeasures.RectangularBinEncoding","text":"RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning)\n\nAn encoding scheme that encodes points œá ‚àà x into their histogram bins.\n\nThe first call signature simply initializes a FixedRectangularBinning and then calls the second call signature.\n\nSee FixedRectangularBinning for info on mapping points to bins.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Contingency-tables","page":"Information API","title":"Contingency tables","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"To estimate discrete information theoretic quantities that are functions of more than one variable, we must estimate empirical joint probability mass functions (pmf). The function contingency_matrix accepts an arbitrary number of equal-length input data and returns the corresponding multidimensional contingency table as a ContingencyMatrix. From this table, we can extract the necessary joint and marginal pmfs for computing any discrete function of multivariate discrete probability distributions. This is essentially the multivariate analogue of Probabilities.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"But why would I use a ContingencyMatrix instead of some other indirect estimation method, you may ask. The answer is that ContingencyMatrix allows you to compute any of the information theoretic quantities offered in this package for any type of input data. You input data can literally be any hashable type, for example String, Tuple{Int, String, Int}, or YourCustomHashableDataType.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"In the case of numeric data, using a ContingencyMatrix is typically a bit slower than other dedicated estimation procedures. For example, quantities like discrete Shannon-type condmutualinfo are faster to estimate using a formulation based on sums of four entropies (the H4-principle). This is faster because we can both utilize the blazingly fast Dataset structure directly, and we can avoid explicitly estimating the entire joint pmf, which demands many extra calculation steps. Whatever you use in practice depends on your use case and available estimation methods, but you can always fall back to contingency matrices for any discrete measure.","category":"page"},{"location":"information_api/#Contingency-matrix-API","page":"Information API","title":"Contingency matrix API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ContingencyMatrix\ncontingency_matrix\nmarginal_encodings","category":"page"},{"location":"information_api/#CausalityTools.ContingencyMatrix","page":"Information API","title":"CausalityTools.ContingencyMatrix","text":"ContingencyMatrix{T, N} <: Probabilities{T, N}\nContingencyMatrix(frequencies::AbstractArray{Int, N})\n\nA contingency matrix is essentially a multivariate analogue of Probabilities that also keep track of raw frequencies.\n\nThe contingency matrix can be constructed directyly from an N-dimensional frequencies array. Alternatively, the contingency_matrix function performs counting for you; this works on both raw categorical data, or by first discretizing data using a a ProbabilitiesEstimator.\n\nDescription\n\nA ContingencyMatrix c is just a simple wrapper around around AbstractArray{T, N}. Indexing c with multiple indices i, j, ‚Ä¶ returns the (i, j, ‚Ä¶)th element of the empirical probability mass function (pmf). The following convencience methods are defined:\n\nfrequencies(c; dims) returns the multivariate raw counts along the given `dims   (default to all available dimensions).\nprobabilities(c; dims) returns a multidimensional empirical   probability mass function (pmf) along the given dims (defaults to all available   dimensions), i.e. the normalized counts.\nprobabilities(c, i::Int) returns the marginal probabilities for the i-th dimension.\noutcomes(c, i::Int) returns the marginal outcomes for the i-th dimension.\n\nOrdering\n\nThe ordering of outcomes are internally consistent, but we make no promise on the ordering of outcomes relative to the input data. This means that if your input data are x = rand([\"yes\", \"no\"], 100); y = rand([\"small\", \"medium\", \"large\"], 100), you'll get a 2-by-3 contingency matrix, but there currently no easy way to determine which outcome the i-j-th row/column of this matrix corresponds to.\n\nSince ContingencyMatrix is intended for use in information theoretic methods that don't care about ordering, as long as the ordering is internally consistent, this is not an issue for practical applications in this package. This may change in future releases.\n\nUsage\n\nContingency matrices is used in the computation of discrete versions of the following quantities:\n\nentropy_joint.\nmutualinfo.\ncondmutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.contingency_matrix","page":"Information API","title":"CausalityTools.contingency_matrix","text":"contingency_matrix(x, y, [z, ...]) ‚Üí c::ContingencyMatrix\ncontingency_matrix(est::ProbabilitiesEstimator, x, y, [z, ...]) ‚Üí c::ContingencyMatrix\n\nEstimate a multidimensional contingency matrix c from input data x, y, ‚Ä¶, where the input data can be of any and different types, as long as length(x) == length(y) == ‚Ä¶.\n\nFor already discretized data, use the first method. For continuous data, you want to discretize the data before computing the contingency table. You can do this manually and then use the first method. Alternatively, you can provide a ProbabilitiesEstimator as the first argument to the constructor. Then the input variables x, y, ‚Ä¶ are discretized separately according to est (enforcing the same outcome space for all variables), by calling marginal_encodings.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#CausalityTools.marginal_encodings","page":"Information API","title":"CausalityTools.marginal_encodings","text":"marginal_encodings(est::ProbabilitiesEstimator, x::VectorOrDataset...)\n\nEncode/discretize each input vector x·µ¢ ‚àà x according to a procedure determined by est. Any x·µ¢ ‚àà X that are multidimensional (Datasets) will be encoded column-wise, i.e. each column of x·µ¢ is treated as a timeseries and is encoded separately.\n\nThis is useful for computing any discrete information theoretic quantity, and is used internally by contingency_matrix.\n\nSupported estimators\n\nValueHistogram. Bin visitation frequencies are counted in the joint space XY,   then marginal visitations are obtained from the joint bin visits.   This behaviour is the same for both FixedRectangularBinning and   RectangularBinning (which adapts the grid to the data).   When using FixedRectangularBinning, the range along the first dimension   is used as a template for all other dimensions.\nSymbolicPermutation. Each timeseries is separately encoded according   to its ordinal pattern.\nDispersion. Each timeseries is separately encoded according to its   dispersion pattern.\n\nMany more implementations are possible. Each new implementation gives one new way of estimating the ContingencyMatrix\n\n\n\n\n\n","category":"function"},{"location":"information_api/#entropies","page":"Information API","title":"Entropies","text":"","category":"section"},{"location":"information_api/#Entropies-API","page":"Information API","title":"Entropies API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The entropies API is defined by","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"EntropyDefinition\nentropy\nDifferentialEntropyEstimator","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Please be sure you have read the Terminology section before going through the API here, to have a good idea of the different \"flavors\" of entropies and how they all come together over the common interface of the entropy function.","category":"page"},{"location":"information_api/#Entropy-definitions","page":"Information API","title":"Entropy definitions","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"EntropyDefinition\nShannon\nRenyi\nTsallis\nKaniadakis\nCurado\nStretchedExponential","category":"page"},{"location":"information_api/#ComplexityMeasures.EntropyDefinition","page":"Information API","title":"ComplexityMeasures.EntropyDefinition","text":"EntropyDefinition\n\nEntropyDefinition is the supertype of all types that encapsulate definitions of (generalized) entropies. These also serve as estimators of discrete entropies, see description below.\n\nCurrently implemented entropy definitions are:\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q ‚Üí 1.\nKaniadakis.\nCurado.\nStretchedExponential.\n\nThese types can be given as inputs to entropy or entropy_normalized.\n\nDescription\n\nMathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amig√≥ et al.'s[Amig√≥2018] summary paper gives a nice overview.\n\nHowever, for a software implementation computing entropies in practice, definitions is not really what matters; estimators matter. Because in the practical sense, one needs to estimate a definition from finite data, and different ways of estimating a quantity come with their own pros and cons.\n\nThat is why the type DiscreteEntropyEstimator exists, which is what is actually given to entropy. Some ways to estimate a discrete entropy only apply to a specific entropy definition. For estimators that can be applied to various entropy definitions, this is specified by providing an instance of EntropyDefinition to the estimator.\n\n[Amig√≥2018]: Amig√≥, J. M., Balogh, S. G., & Hern√°ndez, S. (2018). A brief review of generalized entropies. Entropy, 20(11), 813.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Shannon","page":"Information API","title":"ComplexityMeasures.Shannon","text":"Shannon <: EntropyDefinition\nShannon(; base = 2)\n\nThe Shannon[Shannon1948] entropy, used with entropy to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nThe maximum value of the Shannon entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Renyi","page":"Information API","title":"ComplexityMeasures.Renyi","text":"Renyi <: EntropyDefinition\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe R√©nyi[R√©nyi1960] generalized order-q entropy, used with entropy to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the R√©nyi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nThe maximum value of the R√©nyi entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[R√©nyi1960]: A. R√©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Tsallis","page":"Information API","title":"ComplexityMeasures.Tsallis","text":"Tsallis <: EntropyDefinition\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis[Tsallis1988] generalized order-q entropy, used with entropy to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nThe maximum value of the Tsallis entropy is ``k(L^1 - q - 1)(1 - q), with L the total_outcomes.\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Kaniadakis","page":"Information API","title":"ComplexityMeasures.Kaniadakis","text":"Kaniadakis <: EntropyDefinition\nKaniadakis(; Œ∫ = 1.0, base = 2.0)\n\nThe Kaniadakis entropy (Tsallis, 2009)[Tsallis2009], used with entropy to compute\n\nH_K(p) = -sum_i=1^N p_i f_kappa(p_i)\n\nf_kappa (x) = dfracx^kappa - x^-kappa2kappa\n\nwhere if kappa = 0, regular logarithm to the given base is used, and 0 probabilities are skipped.\n\n[Tsallis2009]: Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Curado","page":"Information API","title":"ComplexityMeasures.Curado","text":"Curado <: EntropyDefinition\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado & Nobre, 2004)[Curado2004], used with entropy to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b ‚àà ‚Ñõ, b > 0, where the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\nThe maximum entropy for Curado is L(1 - exp(-bL)) + exp(-b) - 1 with L the total_outcomes.\n\n[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.StretchedExponential","page":"Information API","title":"ComplexityMeasures.StretchedExponential","text":"StretchedExponential <: EntropyDefinition\nStretchedExponential(; Œ∑ = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999[Anteneodo1999]), used with entropy to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for Œ∑ = 1.0.\n\nThe maximum entropy for StrechedExponential is a rather complicated expression involving incomplete Gamma functions (see source code).\n\n[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Discrete","page":"Information API","title":"Discrete","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"entropy(::EntropyDefinition, ::ProbabilitiesEstimator, ::Any)\nentropy_maximum\nentropy_normalized","category":"page"},{"location":"information_api/#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}","page":"Information API","title":"ComplexityMeasures.entropy","text":"entropy([e::DiscreteEntropyEstimator,] probs::Probabilities)\nentropy([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x)\n\nCompute the discrete entropy h::Real ‚àà [0, ‚àû), using the estimator e, in one of two ways:\n\nDirectly from existing Probabilities probs.\nFrom input data x, by first estimating a probability mass function using the provided ProbabilitiesEstimator, and then computing the entropy from that mass fuction using the provided DiscreteEntropyEstimator.\n\nInstead of providing a DiscreteEntropyEstimator, an EntropyDefinition can be given directly, in which case MLEntropy is used as the estimator. If e is not provided, Shannon() is used by default.\n\nMaximum entropy and normalized entropy\n\nAll discrete entropies have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the entropy_maximum. Or, one can use entropy_normalized to obtain the normalized form of the entropy (divided by the maximum).\n\nExamples\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), CountOccurrences(x), x) # syntactically equivalent to above\nh = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\n\n\n\n\n\n","category":"method"},{"location":"information_api/#ComplexityMeasures.entropy_maximum","page":"Information API","title":"ComplexityMeasures.entropy_maximum","text":"entropy_maximum(e::EntropyDefinition, est::ProbabilitiesEstimator, x)\n\nReturn the maximum value of a discrete entropy with the given probabilities estimator and input data x. Like in outcome_space, for some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to entropy_maximum(e, est).\n\nentropy_maximum(e::EntropyDefinition, L::Int)\n\nSame as above, but computed directly from the number of total outcomes L.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#ComplexityMeasures.entropy_normalized","page":"Information API","title":"ComplexityMeasures.entropy_normalized","text":"entropy_normalized([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) ‚Üí hÃÉ\n\nReturn hÃÉ ‚àà [0, 1], the normalized discrete entropy of x, i.e. the value of entropy divided by the maximum value for e, according to the given probabilities estimator.\n\nInstead of a discrete entropy estimator, an EntropyDefinition can be given as first argument. If e is not given, it defaults to Shannon().\n\nNotice that there is no method entropy_normalized(e::DiscreteEntropyEstimator, probs::Probabilities), because there is no way to know the amount of possible events (i.e., the total_outcomes) from probs.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#Differential/continuous","page":"Information API","title":"Differential/continuous","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"entropy(::EntropyDefinition, ::DifferentialEntropyEstimator, ::Any)","category":"page"},{"location":"information_api/#Table-of-differential-entropy-estimators","page":"Information API","title":"Table of differential entropy estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The following estimators are differential entropy estimators, and can also be used with entropy.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Each DifferentialEntropyEstimators uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, Kraskov estimates the Shannon entropy.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis Curado StretchedExponential\nKozachenkoLeonenko Nearest neighbors Dataset ‚úì x x x x x\nKraskov Nearest neighbors Dataset ‚úì x x x x x\nZhu Nearest neighbors Dataset ‚úì x x x x x\nZhuSingh Nearest neighbors Dataset ‚úì x x x x x\nGao Nearest neighbors Dataset ‚úì x x x x x\nGoria Nearest neighbors Dataset ‚úì x x x x x\nLord Nearest neighbors Dataset ‚úì x x x x x\nVasicek Order statistics Vector ‚úì x x x x x\nEbrahimi Order statistics Vector ‚úì x x x x x\nCorrea Order statistics Vector ‚úì x x x x x\nAlizadehArghami Order statistics Vector ‚úì x x x x x","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"DifferentialEntropyEstimator","category":"page"},{"location":"information_api/#ComplexityMeasures.DifferentialEntropyEstimator","page":"Information API","title":"ComplexityMeasures.DifferentialEntropyEstimator","text":"DifferentialEntropyEstimator\nDiffEntropyEst # alias\n\nThe supertype of all differential entropy estimators. These estimators compute an entropy value in various ways that do not involve explicitly estimating a probability distribution.\n\nSee the table of differential entropy estimators in the docs for all differential entropy estimators.\n\nSee entropy for usage.\n\n\n\n\n\n","category":"type"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Kraskov\nKozachenkoLeonenko\nZhu\nZhuSingh\nGao\nGoria\nLord\nVasicek\nAlizadehArghami\nEbrahimi\nCorrea","category":"page"},{"location":"information_api/#ComplexityMeasures.Kraskov","page":"Information API","title":"ComplexityMeasures.Kraskov","text":"Kraskov <: DiffEntropyEst\nKraskov(; k::Int = 1, w::Int = 1, base = 2)\n\nThe Kraskov estimator computes the Shannon differential entropy of a multi-dimensional Dataset using the k-th nearest neighbor searches method from [Kraskov2004] at the given base.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Kraskov estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Kraskov2004]: Kraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.KozachenkoLeonenko","page":"Information API","title":"ComplexityMeasures.KozachenkoLeonenko","text":"KozachenkoLeonenko <: DiffEntropyEst\nKozachenkoLeonenko(; w::Int = 0, base = 2)\n\nThe KozachenkoLeonenko estimator computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nusing the nearest neighbor method from Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzy≈Ñska and Gambin[Charzy≈Ñska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\nSee also: entropy, Kraskov, DifferentialEntropyEstimator.\n\n[Charzy≈Ñska2016]: Charzy≈Ñska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. EntropyDefinition, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Zhu","page":"Information API","title":"ComplexityMeasures.Zhu","text":"Zhu <: DiffEntropyEst\nZhu(; k = 1, w = 0, base = 2)\n\nThe Zhu estimator (Zhu et al., 2015)[Zhu2015] is an extension to KozachenkoLeonenko, and computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Zhu estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby approximating densities within hyperrectangles surrounding each point x·µ¢ ‚àà x using using k nearest neighbor searches. w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeann√®s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.ZhuSingh","page":"Information API","title":"ComplexityMeasures.ZhuSingh","text":"ZhuSingh <: DiffEntropyEst\nZhuSingh(; k = 1, w = 0, base = 2)\n\nThe ZhuSingh estimator (Zhu et al., 2015)[Zhu2015] computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. ZhuSingh estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point x·µ¢ ‚àà x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeann√®s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Gao","page":"Information API","title":"ComplexityMeasures.Gao","text":"Gao <: DifferentialEntropyEstimator\nGao(; k = 1, w = 0, base = 2, corrected = true)\n\nThe Gao estimator (Gao et al., 2015) computes the Shannon differential entropy, using a k-th nearest-neighbor approach based on Singh et al. (2003)[Singh2003].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nGao et al., 2015 give two variants of this estimator. If corrected == false, then the uncorrected version is used. If corrected == true, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\n[Gao2015]: Gao, S., Ver Steeg, G., & Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PMLR.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Goria","page":"Information API","title":"ComplexityMeasures.Goria","text":"Goria <: DifferentialEntropyEstimator\nGoria(; k = 1, w = 0, base = 2)\n\nThe Goria estimator computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Goria estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSpecifically, let bfn_1 bfn_2 ldots bfn_N be the distance of the samples bfx_1 bfx_2 ldots bfx_N  to their k-th nearest neighbors. Next, let the geometric mean of the distances be\n\nhatrho_k = left( prod_i=1^N right)^dfrac1N\n\nGoria et al. (2005)[Goria2005]'s estimate of Shannon differential entropy is then\n\nhatH = mhatrho_k + log(N - 1) - psi(k) + log c_1(m)\n\nwhere c_1(m) = dfrac2pi^fracm2m Gamma(m2) and psi is the digamma function.\n\n[Goria2005]: Goria, M. N., Leonenko, N. N., Mergel, V. V., & Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Vasicek","page":"Information API","title":"ComplexityMeasures.Vasicek","text":"Vasicek <: DiffEntropyEst\nVasicek(; m::Int = 1, base = 2)\n\nThe Vasicek estimator computes the Shannon differential entropy (in the given base) of a timeseries using the method from Vasicek (1976)[Vasicek1976].\n\nThe Vasicek estimator belongs to a class of differential entropy estimators based on order statistics, of which Vasicek (1976) was the first. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Vasicek estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Vasicek Shannon differential entropy estimate is then\n\nhatH_V(barX m) =\ndfrac1n\nsum_i = 1^n log left dfracn2m (barX_(i+m) - barX_(i-m)) right\n\nUsage\n\nIn practice, choice of m influences how fast the entropy converges to the true value. For small value of m, convergence is slow, so we recommend to scale m according to the time series length n and use m >= n/100 (this is just a heuristic based on the tests written for this package).\n\n[Vasicek1976]: Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.\n\nSee also: entropy, Correa, AlizadehArghami, Ebrahimi, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.AlizadehArghami","page":"Information API","title":"ComplexityMeasures.AlizadehArghami","text":"AlizadehArghami <: DiffEntropyEst\nAlizadehArghami(; m::Int = 1, base = 2)\n\nThe AlizadehArghamiestimator computes the Shannon differential entropy (in the given base) of a timeseries using the method from Alizadeh & Arghami (2010)[Alizadeh2010].\n\nThe AlizadehArghami estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. AlizadehArghami estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X:\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The AlizadehArghami Shannon differential entropy estimate is then the the Vasicek estimate hatH_V(barX m n), plus a correction factor\n\nhatH_A(barX m n) = hatH_V(barX m n) +\ndfrac2nleft(m log(2) right)\n\n[Alizadeh2010]: Alizadeh, N. H., & Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).\n\nSee also: entropy, Correa, Ebrahimi, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Ebrahimi","page":"Information API","title":"ComplexityMeasures.Ebrahimi","text":"Ebrahimi <: DiffEntropyEst\nEbrahimi(; m::Int = 1, base = 2)\n\nThe Ebrahimi estimator computes the Shannon entropy (in the given base) of a timeseries using the method from Ebrahimi (1994)[Ebrahimi1994].\n\nThe Ebrahimi estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Ebrahimi estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Ebrahimi Shannon differential entropy estimate is then\n\nhatH_E(barX m) =\ndfrac1n sum_i = 1^n log\nleft dfracnc_i m (barX_(i+m) - barX_(i-m)) right\n\nwhere\n\nc_i =\nbegincases\n    1 + fraci - 1m  1 geq i geq m \n    2                     m + 1 geq i geq n - m \n    1 + fracn - im  n - m + 1 geq i geq n\nendcases\n\n[Ebrahimi1994]: Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n\nSee also: entropy, Correa, AlizadehArghami, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#ComplexityMeasures.Correa","page":"Information API","title":"ComplexityMeasures.Correa","text":"Correa <: DiffEntropyEst\nCorrea(; m::Int = 1, base = 2)\n\nThe Correa estimator computes the Shannon differential entropy (in the given `base) of a timeseries using the method from Correa (1995)[Correa1995].\n\nThe Correa estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Correa estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, Correa makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n), ensuring that end points are included. The Correa estimate of Shannon differential entropy is then\n\nH_C(barX m n) =\ndfrac1n sum_i = 1^n log\nleft dfrac sum_j=i-m^i+m(barX_(j) -\ntildeX_(i))(j - i)n sum_j=i-m^i+m (barX_(j) - tildeX_(i))^2\nright\n\nwhere\n\ntildeX_(i) = dfrac12m + 1 sum_j = i - m^i + m X_(j)\n\n[Correa1995]: Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.\n\nSee also: entropy, AlizadehArghami, Ebrahimi, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Conditional-entropy","page":"Information API","title":"Conditional entropy","text":"","category":"section"},{"location":"information_api/#Conditional-entropy-API","page":"Information API","title":"Conditional entropy API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The conditional entropy API is defined by","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ConditionalEntropy,\nentropy_conditional,","category":"page"},{"location":"information_api/#Conditional-entropy-definitions","page":"Information API","title":"Conditional entropy definitions","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ConditionalEntropy\nCEShannon\nCETsallisFuruichi\nCETsallisAbe","category":"page"},{"location":"information_api/#CausalityTools.ConditionalEntropy","page":"Information API","title":"CausalityTools.ConditionalEntropy","text":"The supertype for all conditional entropies.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.CEShannon","page":"Information API","title":"CausalityTools.CEShannon","text":"CEShannon <: ConditionalEntropy\nCEShannon(; base = 2,)\n\nTheShannon conditional entropy measure.\n\nDiscrete definition\n\nSum formulation\n\nThe conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH^S(X  Y) = -sum_x in mathcalX y in mathcalY = p(x y) log(p(x  y))\n\nThis is the definition used when calling entropy_conditional with a ContingencyMatrix.\n\nTwo-entropies formulation\n\nEquivalently, the following difference of entropies hold\n\nH^S(X  Y) = H^S(X Y) - H^S(Y)\n\nwhere H^S(cdot and H^S(cdot  cdot) are the Shannon entropy and Shannon joint entropy, respectively. This is the definition used when calling entropy_conditional with a ProbabilitiesEstimator.\n\nDifferential definition\n\nThe differential conditional Shannon entropy is analogously defined as\n\nH^S(X  Y) = h^S(X Y) - h^S(Y)\n\nwhere h^S(cdot and h^S(cdot  cdot) are the Shannon differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling entropy_conditional with a DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.CETsallisFuruichi","page":"Information API","title":"CausalityTools.CETsallisFuruichi","text":"CETsallisFuruichi <: ConditionalEntropy\nCETsallisFuruichi(; base = 2, q = 1.5)\n\nFuruichi (2006)'s discrete Tsallis conditional entropy measure.\n\nDefinition\n\nFuruichi's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T(X  Y) = -sum_x in mathcalX y in mathcalY\np(x y)^q log_q(p(x  y))\n\nwhen q neq 1. For q = 1, H_q^T(X  Y) reduces to the Shannon conditional entropy:\n\nH_q=1^T(X  Y) = -sum_x in mathcalX y in mathcalY =\np(x y) log(p(x  y))\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.CETsallisAbe","page":"Information API","title":"CausalityTools.CETsallisAbe","text":"CETsallisAbe <: ConditionalEntropy\nCETsallisAbe(; base = 2, q = 1.5)\n\nAbe & Rajagopal (2001)'s discrete Tsallis conditional entropy measure.\n\nDefinition\n\nAbe & Rajagopal's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T_A(X  Y) = dfracH_q^T(X Y) - H_q^T(Y)1 + (1-q)H_q^T(Y)\n\nwhere H_q^T(cdot) and H_q^T(cdot cdot) is the Tsallis entropy and the joint Tsallis entropy.\n\n[Abe2001]: Abe, S., & Rajagopal, A. K. (2001). Nonadditive conditional entropy and its significance for local realism. Physica A: Statistical Mechanics and its Applications, 289(1-2), 157-164.\n\n\n\n\n\n","category":"type"},{"location":"information_api/","page":"Information API","title":"Information API","text":"More variants exist in the literature. Pull requests are welcome!","category":"page"},{"location":"information_api/#Discrete-conditional-entropy","page":"Information API","title":"Discrete conditional entropy","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"entropy_conditional(::ConditionalEntropy, ::ContingencyMatrix)","category":"page"},{"location":"information_api/#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}","page":"Information API","title":"CausalityTools.entropy_conditional","text":"entropy_conditional(measure::ConditionalEntropy, c::ContingencyMatrix{T, 2}) where T\n\nEstimate the discrete version of the given ConditionalEntropy measure from its direct (sum) definition, using the probabilities from a pre-computed ContingencyMatrix, constructed from two input variables x and y.\n\nThe convention is to compute the entropy of the variable in the first column of c conditioned on the variable in the second column of c. To do the opposite, call this function with a new contingency matrix where the order of the variables is reversed.\n\nIf measure is not given, then the default is CEShannon().\n\n\n\n\n\n","category":"method"},{"location":"information_api/#contingency_matrix_ce","page":"Information API","title":"Contingency matrix","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Discrete conditional entropy can be computed directly from its sum-definition by using the probabilities from a ContingencyMatrix. This estimation method works for  both numerical and categorical data, and the following ConditionalEntropy definitions are supported.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":" ContingencyMatrix\nCEShannon ‚úì\nCETsallisFuruichi ‚úì\nCETsallisAbe ‚úì","category":"page"},{"location":"information_api/#probabilities_estimators_ce","page":"Information API","title":"Table of discrete conditional entropy estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Here, we list the ProbabilitiesEstimators that are compatible with entropy_conditional, and which definitions they are valid for.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle CEShannon CETsallisAbe CETsallisFuruichi\nCountOccurrences Frequencies ‚úì ‚úì x\nValueHistogram Binning (histogram) ‚úì ‚úì x\nSymbolicPermuation Ordinal patterns ‚úì ‚úì x\nDispersion Dispersion patterns ‚úì ‚úì x","category":"page"},{"location":"information_api/#Differential/continuous-conditional-entropy","page":"Information API","title":"Differential/continuous conditional entropy","text":"","category":"section"},{"location":"information_api/#diffentropy_estimators_ce","page":"Information API","title":"Table of differential conditional entropy estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Continuous/differential mutual information may be estimated using any of our DifferentialEntropyEstimators that support multivariate input data.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle CEShannon CETsallisAbe CETsallisFuruichi\nKraskov Nearest neighbors ‚úì x x\nZhu Nearest neighbors ‚úì x x\nZhuSingh Nearest neighbors ‚úì x x\nGao Nearest neighbors ‚úì x x\nGoria Nearest neighbors ‚úì x x\nLord Nearest neighbors ‚úì x x\nLeonenkoProzantoSavani Nearest neighbors ‚úì x x","category":"page"},{"location":"information_api/#Mutual-information","page":"Information API","title":"Mutual information","text":"","category":"section"},{"location":"information_api/#api_mutualinfo","page":"Information API","title":"Mutual information API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The mutual information API is defined by","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"MutualInformation,\nmutualinfo,\nMutualInformationEstimator.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"We provide a suite of estimators of various mutual information quantities. Many more variants exist in the literature. Pull requests are welcome!","category":"page"},{"location":"information_api/#Definitions","page":"Information API","title":"Definitions","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"MutualInformation","category":"page"},{"location":"information_api/#CausalityTools.MutualInformation","page":"Information API","title":"CausalityTools.MutualInformation","text":"The supertype of all mutual information measures \n\n\n\n\n\n","category":"type"},{"location":"information_api/#Dedicated-estimators","page":"Information API","title":"Dedicated estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"mutualinfo(est::MutualInformationEstimator, ::Any, ::Any) ","category":"page"},{"location":"information_api/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}","page":"Information API","title":"CausalityTools.mutualinfo","text":"mutualinfo([measure::MutualInformation], est::MutualInformationEstimator, x, y)\n\nEstimate the mutual information measure between x and y using the dedicated MutualInformationEstimator est, which can be either discrete, continuous, or a mixture of both, and typically involve some bias correction. If measure is not given, then the default is MIShannon().\n\nSee the online documentation for a list of compatible measures.\n\n\n\n\n\n","category":"method"},{"location":"information_api/","page":"Information API","title":"Information API","text":"MutualInformationEstimator\nKraskovSt√∂gbauerGrassberger1\nKraskovSt√∂gbauerGrassberger2\nGaoKannanOhViswanath\nGaoOhViswanath","category":"page"},{"location":"information_api/#CausalityTools.MutualInformationEstimator","page":"Information API","title":"CausalityTools.MutualInformationEstimator","text":"MutualInformationEstimator\n\nThe supertype of all dedicated mutual information estimators.\n\nMutualInformationEstimators can be either mixed, discrete or a combination of both. Each estimator uses a specialized technique to approximate relevant densities/integrals and/or probabilities, and is typically tailored to a specific type of MutualInformation (mostly MIShannon).\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.KraskovSt√∂gbauerGrassberger1","page":"Information API","title":"CausalityTools.KraskovSt√∂gbauerGrassberger1","text":"KSG1 <: MutualInformationEstimator\nKraskovSt√∂gbauerGrassberger1 <: MutualInformationEstimator\nKraskovSt√∂gbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovSt√∂gbauerGrassberger1 mutual information estimator (you can use KSG1 for short) is the I^(1) k-th nearest neighbor estimator from Kraskov et al. (2004)[Kraskov2004].\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nDescription\n\nLet the joint dataset X = bfX_1 bfX_2 ldots bfX_m  be defined by the concatenation of the marginal datasets  bfX_k _k=1^m, where each bfX_k is potentially multivariate. Let bfx_1 bfx_2 ldots bfx_N be the points in the joint space X.\n\n[Kraskov2004]: Kraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.KraskovSt√∂gbauerGrassberger2","page":"Information API","title":"CausalityTools.KraskovSt√∂gbauerGrassberger2","text":"KSG2 <: MutualInformationEstimator\nKraskovSt√∂gbauerGrassberger2 <: MutualInformationEstimator\nKraskovSt√∂gbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovSt√∂gbauerGrassberger2 mutual information estimator (you can use KSG2 for short) is the I^(2) k-th nearest neighbor estimator from Kraskov et al. (2004)[Kraskov2004].\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nDescription\n\nLet the joint dataset X = bfX_1 bfX_2 ldots bfX_m  be defined by the concatenation of the marginal datasets  bfX_k _k=1^m, where each bfX_k is potentially multivariate. Let bfx_1 bfx_2 ldots bfx_N be the points in the joint space X.\n\nThe KraskovSt√∂gbauerGrassberger2 estimator first locates, for each bfx_i in X, the point bfn_i in X, the k-th nearest neighbor to bfx_i, according to the maximum norm (Chebyshev metric). Let epsilon_i be the distance d(bfx_i bfn_i).\n\nConsider x_i^m in bfX_m, the i-th point in the marginal space bfX_m. For each bfx_i^m, we determine theta_i^m := the number of points bfx_k^m in bfX_m that are a distance less than epsilon_i away from bfx_i^m. That is, we use the distance from a query point bfx_i in X (in the joint space) to count neighbors of x_i^m in bfX_m (in the marginal space).\n\nMutual information between the variables bfX_1 bfX_2 ldots bfX_m is then estimated as\n\nhatI_KSG2(bfX) =\n    psi(k) -\n    dfracm - 1k +\n    (m - 1)psi(N) -\n    dfrac1N sum_i = 1^N sum_j = 1^m psi(theta_i^j + 1)\n\n[Kraskov2004]: Kraskov, A., St√∂gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.GaoKannanOhViswanath","page":"Information API","title":"CausalityTools.GaoKannanOhViswanath","text":"GaoKannanOhViswanath <: MutualInformationEstimator\nGaoKannanOhViswanath(; k = 1, w = 0)\n\nThe GaoKannanOhViswanath (Shannon) estimator is designed for estimating mutual information between variables that may be either discrete, continuous or a mixture of both (Gao et al., 2017).\n\nnote: Explicitly convert your discrete data to floats\nEven though the GaoKannanOhViswanath estimator is designed to handle discrete data, our implementation demands that all input data are Datasets whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to mutualinfo.\n\nDescription\n\nThe estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using k-nearest neighbor distances from empirical samples.\n\nThe estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator \"strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.\"\n\nwarn: Implementation note\nIn Gao et al., (2017), they claim (roughly speaking) that the estimator reduces to the KraskovSt√∂gbauerGrassberger1 estimator for continuous-valued data. However, KraskovSt√∂gbauerGrassberger1 uses the digamma function, while GaoKannanOhViswanath uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the KSG1 approach. The KSG1 estimator uses k-th neighbor distances in the joint space, while the GaoKannanOhViswanath algorithm selects the maximum k-th nearest distances among the two marginal spaces, which are in general not the same as the k-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in GaoKannanOhViswanath. We have modified it in a way that mimics KraskovSt√∂gbauerGrassberger1 for continous data. Note that because of using the log function instead of digamma, there will be slight differences between the methods. See the source code for more details.\n\nSee also: mutualinfo.\n\n[GaoKannanOhViswanath2017]: Gao, W., Kannan, S., Oh, S., & Viswanath, P. (2017). Estimating mutual information for discrete-continuous mixtures. Advances in neural information processing systems, 30.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.GaoOhViswanath","page":"Information API","title":"CausalityTools.GaoOhViswanath","text":"GaoOhViswanath <: MutualInformationEstimator\n\nThe GaoOhViswanath mutual information estimator, also called the bias-improved-KSG estimator, or BI-KSG, by Gao et al. (2018)[Gao2018], is given by\n\nbeginalign*\nhatH_GAO(X Y)\n= hatH_KSG(X) + hatH_KSG(Y) - hatH_KZL(X Y) \n= psi(k) +\n    log(N) +\n    log\n        left(\n            dfracc_d_x 2 c_d_y 2c_d_x + d_y 2\n        right)\n     - \n     dfrac1N sum_i=1^N left( log(n_x i 2) + log(n_y i 2) right)\nendalign*\n\nwhere c_d 2 = dfracpi^fracd2Gamma(dfracd2 + 1) is the volume of a d-dimensional unit mathcall_2-ball.\n\n[Gao2018]: Gao, W., Oh, S., & Viswanath, P. (2018). Demystifying fixed k-nearest neighbor information estimators. IEEE Transactions on Information Theory, 64(8), 5629-5661.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#dedicated_estimators_mi","page":"Information API","title":"Table of dedicated estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Type Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiSarbu MIRenyiJizba\nKraskovSt√∂gbauerGrassberger1 Continuous Nearest neighbors ‚úì x x x x\nKraskovSt√∂gbauerGrassberger2 Continuous Nearest neighbors ‚úì x x x x\nGaoKannanOhViswanath Mixed Nearest neighbors ‚úì x x x x\nGaoOhViswanath Continuous Nearest neighbors ‚úì x x x x","category":"page"},{"location":"information_api/#Discrete-mutual-information","page":"Information API","title":"Discrete mutual information","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"mutualinfo(::ProbabilitiesEstimator, ::Any, ::Any)","category":"page"},{"location":"information_api/#CausalityTools.mutualinfo-Tuple{ProbabilitiesEstimator, Any, Any}","page":"Information API","title":"CausalityTools.mutualinfo","text":"mutualinfo([measure::MutualInformation], est::ProbabilitiesEstimator, x, y)\n\nEstimate the mutual information measure between x and y by a sum of three entropy terms, without any bias correction, using the provided ProbabilitiesEstimator est. If measure is not given, then the default is MIShannon().\n\nJoint and marginal probabilities are computed by jointly discretizing x and y using the approach given by est, and obtaining marginal distributions from the joint distribution.\n\nThis only works for estimators that have an implementation for marginal_encodings. See the online documentation for a list of compatible measures.\n\n\n\n\n\n","category":"method"},{"location":"information_api/#@id-dedicated_probabilities_estimators_mi","page":"Information API","title":"Table of discrete mutual information estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Here, we list the ProbabilitiesEstimators that can be used to compute discrete mutualinformation.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiJizba MIRenyiSarbu\nCountOccurrences Frequencies ‚úì ‚úì ‚úì ‚úì x\nValueHistogram Binning (histogram) ‚úì ‚úì ‚úì ‚úì x\nSymbolicPermuation Ordinal patterns ‚úì ‚úì ‚úì ‚úì x\nDispersion Dispersion patterns ‚úì ‚úì ‚úì ‚úì x","category":"page"},{"location":"information_api/#contingency_matrix_mi","page":"Information API","title":"Contingency matrix","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"mutualinfo(::MutualInformation, ::ContingencyMatrix)","category":"page"},{"location":"information_api/#CausalityTools.mutualinfo-Tuple{MutualInformation, ContingencyMatrix}","page":"Information API","title":"CausalityTools.mutualinfo","text":"mutualinfo(measure::MutualInformation, est::MutualInformationEstimator, x, y)\nmutualinfo(measure::MutualInformation, est::DifferentialEntropyEstimator, x, y)\nmutualinfo(measure::MutualInformation, est::ProbabilitiesEstimator, x, y)\nmutualinfo(measure::MutualInformation, c::ContingencyMatrix)\n\nEstimate the mutual information measure (either MIShannon or MITsallis, ) between x and y using the provided estimator est. Alternatively, compute mutual information from a pre-computed ContingencyMatrix.\n\nCompatible measures/definitions and estimators are listed in the online documentation.\n\n\n\n\n\n","category":"method"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Discrete mutual information can be computed directly from its double-sum definition by using the probabilities from a ContingencyMatrix. This estimation method works for    both numerical and categorical data, and the following MutualInformations are supported.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":" ContingencyMatrix\nMIShannon ‚úì\nMITsallisFuruichi ‚úì\nMITsallisMartin ‚úì\nMIRenyiSarbu ‚úì\nMIRenyiJizba ‚úì","category":"page"},{"location":"information_api/#Differential/continuous-mutual-information","page":"Information API","title":"Differential/continuous mutual information","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"mutualinfo(::DifferentialEntropyEstimator, ::Any, ::Any)","category":"page"},{"location":"information_api/#CausalityTools.mutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any}","page":"Information API","title":"CausalityTools.mutualinfo","text":"mutualinfo([measure::MutualInformation], est::DifferentialEntropyEstimator, x, y)\n\nEstimate the mutual information measure between x and y by a sum of three entropy terms, without any bias correction, using any DifferentialEntropyEstimator compatible with multivariate data. If measure is not given, then the default is MIShannon().\n\nSee the online documentation for a list of compatible measures.\n\n\n\n\n\n","category":"method"},{"location":"information_api/#dedicated_diffentropy_estimators_mi","page":"Information API","title":"Table of differential mutual information estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"In addition to the dedicated differential mutual information estimators listed above, continuous/differential mutual information may also be estimated using any of our DifferentialEntropyEstimator that support multivariate input data. When using these estimators, mutual information is computed as a sum of entropy terms (with different dimensions), and no bias correction is applied.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiJizba MIRenyiSurbu\nKraskov Nearest neighbors ‚úì x x x x\nZhu Nearest neighbors ‚úì x x x x\nZhuSingh Nearest neighbors ‚úì x x x x\nGao Nearest neighbors ‚úì x x x x\nGoria Nearest neighbors ‚úì x x x x\nLord Nearest neighbors ‚úì x x x x\nLeonenkoProzantoSavani Nearest neighbors ‚úì x x x x","category":"page"},{"location":"information_api/#Conditional-mutual-information","page":"Information API","title":"Conditional mutual information","text":"","category":"section"},{"location":"information_api/#CMI-API","page":"Information API","title":"CMI API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The condition mutual information API is defined by","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ConditionalMutualInformation,\nmutualinfo,\nConditionalMutualInformationEstimator.","category":"page"},{"location":"information_api/#CMI-definitions","page":"Information API","title":"CMI definitions","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ConditionalMutualInformation","category":"page"},{"location":"information_api/#CausalityTools.ConditionalMutualInformation","page":"Information API","title":"CausalityTools.ConditionalMutualInformation","text":"ConditionalMutualInformation <: InformationMeasure\nCMI # alias\n\nThe supertype of all conditional mutual informations.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Dedicated-CMI-estimators","page":"Information API","title":"Dedicated CMI estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"condmutualinfo(::ConditionalMutualInformationEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"information_api/#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}","page":"Information API","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::CMIEstimator, x, y, z) ‚Üí cmi::Real\n\nEstimate a conditional mutual information (CMI) of some kind (specified by measure), between x and y, given z, using the given dedicated ConditionalMutualInformationEstimator, which may be discrete, continuous or mixed.\n\n\n\n\n\n","category":"method"},{"location":"information_api/","page":"Information API","title":"Information API","text":"ConditionalMutualInformationEstimator\nFPVP\nMesnerShalisi\nPoczosSchneiderCMI\nRahimzamani","category":"page"},{"location":"information_api/#CausalityTools.ConditionalMutualInformationEstimator","page":"Information API","title":"CausalityTools.ConditionalMutualInformationEstimator","text":"ConditionalMutualInformationEstimator <: InformationEstimator\nCMIEstimator # alias\n\nThe supertype of all conditional mutual information estimators.\n\nSubtypes\n\nFPVP.\nPoczosSchneiderCMI.\nRahimzamani.\nMesnerShalisi.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.FPVP","page":"Information API","title":"CausalityTools.FPVP","text":"FPVP <: ConditionalMutualInformationEstimator\nFPVP(k = 1, w = 0)\n\nThe Frenzel-Pompe-Vejmelka-Palu≈° (or FPVP for short) estimator is used to estimate the differential conditional mutual information using a k-th nearest neighbor approach that is analogous to that of the KraskovSt√∂gbauerGrassberger1 mutual information estimator (Frenzel & Pompe, 2007[Frenzel2007]; Vejmelka & Palu≈°, 2008[Vejmelka2008]).\n\nw is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.\n\n[Frenzel2007]: Frenzel, S., & Pompe, B. (2007). Partial mutual information for coupling analysis of multivariate time series. Physical review letters, 99(20), 204101. w is the Theiler window.\n\n[Vejmelka2008]: Vejmelka, M., & Palu≈°, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E, 77(2), 026214.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.MesnerShalisi","page":"Information API","title":"CausalityTools.MesnerShalisi","text":"MesnerShalisi <: ConditionalMutualInformationEstimator\nMesnerShalisi(k = 1, w = 0)\n\nThe MesnerShalisi estimator is an estimator for conditional mutual information for data that can be mixtures of discrete and continuous data (Mesner & Shalisi et al., 2020)[MesnerShalisi2020].\n\n[MesnerShalisi2020]: Mesner, O. C., & Shalizi, C. R. (2020). Conditional mutual information estimation for mixed, discrete and continuous data. IEEE Transactions on Information Theory, 67(1), 464-484.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.PoczosSchneiderCMI","page":"Information API","title":"CausalityTools.PoczosSchneiderCMI","text":"PoczosSchneiderCMI <: ConditionalMutualInformationEstimator\nPoczosSchneiderCMI(k = 1, w = 0)\n\nThe PoczosSchneiderCMI estimator computes various (differential) conditional mutual informations, using a k-th nearest neighbor approach (P√≥czos & Schneider, 2012)[P√≥czos2012].\n\n[P√≥czos2012]: P√≥czos, B., & Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.Rahimzamani","page":"Information API","title":"CausalityTools.Rahimzamani","text":"Rahimzamani <: ConditionalMutualInformationEstimator\nRahimzamani(k = 1, w = 0)\n\nThe Rahimzamani estimator, short for Rahimzamani-Asnani-Viswanath-Kannan, is an estimator for Shannon conditional mutual information for data that can be mixtures of discrete and continuous data (Rahimzamani et al., 2018)[Rahimzamani2018].\n\nThis is very similar to the GaoKannanOhViswanath mutual information estimator, but has been expanded to the conditional case.\n\n[Rahimzamani2018]: Rahimzamani, A., Asnani, H., Viswanath, P., & Kannan, S. (2018). Estimators for multivariate information measures in general probability spaces. Advances in Neural Information Processing Systems, 31.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#condmutualinfo_dedicated_estimators","page":"Information API","title":"Table of dedicated CMI estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle CMIShannon CMIRenyiPoczos\nFPVP Nearest neighbors ‚úì x\nMesnerShalisi Nearest neighbors ‚úì x\nRahimzamani Nearest neighbors ‚úì x\nPoczosSchneiderCMI Nearest neighbors x ‚úì\nGaussianCMI Parametric ‚úì x","category":"page"},{"location":"information_api/#Estimation-through-mutual-information","page":"Information API","title":"Estimation through mutual information","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"condmutualinfo(::MutualInformationEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"information_api/#CausalityTools.condmutualinfo-Tuple{MutualInformationEstimator, Any, Any, Any}","page":"Information API","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::MutualInformationEstimator, x, y, z) ‚Üí cmi::Real\n\nEstimate the conditional mutual information (CMI) measure between x and y using a difference of mutual information terms, without any bias correction, using the provided MutualInformationEstimator est, which may be continuous/differential, discrete or mixed. If measure is not given, then the default is CMIShannon().\n\n\n\n\n\n","category":"method"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Type Principle CMIShannon\nKraskovSt√∂gbauerGrassberger1 Continuous Nearest neighbors ‚úì\nKraskovSt√∂gbauerGrassberger2 Continuous Nearest neighbors ‚úì\nGaoKannanOhViswanath Mixed Nearest neighbors ‚úì\nGaoOhViswanath Continuous Nearest neighbors ‚úì\nGaussianMI  Parametric ‚úì","category":"page"},{"location":"information_api/#Discrete-CMI","page":"Information API","title":"Discrete CMI","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"condmutualinfo(::ProbabilitiesEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"information_api/#CausalityTools.condmutualinfo-Tuple{ProbabilitiesEstimator, Any, Any, Any}","page":"Information API","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::ProbabilitiesEstimator, x, y, z) ‚Üí cmi::Real ‚àà [0, a)\n\nEstimate the conditional mutual information (CMI) measure between x and y given z using a sum of entropy terms, without any bias correction, using the provided ProbabilitiesEstimator est. If measure is not given, then the default is CMIShannon().\n\nWith a ProbabilitiesEstimator, the returned cmi is guaranteed to be non-negative.\n\n\n\n\n\n","category":"method"},{"location":"information_api/#mutualinfo_overview","page":"Information API","title":"Table of discrete mutual information estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Here, we list the ProbabilitiesEstimators that are compatible with condmutualinfo, and which definitions they are valid for.","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle CMIShannon CMIRenyiSarbu\nCountOccurrences Frequencies ‚úì ‚úì\nValueHistogram Binning (histogram) ‚úì ‚úì\nSymbolicPermuation Ordinal patterns ‚úì ‚úì\nDispersion Dispersion patterns ‚úì ‚úì","category":"page"},{"location":"information_api/#Differential-CMI","page":"Information API","title":"Differential CMI","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"condmutualinfo(::DifferentialEntropyEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"information_api/#CausalityTools.condmutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any, Any}","page":"Information API","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::DifferentialEntropyEstimator, x, y, z) ‚Üí cmi\n\nEstimate the conditional mutual information (CMI) measure between x and y using a sum of entropy terms, without any bias correction, using the provided DifferentialEntropyEstimator est (which must support multivariate data). If measure is not given, then the default is CMIShannon().\n\n\n\n\n\n","category":"method"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Estimator Principle Input data CMIShannon\nKraskov Nearest neighbors Dataset ‚úì\nZhu Nearest neighbors Dataset ‚úì\nGao Nearest neighbors Dataset ‚úì\nGoria Nearest neighbors Dataset ‚úì\nLord Nearest neighbors Dataset ‚úì\nLeonenkoProzantoSavani Nearest neighbors Dataset ‚úì","category":"page"},{"location":"information_api/#Transfer-entropy","page":"Information API","title":"Transfer entropy","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"The transfer entropy API is made up of the following functions and types, which are listed below:","category":"page"},{"location":"information_api/","page":"Information API","title":"Information API","text":"transferentropy.\nTransferEntropy, and its subtypes.\nEmbeddingTE, which exists to provide embedding instructions to   subtypes of TransferEntropy.\nTransferEntropyEstimator, and its subtypes.","category":"page"},{"location":"information_api/#Transfer-entropy-API","page":"Information API","title":"Transfer entropy API","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"transferentropy\nEmbeddingTE\noptimize_marginals_te","category":"page"},{"location":"information_api/#CausalityTools.transferentropy","page":"Information API","title":"CausalityTools.transferentropy","text":"transferentropy([measure::TEShannon], est, s, t, [c])\ntransferentropy(measure::TERenyiJizba, est, s, t, [c])\n\nEstimate the transfer entropy TE^*(S to T) or TE^*(S to T  C) if c is given, using the provided estimator est, where * indicates the given measure. If measure is not given, then TEShannon(; base = 2) is the default.\n\nArguments\n\nmeasure: The transfer entropy measure, e.g. TEShannon or   TERenyi, which dictates which formula is computed.   Embedding parameters are stored in measure.embedding, and   is represented by an EmbeddingTE instance. If calling transferentropy   without giving measure, then the embedding is optimized by finding   suitable delay embedding parameters using the \"traditional\"   approach from DynamicalSystems.jl.\ns: The source timeseries.\nt: The target timeseries.\nc: Optional. A conditional timeseries.\n\nDescription\n\nThe Shannon transfer entropy is defined as TE^S(S to T  C) = I^S(T^+ S^-  T^- C^-), where I^S(T^+ S^-  T^- C^-) is CMIShannon, and marginals for the CMI are constructed as described in EmbeddingTE. The definition is analogous for TERenyiJizba.\n\nIf s, t, and c are univariate timeseries, then the the marginal embedding variables T^+ (target future), T^- (target present/past), S^- (source present/past) and C^- (present/past of conditioning variables) are constructed by first jointly embedding  s, t and c with relevant delay embedding parameters, then subsetting relevant columns of the embedding.\n\nSince estimates of TE^*(S to T) and TE^*(S to T  C) are just a special cases of conditional mutual information where input data are marginals of a particular form of delay embedding, any combination of variables, e.g. S = (A B), T = (C D), C = (D E F) are valid inputs (given as Datasets). In practice, however, s, t and c are most often timeseries, and if  s, t and c are Datasets, it is assumed that the data are pre-embedded and the embedding step is skipped.\n\nCompatible estimators\n\ntransferentropy is just a simple wrapper around condmutualinfo that constructs an appropriate delay embedding from the input data before CMI is estimated. Consequently, any estimator that can be used for ConditionalMutualInformation is, in principle, also a valid transfer entropy estimator. Documentation strings for TEShannon and TERenyiJizba list compatible estimators, and an overview table can be found in the online documentation.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#CausalityTools.EmbeddingTE","page":"Information API","title":"CausalityTools.EmbeddingTE","text":"EmbeddingTE(; dS = 1, dT = 1, dTf = 1, dC = 1, œÑS = -1, œÑT = -1, Œ∑Tf = 1, œÑC = -1)\nEmbeddingTE(opt::OptimiseTraditional, s, t, [c])\n\nEmbeddingTE provide embedding parameters for transfer entropy analysis using either TEShannon, TERenyi, or in general any subtype of TransferEntropy, which in turns dictates the embedding used with transferentropy.\n\nThe second method finds parameters using the \"traditional\" optimised embedding techniques from DynamicalSystems.jl\n\nConvention for generalized delay reconstruction\n\nWe use the following convention. Let s(i) be time series for the source variable, t(i) be the time series for the target variable and c(i) the time series for the conditional variable. To compute transfer entropy, we need the following marginals:\n\nbeginaligned\nT^+ = t(i+eta^1) t(i+eta^2) ldots (t(i+eta^d_T^+)  \nT^- =  (t(i+tau^0_T) t(i+tau^1_T) t(i+tau^2_T) ldots t(t + tau^d_T - 1_T))  \nS^- =  (s(i+tau^0_S) s(i+tau^1_S) s(i+tau^2_S) ldots s(t + tau^d_S - 1_S))  \nC^- =  (c(i+tau^0_C) c(i+tau^1_C) c(i+tau^2_C) ldots c(t + tau^d_C - 1_C)) \nendaligned\n\nDepending on the application, the delay reconstruction lags tau^k_T leq 0, tau^k_S leq 0, and tau^k_C leq 0 may be equally spaced, or non-equally spaced. The same applied to the prediction lag(s), but typically only a only a single predictions lag eta^k is used (so that d_T^+ = 1).\n\nFor transfer entropy, traditionally at least one tau^k_T, one tau^k_S and one tau^k_C equals zero. This way, the T^-, S^- and C^- marginals always contains present/past states, while the mathcal T marginal contain future states relative to the other marginals. However, this is not a strict requirement, and modern approaches that searches for optimal embeddings can return embeddings without the intantaneous lag.\n\nCombined, we get the generalized delay reconstruction mathbbE = (T^+_(d_T^+) T^-_(d_T) S^-_(d_S) C^-_(d_C)). Transfer entropy is then computed as\n\nbeginaligned\nTE_S rightarrow T  C = int_mathbbE P(T^+ T^- S^- C^-)\nlog_bleft(fracP(T^+  T^- S^- C^-)P(T^+  T^- C^-)right)\nendaligned\n\nor, if conditionals are not relevant,\n\nbeginaligned\nTE_S rightarrow T = int_mathbbE P(T^+ T^- S^-)\nlog_bleft(fracP(T^+  T^- S^-)P(T^+  T^-)right)\nendaligned\n\nHere,\n\nT^+ denotes the d_T^+-dimensional set of vectors furnishing the future   states of T (almost always equal to 1 in practical applications),\nT^- denotes the d_T-dimensional set of vectors furnishing the past and   present states of T,\nS^- denotes the d_S-dimensional set of vectors furnishing the past and   present of S, and\nC^- denotes the d_C-dimensional set of vectors furnishing the past and   present of C.\n\nKeyword arguments\n\ndS, dT, dC, dTf (f for future) are the dimensions of the S^-,   T^-, C^- and T^+ marginals. The parameters dS, dT, dC and dTf   must each be a positive integer number.\nœÑS, œÑT, œÑC are the embedding lags for S^-, T^-, C^-.   Each parameter are integers ‚àà ùí©‚Å∞‚Åª, or a vector of integers ‚àà ùí©‚Å∞‚Åª, so   that S^-, T^-, C^- always represents present/past values.   If e.g. œÑT is an integer, then for the T^- marginal is constructed using   lags tau_T = 0 tau 2tau ldots (d_T- 1)tau_T .   If is a vector, e.g. œÑŒ§ = [-1, -5, -7], then the dimension dT must match the lags,   and precisely those lags are used: tau_T = -1 -5 -7 .\nThe prediction lag(s) Œ∑Tf is a positive integer. Combined with the requirement   that the other delay parameters are zero or negative, this ensures that we're   always predicting from past/present to future. In typical applications,   Œ∑Tf = 1 is used for transfer entropy.\n\nExamples\n\nSay we wanted to compute the Shannon transfer entropy TE^S(S to T) = I^S(T^+ S^-  T^-). Using some modern procedure for determining optimal embedding parameters using methods from DynamicalSystems.jl, we find that the optimal embedding of T^- is three-dimensional and is given by the lags [0, -5, -8]. Using the same procedure, we find that the optimal embedding of S^- is two-dimensional with lags -1 -8. We want to predicting a univariate version of the target variable one time step into the future (Œ∑Tf = 1). The total embedding is then the set of embedding vectors\n\nE_TE =  (T(i+1) S(i-1) S(i-8) T(i) T(i-5) T(i-8)) . Translating this to code, we get:\n\nusing CausalityTools\njulia> EmbeddingTE(dT=3, œÑT=[0, -5, -8], dS=2, œÑS=[-1, -4], Œ∑Tf=1)\n\n# output\nEmbeddingTE(dS=2, dT=3, dC=1, dTf=1, œÑS=[-1, -4], œÑT=[0, -5, -8], œÑC=-1, Œ∑Tf=1)\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.optimize_marginals_te","page":"Information API","title":"CausalityTools.optimize_marginals_te","text":"optimize_marginals_te([scheme = OptimiseTraditional()], s, t, [c]) ‚Üí EmbeddingTE\n\nOptimize marginal embeddings for transfer entropy computation from source time series s to target time series t, conditioned on c if c is given, using the provided optimization scheme.\n\n\n\n\n\n","category":"function"},{"location":"information_api/#Transfer-entropy-definitions","page":"Information API","title":"Transfer entropy definitions","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"TransferEntropy","category":"page"},{"location":"information_api/#CausalityTools.TransferEntropy","page":"Information API","title":"CausalityTools.TransferEntropy","text":"The supertype of all transfer entropy measures.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Transfer-entropy-estimators","page":"Information API","title":"Transfer entropy estimators","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"TransferEntropyEstimator\nZhu1\nLindner","category":"page"},{"location":"information_api/#CausalityTools.TransferEntropyEstimator","page":"Information API","title":"CausalityTools.TransferEntropyEstimator","text":"The supertype of all dedicated transfer entropy estimators.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.Zhu1","page":"Information API","title":"CausalityTools.Zhu1","text":"Zhu1 <: DifferentialEntropyEstimator\nZhu1(k = 1, w = 0, base = MathConstants.e)\n\nThe Zhu1 transfer entropy estimator (Zhu et al., 2015)[Zhu2015].\n\nAssumes that the input data have been normalized as described in (Zhu et al., 2015).\n\nThis estimator approximates probabilities within hyperrectangles surrounding each point x·µ¢ ‚àà x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeann√®s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.Lindner","page":"Information API","title":"CausalityTools.Lindner","text":"Lindner <: TransferEntropyEstimator\nLindner(k = 1, w = 0, base = 2)\n\nThe Lindner transfer entropy estimator (Lindner et al., 2011)[Lindner2011], which is also used in the Trentool MATLAB toolbox, and is based on nearest neighbor searches.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nFor a given points in the joint embedding space j·µ¢, this estimator first computes the distance d·µ¢ from j·µ¢ to its k-th nearest neighbor. Then, for each point m‚Çñ[i] in the k-th marginal space, it counts the number of points within radius d·µ¢.\n\nThe transfer entropy is then computed as\n\nTE(X to Y) =\npsi(k) + dfrac1N sum_i^n\nleft\n    sum_k=1^3 left( psi(m_ki + 1) right)\nright\n\nwhere the index k references the three marginal subspaces T, TTf and ST for which neighbor searches are performed.\n\n[Lindner2011]: Lindner, M., Vicente, R., Priesemann, V., & Wibral, M. (2011). TRENTOOL:     A Matlab open source toolbox to analyse information flow in time series data with     transfer entropy. BMC neuroscience, 12(1), 1-22.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#Convenience","page":"Information API","title":"Convenience","text":"","category":"section"},{"location":"information_api/","page":"Information API","title":"Information API","text":"SymbolicTransferEntropy","category":"page"},{"location":"information_api/#CausalityTools.SymbolicTransferEntropy","page":"Information API","title":"CausalityTools.SymbolicTransferEntropy","text":"SymbolicTransferEntropy <: TransferEntropyEstimator\nSymbolicTransferEntropy(; m = 3, œÑ = 1, lt = ComplexityMeasures.isless_rand\n\nA convenience estimator for symbolic transfer entropy (Stanieck & Lenertz, 2008)[Stanieck2008].\n\nDescription\n\nSymbolic transfer entropy consists of two simple steps. First, the input time series are embedded with embedding lag m and delay œÑ. The ordinal patterns of the embedding vectors are then encoded using SymbolicPermutation with marginal_encodings. This transforms the input time series into integer time series using OrdinalPatternEncoding.\n\nTransfer entropy is then estimated as usual on the encoded timeseries with transferentropy and the CountOccurrences naive frequency estimator.\n\n[Stanieck2008]: Staniek, M., & Lehnertz, K. (2008). Symbolic transfer entropy. Physical review letters, 100(15), 158101.\n\n\n\n\n\n","category":"type"},{"location":"information_api/","page":"Information API","title":"Information API","text":"Hilbert\nPhase\nAmplitude","category":"page"},{"location":"information_api/#CausalityTools.Hilbert","page":"Information API","title":"CausalityTools.Hilbert","text":"Hilbert(est;\n    source::InstantaneousSignalProperty = Phase(),\n    target::InstantaneousSignalProperty = Phase(),\n    cond::InstantaneousSignalProperty = Phase())\n) <: TransferDifferentialEntropyEstimator\n\nCompute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are obtained by first applying the Hilbert transform to each signal, then extracting the phases/amplitudes of the resulting complex numbers[Palus2014]. Original time series are thus transformed to instantaneous phase/amplitude time series. Transfer entropy is then estimated using the provided est on those phases/amplitudes (use e.g. VisitationFrequency, or SymbolicPermutation).\n\ninfo: Info\nDetails on estimation of the transfer entropy (conditional mutual information) following the phase/amplitude extraction step is not given in Palus (2014). Here, after instantaneous phases/amplitudes have been obtained, these are treated as regular time series, from which transfer entropy is then computed as usual.\n\nSee also: Phase, Amplitude.\n\n[Palus2014]: Palu≈°, M. (2014). Cross-scale interactions and information transfer. Entropy, 16(10), 5263-5289.\n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.Phase","page":"Information API","title":"CausalityTools.Phase","text":"Phase <: InstantaneousSignalProperty\n\nIndicates that the instantaneous phases of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"information_api/#CausalityTools.Amplitude","page":"Information API","title":"CausalityTools.Amplitude","text":"Amplitude <: InstantaneousSignalProperty\n\nIndicates that the instantaneous amplitudes of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"examples/examples_conditional_entropy/#examples_condentropy","page":"Conditional entropy","title":"Entropy","text":"","category":"section"},{"location":"examples/examples_conditional_entropy/#Discrete:-example-from-Cover-and-Thomas","page":"Conditional entropy","title":"Discrete: example from Cover & Thomas","text":"","category":"section"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"This is essentially example 2.2.1 in Cover & Thomas (2006), where they use the following contingency table as an example. We'll take their example and manually construct a ContingencyMatrix that we can use to compute the conditional entropy. The ContingencyMatrix constructor takes the probabilities as the first argument and the raw frequencies as the second argument. Note also that Julia is column-major, so we need to transpose their example. Then their X is in the first dimension of our contingency matrix (along columns) and their Y is our second dimension (rows).","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"using CausalityTools\nfreqs_yx = [1//8 1//16 1//32 1//32; \n    1//16 1//8  1//32 1//32;\n    1//16 1//16 1//16 1//16; \n    1//4  0//1  0//1  0//1];\nfreqs_xy = transpose(freqs_yx);\nprobs_xy = freqs_xy ./ sum(freqs_xy)\nc_xy = ContingencyMatrix(probs_xy, freqs_xy)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"The marginal distribution for x (first dimension) is","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probabilities(c_xy, 1)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"The marginal distribution for y (second dimension) is","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probabilities(c_xy, 2)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"And the Shannon conditional entropy H^S(X  Y)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"ce_x_given_y = entropy_conditional(CEShannon(), c_xy) |> Rational","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"This is the same as in their example. Hooray! To compute H^S(Y  X), we just need to flip the contingency matrix.","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probs_yx = freqs_yx ./ sum(freqs_yx);\nc_yx = ContingencyMatrix(probs_yx, freqs_yx);\nce_y_given_x = entropy_conditional(CEShannon(), c_yx) |> Rational","category":"page"},{"location":"examples/examples_independence/#examples_independence","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"examples/examples_independence/#[LocalPermutationTest](@ref)","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we'll create a three-variable scenario where X and Z are connected through Y, so that I(X Z  Y) = 0 and I(X Y  Z)  0. We'll test for conditional independence using Shannon conditional mutual information (CMIShannon). To estimate CMI, we'll use the Kraskov differential entropy estimator, which naively computes CMI as a sum of entropy terms without guaranteed bias cancellation.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\n\nX = randn(1000)\nY = X .+ randn(1000) .* 0.4\nZ = randn(1000) .+ Y\nx, y, z = Dataset.((X, Y, Z))\ntest = LocalPermutationTest(CMIShannon(base = 2), Kraskov(k = 10), nsurr = 30)\ntest_result = independence(test, x, y, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We expect there to be a detectable influence from X to Y, if we condition on Z or not, because Z doesn't influence neither X nor Y. The null hypothesis is that the first two variables are conditionally independent given the third, which we reject with a very low p-value. Hence, we accept the alternative hypothesis that the first two variables X and Y. are conditionally dependent given Z.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test_result = independence(test, x, z, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we cannot reject the null hypothesis that X and Z are conditionally independent given Y, because Y is the variable that transmits information from X to Z.","category":"page"},{"location":"examples/examples_independence/#[SurrogateTest](@ref)","page":"Independence testing","title":"SurrogateTest","text":"","category":"section"},{"location":"examples/examples_independence/#examples_surrogatetest_teshannon","page":"Independence testing","title":"TEShannon","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"To demonstrate the SurrogateTest test, we use the transfer entropy measure, which accepts either two input timeseries, or three timeseries when computing the partial/conditional transfer entropy.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nsys = logistic2_unidir(c_xy = 0.5) # x affects y, but not the other way around.\nx, y = columns(trajectory(sys, 1000, Ttr = 10000))\n\ntest = SurrogateTest(TEShannon(), KSG1(k = 4))\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we can reject the null hypothesis that the future of y is independent of  x, because x does actually influence y. This doesn't change if we compute  partial transfer entropy with respect to some random extra time series, because it doesn't influence either variables.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, x, y, rand(length(x)))","category":"page"},{"location":"examples/examples_independence/#examples_surrogatetest_smeasure","page":"Independence testing","title":"SMeasure","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = SMeasure(dx = 3, dy = 3)\ns = s_measure(measure, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The s statistic is larger when there is stronger coupling and smaller when there is weaker coupling. To check whether s is significant (i.e. large enough to claim directional dependence), we can use a SurrogateTest, like here.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test = SurrogateTest(measure)\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The p-value is high, and we can't reject the null at any reasonable significance level. Hence, there isn't evidence in the data to support directional coupling from x to y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"What happens if we use coupled variables?","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"z = x .+ 0.1y\nindependence(test, x, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Now we can confidently reject the null (independence), and conclude that there is evidence in the data to support directional dependence from x to z.","category":"page"},{"location":"experimental/#[Experimental](@ref-experimental_methods)","page":"Experimental","title":"Experimental","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"Here we list implemented methods that do not yet appear in peer-reviewed journals, but are found, for example, in pre-print servers like arXiv.","category":"page"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"The API for these methods, and their return values, may change at any time until they appear as part of the public API. Use them wisely.","category":"page"},{"location":"experimental/#Predictive-asymmetry","page":"Experimental","title":"Predictive asymmetry","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"predictive_asymmetry","category":"page"},{"location":"experimental/#CausalityTools.predictive_asymmetry","page":"Experimental","title":"CausalityTools.predictive_asymmetry","text":"predictive_asymmetry(estimator::TransferEntropyEstimator, Œ∑s; s, t, [c],\n    dTf = 1, dT = 1, dS = 1, œÑT = -1, œÑS = -1, [dC = 1, œÑC = -1],\n    normalize::Bool = false, f::Real = 1.0, base = 2) ‚Üí Vector{Float64}\n\nCompute the predictive asymmetry[Haaga2020] ùî∏(s ‚Üí t) for source time series s and target time series t over prediction lags Œ∑s, using the given estimator and embedding parameters dTf, dT, dS, œÑT, œÑS (see also EmbeddingTE)\n\nIf a conditional time series c is provided, compute ùî∏(s ‚Üí t | c). Then, dC and œÑC controls the embedding dimension and embedding lag for the conditional variable.\n\nReturns\n\nReturns a vector containing the predictive asymmetry for each value of Œ∑s.\n\nNormalization (hypothesis test)\n\nIf normalize == true (the default), then compute the normalized predictive asymmetry ùíú. In this case, for each eta in Œ∑s, compute ùíú(Œ∑) by normalizing ùî∏(Œ∑) to some fraction f of the mean transfer entropy over prediction lags -eta  eta (exluding lag 0). Haaga et al. (2020)[Haaga2020] uses a normalization with f=1.0 as a built-in hypothesis test, avoiding more computationally costly surrogate testing.\n\nEstimators\n\nAny estimator that works for transferentropy will also work with predictive_asymmetry. Check the online documentation for compatiable estimators.\n\nExamples\n\nusing CausalityTools\n# Some example time series\nx, y = rand(100), rand(100)\n# ùî∏(x ‚Üí y) over prediction lags 1:5\nùî∏reg  = predictive_asymmetry(x, y, VisitationFrequency(RectangularBinning(5)), 1:5)\n\ninfo: Experimental!\nThis is a method that does not yet appear in a peer-reviewed scientific journal. Feel free to use, but consider it experimental for now. It will reappear in a 2.X release in new form once published in a peer-reviewed journal.\n\n[Haaga2020]: Haaga, Kristian Agas√∏ster, David Diego, Jo Brendryen, and Bjarte Hannisdal. \"A simple test for causality in complex systems.\" arXiv preprint arXiv:2005.01860 (2020).\n\n\n\n\n\n","category":"function"},{"location":"experimental/#Automated-embedding-for-transfer-entropy","page":"Experimental","title":"Automated embedding for transfer entropy","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"bbnue","category":"page"},{"location":"measures/#Correlation-measures","page":"Association measures","title":"Correlation measures","text":"","category":"section"},{"location":"measures/#Pearson-correlation","page":"Association measures","title":"Pearson correlation","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"PearsonCorrelation\npearson_correlation","category":"page"},{"location":"measures/#CausalityTools.PearsonCorrelation","page":"Association measures","title":"CausalityTools.PearsonCorrelation","text":"PearsonCorrelation\n\nThe Pearson correlation of two variables.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with pearson_correlation to compute the raw correlation coefficient.\n\nDescription\n\nThe sample Pearson correlation coefficient for real-valued random variables X and Y with associated samples x_i_i=1^N and y_i_i=1^N is defined as\n\nrho_xy = dfracsum_i=1^n (x_i - barx)(y_i - bary) sqrtsum_i=1^N (x_i - barx)^2sqrtsum_i=1^N (y_i - bary)^2\n\nwhere barx and bary are the means of the observations x_k and y_k, respectively.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.pearson_correlation","page":"Association measures","title":"CausalityTools.pearson_correlation","text":"pearson_correlation(x::VectorOrDataset, y::VectorOrDataset)\n\nCompute the PearsonCorrelation between x and y, which must each be 1-dimensional.\n\n\n\n\n\n","category":"function"},{"location":"measures/#Partial-correlation","page":"Association measures","title":"Partial correlation","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"PartialCorrelation\npartial_correlation","category":"page"},{"location":"measures/#CausalityTools.PartialCorrelation","page":"Association measures","title":"CausalityTools.PartialCorrelation","text":"PartialCorrelation\n\nThe correlation of two variables, with the effect of a set of conditioning variables removed.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for   conditional dependence.\nUse with partial_correlation to compute the raw correlation coefficient.\n\nDescription\n\nThere are several ways of estimating the partial correlation. We follow the matrix inversion method, because for Datasets, we can very efficiently compute the required joint covariance matrix Sigma for the random variables.\n\nFormally, let X_1 X_2 ldots X_n be a set of n real-valued random variables. Consider the joint precision matrix,P = (p_ij) = Sigma^-1. The partial correlation of any pair of variables (X_i X_j), given the remaining variables bfZ = X_k_i=1 i neq i j^n, is defined as\n\nrho_X_i X_j  bfZ = -dfracp_ijsqrt p_ii p_jj \n\nIn practice, we compute the estimate\n\nhatrho_X_i X_j  bfZ =\n-dfrachatp_ijsqrt hatp_ii hatp_jj \n\nwhere hatP = hatSigma^-1 is the sample precision matrix.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.partial_correlation","page":"Association measures","title":"CausalityTools.partial_correlation","text":"partial_correlation(x::VectorOrDataset, y::VectorOrDataset,\n    z::VectorOrDataset...)\n\nCompute the PartialCorrelation between x and y, given z.\n\n\n\n\n\n","category":"function"},{"location":"measures/#Distance-correlation","page":"Association measures","title":"Distance correlation","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"DistanceCorrelation\ndistance_correlation","category":"page"},{"location":"measures/#CausalityTools.DistanceCorrelation","page":"Association measures","title":"CausalityTools.DistanceCorrelation","text":"DistanceCorrelation\n\nThe distance correlation (Sz√©kely et al., 2007)[Sz√©kely2007] measure quantifies potentially nonlinear associations between pairs of variables.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for   pairwise dependence.\nUse with distance_correlation to compute the raw distance correlation   coefficient.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.distance_correlation","page":"Association measures","title":"CausalityTools.distance_correlation","text":"distance_correlation(x, y) ‚Üí dcor ‚àà [0, 1]\n\nCompute the empirical/sample distance correlation (Sz√©kely et al., 2007)[Sz√©kely2007], here called dcor, between datasets x and y.\n\n[Sz√©kely2007]: Sz√©kely, G. J., Rizzo, M. L., & Bakirov, N. K. (2007). Measuring and testing dependence by correlation of distances. The annals of statistics, 35(6), 2769-2794.\n\n\n\n\n\n","category":"function"},{"location":"measures/#Closeness-measures","page":"Association measures","title":"Closeness measures","text":"","category":"section"},{"location":"measures/#Joint-distance-distribution","page":"Association measures","title":"Joint distance distribution","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"JointDistanceDistribution\njdd","category":"page"},{"location":"measures/#CausalityTools.JointDistanceDistribution","page":"Association measures","title":"CausalityTools.JointDistanceDistribution","text":"JointDistanceDistribution <: AssociationMeasure end\nJointDistanceDistribution(; metric = Euclidean(), B = 10, D = 2, œÑ = 1, Œº = 0.0)\n\nThe joint distance distribution (JDD) measure (Amig√≥ & Hirata, 2018)[Amigo2018].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for directional dependence.\nUse with jdd to compute the joint distance distribution Œî from Amig√≥ & Hirata (2018).\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.   Defaults to Euclidean().\nB::Int: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances.\nD::Int: Embedding dimension.\nœÑ::Int: Embedding delay.\nŒº: The hypothetical mean value of the joint distance distribution if there   is no coupling between x and y (default is Œº = 0.0).\n\nExamples\n\nComputing the JJD\nIndependence testing using JJD\n\n[Amigo2018]: Amig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.jdd","page":"Association measures","title":"CausalityTools.jdd","text":"jdd(measure::JointDistanceDistribution, source, target) ‚Üí Œî\n\nCompute the joint distance distribution (Amig√≥ & Hirata, 2018[Amigo2018]) from source to target using the given JointDistanceDistribution measure.\n\nReturns the distribution Œî from the paper directly (example). Use JointDistanceDistributionTest to perform a formal indepencence test.\n\n[Amigo2018]: Amig√≥, Jos√© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"function"},{"location":"measures/#S-measure","page":"Association measures","title":"S-measure","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"SMeasure\ns_measure","category":"page"},{"location":"measures/#CausalityTools.SMeasure","page":"Association measures","title":"CausalityTools.SMeasure","text":"SMeasure < AssociationMeasure\nSMeasure(K::Int = 2, metric = SqEuclidean(), tree_metric = Euclidean(),\n    dx::Int = 2, my::Int = 2, œÑx::Int = 1, œÑy::Int = 1))\n\nSMeasure is a bivariate association measure from Grassberger et al. (1999)[Grassberger1999] and Quiroga et al. (2000) [Quiroga2000] that measure directional dependence  between two input (potentially multivariate) time series.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for directional dependence.\nUse with s_measure to compute the raw s-measure statistic.\n\nDescription\n\nThe steps of the algorithm are:\n\nLet r_ij and s_ij be the indices of the K-th nearest neighbors   of x_i and y_i, respectively.\nCompute the the mean squared Euclidean distance to the K nearest neighbors   for each x_i, using the indices r_i j.\n\nR_i^(k)(x) = dfrac1k sum_i=1^k(x_i x_r_ij)^2\n\nCompute the y-conditioned mean squared Euclidean distance to the K nearest    neighbors for each x_i, now using the indices s_ij.\n\nR_i^(k)(xy) = dfrac1k sum_i=1^k(x_i x_s_ij)^2\n\nDefine the following measure of independence, where 0 leq S leq 1, and    low values indicate independence and values close to one occur for    synchronized signals.\n\nS^(k)(xy) = dfrac1N sum_i=1^N dfracR_i^(k)(x)R_i^(k)(xy)\n\nInput data\n\nThe algorithm is slightly modified from [Grassberger1999] to allow univariate timeseries as input.\n\nIf x and y are Datasets then use x and y as is and ignore the parameters   dx/œÑx and dy/œÑy.\nIf x and y are scalar time series, then create dx and dy dimensional embeddings,   respectively, of both x and y, resulting in N different m-dimensional embedding points   X = x_1 x_2 ldots x_N  and Y = y_1 y_2 ldots y_N .   œÑx and œÑy control the embedding lags for x and y. \nIf x is a scalar-valued vector and y is a Dataset, or vice versa,    then create an embedding of the scalar timeseries using parameters dx/œÑx or dy/œÑy.\n\nIn all three cases, input datasets are length-matched by eliminating points at the end of  the longest dataset (after the embedding step, if relevant) before analysis.\n\n[Quiroga2000]: Quian Quiroga, R., Arnhold, J. & Grassberger, P. [2000] ‚ÄúLearning driver-response relationships from synchronization patterns,‚Äù Phys. Rev. E61(5), 5142‚Äì5148.\n\n[Grassberger1999]: Arnhold, J., Grassberger, P., Lehnertz, K., & Elger, C. E. (1999). A robust method for detecting interdependences: application to intracranially recorded EEG. Physica D: Nonlinear Phenomena, 134(4), 419-430.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.s_measure","page":"Association measures","title":"CausalityTools.s_measure","text":"s_measure(measure::SMeasure, x::VectorOrDataset, y::VectorOrDataset) ‚Üí s ‚àà [0, 1]\n\nCompute the given measure to quantify the directional dependence between  univariate/multivariate time series x and y. \n\nReturns a scalar s where s = 0 indicates independence between x and y,  and higher values indicate synchronization between x and y, with complete  synchronization for s = 1.0.\n\nExample\n\nusing CausalityTools\n\n# A two-dimensional Ulam lattice map\nsys = ulam(2)\n\n# Sample 1000 points after discarding 5000 transients\norbit = trajectory(sys, 1000, Ttr = 5000)\nx, y = orbit[:, 1], orbit[:, 2]\n\n# 4-dimensional embedding for `x`, 5-dimensional embedding for `y`\nm = SMeasure(dx = 4, œÑx = 3, dy = 5, œÑy = 1)\ns_measure(m, x, y)\n\n\n\n\n\n","category":"function"},{"location":"measures/#Cross-map-measures","page":"Association measures","title":"Cross-map measures","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"See also the cross mapping API for estimators.","category":"page"},{"location":"measures/#Convergent-cross-mapping","page":"Association measures","title":"Convergent cross mapping","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"ConvergentCrossMapping","category":"page"},{"location":"measures/#CausalityTools.ConvergentCrossMapping","page":"Association measures","title":"CausalityTools.ConvergentCrossMapping","text":"ConvergentCrossMapping <: CrossmapMeasure\nConvergentCrossMapping(; d::Int = 2, œÑ::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe convergent cross mapping (CCM) measure  (Sugihara et al., 2012)[Sugihara2012]).\n\nSpecifies embedding dimension d, embedding lag œÑ to be used, as described below, with predict or crossmap. The Theiler window w controls how many temporal neighbors are excluded during neighbor searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nLet S(i) be the source time series variable and T(i) be the target time series variable. This version produces regular embeddings with fixed dimension d and embedding lag œÑ as follows:\n\n( S(i) S(i+tau) S(i+2tau) ldots S(i+(d-1)tau T(i))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D-1 variables, while the last (D-th) variable is to be predicted.\n\nWith this convention, œÑ < 0 implies \"past/present values of source used to predict target\", and œÑ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if œÑ > 0 (embed_warn = false turns off warnings).\n\n[Sugihara2012]: Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Pairwise-asymmetric-inference","page":"Association measures","title":"Pairwise asymmetric inference","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"PairwiseAsymmetricInference","category":"page"},{"location":"measures/#CausalityTools.PairwiseAsymmetricInference","page":"Association measures","title":"CausalityTools.PairwiseAsymmetricInference","text":"PairwiseAsymmetricInference <: CrossmapMeasure\nPairwiseAsymmetricInference(; d::Int = 2, œÑ::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe pairwise asymmetric inference (PAI) cross mapping  measure (McCracken & Weigel (2014)[McCracken2014]) is a version of  ConvergentCrossMapping that searches for neighbors in mixed embeddings (i.e. both source and target variables included); otherwise, the algorithms are identical.\n\nSpecifies embedding dimension d, embedding lag œÑ to be used, as described below, with predict or crossmap. The Theiler window w controls how many temporal neighbors are excluded during neighbor searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nThere are many possible ways of defining the embedding for PAI. Currently, we only implement the \"add one non-lagged source timeseries to an embedding of the target\" approach, which is used as an example in McCracken & Weigel's paper. Specifically: Let S(i) be the source time series variable and T(i) be the target time series variable. PairwiseAsymmetricInference produces regular embeddings with fixed dimension d and embedding lag œÑ as follows:\n\n(S(i) T(i+(d-1)tau ldots T(i+2tau) T(i+tau) T(i)))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D variables, while the last variable is to be predicted.\n\nWith this convention, œÑ < 0 implies \"past/present values of source used to predict target\", and œÑ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if œÑ > 0 (embed_warn = false turns off warnings).\n\n[McCracken2014]: McCracken, J. M., & Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E, 90(6), 062903.\n\n\n\n\n\n","category":"type"},{"location":"measures/#information_measures","page":"Association measures","title":"Information measures","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"Association measures that are information-based are listed here. Available estimators are listed in the information API.","category":"page"},{"location":"measures/#Mutual-information-(Shannon)","page":"Association measures","title":"Mutual information (Shannon)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MIShannon","category":"page"},{"location":"measures/#CausalityTools.MIShannon","page":"Association measures","title":"CausalityTools.MIShannon","text":"MIShannon <: MutualInformation\nMIShannon(; base = 2)\n\nThe Shannon mutual information I^S(X Y).\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information. \n\nDiscrete definition\n\nThere are many equivalent formulations of discrete Shannon mutual information. In this package, we currently use the double-sum and the three-entropies formulations.\n\nDouble sum formulation\n\nAssume we observe samples barbfX_1N_y = barbfX_1 ldots barbfX_n  and barbfY_1N_x = barbfY_1 ldots barbfY_n  from two discrete random variables X and Y with finite supports mathcalX =  x_1 x_2 ldots x_M_x  and mathcalY = y_1 y_2 ldots x_M_y. The double-sum estimate is obtained by replacing the double sum\n\nhatI_DS(X Y) =\n sum_x_i in mathcalX y_i in mathcalY p(x_i y_j) log left( dfracp(x_i y_i)p(x_i)p(y_j) right)\n\nwhere  hatp(x_i) = fracn(x_i)N_x, hatp(y_i) = fracn(y_j)N_y, and hatp(x_i x_j) = fracn(x_i)N, and N = N_x N_y. This definition is used by mutualinfo when called with a ContingencyMatrix.\n\nThree-entropies formulation\n\nAn equivalent formulation of discrete Shannon mutual information is\n\nI^S(X Y) = H^S(X) + H_q^S(Y) - H^S(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint discrete Shannon entropies. This definition is used by mutualinfo when called with a ProbabilitiesEstimator.\n\nDifferential mutual information\n\nOne possible formulation of differential Shannon mutual information is\n\nI^S(X Y) = h^S(X) + h_q^S(Y) - h^S(X Y)\n\nwhere h^S(cdot) and h^S(cdot cdot) are the marginal and joint differential Shannon entropies. This definition is used by mutualinfo when called with a DifferentialEntropyEstimator.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(Tsallis,-Furuichi)","page":"Association measures","title":"Mutual information (Tsallis, Furuichi)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MITsallisFuruichi","category":"page"},{"location":"measures/#CausalityTools.MITsallisFuruichi","page":"Association measures","title":"CausalityTools.MITsallisFuruichi","text":"MITsallisFuruichi <: MutualInformation\nMITsallisFuruichi(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Furuichi (2006)[Furuichi2006], which in that paper is called the mutual entropy.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information. \n\nDescription\n\nFuruichi's Tsallis mutual entropy between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_q^T(X Y) = H_q^T(X) - H_q^T(X  Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X Y)\n\nwhere H^T(cdot) and H^T(cdot cdot) are the marginal and joint Tsallis entropies, and q is the Tsallis-parameter. ```\n\n[Furuichi2006]: Furuichi, S. (2006). Information theoretical properties of Tsallis entropies. Journal of Mathematical Physics, 47(2), 023302.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(Tsallis,-Martin)","page":"Association measures","title":"Mutual information (Tsallis, Martin)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MITsallisMartin","category":"page"},{"location":"measures/#CausalityTools.MITsallisMartin","page":"Association measures","title":"CausalityTools.MITsallisMartin","text":"MITsallisMartin <: MutualInformation\nMITsallisMartin(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Martin et al. (2005)[Martin2004].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information. \n\nDescription\n\nMartin et al.'s Tsallis mutual information between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_textMartin^T(X Y q) = H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint Shannon entropies, and q is the Tsallis-parameter.\n\n[Martin2004]: Martin, S., Morison, G., Nailon, W., & Durrani, T. (2004). Fast and accurate image registration using Tsallis entropy and simultaneous perturbation stochastic approximation. Electronics Letters, 40(10), 1.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(R√©nyi,-Sarbu)","page":"Association measures","title":"Mutual information (R√©nyi, Sarbu)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MIRenyiSarbu","category":"page"},{"location":"measures/#CausalityTools.MIRenyiSarbu","page":"Association measures","title":"CausalityTools.MIRenyiSarbu","text":"MIRenyiSarbu <: MutualInformation\nMIRenyiSarbu(; base = 2, q = 1.5)\n\nThe discrete R√©nyi mutual information from Sarbu (2014)[Sarbu2014].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information. \n\nDescription\n\nSarbu (2014) defines discrete R√©nyi mutual information as the R√©nyi alpha-divergence between the conditional joint probability mass function p(x y) and the product of the conditional marginals, p(x) cdot p(y):\n\nI(X Y)^R_q =\ndfrac1q-1\nlog left(\n    sum_x in X y in Y\n    dfracp(x y)^qleft( p(x)cdot p(y) right)^q-1\nright)\n\n[Sarbu2014]: Sarbu, S. (2014, May). R√©nyi information transfer: Partial R√©nyi transfer entropy and partial R√©nyi mutual information. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5666-5670). IEEE.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(R√©nyi,-Jizba)","page":"Association measures","title":"Mutual information (R√©nyi, Jizba)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MIRenyiJizba","category":"page"},{"location":"measures/#CausalityTools.MIRenyiJizba","page":"Association measures","title":"CausalityTools.MIRenyiJizba","text":"MIRenyiJizba <: MutualInformation\n\nThe R√©nyi mutual information I_q^R_J(X Y) defined in Jizba et al. (2012)[Jizba2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information. \n\nDefinition\n\nI_q^R_J(X Y) = S_q^R(X) + S_q^R(Y) - S_q^R(X Y)\n\nwhere S_q^R(cdot) and S_q^R(cdot cdot) the R√©nyi entropy and the joint R√©nyi entropy.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). R√©nyi's information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Conditional-mutual-information-(Shannon)","page":"Association measures","title":"Conditional mutual information (Shannon)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"CMIShannon","category":"page"},{"location":"measures/#CausalityTools.CMIShannon","page":"Association measures","title":"CausalityTools.CMIShannon","text":"CMIShannon <: ConditionalMutualInformation\nCMIShannon(; base = 2)\n\nThe Shannon conditional mutual information (CMI) I^S(X Y  Z).\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with condmutualinfo to compute the raw conditional mutual information. \n\nSupported definitions\n\nConsider random variables X in mathbbR^d_X and Y in mathbbR^d_Y, given Z in mathbbR^d_Z. The Shannon conditional mutual information is defined as\n\nbeginalign*\nI(X Y  Z)\n= H^S(X Z) + H^S(Y z) - H^S(X Y Z) - H^S(Z) \n= I^S(X Y Z) + I^S(X Y)\nendalign*\n\nwhere I^S(cdot cdot) is the Shannon mutual information MIShannon, and H^S(cdot) is the Shannon entropy.\n\nDifferential Shannon CMI is obtained by replacing the entropies by differential entropies.\n\nSee also: condmutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Conditional-mutual-information-(R√©nyi,-Jizba)","page":"Association measures","title":"Conditional mutual information (R√©nyi, Jizba)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"CMIRenyiJizba","category":"page"},{"location":"measures/#CausalityTools.CMIRenyiJizba","page":"Association measures","title":"CausalityTools.CMIRenyiJizba","text":"CMIRenyiJizba <: ConditionalMutualInformation\n\nThe R√©nyi conditional mutual information I_q^R_J(X Y  Z defined in Jizba et al. (2012)[Jizba2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with condmutualinfo to compute the raw conditional mutual information. \n\nDefinition\n\nI_q^R_J(X Y  Z) = I_q^R_J(X Y Z) - I_q^R_J(X Z)\n\nwhere I_q^R_J(X Z) is the MIRenyiJizba mutual information.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). R√©nyi‚Äôs information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Conditional-mutual-information-(R√©nyi,-Poczos)","page":"Association measures","title":"Conditional mutual information (R√©nyi, Poczos)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"CMIRenyiPoczos","category":"page"},{"location":"measures/#CausalityTools.CMIRenyiPoczos","page":"Association measures","title":"CausalityTools.CMIRenyiPoczos","text":"CMIRenyiPoczos <: ConditionalMutualInformation\n\nThe differential R√©nyi conditional mutual information I_q^R_P(X Y  Z) defined in (P√≥czos & Schneider, 2012)[P√≥czos2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with condmutualinfo to compute the raw conditional mutual information. \n\nDefinition\n\nbeginalign*\nI_q^R_P(X Y  Z) = dfrac1q-1\nint int int dfracp_Z(z) p_X Y  Z^q( p_XZ(xz) p_YZ(yz) )^q-1 \nmathbbE_X Y Z sim p_X Y Z\nleft dfracp_X Z^1-q(X Z) p_Y Z^1-q(Y Z) p_X Y Z^1-q(X Y Z) p_Z^1-q(Z) right\nendalign*\n\n[P√≥czos2012]: P√≥czos, B., & Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Transfer-entropy-(Shannon)","page":"Association measures","title":"Transfer entropy (Shannon)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"TEShannon","category":"page"},{"location":"measures/#CausalityTools.TEShannon","page":"Association measures","title":"CausalityTools.TEShannon","text":"TEShannon <: TransferEntropy\nTEShannon(; base = 2; embedding = EmbeddingTE()) <: TransferEntropy\n\nThe Shannon-type transfer entropy measure.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise   and conditional dependence.\nUse with transferentropy to compute the raw transfer entropy.\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I^S(T^+ S^-  T^-) \nTE(S to T  C) = I^S(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I(T^+ S^-  T^-) is the Shannon conditional mutual information (CMIShannon). The variables T^+, T^-, S^- and C^- are described in the docstring for transferentropy.\n\nCompatible estimators\n\nProbabilitiesEstimator: Any probabilities estimator that accepts   multivariate input data or has an implementation for marginal_encodings.   Transfer entropy is computed a sum of marginal (discrete) entropy estimates.   Example: ValueHistogram.\nDifferentialEntropyEstimator. Any differential entropy   estimator that accepts multivariate input data.   Transfer entropy is computed a sum of marginal differential entropy estimates.   Example: Kraskov.\nMutualInformationEstimator. Any mutual information estimator.   Formulates the transfer entropy as a sum of mutual information terms, which are   estimated separately using mutualinfo. Example: KraskovSt√∂gbauerGrassberger2.\nConditionalMutualInformationEstimator. Dedicated CMI estimators.   Example: FPVP.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Transfer-entropy-(R√©nyi,-Jizba)","page":"Association measures","title":"Transfer entropy (R√©nyi, Jizba)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"TERenyiJizba","category":"page"},{"location":"measures/#CausalityTools.TERenyiJizba","page":"Association measures","title":"CausalityTools.TERenyiJizba","text":"TERenyiJizba() <: TransferEntropy\n\nThe R√©nyi transfer entropy from Jizba et al. (2012)[Jizba2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise   and conditional dependence.\nUse with transferentropy to compute the raw transfer entropy.\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I_q^R_J(T^+ S^-  T^-) \nTE(S to T  C) = I_q^R_J(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I_q^R_J(T^+ S^-  T^-) is Jizba et al. (2012)'s definition of conditional mutual information (CMIRenyiJizba). The variables T^+, T^-, S^- and C^- are described in the docstring for transferentropy.\n\nCompatible estimators\n\nProbabilitiesEstimator: Any probabilities estimator that accepts   multivariate input data or has an implementation for marginal_encodings.   Transfer entropy is computed a sum of marginal (discrete) entropy estimates.   Example: ValueHistogram.\nDifferentialEntropyEstimator. Any differential entropy   estimator that accepts multivariate input data.   Transfer entropy is computed a sum of marginal differential entropy estimates.   Example: Kraskov.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). R√©nyi‚Äôs information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"examples/examples_mutualinfo/#examples_mutualinfo","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(discrete):-synthetic-system","page":"Mutual information","title":"MIShannon (discrete): synthetic system","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In this example we generate realizations of two different systems where we know the strength of coupling between the variables. Our aim is to compute Shannon mutual information I^S(X Y) (MIShannon) between time series of each variable and assess how the magnitude of I^S(X Y) changes as we change the strength of coupling between X and Y.","category":"page"},{"location":"examples/examples_mutualinfo/#Defining-the-systems","page":"Mutual information","title":"Defining the systems","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here we implement two of the example systems that come with the CausalityTools.jl:","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"A stochastic system consisting of two unidirectionally coupled first-order autoregressive processes (ar1_unidir)\nA deterministic, chaotic system consisting of two unidirectionally coupled logistic maps (logistic2_unidir)","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"We use the default input parameter values (see ar1_unidir and logistic2_unidir for details) and below we toggle only the random initial conditions and the coupling strength parameter c_xy. For each value of c_xy we generate 1,000 unique realizations of the system and obtain 500-point time series of the coupled variables.","category":"page"},{"location":"examples/examples_mutualinfo/#Estimating-mutual-information","page":"Mutual information","title":"Estimating mutual information","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here we use the binning-based ValueHistogram estimator. We summarize the distribution of I(X Y) values across all realizations using the median and quantiles encompassing 95 % of the values.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Statistics\nusing CairoMakie\n\n# Span a range of x-y coupling strengths\nc = 0.0:0.1:1.0\n\n# Number of observations in each time series\nnpts = 500\n\n# Number of unique realizations of each system\nn_realizations = 1000\n\n# Get MI for multiple realizations of two systems, \n# saving three quantiles for each c value\nmi = zeros(length(c), 3, 2)\n\n# Define an estimator for MI\nb = RectangularBinning(4)\nestimator = VisitationFrequency(b)\n\nfor i in 1 : length(c)\n    \n    tmp = zeros(n_realizations, 2)\n    \n    for k in 1 : n_realizations\n        \n        # Obtain time series realizations of the two 2D systems \n        # for a given coupling strength and random initial conditions\n        lmap = trajectory(logistic2_unidir(u‚ÇÄ = rand(2), c_xy = c[i]), npts - 1, Ttr = 1000)\n        ar1 = trajectory(ar1_unidir(u‚ÇÄ = rand(2), c_xy = c[i]), npts - 1)\n        \n        # Compute the MI between the two coupled components of each system\n        tmp[k, 1] = mutualinfo(MIShannon(), estimator, lmap[:, 1], lmap[:, 2])\n        tmp[k, 2] = mutualinfo(MIShannon(), estimator, ar1[:, 1], ar1[:, 2])\n    end\n    \n    # Compute lower, middle, and upper quantiles of MI for each coupling strength\n    mi[i, :, 1] = quantile(tmp[:, 1], [0.025, 0.5, 0.975])\n    mi[i, :, 2] = quantile(tmp[:, 2], [0.025, 0.5, 0.975])\nend\n\n# Plot distribution of MI values as a function of coupling strength for both systems\nfig =with_theme(theme_minimal()) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"Coupling strength\", ylabel = \"Mutual information\")\n    band!(ax, c, mi[:, 1, 1], mi[:, 3, 1], color = (:black, 0.3))\n    lines!(ax, c, mi[:, 2, 1], label = \"2D chaotic logistic maps\", color = :black)\n    band!(ax, c, mi[:, 1, 2], mi[:, 3, 2], color = (:red, 0.3))\n    lines!(ax, c, mi[:, 2, 2],  label = \"2D order-1 autoregressive\", color = :red)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"As expected, I(X Y) increases with coupling strength in a system-specific manner.","category":"page"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(differential):-Reproducing-Kraskov-et-al.-(2004)","page":"Mutual information","title":"MIShannon (differential): Reproducing Kraskov et al. (2004)","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here, we'll reproduce Figure 4 from Kraskov et al. (2004)'s seminal paper on the nearest-neighbor based mutual information estimator. We'll estimate the mutual information between marginals of a bivariate Gaussian for a fixed time series length of 2000, varying the number of neighbors. Note: in the original paper, they show multiple curves corresponding to different time series length. We only show two single curves: one for the KSG1 estimator and one for the KSG2 estimator.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing LinearAlgebra: det\nusing Distributions: MvNormal\nusing StateSpaceSets: Dataset\nusing CairoMakie\nusing Statistics\n\nN = 2000\nc = 0.9\nŒ£ = [1 c; c 1]\nN2 = MvNormal([0, 0], Œ£)\nmitrue = -0.5*log(det(Œ£)) # in nats\nks = [2; 5; 7; 10:10:70] .* 2\n\nnreps = 30\nmis_ksg1 = zeros(nreps, length(ks))\nmis_ksg2 = zeros(nreps, length(ks))\nfor i = 1:nreps\n    D2 = Dataset([rand(N2) for i = 1:N])\n    X = D2[:, 1] |> Dataset\n    Y = D2[:, 2] |> Dataset\n    measure = MIShannon(; base = ‚ÑØ)\n    mis_ksg1[i, :] = map(k -> mutualinfo(measure, KSG1(; k), X, Y), ks)\n    mis_ksg2[i, :] = map(k -> mutualinfo(measure, KSG2(; k), X, Y), ks)\nend\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"k / N\", ylabel = \"Mutual infomation (nats)\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg1, dims = 1) |> vec, label = \"KSG1\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg2, dims = 1) |> vec, label = \"KSG2\")\nhlines!(ax, [mitrue], color = :black, linewidth = 3, label = \"I (true)\")\naxislegend()\nfig","category":"page"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(differential):-estimator-comparison","page":"Mutual information","title":"MIShannon (differential): estimator comparison","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Let's compare the performance of a subset of the implemented mutual information estimators. We'll use example data from Lord et al., where the analytical mutual information is known.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing LinearAlgebra: det\nusing StateSpaceSets: Dataset\nusing Distributions: MvNormal\nusing LaTeXStrings\nusing CairoMakie\n\n# adapted from https://juliadatascience.io/makie_colors\nfunction new_cycle_theme()\n    # https://nanx.me/ggsci/reference/pal_locuszoom.html\n    my_colors = [\"#D43F3AFF\", \"#EEA236FF\", \"#5CB85CFF\", \"#46B8DAFF\",\n        \"#357EBDFF\", \"#9632B8FF\", \"#B8B8B8FF\"]\n    cycle = Cycle([:color, :linestyle, :marker], covary=true) # alltogether\n    my_markers = [:circle, :rect, :utriangle, :dtriangle, :diamond,\n        :pentagon, :cross, :xcross]\n    my_linestyle = [nothing, :dash, :dot, :dashdot, :dashdotdot]\n    return Theme(\n        fontsize = 22, font=\"CMU Serif\",\n        colormap = :linear_bmy_10_95_c78_n256,\n        palette = (\n            color = my_colors, \n            marker = my_markers, \n            linestyle = my_linestyle,\n        ),\n        Axis = (\n            backgroundcolor= (:white, 0.2), \n            xgridstyle = :dash, \n            ygridstyle = :dash\n        ),\n        Lines = (\n            cycle= cycle,\n        ), \n        ScatterLines = (\n            cycle = cycle,\n        ),\n        Scatter = (\n            cycle = cycle,\n        ),\n        Legend = (\n            bgcolor = (:grey, 0.05), \n            framecolor = (:white, 0.2),\n            labelsize = 13,\n        )\n    )\nend\n\nrun(est; f::Function, # function that generates data\n        base::Real = ‚ÑØ, \n        nreps::Int = 10, \n        Œ±s = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], \n        n::Int = 1000) =\n    map(Œ± -> mutualinfo(MIShannon(; base), est, f(Œ±, n)...), Œ±s)\n\nfunction compute_results(f::Function; estimators, k = 5, k_lord = 20,\n        n = 1000, base = ‚ÑØ, nreps = 10,\n        as = 7:-1:0,\n        Œ±s = [1/10^(a) for a in as])\n    \n    is = [zeros(length(Œ±s)) for est in estimators]\n    for (k, est) in enumerate(estimators)\n        tmp = zeros(length(Œ±s))\n        for i = 1:nreps\n            tmp .+= run(est; f = f, Œ±s, base, n)\n        end\n        is[k] .= tmp ./ nreps\n    end\n\n    return is\nend\n\nfunction plot_results(f::Function, ftrue::Function; \n        base, estimators, k_lord, k, \n        as = 7:-1:0, Œ±s = [1/10^(a) for a in as], kwargs...\n    )\n    is = compute_results(f; \n        base, estimators, k_lord, k, as, Œ±s, kwargs...)\n    itrue = [ftrue(Œ±; base) for Œ± in Œ±s]\n\n    xmin, xmax = minimum(Œ±s), maximum(Œ±s)\n    \n    ymin = floor(Int, min(minimum(itrue), minimum(Iterators.flatten(is))))\n    ymax = ceil(Int, max(maximum(itrue), maximum(Iterators.flatten(is))))\n    f = Figure()\n    ax = Axis(f[1, 1],\n        xlabel = \"Œ±\", ylabel = \"I (nats)\",\n        xscale = log10, aspect = 1,\n        xticks = (Œ±s, [latexstring(\"10^{$(-a)}\") for a in as]),\n        yticks = (ymin:ymax)\n        )\n    xlims!(ax, (1/10^first(as), 1/10^last(as)))\n    ylims!(ax, (ymin, ymax))\n    lines!(ax, Œ±s, itrue, \n        label = \"I (true)\", linewidth = 4, color = :black)\n    for (i, est) in enumerate(estimators)\n        es = string(typeof(est).name.name)\n        lbl = occursin(\"Lord\", es) ? \"$es (k = $k_lord)\" : \"$es (k = $k)\"\n        scatter!(ax, Œ±s, is[i], label = lbl)\n        lines!(ax, Œ±s, is[i])\n\n    end\n    axislegend()\n    return f\nend\n\nset_theme!(new_cycle_theme())\nk_lord = 20\nk = 5\nbase = ‚ÑØ\n\nestimators = [\n    Kraskov(; k), \n    KozachenkoLeonenko(),\n    Zhu(; k), \n    ZhuSingh(; k),\n    Gao(; k),\n    Lord(; k = k_lord),\n    KSG1(; k), \n    KSG2(; k),\n    GaoOhViswanath(; k),\n    GaoKannanOhViswanath(; k),\n    GaussianMI(),\n]","category":"page"},{"location":"examples/examples_mutualinfo/#Family-2","page":"Mutual information","title":"Family 2","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"function family2(Œ±, n::Int)\n    Œ£ = [1 Œ±; Œ± 1]\n    N2 = MvNormal(zeros(2), Œ£)\n    D2 = Dataset([rand(N2) for i = 1:n])\n    X = Dataset(D2[:, 1])\n    Y = Dataset(D2[:, 2])\n    return X, Y\nend\n\nfunction ifamily2(Œ±; base = ‚ÑØ)\n    return (-0.5 * log(1 - Œ±^2)) / log(base, ‚ÑØ)\nend\n\nŒ±s = 0.05:0.05:0.95\nestimators = estimators\nwith_theme(new_cycle_theme()) do\n    f = Figure();\n    ax = Axis(f[1, 1], xlabel = \"Œ±\", ylabel = \"I (nats)\")\n    is_true = map(Œ± -> ifamily2(Œ±), Œ±s)\n    is_est = map(est -> run(est; f = family2, Œ±s, nreps = 20), estimators)\n    lines!(ax, Œ±s, is_true, \n        label = \"I (true)\", color = :black, linewidth = 3)\n    for (i, est) in enumerate(estimators)\n        estname = typeof(est).name.name |> String\n        scatterlines!(ax, Œ±s, is_est[i], label = estname)\n    end\n    axislegend(position = :lt)\n    return f\nend","category":"page"},{"location":"examples/examples_mutualinfo/#Family-1","page":"Mutual information","title":"Family 1","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In this system, samples are concentrated around the diagonal X = Y, and the strip of samples gets thinner as alpha to 0.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"function family1(Œ±, n::Int)\n    x = rand(n)\n    v = rand(n)\n    y = x + Œ± * v\n    return Dataset(x), Dataset(y)\nend\n\n# True mutual information values for these data\nfunction ifamily1(Œ±; base = ‚ÑØ)\n    mi = -log(Œ±) - Œ± - log(2)\n    return mi / log(base, ‚ÑØ)\nend\n\nfig = plot_results(family1, ifamily1; \n    k_lord = k_lord, k = k, nreps = 10,\n    estimators = estimators,\n    base = base)","category":"page"},{"location":"examples/examples_mutualinfo/#Family-3","page":"Mutual information","title":"Family 3","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In this system, we draw samples from a 4D Gaussian distribution distributed as specified in the ifamily3 function below. We let X be the two first variables, and Y be the two last variables.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"function ifamily3(Œ±; base = ‚ÑØ)\n    Œ£ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+Œ±]\n    Œ£x = Œ£[1:2, 1:2]; Œ£y = Œ£[3:4, 3:4]\n    mi = 0.5*log(det(Œ£x) * det(Œ£y) / det(Œ£))\n    return mi / log(base, ‚ÑØ)\nend\n\nfunction family3(Œ±, n::Int)\n    Œ£ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+Œ±]\n    N4 = MvNormal(zeros(4), Œ£)\n    D4 = Dataset([rand(N4) for i = 1:n])\n    X = D4[:, 1:2]\n    Y = D4[:, 3:4]\n    return X, Y\nend\n\nfig = plot_results(family3, ifamily3; \n    k_lord = k_lord, k = k, nreps = 10,\n    n = 2000,\n    estimators = estimators, base = base)","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"We see that the Lord estimator, which estimates local volume elements using a singular-value decomposition (SVD) of local neighborhoods, outperforms the other estimators by a large margin.","category":"page"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(continuous/discrete):-estimator-comparison","page":"Mutual information","title":"MIShannon (continuous/discrete): estimator comparison","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Most estimators suffer from significant bias when applied to discrete data. One possible resolution is to add a small amount of noise to discrete variables, so that the data becomes continuous in practice.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Instead of adding noise to your data, you can consider using an estimator that is specifically designed to deal with continuous-discrete mixture data. One example is the GaoKannanOhViswanath estimator.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here, we compare its performance to KSG1 on uniformly  distributed discrete multivariate data. The true mutual information is zero.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Statistics\nusing StateSpaceSets: Dataset\nusing Statistics: mean\nusing CairoMakie\n\nfunction compare_ksg_gkov(;\n        k = 5,\n        base = 2,\n        nreps = 15,\n        Ls = [500:100:1000; 1500; 2000; 3000; 4000; 5000; 1000])\n\n    est_gkov = GaoKannanOhViswanath(; k)\n    est_ksg1 = KSG1(; k)\n\n    mis_ksg1_mix = zeros(nreps, length(Ls))\n    mis_ksg1_discrete = zeros(nreps, length(Ls))\n    mis_ksg1_cont = zeros(nreps, length(Ls))\n    mis_gkov_mix = zeros(nreps, length(Ls))\n    mis_gkov_discrete = zeros(nreps, length(Ls))\n    mis_gkov_cont = zeros(nreps, length(Ls))\n\n    for (j, L) in enumerate(Ls)\n        for i = 1:nreps\n            X = Dataset(float.(rand(1:8, L, 2)))\n            Y = Dataset(float.(rand(1:8, L, 2)))\n            Z = Dataset(rand(L, 2))\n            W = Dataset(rand(L, 2))\n            measure = MIShannon(; base = ‚ÑØ)\n            mis_ksg1_discrete[i, j] = mutualinfo(measure, est_ksg1, X, Y)\n            mis_gkov_discrete[i, j] = mutualinfo(measure, est_gkov, X, Y)\n            mis_ksg1_mix[i, j] = mutualinfo(measure, est_ksg1, X, Z)\n            mis_gkov_mix[i, j] = mutualinfo(measure, est_gkov, X, Z)\n            mis_ksg1_cont[i, j] = mutualinfo(measure, est_ksg1, Z, W)\n            mis_gkov_cont[i, j] = mutualinfo(measure, est_gkov, Z, W)\n        end\n    end\n    return mis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n        mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont\nend\n\nfig = Figure()\nax = Axis(fig[1, 1], \n    xlabel = \"Sample size\", \n    ylabel = \"Mutual information (bits)\")\nLs = [100; 200; 500; 1000; 2500; 5000; 10000]\nnreps = 5\nk = 3\nmis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n    mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont = \n    compare_ksg_gkov(; nreps, k, Ls)\n\nscatterlines!(ax, Ls, mean(mis_ksg1_mix, dims = 1) |> vec, \n    label = \"KSG1 (mixed)\", color = :black, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_ksg1_discrete, dims = 1) |> vec, \n    label = \"KSG1 (discrete)\", color = :black, \n    linestyle = :dash, marker = '‚ñ≤')\nscatterlines!(ax, Ls, mean(mis_ksg1_cont, dims = 1) |> vec, \n    label = \"KSG1 (continuous)\", color = :black, \n    linestyle = :dot, marker = '‚óè')\nscatterlines!(ax, Ls, mean(mis_gkov_mix, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (mixed)\", color = :red, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_gkov_discrete, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (discrete)\", color = :red, \n    linestyle = :dash, marker = '‚ñ≤')\nscatterlines!(ax, Ls, mean(mis_gkov_cont, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (continuous)\", color = :red, \n    linestyle = :dot, marker = '‚óè')\naxislegend(position = :rb)\nfig","category":"page"},{"location":"estimator_list/#Continuous/differential-entropy","page":"-","title":"Continuous/differential entropy","text":"","category":"section"},{"location":"estimator_list/","page":"-","title":"-","text":"Continuous (differential) entropies are defined by an integral, and are  related to but don't share all the same properties as discrete entropies. For example, Shannon differential entropy may even be negative for  some distributions.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Continuous entropies must be estimated using some form of \"plug-in\" estimator. For example, the Shannon differential entropy for a random variable X with support mathcalX is defined as","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"h(x) = mathbbE-log(f(X)) = -int_mathcalXf(x) log f(x) dx","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"There are several ways of estimating this integral from observed data, using what is called \"plug-in\" estimators. A common plug-in estimator is the resubstitution estimator","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"hatH(x) = -frac1Nsum_i=1^N log(hatp(X_i))","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"where hatp is estimated using the samples X_1 X_2 ldots X_N, is a plug-in estimator for Shannon differential entropy.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Subtypes of DifferentialEntropyEstimators use various forms of plug-in estimators to estimate differential entropy. For example, Kraskov estimates Shannon differential entropy. LeonenkoProzantoSavani, on the other hand, estimates both Shannon, Renyi and  Tsallis differential entropy.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"note: Plug-in estimators for differential entropy\nWhen using entropy with a ProbabilitiesEstimator, it is always the discrete entropy that is computed. When using entropy with an DifferentialEntropyEstimator, it is the differential entropy that is computed.","category":"page"},{"location":"estimator_list/#Generalized-entropies","page":"-","title":"Generalized entropies","text":"","category":"section"},{"location":"estimator_list/","page":"-","title":"-","text":"There exists a multitude of various entropy measures in the scientific literature, which appear either in discrete form, differential/continuous form, or both.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Discrete entropies are simply functions of sums over","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"probability mass functions (pmf). Hence, every ProbabilitiesEstimator yields a naive plug-in estimator for generalized entropy (i.e. just plug the probabilities into the relevant entropy formulas). No bias correction is currently performed for discrete estimators.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Continuous (differential) entropies are functions of","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"probability density functions (pdf). Therefore, continuous entropy estimators approximate integrals instead of sums, as in the discrete case, and boils down to density function estimation. Every DifferentialEntropyEstimators has a slightly different way of estimating densities,  hence yielding slightly different differential entropy estimates.","category":"page"},{"location":"examples/examples_entropy/#examples_entropy","page":"Entropy","title":"Entropy","text":"","category":"section"},{"location":"examples/examples_entropy/#Differential-entropy:-estimator-comparison","page":"Entropy","title":"Differential entropy: estimator comparison","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Here, we'll test the different nearest-neighbor based differential entropy estimators on a three-dimensional normal distribution mathcalN (mu Sigma) with zero means and covariance matrix Sigma = diag(r_1 r_2 r_3) with r_1 = r_2 = r_3 = 05.  The analytical entropy for multivariate Gaussian is H(mathcalN (mu Sigma)) = dfrac12log(det(2pi e Sigma)). In our case, Sigma is diagonal, so det(Sigma) = (05)^3 and H = 05log(2pi e (05)^3)approx 3217.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Several of these estimators have been shown to convergence to the true entropy with an increasing number of samples. Therefore, we test the  estimators on samples of increasing size N, where N ranges from 1000 to 30000. Since we're estimating entropy from samples of a normal distribution, we don't expect the estimates to perfectly match the analytical entropy every time. On average, however, they should hit the target when the sample size gets large enough.","category":"page"},{"location":"examples/examples_entropy/#Analytical-and-estimated-entropies","page":"Entropy","title":"Analytical and estimated entropies","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"We'll first make two helper functions.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"analytical_entropy(estimators, Ls; d::Int, r, base = 2): Computes the analytical     Shannon differential entropy to the given base of a multivariate normal distribution   with covariance matrix with diagonal elements r and zeros on the off-diagonal.   Does so for each of the given estimators for each   sample size in Ls.\nmvnormal_entropies(; d::Int, r, base = 2, kwargs...): Estimates  the Shannon    entropy to the given base of samples from a multivariate normal distribution as   specified as above.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"using CausalityTools\nusing Distributions: MvNormal\nusing LinearAlgebra\nusing Statistics: quantile\nusing Random; rng = MersenneTwister(12345678)\nusing CairoMakie\n\nanalytical_entropy(; d::Int, r, base = 2) = \n    0.5*log(det(2*pi*‚ÑØ*diagm(repeat([r], d)))) / log(base, ‚ÑØ) # convert to desired base\n\nfunction mvnormal_entropies(estimators, Ls; \n        d = 3,\n        base = 2,\n        nreps = 50,\n        r = 0.5,\n    )\n    Œº = zeros(d)\n    Œ£ = diagm(repeat([r], d))\n    N = MvNormal(Œº, Œ£)    \n    Hs = [[zeros(nreps) for L in Ls] for est in estimators]\n    data = [Dataset([rand(rng, N) for i = 1:maximum(Ls)]) for i = 1:nreps]\n    for (e, est) in enumerate(estimators)\n        for (l, L) in enumerate(Ls)\n            for i = 1:nreps\n                Hs[e][l][i] = entropy(Shannon(; base), est, data[i][1:L])\n            end\n        end\n    end\n    return Hs\nend;","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"We'll also need a function to summarize the estimates.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"# A helper to get the estimator name for plotting.\ngetname(est::DifferentialEntropyEstimator) = typeof(est).name.name  |> string\nfunction medians_and_quantiles(Hs, Ls; q = 0.95)\n    medians = [zeros(length(Ls)) for est in estimators]\n    lb = [zeros(length(Ls)) for est in estimators]\n    ub = [zeros(length(Ls)) for est in estimators]\n\n    for (e, est) in enumerate(estimators)\n        for (l, L) in enumerate(Ls)\n            hÃÇs = Hs[e][l] # nreps estimates for this combinations of e and l\n            medians[e][l] = quantile(hÃÇs, 0.5)\n            lb[e][l] = quantile(hÃÇs, (1 - q) / 2)\n            ub[e][l] = quantile(hÃÇs, 1 - ((1 - q) / 2))\n        end\n    end\n\n    return medians, lb, ub\nend;","category":"page"},{"location":"examples/examples_entropy/#Plotting-utilities","page":"Entropy","title":"Plotting utilities","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Now, make some plotting helper functions.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"struct Cyclor{T} <: AbstractVector{T}\n    c::Vector{T}\n    n::Int\nend\nCyclor(c) = Cyclor(c, 0)\n\nBase.length(c::Cyclor) = length(c.c)\nBase.size(c::Cyclor) = size(c.c)\nBase.iterate(c::Cyclor, state=1) = Base.iterate(c.c, state)\nBase.getindex(c::Cyclor, i) = c.c[(i-1)%length(c.c) + 1]\nBase.getindex(c::Cyclor, i::AbstractArray) = c.c[i]\nfunction Base.getindex(c::Cyclor)\n    c.n += 1\n    c[c.n]\nend\nBase.iterate(c::Cyclor, i = 1) = iterate(c.c, i)\n\nCOLORSCHEME = [\n    \"#D43F3AFF\", \"#EEA236FF\", \"#5CB85CFF\", \"#46B8DAFF\",\n    \"#357EBDFF\", \"#9632B8FF\", \"#B8B8B8FF\",\n]\n\nCOLORS = Cyclor(COLORSCHEME)\nLINESTYLES = Cyclor(string.([\"--\", \".-\", \".\", \"--.\", \"---...\"]))\nMARKERS = Cyclor(string.([:circle, :rect, :utriangle, :dtriangle, :diamond,\n    :pentagon, :cross, :xcross]))\n\nfunction plot_entropy_estimates(Hs, Ls, Htrue)\n    # Summarize data (medians[e][l]) is the median of the e-th estimator for the \n    # l-th sample size).\n    medians, lbs, ubs = medians_and_quantiles(Hs, Ls);\n\n    fig = Figure(resolution = (800, 1000))\n    ymax = (vcat(Hs...) |> Iterators.flatten |> maximum) * 1.1\n    ymin = (vcat(Hs...) |> Iterators.flatten |> minimum) * 0.9\n\n    # We have 9 estimators, so place them on a 5-by-2 grid\n    positions = (Tuple(c) for c in CartesianIndices((5, 2)))\n    for (i, (est, c)) in enumerate(zip(estimators, positions))\n        ax = Axis(fig[first(c), last(c)],\n            xlabel = \"Sample size (L)\",\n            ylabel = \"HÃÇ (bits)\",\n            title = getname(est)\n        )\n        ylims!(ax, (ymin, ymax))\n        # Ground truth\n        hlines!(ax, [Htrue], \n            linestyle = :dash, \n            color = :black,\n            linewidth = 2,\n        )\n        # Estimates\n        band!(ax, Ls, lbs[i], ubs[i], color = (COLORS[i], 0.5))\n        lines!(ax, Ls, medians[i], \n            label = getname(est),\n            linestyle = LINESTYLES[i],\n            color = COLORS[i],\n            marker = MARKERS[i],\n            linewidth = 2\n        )\n    end\n    fig\nend;","category":"page"},{"location":"examples/examples_entropy/#Results","page":"Entropy","title":"Results","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Now, we can finally run an ensemble of tests and plot the confidence bands against the ground truth. This","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"k = 4\nestimators = [\n    Kraskov(; k), \n    KozachenkoLeonenko(), \n    Gao(; k),\n    ZhuSingh(; k),\n    Zhu(; k),\n    Goria(; k),\n    LeonenkoProzantoSavani(; k),\n    Lord(; k = k*5)\n]\n\nLs = [100:100:1000 |> collect; 2500:2500:5000 |> collect]\nd = 3\nr = 0.5\nnreps = 30\nHs = mvnormal_entropies(estimators, Ls; d, r, nreps)\nHtrue = analytical_entropy(; d, r)\nplot_entropy_estimates(Hs, Ls, Htrue)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"(Image: CausalityTools.jl static logo)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"info: Info\nYou are reading the development version of the documentation of CausalityTools.jl that will become version 2.0.","category":"page"},{"location":"#Quantifying-associations","page":"Overview","title":"Quantifying associations","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"CausalityTools is a Julia package that provides algorithms for detecting associations, dynamical influences and causal inference from data. The core function for quantifying associations is independence, which performs either a parametric or nonparametric (conditional) independence test, using any of the following measures:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Type Measure Pairwise Conditional\nCorrelation PearsonCorrelation ‚úì ‚úñ\nCorrelation DistanceCorrelation ‚úì ‚úñ\nCloseness SMeasure ‚úì ‚úñ\nCloseness JointDistanceDistribution ‚úì ‚úñ\nCross-mapping PairwiseAsymmetricInference ‚úì ‚úñ\nCross-mapping ConvergentCrossMapping ‚úì ‚úñ\nShared information MIShannon ‚úì ‚úñ\nShared information MIRenyiJizba ‚úì ‚úñ\nShared information MIRenyiSarbu ‚úì ‚úñ\nShared information MITsallisFuruichi ‚úì ‚úñ\nShared information PartialCorrelation ‚úñ ‚úì\nShared information CMIShannon ‚úñ ‚úì\nShared information CMIRenyiSarbu ‚úñ ‚úì\nShared information CMIRenyiJizba ‚úñ ‚úì\nInformation transfer TransferEntropy ‚úì ‚úì","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"All of these measure are also available in a simple function form that allows you to compute the raw measures outside an independence testing context.","category":"page"},{"location":"#Goals","page":"Overview","title":"Goals","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Causal inference, and quantification of association in general, is fundamental to most scientific disciplines. There exists a multitude of bivariate and multivariate association measures in the scientific literature. However, beyond the most basic measures, most methods aren't readily available for practical use. Most scientific papers don't provide code, which makes reproducing them difficult or impossible, without investing significant time and resources into deciphering and understanding the original papers to the point where an implementation is possible.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Our main goal with this package is to provide an easily extendible library of association measures, and an as-complete-as-possible set of their estimators. We also want to lower the entry-point to the field of causal inference and association quantification, by providing well-documented implementations of literature methods with runnable code examples.","category":"page"},{"location":"#Input-data","page":"Overview","title":"Input data","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Input data for CausalityTools are given as:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Univariate timeseries, which are given as standard Julia Vectors.\nMultivariate timeseries, datasets, or state space sets, which are given as   Datasets. Many methods convert timeseries inputs to Dataset   for faster internal computations.\nCategorical data can be used with ContingencyMatrix to compute various   information theoretic measures and is represented using any iterable whose elements   can be any arbitrarily complex data type (as long as it's hashable), for example   Vector{String}, {Vector{Int}}, or Vector{Tuple{Int, String}}.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Dataset","category":"page"},{"location":"#StateSpaceSets.Dataset","page":"Overview","title":"StateSpaceSets.Dataset","text":"Dataset{D, T} <: AbstractDataset{D,T}\n\nA dedicated interface for datasets. It contains equally-sized datapoints of length D, represented by SVector{D, T}. These data are a standard Julia Vector{SVector}, and can be obtained with vec(dataset).\n\nWhen indexed with 1 index, a dataset is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.\n\nDataset also supports most sensible operations like append!, push!, hcat, eachrow, among others, and when iterated over, it iterates over its contained points.\n\nDescription of indexing\n\nIn the following let i, j be integers,  typeof(data) <: AbstractDataset and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges, and for massive performance benefits make v2 an SVector{X, Int}).\n\ndata[i] == data[i, :] gives the ith datapoint (returns an SVector)\ndata[v1] == data[v1, :], returns a Dataset with the points in those indices.\ndata[:, j] gives the jth variable timeseries, as Vector\ndata[v1, v2], data[:, v2] returns a Dataset with the appropriate entries (first indices being \"time\"/point index, while second being variables)\ndata[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(dataset) or Dataset(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like Dataset(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"#Pull-requests-and-issues","page":"Overview","title":"Pull requests and issues","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"This package has been and is under heavy development. Don't hesitate to submit an issue if you find something that doesn't work or doesn't make sense, or if there's some functionality that you're missing. Pull requests are also very welcome!","category":"page"},{"location":"#Maintainers-and-contributors","page":"Overview","title":"Maintainers and contributors","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The CausalityTools.jl software is maintained by Kristian Agas√∏ster Haaga, who also curates and writes this documentation. Significant contributions to the API and documentation design has been made by George Datseris, which also co-authors ComplexityMeasures.jl, which we develop in tandem with this package.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"A complete list of contributors to this repo are listed on the main Github page. Some important contributions are:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Norbert Genera contributed bug reports and   investigations that led to subsequent improvements for the pairwise asymmetric   inference algorithm and an improved cross mapping API.\nDavid Diego's contributions were   invaluable in the initial stages of development. His MATLAB code provided the basis   for several transfer entropy methods and binning-related code.\nGeorge Datseris also ported KSG1 and KSG2 mutual   information estimators to Neighborhood.jl.\nBjarte Hannisdal provided tutorials for mutual information.\nTor Einar M√∏ller contributed to cross-mapping methods in initial stages of development.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Many individuals has contributed code to other packages in the JuliaDynamics ecosystem which we use here. Contributors are listed in the respective GitHub repos and webpages.","category":"page"},{"location":"#Related-packages","page":"Overview","title":"Related packages","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"TransferEntropy.jl previously   provided mutual infromation and transfer entropy estimators. These have been   re-implemented from scratch and moved here.","category":"page"},{"location":"examples/examples_cross_mappings/#examples_crossmappings","page":"Cross mappings","title":"Cross mappings","text":"","category":"section"},{"location":"examples/examples_cross_mappings/#Convergent-cross-mapping-(reproducing-Sugihara-et-al.,-2012)","page":"Cross mappings","title":"Convergent cross mapping (reproducing Sugihara et al., 2012)","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"note: Run blocks consecutively\nIf copying these examples and running them locally, make sure the relevant packages (given in the first block) are loaded first.","category":"page"},{"location":"examples/examples_cross_mappings/#Figures-3C-and-3D","page":"Cross mappings","title":"Figures 3C and 3D","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's reproduce figures 3C and 3D in Sugihara et al. (2012)[Sugihara2012], which introduced the ConvergentCrossMapping measure. Equations and parameters can be found in their supplementary material. Simulatenously, we also compute the PairwiseAsymmetricInference measure from McCracken & Weigel (2014)[McCracken2014], which is a related method, but uses a slightly different embedding.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"[Sugihara2012]: Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"[McCracken2014]: McCracken, J. M., & Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E, 90(6), 062903.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n\n# -----------------------------------------------------------------------------------------\n# Create 400-point long time series for Sugihara et al. (2012)'s example for figure 3.\n# -----------------------------------------------------------------------------------------\nfunction eom_logistic_sugi(u, p, t)\n    (; rx, ry, Œ≤xy, Œ≤yx) = p\n    (; x, y) = u\n\n    dx = x*(rx - rx*x - Œ≤xy*y)\n    dy = y*(ry - ry*y - Œ≤yx*x)\n    return SVector{2}(dx, dy)\nend\n\n# Œ≤xy := effect on x of y\n# Œ≤yx := effect on y of x\nfunction logistic_sugi(; u0 = rand(2), rx, ry, Œ≤xy, Œ≤yx)\n    p = @LArray [rx, ry, Œ≤xy, Œ≤yx] (:rx, :ry, :Œ≤xy, :Œ≤yx)\n    DiscreteDynamicalSystem(eom_logistic_sugi, u0, p)\nend\n\nsys_unidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, Œ≤xy = 0.00, Œ≤yx = 0.32);\nx, y = columns(trajectory(sys_unidir, 1000, Ttr = 10000));\n\n# -----------------------------------------------------------------------------------------\n# Cross map.\n# -----------------------------------------------------------------------------------------\nm_ccm = ConvergentCrossMapping(d = 2)\nm_pai = PairwiseAsymmetricInference(d = 2)\n# Make predictions xÃÇy, i.e. predictions `xÃÇ` made from embedding of y (AND x, if PAI)\ntÃÇccm_xÃÇy, tccm_xÃÇy, œÅccm_xÃÇy = predict(m_ccm, x, y)\ntÃÇpai_xÃÇy, tpai_xÃÇy, œÅpai_xÃÇy = predict(m_pai, x, y);\n# Make predictions yÃÇx, i.e. predictions `yÃÇ` made from embedding of x (AND y, if PAI)\ntÃÇccm_yÃÇx, tccm_yÃÇx, œÅccm_yÃÇx = predict(m_ccm, y, x)\ntÃÇpai_yÃÇx, tpai_yÃÇx, œÅpai_yÃÇx = predict(m_pai, y, x);\n\n# -----------------------------------------------------------------------------------------\n# Plot results\n# -----------------------------------------------------------------------------------------\nœÅs = (œÅccm_xÃÇy, œÅpai_xÃÇy, œÅccm_yÃÇx, œÅpai_yÃÇx)\nsccm_xÃÇy, spai_xÃÇy, sccm_yÃÇx, spai_yÃÇx = (map(œÅ -> (@sprintf \"%.3f\" œÅ), œÅs)...,)\n\nœÅs = (œÅccm_xÃÇy, œÅpai_xÃÇy, œÅccm_yÃÇx, œÅpai_yÃÇx)\nsccm_xÃÇy, spai_xÃÇy, sccm_yÃÇx, spai_yÃÇx = (map(œÅ -> (@sprintf \"%.3f\" œÅ), œÅs)...,)\n\nwith_theme(theme_minimal(),\n    markersize = 5) do\n    fig = Figure();\n    ax_yÃÇx = Axis(fig[2,1], aspect = 1, xlabel = \"y(t) (observed)\", ylabel = \"yÃÇ(t) | x (predicted)\")\n    ax_xÃÇy = Axis(fig[2,2], aspect = 1, xlabel = \"x(t) (observed)\", ylabel = \"xÃÇ(t) | y (predicted)\")\n    xlims!(ax_yÃÇx, (0, 1)), ylims!(ax_yÃÇx, (0, 1))\n    xlims!(ax_xÃÇy, (0, 1)), ylims!(ax_xÃÇy, (0, 1))\n    ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n    scatterlines!(ax_ts, x[1:300], label = \"x\")\n    scatterlines!(ax_ts, y[1:300], label = \"y\")\n    axislegend()\n    scatter!(ax_yÃÇx, tccm_yÃÇx, tÃÇccm_yÃÇx, label = \"CCM (œÅ = $sccm_yÃÇx)\", color = :black)\n    scatter!(ax_yÃÇx, tpai_yÃÇx, tÃÇpai_yÃÇx, label = \"PAI (œÅ = $spai_yÃÇx)\", color = :red)\n    axislegend(ax_yÃÇx, position = :lt)\n    scatter!(ax_xÃÇy, tccm_xÃÇy, tÃÇccm_xÃÇy, label = \"CCM (œÅ = $sccm_xÃÇy)\", color = :black)\n    scatter!(ax_xÃÇy, tpai_xÃÇy, tÃÇpai_xÃÇy, label = \"PAI (œÅ = $spai_xÃÇy)\", color = :red)\n    axislegend(ax_xÃÇy, position = :lt)\n    fig\nend","category":"page"},{"location":"examples/examples_cross_mappings/#Figure-3A","page":"Cross mappings","title":"Figure 3A","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's reproduce figure 3A too, focusing only on ConvergentCrossMapping this time. In this figure, they compute the cross mapping for libraries of increasing size, always starting at time index 1. This approach - which we here call the ExpandingSegment estimator - is one of many ways of estimating the correspondence between observed and predicted value.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"For this example, they use a bidirectional system with asymmetrical coupling strength.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n# Used in `reproduce_figure_3A_naive`, and `reproduce_figure_3A_ensemble` below.\nfunction add_to_fig!(fig_pos, libsizes, œÅs_xÃÇy, œÅs_yÃÇx; title = \"\", quantiles = false)\n    ax = Axis(fig_pos; title, aspect = 1,\n        xlabel = \"Library size\", ylabel = \"Correlation (œÅ)\")\n    ylims!(ax, (-1, 1))\n    hlines!([0], linestyle = :dash, alpha = 0.5, color = :grey)\n    scatterlines!(libsizes, median.(œÅs_xÃÇy), label = \"xÃÇ|y\", color = :blue)\n    scatterlines!(libsizes, median.(œÅs_yÃÇx), label = \"yÃÇ|x\", color = :red)\n    if quantiles\n        band!(libsizes, quantile.(œÅs_xÃÇy, 0.05), quantile.(œÅs_xÃÇy, 0.95), color = (:blue, 0.5))\n        band!(libsizes, quantile.(œÅs_yÃÇx, 0.05), quantile.(œÅs_yÃÇx, 0.95), color = (:red, 0.5))\n    end\n    axislegend(ax, position = :rb)\nend\n\nfunction reproduce_figure_3A_naive(measure::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, Œ≤xy = 0.02, Œ≤yx = 0.32);\n    x, y = columns(trajectory(sys_bidir, 3100, Ttr = 10000));\n    libsizes = [20:2:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:3000]\n    est = ExpandingSegment(; libsizes);\n    œÅs_xÃÇy = crossmap(measure, est, x, y)\n    œÅs_yÃÇx = crossmap(measure, est, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, œÅs_xÃÇy, œÅs_yÃÇx; title = \"`ExpandingSegment`\")\n        fig\n    end\nend\n\nreproduce_figure_3A_naive(ConvergentCrossMapping(d = 3))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Hm. This looks a bit like the paper, but the curve is not smooth. We can do better!","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"It is not clear from the paper exactly what they plot in their Figure 3A, if they plot an average of some kind, or precisely what parameters and initial conditions they use. However, we can get a smoother plot by using a Ensemble. Combined with a CrossmapEstimator, it uses Monte Carlo resampling on subsets of the input data to compute an ensemble of œÅs that we here use to compute the median and 90-th percentile range for each library size.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"function reproduce_figure_3A_ensemble(measure::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.4, 0.2], rx = 3.8, ry = 3.5, Œ≤xy = 0.02, Œ≤yx = 0.1);\n    x, y = columns(trajectory(sys_bidir, 10000, Ttr = 10000));\n    # Note: our time series are 1000 points long. When embedding, some points are\n    # lost, so we must use slightly less points for the segments than \n    # there are points in the original time series.\n    libsizes = [20:5:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:3000]\n    # No point in doing more than one rep, because there data are always the same\n    # for `ExpandingSegment.`\n    ensemble_ev = Ensemble(measure, ExpandingSegment(; libsizes); nreps = 1)\n    ensemble_rs = Ensemble(measure, RandomSegment(; libsizes); nreps = 50)\n    ensemble_rv = Ensemble(measure, RandomVectors(; libsizes); nreps = 50)\n    œÅs_xÃÇy_es = crossmap(ensemble_ev, x, y)\n    œÅs_yÃÇx_es = crossmap(ensemble_ev, y, x)\n    œÅs_xÃÇy_rs = crossmap(ensemble_rs, x, y)\n    œÅs_yÃÇx_rs = crossmap(ensemble_rs, y, x)\n    œÅs_xÃÇy_rv = crossmap(ensemble_rv, x, y)\n    œÅs_yÃÇx_rv = crossmap(ensemble_rv, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, œÅs_xÃÇy_es, œÅs_yÃÇx_es; title = \"`ExpandingSegment`\", quantiles = false) # quantiles make no sense for `ExpandingSegment`\n        add_to_fig!(fig[1, 2], libsizes, œÅs_xÃÇy_rs, œÅs_yÃÇx_rs; title = \"`RandomSegment`\", quantiles = true)\n        add_to_fig!(fig[1, 3], libsizes, œÅs_xÃÇy_rv, œÅs_yÃÇx_rv; title = \"`RandomVector`\", quantiles = true)\n        fig\n    end\nend\n\nreproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, œÑ = -1))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"With the RandomVectors estimator, the mean of our ensemble œÅs seem to look pretty much identical to Figure 3A in Sugihara et al. The RandomSegment estimator also performs pretty well, but since subsampled segments are contiguous, there are probably some autocorrelation effects at play.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"We can avoid the autocorrelation issue by tuning the w parameter of the ConvergentCrossMapping measure, which is the  Theiler window. Setting the Theiler window to w > 0, we can exclude neighbors of a query point p that are close to p in time, and thus deal with autocorrelation issues that way (the default w = 0 excludes only the point itself). Let's re-do the analysis with w = 5, just for fun.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"reproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, œÑ = -1, w = 5))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"There wasn't really that much of a difference, since for the logistic map, the autocorrelation function flips sign for every lag increase. However, for examples from other systems, tuning w may be important.","category":"page"},{"location":"examples/examples_cross_mappings/#Figure-3B","page":"Cross mappings","title":"Figure 3B","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"What about figure 3B? Here they generate time series of length 400 for a range of values for both coupling parameters, and plot the dominant direction Delta = rho(hatx  y) - rho(haty  x).","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"In the paper, they use a 1000 different parameterizations for the logistic map parameters, but don't state what is summarized in the plot. For simplicity, we'll therefore just stick to rx = ry = 3.7, as in the examples above, and just loop over the coupling strengths in either direction.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"function reproduce_figure_3B()\n    Œ≤xys = 0.0:0.02:0.4\n    Œ≤yxs = 0.0:0.02:0.4\n    œÅxÃÇys = zeros(length(Œ≤xys), length(Œ≤yxs))\n    œÅyÃÇxs = zeros(length(Œ≤xys), length(Œ≤yxs))\n\n    for (i, Œ≤xy) in enumerate(Œ≤xys)\n        for (j, Œ≤yx) in enumerate(Œ≤yxs)\n            sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.7, Œ≤xy, Œ≤yx);\n            # Generate 1000 points. Randomly select a 400-pt long segment.\n            x, y = columns(trajectory(sys_bidir, 1300, Ttr = 10000));\n            ensemble = Ensemble(CCM(d = 3, w = 5, œÑ = -1), RandomVectors(libsizes = 400), nreps = 10)\n            œÅxÃÇys[i, j] = mean(crossmap(ensemble, x, y))\n            œÅyÃÇxs[i, j] = mean(crossmap(ensemble, y, x))\n        end\n    end\n    Œî = œÅyÃÇxs .- œÅxÃÇys\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure();\n        ax = Axis(fig[1, 1], xlabel = \"Œ≤xy\", ylabel = \"Œ≤yx\")\n        cont = contourf!(ax, Œî, levels = range(-1, 1, length = 10),\n            colormap = :curl)\n        ax.xticks = 1:length(Œ≤xys), string.([i % 2 == 0 ? Œ≤xys[i] : \"\" for i in 1:length(Œ≤xys)])\n        ax.yticks = 1:length(Œ≤yxs), string.([i % 2 == 0 ? Œ≤yxs[i] : \"\" for i in 1:length(Œ≤yxs)])\n        Colorbar(fig[1 ,2], cont, label = \"Œî (œÅ(yÃÇ|x) - œÅ(xÃÇ|y))\")\n        tightlimits!(ax)\n        fig\n    end\nend\n\nreproduce_figure_3B()","category":"page"},{"location":"examples/examples_cross_mappings/#Pairwise-asymmetric-inference-(reproducing-McCracken-and-Weigel,-2014)","page":"Cross mappings","title":"Pairwise asymmetric inference (reproducing McCracken & Weigel, 2014)","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's try to reproduce figure 8 from McCracken & Weigel (2014)'s[McCracken2014] paper on PairwiseAsymmetricInference (PAI). We'll start by defining the their example B (equations 6-7). This system consists of two variables X and Y, where X drives Y.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"After we have computed the PAI in both directions, we define a measure of directionality as the difference between PAI in the X to Y direction and in the Y to X direction, so that if X drives Y, then Delta  0.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\nusing Distributions: Normal\nusing Statistics: mean, std\n\nfunction eom_nonlinear_sindriver(dx, x, p, n)\n    a, b, c, t, Œît = (p...,)\n    x, y = x[1], x[2]\n    ùí© = Normal(0, 1)\n    \n    dx[1] = sin(t)\n    dx[2] = a*x * (1 - b*x) + c* rand(ùí©)\n    p[end-1] += 1 # update t\n\n    return\nend\n\nfunction nonlinear_sindriver(;u‚ÇÄ = rand(2), a = 1.0, b = 1.0, c = 2.0, Œît = 1)\n    DiscreteDynamicalSystem(eom_nonlinear_sindriver, u‚ÇÄ, [a, b, c, 0, Œît])\nend\n\nfunction reproduce_figure_8_mccraken(; \n        c = 2.0, Œît = 0.2,\n        as = 0.25:0.25:5.0,\n        bs = 0.25:0.25:5.0)\n    # -----------------------------------------------------------------------------------------\n    # Generate many time series for many different values of the parameters `a` and `b`,\n    # and compute PAI. This will replicate the upper right panel of \n    # figure 8 in McCracken & Weigel (2014).\n    # -----------------------------------------------------------------------------------------\n    \n    measure = PairwiseAsymmetricInference(d = 3)\n\n    # Manually resample `nreps` length-`L` time series and use mean œÅ(xÃÇ|XÃÑy) - œÅ(yÃÇ|YÃÑx)\n    # for each parameter combination.\n    nreps = 50\n    L = 300 # length of timeseries\n    Œî = zeros(length(as), length(bs))\n    for (i, a) in enumerate(as)\n        for (j, b) in enumerate(bs)\n            s = nonlinear_sindriver(; a, b, c,  Œît)\n            x, y = columns(trajectory(s, 1000, Ttr = 10000))\n            Œîreps = zeros(nreps)\n            for i = 1:nreps\n                # Ensure we're subsampling at the same time indices. \n                ind_start = rand(1:(1000-L))\n                r = ind_start:(ind_start + L)\n                Œîreps[i] = @views crossmap(measure, y[r], x[r]) - \n                    crossmap(measure, x[r], y[r])\n            end\n            Œî[i, j] = mean(Œîreps)\n        end\n    end\n\n    # -----------------------------------------------------------------------------------------\n    # An example time series for plotting.\n    # -----------------------------------------------------------------------------------------\n    sys = nonlinear_sindriver(; a = 1.0, b = 1.0, c, Œît)\n    npts = 500\n    orbit = trajectory(sys, npts, Ttr = 10000)\n    x, y = columns(orbit)\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        \n        X = x[1:300]\n        Y = y[1:300]\n        fig = Figure();\n        ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n        scatterlines!(ax_ts, (X .- mean(X)) ./ std(X), label = \"x\")\n        scatterlines!(ax_ts, (Y .- mean(Y)) ./ std(Y), label = \"y\")\n        axislegend()\n\n        ax_hm = Axis(fig[2, 1:2], xlabel = \"a\", ylabel = \"b\")\n        ax_hm.yticks = (1:length(as), string.([i % 2 == 0 ? as[i] : \"\" for i = 1:length(as)]))\n        ax_hm.xticks = (1:length(bs), string.([i % 2 == 0 ? bs[i] : \"\" for i = 1:length(bs)]))\n        hm = heatmap!(ax_hm, Œî,  colormap = :viridis)\n        Colorbar(fig[2, 3], hm; label = \"Œî' = œÅ(yÃÇ | yx) - œÅ(xÃÇ | xy)\")\n        fig\n    end\nend\n\nreproduce_figure_8_mccraken()","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"As expected, Delta  0 for all parameter combinations, implying that X \"PAI drives\" Y.","category":"page"},{"location":"quickstart/quickstart_independence/#quickstart_independence","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#quickstart_jddtest","page":"Independence testing","title":"JointDistanceDistributionTest","text":"","category":"section"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"Let's use the built-in logistic2_bidir discrete dynamical system to create a pair of bidirectionally coupled time series and use the JointDistanceDistributionTest to see if we can confirm from observed time series that these variables are bidirectionally coupled. We'll use a significance level of 1 - Œ± = 0.99, i.e. Œ± = 0.01.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"We start by generating some time series and configuring the test.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nsys = logistic2_bidir(c_xy = 0.5, c_yx = 0.4)\nx, y = columns(trajectory(sys, 2000, Ttr = 10000))\nmeasure = JointDistanceDistribution(D = 5, B = 5)\ntest = JointDistanceDistributionTest(measure)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"Now, we test for independence in both directions.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, x, y)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, y, x)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the null hypothesis is rejected in both directions at the pre-determined  significance level, and hence we detect directional coupling in both directions. But what happens if there is no coupling?","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"sys = logistic2_bidir(c_xy = 0.00, c_yx = 0.0)\nx, y = columns(trajectory(sys, 1000, Ttr = 10000));\nrxy = independence(test, x, y)\nryx = independence(test, y, x)\npvalue(rxy), pvalue(ryx)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"At significance level 0.99, we can't reject the null in either direction, hence there's not enough evidence in the data to suggest directional coupling.","category":"page"},{"location":"quickstart/quickstart_independence/#quickstart_surrogatetest","page":"Independence testing","title":"SurrogateTest","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Mutual-information","page":"Independence testing","title":"Mutual information","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Categorical","page":"Independence testing","title":"Categorical","text":"","category":"section"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"In this example, we expect the preference and the food variables to be independent.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\n# Simulate \nn = 1000\npreference = rand([\"yes\", \"no\"], n)\nfood = rand([\"veggies\", \"meat\", \"fish\"], n)\ntest = SurrogateTest(MIShannon(), Contingency())\nindependence(test, preference, food)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, there's not enough evidence to reject the null hypothesis that the variables are independent.","category":"page"},{"location":"quickstart/quickstart_independence/#Conditional-mutual-information","page":"Independence testing","title":"Conditional mutual information","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Categorical-2","page":"Independence testing","title":"Categorical","text":"","category":"section"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we simulate a survey at a ski resort. The data are such that the place a person grew up is associated with how many times they fell while going skiing. The control happens through an intermediate variable preferred_equipment, which indicates what type of physical activity the person has engaged with in the past. Some activities like skateboarding leads to better overall balance, so people that are good on a skateboard also don't fall, and people that to less challenging activities fall more often.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"We should be able to reject places ‚´´ experience, but not reject places ‚´´ experience |¬†preferred_equipment.  Let's see if we can detect these relationships using (conditional) mutual information.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nn = 10000\n\nplaces = rand([\"city\", \"countryside\", \"under a rock\"], n);\npreferred_equipment = map(places) do place\n    if cmp(place, \"city\") == 1\n        return rand([\"skateboard\", \"bmx bike\"])\n    elseif cmp(place, \"countryside\") == 1\n        return rand([\"sled\", \"snowcarpet\"])\n    else\n        return rand([\"private jet\", \"ferris wheel\"])\n    end\nend;\nexperience = map(preferred_equipment) do equipment\n    if equipment ‚àà [\"skateboard\", \"bmx bike\"]\n        return \"didn't fall\"\n    elseif equipment ‚àà [\"sled\", \"snowcarpet\"]\n        return \"fell 3 times or less\"\n    else\n        return \"fell uncontably many times\"\n    end\nend;\n\ntest_mi = independence(SurrogateTest(MIShannon(), Contingency()), places, experience)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the evidence favors the alternative hypothesis that places and  experience are dependent.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"test_cmi = independence(SurrogateTest(CMIShannon(), Contingency()), places, experience, preferred_equipment)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"Again, as expected, when conditioning on the mediating variable, the dependence disappears, and we can't reject the null hypothesis of independence.","category":"page"},{"location":"quickstart/quickstart_independence/#quickstart_localpermutationtest","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#[CMIShannon](@ref)","page":"Independence testing","title":"CMIShannon","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Continuous-data","page":"Independence testing","title":"Continuous data","text":"","category":"section"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nx = randn(1000)\ny = randn(1000) .+ 0.7 .* x\nz = sin.(randn(1000)) .* 0.5 .* y\ntest = LocalPermutationTest(CMIShannon(; base = 2), FPVP(k = 10), nshuffles = 30)\nindependence(test, x, z, y)","category":"page"}]
}
