<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Information measures · CausalityTools.jl</title><meta name="title" content="Information measures · CausalityTools.jl"/><meta property="og:title" content="Information measures · CausalityTools.jl"/><meta property="twitter:title" content="Information measures · CausalityTools.jl"/><meta name="description" content="Documentation for CausalityTools.jl."/><meta property="og:description" content="Documentation for CausalityTools.jl."/><meta property="twitter:description" content="Documentation for CausalityTools.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="CausalityTools.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="CausalityTools.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">CausalityTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">CausalityTools.jl</a></li><li><a class="tocitem" href="../associations/">Association measures</a></li><li><a class="tocitem" href="../independence/">Independence</a></li><li><a class="tocitem" href="../causal_graphs/">Network/graph inference</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/examples_associations/">Association measures</a></li><li><a class="tocitem" href="../examples/examples_infer_graphs/">Graph inference</a></li><li><span class="tocitem">Extended examples</span><ul><li><a class="tocitem" href="../extended_examples/cross_mapping/"><code>ConvergentCrossMapping</code></a></li><li><a class="tocitem" href="../extended_examples/pairwise_asymmetric_inference/"><code>PairwiseAsymmetricInference</code></a></li><li><a class="tocitem" href="../extended_examples/mutual_information/"><code>MIShannon</code></a></li></ul></li></ul></li><li><span class="tocitem">Basics and tutorials</span><ul><li><a class="tocitem" href="../encoding_tutorial/">Encoding elements</a></li><li><a class="tocitem" href="../discretization_tutorial/">Encoding input datasets</a></li><li><a class="tocitem" href="../probabilities_tutorial/">Counts and probabilities</a></li><li class="is-active"><a class="tocitem" href>Information measures</a><ul class="internal"><li><a class="tocitem" href="#Definitions"><span>Definitions</span></a></li><li><a class="tocitem" href="#Basic-strategy"><span>Basic strategy</span></a></li><li><a class="tocitem" href="#Distances/divergences"><span>Distances/divergences</span></a></li><li><a class="tocitem" href="#Conditional-entropies"><span>Conditional entropies</span></a></li><li><a class="tocitem" href="#Joint-entropies"><span>Joint entropies</span></a></li><li><a class="tocitem" href="#Mutual-informations"><span>Mutual informations</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Basics and tutorials</a></li><li class="is-active"><a href>Information measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Information measures</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/CausalityTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/main/docs/src/info_tutorial.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="info_tutorial"><a class="docs-heading-anchor" href="#info_tutorial">Information measure tutorial</a><a id="info_tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#info_tutorial" title="Permalink"></a></h1><p>CausalityTools.jl extends the single-variate information API in <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl">ComplexityMeasures.jl</a> to information measures of multiple variables. </p><h2 id="Definitions"><a class="docs-heading-anchor" href="#Definitions">Definitions</a><a id="Definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Definitions" title="Permalink"></a></h2><p>We define <strong>&quot;information measure&quot;</strong> as some functional of probability  mass functions or probability densities. This definition may or may not agree with literature usage, depending on the context. We made this choice pragmatically based on user-friendlyness and coding-friendlyness, but still trying to maintain some level of meaningful terminology.</p><h2 id="Basic-strategy"><a class="docs-heading-anchor" href="#Basic-strategy">Basic strategy</a><a id="Basic-strategy-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-strategy" title="Permalink"></a></h2><p>To <em>estimate</em> a multivariate information measure in practice, you must first specify the <a href="@ref">definition</a> of the measure, which is then used as input to an  <a href="@ref">estimator</a>. This estimator is then given to <a href="@ref"><code>information</code></a>, or one  of the <a href="@ref convenience_info">convenience methods</a>.</p><div class="admonition is-info"><header class="admonition-header">Naming convention: The same name for different things</header><div class="admonition-body"><p>Upon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use <em>the same name for different concepts</em>. For novices, and experienced practitioners too, this can be confusing. Our API clearly distinguishes between methods that are conceptually the same but named differently in the literature due to differing <em>estimation</em> strategies, from methods that actually have different definitions.</p><ul><li>Multiple, equivalent definitions occur for example for the Shannon mutual   information (MI; <a href="../associations/#CausalityTools.MIShannon"><code>MIShannon</code></a>), which has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral   over a joint probability mass function (pmf), as a sum of three entropy terms, and as   a Kullback-Leibler divergence between the joint pmf and the product of the marginal   distributions. Since these definitions are all equivalent, we only need once type   (<a href="../associations/#CausalityTools.MIShannon"><code>MIShannon</code></a>) to represent them.</li><li>But Shannon MI is not the  only type of mutual information! For example, &quot;Tsallis mutual information&quot;   has been proposed in different variants by various authors. Despite sharing the   same name, these are actually <em>nonequivalent definitions</em>. We&#39;ve thus assigned   them entirely different measure names (e.g. <a href="../associations/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a> and   <a href="../associations/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a>), with the author name at the end.</li></ul></div></div><h2 id="Distances/divergences"><a class="docs-heading-anchor" href="#Distances/divergences">Distances/divergences</a><a id="Distances/divergences-1"></a><a class="docs-heading-anchor-permalink" href="#Distances/divergences" title="Permalink"></a></h2><p>There are many information measures in the literature that aim to quantify the  distance/divergence between two probability mass functions (pmf) or densities. You can  find those that we implement <a href="../associations/#divergences_and_distances">here</a>.</p><p>As an example, let&#39;s quantify the <a href="../associations/#CausalityTools.KLDivergence"><code>KLDivergence</code></a> between two probability  mass functions estimated by symbolizing two input vectors <code>x</code> and <code>y</code> using  <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a>. Since the discrete <a href="../associations/#CausalityTools.KLDivergence"><code>KLDivergence</code></a> can be  expressed as a function of a joint pmf, we can use the <a href="../associations/#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator.</p><pre><code class="language- hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x, y = rand(rng, 1000), rand(rng, 1000)
est = JointProbabilities(KLDivergence(), OrdinalPatterns(m=2))
information(est, x, y) # should be close to 0</code></pre><p>Divergences are examples of <em>asymmetric</em> information measures, which we can see by  flipping the order of the input data.</p><pre><code class="language- hljs">information(est, y, x)</code></pre><h2 id="Conditional-entropies"><a class="docs-heading-anchor" href="#Conditional-entropies">Conditional entropies</a><a id="Conditional-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropies" title="Permalink"></a></h2><p><a href="../associations/#conditional_entropies">Conditional entropies</a> are another example of asymmetric information measures. They all have in common that  they are functions of a joint pmf, and can therefore also be estimated using the <a href="../associations/#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator. This time, we&#39;ll use a rectangular binning with 3 bins along each dimension to discretize the data.</p><pre><code class="language- hljs">x, y = randn(rng, 1000), randn(rng, 1000)
est = JointProbabilities(ConditionalEntropyShannon(base = 2), ValueBinning(3))
information(est, x, y)</code></pre><h2 id="Joint-entropies"><a class="docs-heading-anchor" href="#Joint-entropies">Joint entropies</a><a id="Joint-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-entropies" title="Permalink"></a></h2><p><a href="../associations/#joint_entropies">Joint entropies</a>, on the other hand, are <em>symmetric</em>. Joint entropies are functionals of a joint pmf, so we can still use the <a href="../associations/#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator. This time, we use a <a href="../discretization_tutorial/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a> based discretization.</p><pre><code class="language- hljs">x, y = randn(rng, 1000), randn(rng, 1000)
est = JointProbabilities(JointEntropyShannon(base = 2), Dispersion())
information(est, x, y) == information(est, y, x) # should be true</code></pre><h2 id="Mutual-informations"><a class="docs-heading-anchor" href="#Mutual-informations">Mutual informations</a><a id="Mutual-informations-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-informations" title="Permalink"></a></h2><p>Mutual informations, in particular <a href="../associations/#CausalityTools.MIShannon"><code>MIShannon</code></a> is an often-used symmetric  measure for quantifing the (possibly nonlinear) association between variables. It appears in both  discrete and differential form, and can be estimated in a multitude of ways. For  example, one can use dedicated <a href="../associations/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s such as  <a href="@ref"><code>KSG2</code></a> or <a href="../associations/#CausalityTools.GaussianMI"><code>GaussianMI</code></a>:</p><pre><code class="language- hljs">x, y = randn(rng, 1000), randn(rng, 1000)
est = KSG1(MIShannon(base = 2), k = 10)
information(est, x, y)</code></pre><p>The result should be symmetric:</p><pre><code class="language- hljs">information(est, x, y) == information(est, y, x) # should be true</code></pre><p>One can also estimate mutual information using the <a href="../associations/#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>  estimator, or (like above) using the <a href="../associations/#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator. Let&#39;s construct a differential entropy based estimator based on the <a href="../associations/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimator.</p><pre><code class="language-julia hljs">est_diff = EntropyDecomposition(MIShannon(base = 2), Kraskov(Shannon(), k=10))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr1">EntropyDecomposition estimator</span>

  Formula: Iₛ(X, Y) = hₛ(X) + hₛ(Y) - hₛ(X, Y)

<span class="sgr31"> Measure to be decomposed: </span><span class="sgr91">MIShannon{Int64}</span>
<span class="sgr32"> Estimator for components: </span><span class="sgr92">Kraskov{Shannon{Int64}, Int64}</span></code></pre><p>We can also construct a discrete entropy based estimator based on e.g. <a href="../associations/#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator of <a href="../associations/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><pre><code class="language- hljs">est_disc = EntropyDecomposition(MIShannon(base = 2), PlugIn(Shannon()), ValueBinning(2))</code></pre><p>These estimators use different estimation methods, so give different results:</p><pre><code class="language- hljs">information(est_diff, x, y)</code></pre><pre><code class="language- hljs">information(est_disc, x, y)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities_tutorial/">« Counts and probabilities</a><a class="docs-footer-nextpage" href="../references/">References »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Wednesday 24 July 2024 01:54">Wednesday 24 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
