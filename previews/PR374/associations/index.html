<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Association measures · CausalityTools.jl</title><meta name="title" content="Association measures · CausalityTools.jl"/><meta property="og:title" content="Association measures · CausalityTools.jl"/><meta property="twitter:title" content="Association measures · CausalityTools.jl"/><meta name="description" content="Documentation for CausalityTools.jl."/><meta property="og:description" content="Documentation for CausalityTools.jl."/><meta property="twitter:description" content="Documentation for CausalityTools.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="CausalityTools.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="CausalityTools.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">CausalityTools.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">CausalityTools.jl</a></li><li class="is-active"><a class="tocitem" href>Association measures</a><ul class="internal"><li><a class="tocitem" href="#Associations-API"><span>Associations API</span></a></li><li><a class="tocitem" href="#information_api"><span>Information measures</span></a></li><li><a class="tocitem" href="#correlation_api"><span>Correlation measures</span></a></li><li><a class="tocitem" href="#cross_map_api"><span>Cross-map measures</span></a></li><li><a class="tocitem" href="#closeness_api"><span>Closeness measures</span></a></li><li><a class="tocitem" href="#Recurrence-measures"><span>Recurrence measures</span></a></li></ul></li><li><a class="tocitem" href="../independence/">Independence</a></li><li><a class="tocitem" href="../causal_graphs/">Network/graph inference</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/examples_associations/">Association measures</a></li><li><a class="tocitem" href="../examples/examples_infer_graphs/">Graph inference</a></li><li><span class="tocitem">Extended examples</span><ul><li><a class="tocitem" href="../extended_examples/cross_mapping/"><code>ConvergentCrossMapping</code></a></li><li><a class="tocitem" href="../extended_examples/pairwise_asymmetric_inference/"><code>PairwiseAsymmetricInference</code></a></li><li><a class="tocitem" href="../extended_examples/mutual_information/"><code>MIShannon</code></a></li></ul></li></ul></li><li><span class="tocitem">Basics and tutorials</span><ul><li><a class="tocitem" href="../encoding_tutorial/">Encoding elements</a></li><li><a class="tocitem" href="../discretization_tutorial/">Encoding input datasets</a></li><li><a class="tocitem" href="../probabilities_tutorial/">Counts and probabilities</a></li><li><a class="tocitem" href="../info_tutorial/">Information measures</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Association measures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Association measures</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/CausalityTools.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/main/docs/src/associations.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Association-measures"><a class="docs-heading-anchor" href="#Association-measures">Association measures</a><a id="Association-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Association-measures" title="Permalink"></a></h1><h2 id="Associations-API"><a class="docs-heading-anchor" href="#Associations-API">Associations API</a><a id="Associations-API-1"></a><a class="docs-heading-anchor-permalink" href="#Associations-API" title="Permalink"></a></h2><p>The most basic components of CausalityTools.jl are a collection of statistics that in some manner quantify the &quot;association&quot; between input datasets. Precisely what is meant by &quot;association&quot; depends on the measure, and precisely what is meant by &quot;quantify&quot; depends on the <em>estimator</em> of that measure. We formalize this notion below with the <a href="#CausalityTools.association"><code>association</code></a> function, which dispatches on <a href="#CausalityTools.AssociationMeasureEstimator"><code>AssociationMeasureEstimator</code></a> and <a href="#CausalityTools.AssociationMeasure"><code>AssociationMeasure</code></a>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.association" href="#CausalityTools.association"><code>CausalityTools.association</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">association(estimator::AssociationMeasureEstimator, x, y, [z, ...]) → r
association(definition::AssociationMeasure, x, y, [z, ...]) → r</code></pre><p>Estimate the (conditional) association between input variables <code>x, y, z, …</code> using  the given <code>estimator</code> (an <a href="#CausalityTools.AssociationMeasureEstimator"><code>AssociationMeasureEstimator</code></a>) or <code>definition</code> (an <a href="#CausalityTools.AssociationMeasure"><code>AssociationMeasure</code></a>).  </p><p>The type of the return value <code>r</code> depends on the <code>measure</code>/<code>estimator</code>.</p><p><strong>Examples</strong></p><p>Some <a href="#CausalityTools.AssociationMeasure"><code>AssociationMeasure</code></a>s have no estimators and are given directly. For other association measures, you need to provide an <a href="#CausalityTools.AssociationMeasureEstimator"><code>AssociationMeasureEstimator</code></a>  of some kind, which takes the definition as its first argument.</p><pre><code class="language-julia hljs">using CausalityTools
x, y, z = rand(1000), rand(1000), rand(1000)

# Pairwise association using different measures
association(DistanceCorrelation(), x, y)
association(PartialCorrelation(), x, y)
association(JointProbabilities(ConditionalEntropyTsallisAbe(), ValueBinning(3)), x, y)
association(JointProbabilities(JointEntropyShannon(), Dispersion(c = 3, m = 2)), x, y)
association(EntropyDecomposition(MIShannon(), PlugIn(Shannon()), OrdinalPatterns(m=3)), x, y)
association(KSG2(MIShannon(base = 2)), x, y)

# Conditional association using different measures
association(JointProbabilities(PartialMutualInformation(), OrdinalPatterns(m=3)), x, y, z)
association(FPVP(CMIShannon(base = 2)), x, y, z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/core.jl#L132-L164">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.AssociationMeasure" href="#CausalityTools.AssociationMeasure"><code>CausalityTools.AssociationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AssociationMeasure</code></pre><p>The supertype of all association measures. </p><p><strong>Abstract implementations</strong></p><p>Currently, the association measures are classified by abstract classes listed below. These abstract classes offer common functionality among association measures that are  conceptually similar. This makes maintenance and framework extension easier than  if each measure was implemented &quot;in isolation&quot;.</p><ul><li><a href="#CausalityTools.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a></li><li><a href="#CausalityTools.CrossmapMeasure"><code>CrossmapMeasure</code></a></li><li><a href="@ref"><code>ClosenessMeasure</code></a></li><li><a href="@ref"><code>CorrelationMeasure</code></a></li></ul><p><strong>Concrete implementations</strong></p><p>Concrete subtypes are given as input to <a href="#CausalityTools.association"><code>association</code></a>.</p><table><tr><th style="text-align: right">Type</th><th style="text-align: right"><a href="#CausalityTools.AssociationMeasure"><code>AssociationMeasure</code></a></th><th style="text-align: center">Pairwise</th><th style="text-align: center">Conditional</th></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#CausalityTools.PearsonCorrelation"><code>PearsonCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Correlation</td><td style="text-align: right"><a href="#CausalityTools.DistanceCorrelation"><code>DistanceCorrelation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.SMeasure"><code>SMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.HMeasure"><code>HMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.MMeasure"><code>MMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness (ranks)</td><td style="text-align: right"><a href="#CausalityTools.LMeasure"><code>LMeasure</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Closeness</td><td style="text-align: right"><a href="#CausalityTools.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Cross-mapping</td><td style="text-align: right"><a href="#CausalityTools.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Cross-mapping</td><td style="text-align: right"><a href="#CausalityTools.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Conditional recurrence</td><td style="text-align: right"><a href="#CausalityTools.MCR"><code>MCR</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Conditional recurrence</td><td style="text-align: right"><a href="#CausalityTools.RMCD"><code>RMCD</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.PartialCorrelation"><code>PartialCorrelation</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Shared information</td><td style="text-align: right"><a href="#CausalityTools.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Information transfer</td><td style="text-align: right"><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Information transfer</td><td style="text-align: right"><a href="#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Part mutual information</td><td style="text-align: right"><a href="@ref"><code>PMI</code></a></td><td style="text-align: center">✖</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Information asymmetry</td><td style="text-align: right"><a href="@ref"><code>PA</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#CausalityTools.JointEntropyShannon"><code>JointEntropyShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#CausalityTools.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#CausalityTools.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#CausalityTools.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#CausalityTools.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Information measure</td><td style="text-align: right"><a href="#CausalityTools.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#CausalityTools.HellingerDistance"><code>HellingerDistance</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#CausalityTools.KLDivergence"><code>KLDivergence</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#CausalityTools.RenyiDivergence"><code>RenyiDivergence</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right">Divergence</td><td style="text-align: right"><a href="#CausalityTools.VariationDistance"><code>VariationDistance</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr></table><p>See also: <a href="#CausalityTools.AssociationMeasureEstimator"><code>AssociationMeasureEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/core.jl#L13-L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.AssociationMeasureEstimator" href="#CausalityTools.AssociationMeasureEstimator"><code>CausalityTools.AssociationMeasureEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AssociationMeasureEstimator</code></pre><p>The supertype of all association measure estimators.</p><p>Concrete subtypes are given as input to <a href="#CausalityTools.association"><code>association</code></a>.</p><p><strong>Abstract subtypes</strong></p><ul><li><a href="#CausalityTools.MultivariateInformationMeasureEstimator"><code>MultivariateInformationMeasureEstimator</code></a></li><li><a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a></li></ul><p><strong>Concrete implementations</strong></p><table><tr><th style="text-align: left">AssociationMeasure</th><th style="text-align: left">Estimators</th></tr><tr><td style="text-align: left"><a href="#CausalityTools.PearsonCorrelation"><code>PearsonCorrelation</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.DistanceCorrelation"><code>DistanceCorrelation</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.PartialCorrelation"><code>PartialCorrelation</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.SMeasure"><code>SMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.HMeasure"><code>HMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.MMeasure"><code>MMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.LMeasure"><code>LMeasure</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a></td><td style="text-align: left"><a href="#CausalityTools.RandomVectors"><code>RandomVectors</code></a>, <a href="#CausalityTools.RandomSegment"><code>RandomSegment</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a></td><td style="text-align: left"><a href="#CausalityTools.RandomVectors"><code>RandomVectors</code></a>, <a href="#CausalityTools.RandomSegment"><code>RandomSegment</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.MCR"><code>MCR</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.RMCD"><code>RMCD</code></a></td><td style="text-align: left">Not required</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>, <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>, <a href="@ref"><code>KSG1</code></a>, <a href="@ref"><code>KSG2</code></a>, <a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a>, <a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a>, <a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>, <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>, <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>, <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>, <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>, <a href="#CausalityTools.MIDecomposition"><code>MIDecomposition</code></a>, <a href="#CausalityTools.GaussianCMI"><code>GaussianCMI</code></a>, <a href="#CausalityTools.FPVP"><code>FPVP</code></a>, <a href="#CausalityTools.MesnerShalizi"><code>MesnerShalizi</code></a>, <a href="#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>, <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></td><td style="text-align: left"><a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>, <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>, <a href="#CausalityTools.Zhu1"><code>Zhu1</code></a>, <a href="#CausalityTools.Lindner"><code>Lindner</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="@ref"><code>PMI</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="@ref"><code>PA</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.JointEntropyShannon"><code>JointEntropyShannon</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.HellingerDistance"><code>HellingerDistance</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.KLDivergence"><code>KLDivergence</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.RenyiDivergence"><code>RenyiDivergence</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr><tr><td style="text-align: left"><a href="#CausalityTools.VariationDistance"><code>VariationDistance</code></a></td><td style="text-align: left"><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/core.jl#L76-L129">source</a></section></article><h2 id="information_api"><a class="docs-heading-anchor" href="#information_api">Information measures</a><a id="information_api-1"></a><a class="docs-heading-anchor-permalink" href="#information_api" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MultivariateInformationMeasure" href="#CausalityTools.MultivariateInformationMeasure"><code>CausalityTools.MultivariateInformationMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MultivariateInformationMeasure &lt;: AssociationMeasure</code></pre><p>The supertype for all multivariate information-based measure definitions.</p><p><strong>Definition</strong></p><p>Following <a href="../references/#Datseris2024">Datseris and Haaga (2024)</a>, we define a multivariate information measure as <em>any functional  of a multidimensional probability mass functions (PMFs) or multidimensional probability density</em>.</p><p><strong>Implementations</strong></p><p><a href="#CausalityTools.JointEntropy"><code>JointEntropy</code></a> definitions:</p><ul><li><a href="#CausalityTools.JointEntropyShannon"><code>JointEntropyShannon</code></a></li><li><a href="#CausalityTools.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></li><li><a href="#CausalityTools.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></li></ul><p><a href="#CausalityTools.ConditionalEntropy"><code>ConditionalEntropy</code></a> definitions:</p><ul><li><a href="#CausalityTools.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></li><li><a href="#CausalityTools.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></li><li><a href="#CausalityTools.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></li></ul><p><a href="@ref"><code>DivergenceOrDistance</code></a> definitions:</p><ul><li><a href="#CausalityTools.HellingerDistance"><code>HellingerDistance</code></a></li><li><a href="#CausalityTools.KLDivergence"><code>KLDivergence</code></a></li><li><a href="#CausalityTools.RenyiDivergence"><code>RenyiDivergence</code></a></li><li><a href="#CausalityTools.VariationDistance"><code>VariationDistance</code></a></li></ul><p><a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a> definitions:</p><ul><li><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></li><li><a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></li><li><a href="@ref"><code>MIRenyiMartin</code></a></li><li><a href="@ref"><code>MITsallisAbe</code></a></li><li><a href="@ref"><code>MITsallisFuruchi</code></a></li></ul><p><a href="#CausalityTools.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> definitions:</p><ul><li><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></li><li><a href="#CausalityTools.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></li><li><a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></li><li><a href="#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></li><li><a href="#CausalityTools.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></li></ul><p><a href="#CausalityTools.TransferEntropy"><code>TransferEntropy</code></a> definitions:</p><ul><li><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></li><li><a href="#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a></li></ul><p>Other definitions:</p><ul><li><a href="#CausalityTools.PartialMutualInformation"><code>PartialMutualInformation</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/core.jl#L82-L137">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MultivariateInformationMeasureEstimator" href="#CausalityTools.MultivariateInformationMeasureEstimator"><code>CausalityTools.MultivariateInformationMeasureEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MultivariateInformationMeasureEstimator</code></pre><p>The supertype for all estimators of multivariate information measures.</p><p><strong>Generic implementations</strong></p><ul><li><a href="@ref"><code>JointProbababilities</code></a></li><li><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></li><li><a href="#CausalityTools.MIDecomposition"><code>MIDecomposition</code></a></li><li><a href="#CausalityTools.CMIDecomposition"><code>CMIDecomposition</code></a></li></ul><p><strong>Dedicated implementations</strong></p><p><a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s:</p><ul><li><a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a></li><li><a href="#CausalityTools.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a></li><li><a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></li><li><a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></li><li><a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a></li></ul><p><a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>s:</p><ul><li><a href="#CausalityTools.FPVP"><code>FPVP</code></a></li><li><a href="#CausalityTools.MesnerShalizi"><code>MesnerShalizi</code></a></li><li><a href="#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a></li><li><a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li><li><a href="#CausalityTools.GaussianCMI"><code>GaussianCMI</code></a></li></ul><p><a href="#CausalityTools.TransferEntropyEstimator"><code>TransferEntropyEstimator</code></a>s:</p><ul><li><a href="#CausalityTools.Zhu1"><code>Zhu1</code></a></li><li><a href="#CausalityTools.Lindner"><code>Lindner</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/core.jl#L14-L48">source</a></section></article><h3 id="Generic-information-estimators"><a class="docs-heading-anchor" href="#Generic-information-estimators">Generic information estimators</a><a id="Generic-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Generic-information-estimators" title="Permalink"></a></h3><p>We provide a set of generic estimators that can be used to calculate  potentially several types of information measures.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.JointProbabilities" href="#CausalityTools.JointProbabilities"><code>CausalityTools.JointProbabilities</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointProbabilities &lt;: InformationMeasureEstimator
JointProbabilities(
    definition::MultivariateInformationMeasure,
    discretization::Discretization
)</code></pre><p><code>JointProbabilities</code> is a generic estimator for multivariate discrete information measures.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute an information measure from input data.</li></ul><p><strong>Description</strong></p><p>It first encodes the input data according to the given <code>discretization</code>, then constructs  <code>probs</code>, a multidimensional <a href="../probabilities_tutorial/#Probabilities"><code>Probabilities</code></a> instance. Finally, <code>probs</code> are  forwarded to a <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, which computes the measure according to  <code>definition</code>.</p><p><strong>Compatible encoding schemes</strong></p><ul><li><a href="../discretization_tutorial/#CausalityTools.CodifyVariables"><code>CodifyVariables</code></a> (encode each <em>variable</em>/column of the input data independently by    applying an encoding in a sliding window over each input variable).  </li><li><a href="../discretization_tutorial/#CausalityTools.CodifyPoints"><code>CodifyPoints</code></a> (encode each <em>point</em>/column of the input data)</li></ul><p>Works for any <a href="@ref"><code>OutcomeSpace</code></a> that implements <a href="../discretization_tutorial/#ComplexityMeasures.codify"><code>codify</code></a>.</p><div class="admonition is-info"><header class="admonition-header">Joint probabilities vs decomposition methods</header><div class="admonition-body"><p>Using <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> to compute an information measure,  e.g. conditional mutual estimation, is typically slower than other dedicated estimation procedures like <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>. The reason is that measures such as <a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a> can be formulated as a sum of four entropies, which can be estimated individually and summed afterwards.  This decomposition is fast because because we avoid <em>explicitly</em> estimating the entire joint pmf,  which demands many extra calculation steps, However, the decomposition is biased,  because it fails to fully take into consideration the joint relationships between the variables. Pick your estimator according to your needs.</p></div></div><p>See also: <a href="../probabilities_tutorial/#Counts"><code>Counts</code></a>, <a href="../probabilities_tutorial/#Probabilities"><code>Probabilities</code></a>, <a href="@ref"><code>ProbabilitiesEstimator</code></a>, <a href="@ref"><code>OutcomeSpace</code></a>, <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/JointProbabilities.jl#L4-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.EntropyDecomposition" href="#CausalityTools.EntropyDecomposition"><code>CausalityTools.EntropyDecomposition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EntropyDecomposition(definition::MultivariateInformationMeasure, 
    est::DifferentialInfoEstimator)
EntropyDecomposition(definition::MultivariateInformationMeasure,
    est::DiscreteInfoEstimator,
    discretization::CodifyVariables{&lt;:OutcomeSpace},
    pest::ProbabilitiesEstimator = RelativeAmount())</code></pre><p>Estimate the multivariate information measure specified by <code>definition</code> by rewriting its formula into some combination of entropy terms. </p><p>If calling the second method (discrete variant), then discretization is always done  per variable/column and each column is encoded into integers using <a href="../discretization_tutorial/#ComplexityMeasures.codify"><code>codify</code></a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute a <a href="#CausalityTools.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>   from input data.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>The entropy terms are estimated using <code>est</code>, and then combined to form the final  estimate of <code>definition</code>. No bias correction is applied. If <code>est</code> is a <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>, then <code>discretization</code> and <code>pest</code>  are ignored. If <code>est</code> is a <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>, then <code>discretization</code> and a probabilities estimator <code>pest</code> must also be provided (default to <code>RelativeAmount</code>,  which uses naive plug-in probabilities).</p><p><strong>Compatible differential information estimators</strong></p><p>If using the first signature, any compatible <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> can be  used.</p><p><strong>Compatible outcome spaces for discrete estimation</strong></p><p>If using the second signature, the outcome spaces can be used for discretisation.  Note that not all outcome spaces will work with all measures.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Principle</th><th style="text-align: left">Note</th></tr><tr><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a></td><td style="text-align: left">Count of unique elements</td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a></td><td style="text-align: left">Binning (histogram)</td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a></td><td style="text-align: left">Ordinal patterns</td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: left">Dispersion patterns</td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.CosineSimilarityBinning"><code>CosineSimilarityBinning</code></a></td><td style="text-align: left">Cosine similarities histogram</td><td style="text-align: left"></td></tr><tr><td style="text-align: left"><a href="@ref"><code>TransferOperator</code></a></td><td style="text-align: left">Transfer operator on rectangular bins</td><td style="text-align: left"><code>binning.precise</code> must be <code>true</code></td></tr></table><p><strong>Bias</strong></p><p>Estimating the <code>definition</code> by decomposition into a combination of entropy terms, which are estimated independently, will in general be more biased than when using a dedicated estimator. One reason is that this decomposition may miss out on crucial information in the joint space. To remedy this, dedicated information measure  estimators typically derive the marginal estimates by first considering the joint space, and then does some clever trick to eliminate the bias that is introduced through a naive decomposition. Unless specified below, no bias correction is  applied for <code>EntropyDecomposition</code>.</p><p><strong>Handling of overlapping parameters</strong></p><p>If there are overlapping parameters between the measure to be estimated, and the lower-level decomposed measures, then the top-level measure parameter takes precedence. For example, if we want to estimate <code>CMIShannon(base = 2)</code> through a decomposition  of entropies using the <code>Kraskov(Shannon(base = ℯ))</code> Shannon entropy estimator, then <code>base = 2</code> is used.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Not all measures have the property that they can be decomposed into more fundamental information theoretic quantities. For example, <a href="#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a> <em>can</em> be  decomposed into a combination of marginal entropies, while <a href="#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a> cannot. An error will be thrown if decomposition is not possible.</p></div></div><p><strong>Discrete entropy decomposition</strong></p><p>The second signature is for discrete estimation using <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s, for example <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a>. The given <code>discretization</code> scheme (typically an  <a href="@ref"><code>OutcomeSpace</code></a>) controls how the joint/marginals are discretized, and the probabilities estimator <code>pest</code> controls how probabilities are estimated from counts.</p><div class="admonition is-info"><header class="admonition-header">Bias</header><div class="admonition-body"><p>Like for <a href="@ref"><code>DifferentialDecomposition</code></a>, using a dedicated estimator  for the measure in question will be more reliable than using a decomposition estimate. Here&#39;s how different <code>discretization</code>s are applied:</p><ul><li><a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a>. Bin visitation frequencies are counted in the joint space   <code>XY</code>, then marginal visitations are obtained from the joint bin visits.   This behaviour is the same for both <a href="../discretization_tutorial/#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> and   <a href="../discretization_tutorial/#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a> (which adapts the grid to the data).   When using <a href="../discretization_tutorial/#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a>, the range along the first dimension   is used as a template for all other dimensions. This is a bit slower than naively    binning each marginal, but lessens bias.</li><li><a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a>. Each timeseries is separately <a href="../discretization_tutorial/#ComplexityMeasures.codify"><code>codify</code></a>-ed   according to its ordinal pattern (no bias correction).</li><li><a href="../discretization_tutorial/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a>. Each timeseries is separately <a href="../discretization_tutorial/#ComplexityMeasures.codify"><code>codify</code></a>-ed according   to its dispersion pattern  (no bias correction).</li></ul></div></div><p><strong>Examples</strong></p><p>Both Shannon-type mutual information and conditional mutual information can be  written as a sum of marginal entropy terms. First a discrete example for mutual  information:</p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)

x = StateSpaceSet(rand(rng, 1000000, 2))
y = StateSpaceSet(rand(rng, 1000000, 2))
# Compute Shannon mutual information by discretizing each marginal column-wise
# (per variable) using length-`3` ordinal patterns.
est = EntropyDecomposition(MIShannon(), PlugIn(Shannon()), OrdinalPatterns(m=3))
association(est, x, y) # should be close to 0</code></pre><p>Here, we estimate Shannon-type conditional mutual information using the <code>ZhuSingh</code> entropy estimator.</p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 100000)
y = rand(rng, 100000) .+ x
z = rand(rng, 100000) .+ y

est = EntropyDecomposition(CMIShannon(), ZhuSingh(k = 3))
association(est, x, z, y) # should be near 0 (and can be negative)</code></pre><p>See also: <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>, <a href="#CausalityTools.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/decomposition/EntropyDecomposition.jl#L4-L135">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MIDecomposition" href="#CausalityTools.MIDecomposition"><code>CausalityTools.MIDecomposition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIDecomposition(definition::MultivariateInformationMeasure, 
    est::MutualInformationEstimator)</code></pre><p>Estimate the <a href="#CausalityTools.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a> specified by <code>definition</code> by by decomposing, the measure, if possible, into a combination of mutual information terms. These terms are individually estimated using the given <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> <code>est</code>, and finally combined to form the final  value of the measure. </p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute a <a href="#CausalityTools.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>   from input data.</li></ul><p><strong>Examples</strong></p><p>One common application is computing Shannon-type conditional mutual information. It can be decomposed as a sum of mutual information terms, which we can each  estimate with any dedicated <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> estimator.</p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 100000)
y = rand(rng, 100000) .+ x
z = rand(rng, 100000) .+ y

est = MIDecomposition(CMIShannon(), KSG1(MIShannon(base = 2), k = 3))
association(est, x, z, y) # should be near 0 (and can be negative)</code></pre><p>See also: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/decomposition/MIDecomposition.jl#L4-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CMIDecomposition" href="#CausalityTools.CMIDecomposition"><code>CausalityTools.CMIDecomposition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIDecomposition(definition::MultivariateInformationMeasure, 
    est::ConditionalMutualInformationEstimator)</code></pre><p>Estimate some multivariate information measure specified by <code>definition</code>, by decomposing it into a combination of conditional mutual information terms. Each of these  terms are then estimated using <code>est</code>, which can be any  <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>. Finally, these estimates are combined according to the relevant decomposition formula.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute a <a href="#CausalityTools.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>   from input data.</li></ul><p><strong>Description</strong></p><p>Similar to <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>, but <code>definition</code> is expressed as  conditional mutual information terms instead of entropy terms.</p><p><strong>Usage</strong></p><ul><li><a href="@ref"><code>information</code></a><code>(est::CMIDecomposition, x...)</code>.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 100000)
y = rand(rng, 100000) .+ x
z = rand(rng, 100000) .+ y

# Estimate transfer entropy by representing it as a CMI and using the `FPVP` estimator.
est = CMIDecomposition(TEShannon(base = 2), FPVP(k = 3))
association(est, x, z, y) # should be near 0 (and can be negative)</code></pre><p>See also: <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>, <a href="#CausalityTools.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/decomposition/CMIDecomposition.jl#L3-L42">source</a></section></article><h3 id="conditional_entropies"><a class="docs-heading-anchor" href="#conditional_entropies">Conditional entropies</a><a id="conditional_entropies-1"></a><a class="docs-heading-anchor-permalink" href="#conditional_entropies" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ConditionalEntropy" href="#CausalityTools.ConditionalEntropy"><code>CausalityTools.ConditionalEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropy &lt;: MultivariateInformationMeasure</code></pre><p>The supertype for all conditional entropy measures.</p><p><strong>Concrete subtypes</strong></p><ul><li><a href="#CausalityTools.ConditionalEntropyShannon"><code>ConditionalEntropyShannon</code></a></li><li><a href="#CausalityTools.ConditionalEntropyTsallisAbe"><code>ConditionalEntropyTsallisAbe</code></a></li><li><a href="#CausalityTools.ConditionalEntropyTsallisFuruichi"><code>ConditionalEntropyTsallisFuruichi</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_entropies/conditional_entropies.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ConditionalEntropyShannon" href="#CausalityTools.ConditionalEntropyShannon"><code>CausalityTools.ConditionalEntropyShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropyShannon &lt;: ConditionalEntropy
ConditionalEntropyShannon(; base = 2)</code></pre><p>The <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> conditional entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the Shannon conditional entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Discrete definition</strong></p><p><strong>Sum formulation</strong></p><p>The conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H^{S}(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log(p(x | y)).\]</p><p>This is the definition used when calling <a href="@ref"><code>entropy_conditional</code></a> with a <a href="@ref"><code>ContingencyMatrix</code></a>.</p><p><strong>Two-entropies formulation</strong></p><p>Equivalently, the following differenConditionalEntropy of entropies hold</p><p class="math-container">\[H^S(X | Y) = H^S(X, Y) - H^S(Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot | \cdot)$</span> are the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy and Shannon joint entropy, respectively. This is the definition used when calling <a href="@ref"><code>entropy_conditional</code></a> with a <a href="@ref"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Differential definition</strong></p><p>The differential conditional Shannon entropy is analogously defined as</p><p class="math-container">\[H^S(X | Y) = h^S(X, Y) - h^S(Y),\]</p><p>where <span>$h^S(\cdot)$</span> and <span>$h^S(\cdot | \cdot)$</span> are the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling <a href="@ref"><code>entropy_conditional</code></a> with a <a href="@ref"><code>DifferentialEntropyEstimator</code></a>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConditionalEntropyShannon_analytical">Example 1</a>: Analytical example from Cover &amp; Thomas&#39;s book.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyShannon_JointProbabilities_CodifyVariables_UniqueElements">Example 2</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator with<a href="../discretization_tutorial/#CausalityTools.CodifyVariables"><code>CodifyVariables</code></a> discretization and    <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on categorical data.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyShannon_JointProbabilities_CodifyPoints_UniqueElementsEncoding">Example 3</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator with <a href="../discretization_tutorial/#CausalityTools.CodifyPoints"><code>CodifyPoints</code></a> discretization and <a href="../encoding_tutorial/#ComplexityMeasures.UniqueElementsEncoding"><code>UniqueElementsEncoding</code></a>   encoding of points on numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_entropies/ConditionalEntropyShannon.jl#L6-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ConditionalEntropyTsallisFuruichi" href="#CausalityTools.ConditionalEntropyTsallisFuruichi"><code>CausalityTools.ConditionalEntropyTsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropyTsallisFuruichi &lt;: ConditionalEntropy
ConditionalEntropyTsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>Furuichi (2006)&#39;s discrete Tsallis conditional entropy definition.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the Tsallis-Furuichi conditional entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Furuichi&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}}
p(x, y)^q \log_q(p(x | y)),\]</p><p><span>$\ln_q(x) = \frac{x^{1-q} - 1}{1 - q}$</span> and <span>$q \neq 1$</span>. For <span>$q = 1$</span>, <span>$H_q^T(X | Y)$</span> reduces to the Shannon conditional entropy:</p><p class="math-container">\[H_{q=1}^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} =
p(x, y) \log(p(x | y))\]</p><p>If any of the entries of the marginal distribution for <code>Y</code> are zero, or the q-logarithm  is undefined for a particular value, then the measure is undefined and <code>NaN</code> is returned.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisFuruichi_JointProbabilities_CodifyVariables_UniqueElements">Example 1</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator with<a href="../discretization_tutorial/#CausalityTools.CodifyVariables"><code>CodifyVariables</code></a> discretization and    <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on categorical data.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisFuruichi_JointProbabilities_CodifyPoints_UniqueElementsEncoding">Example 2</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator with <a href="../discretization_tutorial/#CausalityTools.CodifyPoints"><code>CodifyPoints</code></a> discretization and <a href="../encoding_tutorial/#ComplexityMeasures.UniqueElementsEncoding"><code>UniqueElementsEncoding</code></a>   encoding of points on numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_entropies/ConditionalEntropyTsallisFuruichi.jl#L3-L47">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ConditionalEntropyTsallisAbe" href="#CausalityTools.ConditionalEntropyTsallisAbe"><code>CausalityTools.ConditionalEntropyTsallisAbe</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalEntropyTsallisAbe &lt;: ConditionalEntropy
ConditionalEntropyTsallisAbe(; base = 2, q = 1.5)</code></pre><p><a href="../references/#Abe2001">Abe and Rajagopal (2001)</a>&#39;s discrete Tsallis conditional entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the Tsallis-Abe conditional entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Abe &amp; Rajagopal&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^{T_A}(X | Y) = \dfrac{H_q^T(X, Y) - H_q^T(Y)}{1 + (1-q)H_q^T(Y)},\]</p><p>where <span>$H_q^T(\cdot)$</span> and <span>$H_q^T(\cdot, \cdot)$</span> is the <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a> entropy and the joint Tsallis entropy.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisAbe_JointProbabilities_CodifyVariables_UniqueElements">Example 1</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator with<a href="../discretization_tutorial/#CausalityTools.CodifyVariables"><code>CodifyVariables</code></a> discretization and    <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on categorical data.</li><li><a href="../examples/examples_associations/#example_ConditionalEntropyTsallisAbe_JointProbabilities_CodifyPoints_UniqueElementsEncoding">Example 2</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> estimator with <a href="../discretization_tutorial/#CausalityTools.CodifyPoints"><code>CodifyPoints</code></a> discretization and <a href="../encoding_tutorial/#ComplexityMeasures.UniqueElementsEncoding"><code>UniqueElementsEncoding</code></a>   encoding of points on numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_entropies/ConditionalEntropyTsallisAbe.jl#L5-L40">source</a></section></article><h3 id="divergences_and_distances"><a class="docs-heading-anchor" href="#divergences_and_distances">Divergences and distances</a><a id="divergences_and_distances-1"></a><a class="docs-heading-anchor-permalink" href="#divergences_and_distances" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.HellingerDistance" href="#CausalityTools.HellingerDistance"><code>CausalityTools.HellingerDistance</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HellingerDistance &lt;: DivergenceOrDistance</code></pre><p>The Hellinger distance.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the compute the Hellinger distance between two pre-computed   probability distributions, or from raw data using one of the estimators listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Description</strong></p><p>The Hellinger distance between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="@ref"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is <a href="https://en.wikipedia.org/wiki/Hellinger_distance">defined</a> as</p><p class="math-container">\[D_{H}(P_Y(\Omega) || P_Y(\Omega)) =
\dfrac{1}{\sqrt{2}} \sum_{\omega \in \Omega} (\sqrt{p_x(\omega)} - \sqrt{p_y(\omega)})^2\]</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_HellingerDistance_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_HellingerDistance_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/divergences_and_distances/HellingerDistance.jl#L3-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.KLDivergence" href="#CausalityTools.KLDivergence"><code>CausalityTools.KLDivergence</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KLDivergence &lt;: DivergenceOrDistance</code></pre><p>The Kullback-Leibler (KL) divergence.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the compute the KL-divergence between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></li></ul><p><strong>Estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>.</li></ul><p><strong>Description</strong></p><p>The KL-divergence between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="@ref"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is defined as</p><p class="math-container">\[D_{KL}(P_Y(\Omega) || P_Y(\Omega)) =
\sum_{\omega \in \Omega} p_x(\omega) \log\dfrac{p_x(\omega)}{p_y(\omega)}\]</p><p><strong>Implements</strong></p><ul><li><a href="#CausalityTools.association"><code>association</code></a>. Used to compute the KL-divergence between two pre-computed   probability distributions. If used with <a href="@ref"><code>RelativeAmount</code></a>, the KL divergence may   be undefined to due some outcomes having zero counts. Use some other   <a href="@ref"><code>ProbabilitiesEstimator</code></a> like <a href="@ref"><code>BayesianRegularization</code></a> to ensure   all estimated probabilities are nonzero.</li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Distances.jl also defines <code>KLDivergence</code>. Quality it if you&#39;re loading both  packages, i.e. do <code>association(CausalityTools.KLDivergence(), x, y)</code>.</p></div></div><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_KLDivergence_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_KLDivergence_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/divergences_and_distances/KLDivergence.jl#L3-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.RenyiDivergence" href="#CausalityTools.RenyiDivergence"><code>CausalityTools.RenyiDivergence</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RenyiDivergence &lt;: DivergenceOrDistance
RenyiDivergence(q; base = 2)</code></pre><p>The Rényi divergence of positive order <code>q</code>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the compute the Rényi divergence between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></li></ul><p><strong>Description</strong></p><p>The Rényi divergence between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="@ref"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is defined as <a href="../references/#vanErven2014">van Erven and Harremos (2014)</a>.</p><p class="math-container">\[D_{q}(P_Y(\Omega) || P_Y(\Omega)) =
\dfrac{1}{q - 1} \log \sum_{\omega \in \Omega}p_x(\omega)^{q}p_y(\omega)^{1-\alpha}\]</p><p><strong>Implements</strong></p><ul><li><a href="@ref"><code>information</code></a>. Used to compute the Rényi divergence between two pre-computed   probability distributions. If used with <a href="@ref"><code>RelativeAmount</code></a>, the KL divergence may   be undefined to due some outcomes having zero counts. Use some other   <a href="@ref"><code>ProbabilitiesEstimator</code></a> like <a href="@ref"><code>BayesianRegularization</code></a> to ensure   all estimated probabilities are nonzero.</li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Distances.jl also defines <code>RenyiDivergence</code>. Quality it if you&#39;re loading both  packages, i.e. do <code>association(CausalityTools.RenyiDivergence(), x, y)</code>.</p></div></div><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_RenyiDivergence_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_RenyiDivergence_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/divergences_and_distances/RenyiDivergence.jl#L3-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.VariationDistance" href="#CausalityTools.VariationDistance"><code>CausalityTools.VariationDistance</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">VariationDistance &lt;: DivergenceOrDistance</code></pre><p>The variation distance.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the compute the variation distance between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointDistanceDistribution"><code>JointDistanceDistribution</code></a></li></ul><p><strong>Description</strong></p><p>The variation distance between two probability distributions <span>$P_X = (p_x(\omega_1), \ldots, p_x(\omega_n))$</span> and <span>$P_Y = (p_y(\omega_1), \ldots, p_y(\omega_m))$</span>, both defined over the same <a href="@ref"><code>OutcomeSpace</code></a> <span>$\Omega = \{\omega_1, \ldots, \omega_n \}$</span>, is <a href="https://en.wikipedia.org/wiki/Variation_distance">defined</a> as</p><p class="math-container">\[D_{V}(P_Y(\Omega) || P_Y(\Omega)) =
\dfrac{1}{2} \sum_{\omega \in \Omega} | p_x(\omega) - p_y(\omega) |\]</p><p><strong>Examples</strong></p><ul><li><a href="../examples/examples_associations/#example_VariationDistance_precomputed_probabilities">Example 1</a>: From precomputed probabilities</li><li><a href="../examples/examples_associations/#example_VariationDistance_JointProbabilities_OrdinalPatterns">Example 2</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/divergences_and_distances/VariationDistance.jl#L3-L36">source</a></section></article><h3 id="joint_entropies"><a class="docs-heading-anchor" href="#joint_entropies">Joint entropies</a><a id="joint_entropies-1"></a><a class="docs-heading-anchor-permalink" href="#joint_entropies" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.JointEntropy" href="#CausalityTools.JointEntropy"><code>CausalityTools.JointEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropy &lt;: BivariateInformationMeasure</code></pre><p>The supertype for all joint entropy measures.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#CausalityTools.JointEntropyShannon"><code>JointEntropyShannon</code></a></li><li><a href="#CausalityTools.JointEntropyRenyi"><code>JointEntropyRenyi</code></a></li><li><a href="#CausalityTools.JointEntropyTsallis"><code>JointEntropyTsallis</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/joint_entropies/joint_entropies.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.JointEntropyShannon" href="#CausalityTools.JointEntropyShannon"><code>CausalityTools.JointEntropyShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropyShannon &lt;: JointEntropy
JointEntropyShannon(; base = 2)</code></pre><p>The Shannon joint entropy measure (<a href="../references/#CoverThomas1999">Cover, 1999</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the Shannon joint entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Given two two discrete random variables <span>$X$</span> and <span>$Y$</span> with ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{X}$</span>, <a href="../references/#CoverThomas1999">Cover (1999)</a> defines the Shannon joint entropy as</p><p class="math-container">\[H^S(X, Y) = -\sum_{x\in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y),\]</p><p>where we define <span>$log(p(x, y)) := 0$</span> if <span>$p(x, y) = 0$</span>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_JointEntropyShannon_Dispersion">Example 1</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/joint_entropies/JointEntropyShannon.jl#L5-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.JointEntropyTsallis" href="#CausalityTools.JointEntropyTsallis"><code>CausalityTools.JointEntropyTsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropyTsallis &lt;: JointEntropy
JointEntropyTsallis(; base = 2, q = 1.5)</code></pre><p>The Tsallis joint entropy definition from <a href="../references/#Furuichi2006">Furuichi (2006)</a>. </p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the Furuichi-Tsallis joint entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Given two two discrete random variables <span>$X$</span> and <span>$Y$</span> with ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{X}$</span>, <a href="../references/#Furuichi2006">Furuichi (2006)</a> defines the Tsallis joint entropy as</p><p class="math-container">\[H_q^T(X, Y) = -\sum_{x\in \mathcal{X}, y \in \mathcal{Y}} p(x, y)^q \log_q p(x, y),\]</p><p>where <span>$log_q(x, q) = \dfrac{x^{1-q} - 1}{1-q}$</span> is the q-logarithm, and  we define <span>$log_q(x, q) := 0$</span> if <span>$q = 0$</span>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_JointEntropyTsallis_OrdinalPatterns">Example 1</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/joint_entropies/JointEntropyTsallis.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.JointEntropyRenyi" href="#CausalityTools.JointEntropyRenyi"><code>CausalityTools.JointEntropyRenyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointEntropyRenyi &lt;: JointEntropy
JointEntropyRenyi(; base = 2, q = 1.5)</code></pre><p>The Rényi joint entropy measure (<a href="../references/#Golshani2009">Golshani <em>et al.</em>, 2009</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the Golshani-Rényi joint entropy between    two variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Given two two discrete random variables <span>$X$</span> and <span>$Y$</span> with ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{X}$</span>, <a href="../references/#Golshani2009">Golshani <em>et al.</em> (2009)</a> defines the Rényi joint entropy as</p><p class="math-container">\[H_q^R(X, Y) = \dfrac{1}{1-\alpha} \log \sum_{i = 1}^N p_i^q,\]</p><p>where <span>$q &gt; 0$</span> and <span>$q != 1$</span>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_JointEntropyRenyi_ValueBinning">Example 1</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> outcome space</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/joint_entropies/JointEntropyRenyi.jl#L5-L35">source</a></section></article><h3 id="Mutual-informations"><a class="docs-heading-anchor" href="#Mutual-informations">Mutual informations</a><a id="Mutual-informations-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-informations" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MutualInformation" href="#CausalityTools.MutualInformation"><code>CausalityTools.MutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MutualInformation</code></pre><p>Abstract type for all mutual information measures.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></li><li><a href="#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></li><li><a href="#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></li><li><a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></li><li><a href="#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></li></ul><p>See also: <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/mutual_informations/mutual_informations.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MIShannon" href="#CausalityTools.MIShannon"><code>CausalityTools.MIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIShannon &lt;: BivariateInformationMeasure
MIShannon(; base = 2)</code></pre><p>The Shannon mutual information <span>$I_S(X; Y)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Shannon mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Shannon mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> (generic)</li><li><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> (generic)</li><li><a href="@ref"><code>KSG1</code></a></li><li><a href="@ref"><code>KSG2</code></a></li><li><a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></li><li><a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></li><li><a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a></li></ul><p><strong>Discrete definition</strong></p><p>There are many equivalent formulations of discrete Shannon mutual information, meaning that  it can be estimated in several ways, either using <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>  (double-sum formulation), <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> (three-entropies decomposition), or some dedicated estimator.</p><p><strong>Double sum formulation</strong></p><p>Assume we observe samples <span>$\bar{\bf{X}}_{1:N_y} = \{\bar{\bf{X}}_1, \ldots, \bar{\bf{X}}_n \}$</span> and <span>$\bar{\bf{Y}}_{1:N_x} = \{\bar{\bf{Y}}_1, \ldots, \bar{\bf{Y}}_n \}$</span> from two discrete random variables <span>$X$</span> and <span>$Y$</span> with finite supports <span>$\mathcal{X} = \{ x_1, x_2, \ldots, x_{M_x} \}$</span> and <span>$\mathcal{Y} = y_1, y_2, \ldots, x_{M_y}$</span>. The double-sum estimate is obtained by replacing the double sum</p><p class="math-container">\[\hat{I}_{DS}(X; Y) =
 \sum_{x_i \in \mathcal{X}, y_i \in \mathcal{Y}} p(x_i, y_j) \log \left( \dfrac{p(x_i, y_i)}{p(x_i)p(y_j)} \right)\]</p><p>where  <span>$\hat{p}(x_i) = \frac{n(x_i)}{N_x}$</span>, <span>$\hat{p}(y_i) = \frac{n(y_j)}{N_y}$</span>, and <span>$\hat{p}(x_i, x_j) = \frac{n(x_i)}{N}$</span>, and <span>$N = N_x N_y$</span>. This definition is used by <a href="@ref"><code>mutualinfo</code></a> when called with a <a href="@ref"><code>ContingencyMatrix</code></a>.</p><p><strong>Three-entropies formulation</strong></p><p>An equivalent formulation of discrete Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = H^S(X) + H_q^S(Y) - H^S(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint discrete Shannon entropies. This definition is used by <a href="@ref"><code>mutualinfo</code></a> when called with a <a href="@ref"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Differential mutual information</strong></p><p>One possible formulation of differential Shannon mutual information is</p><p class="math-container">\[I^S(X; Y) = h^S(X) + h_q^S(Y) - h^S(X, Y),\]</p><p>where <span>$h^S(\cdot)$</span> and <span>$h^S(\cdot, \cdot)$</span> are the marginal and joint differential Shannon entropies. This definition is used by <a href="@ref"><code>mutualinfo</code></a> when called with a <a href="@ref"><code>DifferentialEntropyEstimator</code></a>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MIShannon_JointProbabilities_ValueBinning">Example 1</a>: <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MIShannon_JointProbabilities_UniqueElements">Example 2</a>: <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space on string data.</li><li><a href="../examples/examples_associations/#example_MIShannon_GaussianMI">Example 3</a>: Dedicated <a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_KSG1">Example 4</a>: Dedicated <a href="@ref"><code>KSG1</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_KSG2">Example 5</a>: Dedicated <a href="@ref"><code>KSG2</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_GaoKannanOhViswanath">Example 6</a>: Dedicated <a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_EntropyDecomposition_Kraskov">Example 7</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MIShannon_EntropyDecomposition_BubbleSortSwaps">Example 8</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.BubbleSortSwaps"><code>BubbleSortSwaps</code></a>.</li><li><a href="../examples/examples_associations/#example_MIShannon_EntropyDecomposition_Jackknife_ValueBinning">Example 9</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a> estimator and <a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> outcome space.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/mutual_informations/MIShannon.jl#L5-L89">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MITsallisFuruichi" href="#CausalityTools.MITsallisFuruichi"><code>CausalityTools.MITsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisFuruichi &lt;: BivariateInformationMeasure
MITsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from Furuichi (2006)(<a href="../references/#Furuichi2006">Furuichi, 2006</a>), which in that paper is called the <em>mutual entropy</em>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Tsallis-Furuichi mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Tsallis-Furuichi mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></li></ul><p><strong>Description</strong></p><p>Furuichi&#39;s Tsallis mutual entropy between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_q^T(X; Y) = H_q^T(X) - H_q^T(X | Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X, Y),\]</p><p>where <span>$H^T(\cdot)$</span> and <span>$H^T(\cdot, \cdot)$</span> are the marginal and joint Tsallis entropies, and <code>q</code> is the <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MITsallisFuruichi_JointProbabilities_UniqueElements">Example 1</a>: <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MITsallisFuruichi_EntropyDecomposition_LeonenkoProsantoSavani">Example 2</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="@ref"><code>LeonenkoProsantoSavani</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MITsallisFuruichi_EntropyDecomposition_Dispersion">Example 3</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/mutual_informations/MITsallisFuruichi.jl#L4-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MITsallisMartin" href="#CausalityTools.MITsallisMartin"><code>CausalityTools.MITsallisMartin</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MITsallisMartin &lt;: BivariateInformationMeasure
MITsallisMartin(; base = 2, q = 1.5)</code></pre><p>The discrete Tsallis mutual information from <a href="../references/#Martin2004">Martin <em>et al.</em> (2004)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Tsallis-Martin mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Tsallis-Martin mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></li></ul><p><strong>Description</strong></p><p>Martin et al.&#39;s Tsallis mutual information between variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span> is defined as</p><p class="math-container">\[I_{\text{Martin}}^T(X, Y, q) := H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X, Y),\]</p><p>where <span>$H^S(\cdot)$</span> and <span>$H^S(\cdot, \cdot)$</span> are the marginal and joint Shannon entropies, and <code>q</code> is the <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>-parameter.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MITsallisMartin_JointProbabilities_UniqueElements">Example 1</a>: <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MITsallisMartin_EntropyDecomposition_LeonenkoProsantoSavani">Example 2</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="@ref"><code>LeonenkoProsantoSavani</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_MITsallisMartin_EntropyDecomposition_OrdinalPatterns">Example 3</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/mutual_informations/MITsallisMartin.jl#L5-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MIRenyiJizba" href="#CausalityTools.MIRenyiJizba"><code>CausalityTools.MIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiJizba &lt;: &lt;: BivariateInformationMeasure
MIRenyiJizba(; q = 1.5, base = 2)</code></pre><p>The Rényi mutual information <span>$I_q^{R_{J}}(X; Y)$</span> defined in (<a href="../references/#Jizba2012">Jizba <em>et al.</em>, 2012</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Rényi-Jizba mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Rényi-Jizba mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>.</li><li><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a>.</li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y) = H_q^{R}(X) + H_q^{R}(Y) - H_q^{R}(X, Y),\]</p><p>where <span>$H_q^{R}(\cdot)$</span> is the <a href="@ref"><code>Rényi</code></a> entropy.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MIRenyiJizba_JointProbabilities_UniqueElements">Example 1</a>: <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_MIRenyiJizba_JointProbabilities_LeonenkoProzantoSavani">Example 2</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a>.</li><li><a href="../examples/examples_associations/#example_MIRenyiJizba_EntropyDecomposition_ValueBinning">Example 3</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/mutual_informations/MIRenyiJizba.jl#L5-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MIRenyiSarbu" href="#CausalityTools.MIRenyiSarbu"><code>CausalityTools.MIRenyiSarbu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIRenyiSarbu &lt;: BivariateInformationMeasure
MIRenyiSarbu(; base = 2, q = 1.5)</code></pre><p>The discrete Rényi mutual information from <a href="../references/#Sarbu2014">Sarbu (2014)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Rényi-Sarbu mutual information from input data   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence using   the Rényi-Sarbu mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a>.</li></ul><p><strong>Description</strong></p><p>Sarbu (2014) defines discrete Rényi mutual information as the Rényi <span>$\alpha$</span>-divergence between the conditional joint probability mass function <span>$p(x, y)$</span> and the product of the conditional marginals, <span>$p(x) \cdot p(y)$</span>:</p><p class="math-container">\[I(X, Y)^R_q =
\dfrac{1}{q-1}
\log \left(
    \sum_{x \in X, y \in Y}
    \dfrac{p(x, y)^q}{\left( p(x)\cdot p(y) \right)^{q-1}}
\right)\]</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MIRenyiSarbu_JointProbabilities_UniqueElements">Example 1</a>: <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a> for categorical data.</li><li><a href="../examples/examples_associations/#example_MIRenyiSarbu_JointProbabilities_CosineSimilarityBinning">Example 2</a>: <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.CosineSimilarityBinning"><code>CosineSimilarityBinning</code></a> for numerical data.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/mutual_informations/MIRenyiSarbu.jl#L5-L41">source</a></section></article><h3 id="Mutual-information-estimators"><a class="docs-heading-anchor" href="#Mutual-information-estimators">Mutual information estimators</a><a id="Mutual-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MutualInformationEstimator" href="#CausalityTools.MutualInformationEstimator"><code>CausalityTools.MutualInformationEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MutualInformationEstimator</code></pre><p>The supertype for dedicated <a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a> estimators.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="@ref"><code>KSG1</code></a></li><li><a href="@ref"><code>KSG2</code></a></li><li><a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></li><li><a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></li><li><a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/core.jl#L52-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.KraskovStögbauerGrassberger1" href="#CausalityTools.KraskovStögbauerGrassberger1"><code>CausalityTools.KraskovStögbauerGrassberger1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger1</code> mutual information estimator (you can use <code>KSG1</code> for short) is the <span>$I^{(1)}$</span> <code>k</code>-th nearest neighbor estimator from <a href="../references/#Kraskov2004">Kraskov <em>et al.</em> (2004)</a>.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute Shannon mutual information from input data.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(KSG1(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/mutual_info_estimators/KSG1.jl#L16-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.KraskovStögbauerGrassberger2" href="#CausalityTools.KraskovStögbauerGrassberger2"><code>CausalityTools.KraskovStögbauerGrassberger2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger2</code> mutual information estimator (you can use <code>KSG2</code> for short) is the <span>$I^{(2)}$</span> <code>k</code>-th nearest neighbor estimator from (<a href="../references/#Kraskov2004">Kraskov <em>et al.</em>, 2004</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute Shannon mutual information from input data.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Description</strong></p><p>Let the joint StateSpaceSet <span>$X := \{\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m \}$</span> be defined by the concatenation of the marginal StateSpaceSets <span>$\{ \bf{X}_k \}_{k=1}^m$</span>, where each <span>$\bf{X}_k$</span> is potentially multivariate. Let <span>$\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N$</span> be the points in the joint space <span>$X$</span>.</p><p>The <code>KraskovStögbauerGrassberger2</code> estimator first locates, for each <span>$\bf{x}_i \in X$</span>, the point <span>$\bf{n}_i \in X$</span>, the <code>k</code>-th nearest neighbor to <span>$\bf{x}_i$</span>, according to the maximum norm (<code>Chebyshev</code> metric). Let <span>$\epsilon_i$</span> be the distance <span>$d(\bf{x}_i, \bf{n}_i)$</span>.</p><p>Consider <span>$x_i^m \in \bf{X}_m$</span>, the <span>$i$</span>-th point in the marginal space <span>$\bf{X}_m$</span>. For each <span>$\bf{x}_i^m$</span>, we determine <span>$\theta_i^m$</span> := the number of points <span>$\bf{x}_k^m \in \bf{X}_m$</span> that are a distance less than <span>$\epsilon_i$</span> away from <span>$\bf{x}_i^m$</span>. That is, we use the distance from a query point <span>$\bf{x}_i \in X$</span> (in the <em>joint</em> space) to count neighbors of <span>$x_i^m \in \bf{X}_m$</span> (in the marginal space).</p><p>Shannon mutual information between the variables <span>$\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m$</span> is then estimated as</p><p class="math-container">\[\hat{I}_{KSG2}(\bf{X}) =
    \psi{(k)} -
    \dfrac{m - 1}{k} +
    (m - 1)\psi{(N)} -
    \dfrac{1}{N} \sum_{i = 1}^N \sum_{j = 1}^m \psi{(\theta_i^j + 1)}\]</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(KSG2(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/mutual_info_estimators/KSG2.jl#L9-L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.GaoKannanOhViswanath" href="#CausalityTools.GaoKannanOhViswanath"><code>CausalityTools.GaoKannanOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoKannanOhViswanath &lt;: MutualInformationEstimator
GaoKannanOhViswanath(; k = 1, w = 0)</code></pre><p>The <code>GaoKannanOhViswanath</code> (Shannon) estimator is designed for estimating Shannon mutual information between variables that may be either discrete, continuous or a mixture of both (<a href="../references/#GaoKannanOhViswanath2017">Gao <em>et al.</em>, 2017</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute Shannon mutual information from input data.</li></ul><p><strong>Description</strong></p><p>The estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using <code>k</code>-nearest neighbor distances from empirical samples.</p><p>The estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator <em>&quot;strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.&quot;</em></p><div class="admonition is-category-warn"><header class="admonition-header">Implementation note</header><div class="admonition-body"><p>In <a href="../references/#GaoKannanOhViswanath2017">Gao <em>et al.</em> (2017)</a>, they claim (roughly speaking) that the estimator reduces to the <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> estimator for continuous-valued data. However, <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> uses the digamma function, while <code>GaoKannanOhViswanath</code> uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.</p><p>Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the <code>KSG1</code> approach. The <code>KSG1</code> estimator uses <code>k</code>-th neighbor distances in the <em>joint</em> space, while the <code>GaoKannanOhViswanath</code> algorithm selects the maximum <code>k</code>-th nearest distances among the two marginal spaces, which are in general not the same as the <code>k</code>-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in <code>GaoKannanOhViswanath</code>. We have modified it in a way that mimics <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> for continous data. Note that because of using the <code>log</code> function instead of <code>digamma</code>, there will be slight differences between the methods. See the source code for more details.</p></div></div><div class="admonition is-info"><header class="admonition-header">Explicitly convert your discrete data to floats</header><div class="admonition-body"><p>Even though the <code>GaoKannanOhViswanath</code> estimator is designed to handle discrete data, our implementation demands that all input data are <code>StateSpaceSet</code>s whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to <a href="@ref"><code>mutualinfo</code></a>.</p></div></div><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(GaoKannanOhViswanath(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/mutual_info_estimators/GaoKannanOhViswanath.jl#L5-L65">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.GaoOhViswanath" href="#CausalityTools.GaoOhViswanath"><code>CausalityTools.GaoOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoOhViswanath &lt;: MutualInformationEstimator</code></pre><p>The <code>GaoOhViswanath</code> is a mutual information estimator based on nearest neighbors, and is also called the bias-improved-KSG estimator, or BI-KSG, by (<a href="../references/#Gao2018">Gao <em>et al.</em>, 2018</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute Shannon mutual information from input data.</li></ul><p><strong>Description</strong></p><p>The estimator is given by</p><p class="math-container">\[\begin{align*}
\hat{H}_{GAO}(X, Y)
&amp;= \hat{H}_{KSG}(X) + \hat{H}_{KSG}(Y) - \hat{H}_{KZL}(X, Y) \\
&amp;= \psi{(k)} +
    \log{(N)} +
    \log{
        \left(
            \dfrac{c_{d_{x}, 2} c_{d_{y}, 2}}{c_{d_{x} + d_{y}, 2}}
        \right)
    } - \\
    &amp; \dfrac{1}{N} \sum_{i=1}^N \left( \log{(n_{x, i, 2})} + \log{(n_{y, i, 2})} \right)
\end{align*},\]</p><p>where <span>$c_{d, 2} = \dfrac{\pi^{\frac{d}{2}}}{\Gamma{(\dfrac{d}{2} + 1)}}$</span> is the volume of a <span>$d$</span>-dimensional unit <span>$\mathcal{l}_2$</span>-ball.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(GaoOhViswanath(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/mutual_info_estimators/GaoOhViswanath.jl#L4-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.GaussianMI" href="#CausalityTools.GaussianMI"><code>CausalityTools.GaussianMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianMI &lt;: MutualInformationEstimator
GaussianMI(; normalize::Bool = false)</code></pre><p><code>GaussianMI</code> is a parametric estimator for Shannon mutual information.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute Shannon mutual information from input data.</li></ul><p><strong>Description</strong></p><p>Given <span>$d_x$</span>-dimensional and <span>$d_y$</span>-dimensional input data <code>X</code> and <code>Y</code>, <code>GaussianMI</code> first constructs the <span>$d_x + d_y$</span>-dimensional joint <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> <code>XY</code>. If <code>normalize == true</code>, then we follow the approach in Vejmelka &amp; Palus (2008)(<a href="../references/#Vejmelka2008">Vejmelka and Paluš, 2008</a>) and transform each column in <code>XY</code> to have zero mean and unit standard deviation. If <code>normalize == false</code>, then the algorithm proceeds without normalization.</p><p>Next, the <code>C_{XY}</code>, the correlation matrix for the (normalized) joint data <code>XY</code> is computed. The mutual information estimate <code>GaussianMI</code> assumes the input variables are distributed according to normal distributions with zero means and unit standard deviations. Therefore, given <span>$d_x$</span>-dimensional and <span>$d_y$</span>-dimensional input data <code>X</code> and <code>Y</code>, <code>GaussianMI</code> first constructs the joint <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> <code>XY</code>, then transforms each column in <code>XY</code> to have zero mean and unit standard deviation, and finally computes the <code>\Sigma</code>, the correlation matrix for <code>XY</code>.</p><p>The mutual information estimated (for <code>normalize == false</code>) is then estimated as</p><p class="math-container">\[\hat{I}^S_{Gaussian}(X; Y) = \dfrac{1}{2}
\dfrac{ \det(\Sigma_X) \det(\Sigma_Y)) }{\det(\Sigma))},\]</p><p>where we <span>$\Sigma_X$</span> and <span>$\Sigma_Y$</span> appear in <span>$\Sigma$</span> as</p><p class="math-container">\[\Sigma = \begin{bmatrix}
\Sigma_{X} &amp; \Sigma^{&#39;}\\
\Sigma^{&#39;} &amp; \Sigma_{Y}
\end{bmatrix}.\]</p><p>If <code>normalize == true</code>, then the mutual information is estimated as</p><p class="math-container">\[\hat{I}^S_{Gaussian}(X; Y) = -\dfrac{1}{2} \sum_{i = 1}^{d_x + d_y} \sigma_i,\]</p><p>where <span>$\sigma_i$</span> are the eigenvalues for <span>$\Sigma$</span>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(GaussianMI(), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/mutual_info_estimators/GaussianMI.jl#L6-L69">source</a></section></article><h3 id="Conditional-mutual-informations"><a class="docs-heading-anchor" href="#Conditional-mutual-informations">Conditional mutual informations</a><a id="Conditional-mutual-informations-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-informations" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ConditionalMutualInformation" href="#CausalityTools.ConditionalMutualInformation"><code>CausalityTools.ConditionalMutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CondiitionalMutualInformation</code></pre><p>Abstract type for all mutual information measures.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></li><li><a href="#CausalityTools.CMITsallisPapapetrou"><code>CMITsallisPapapetrou</code></a></li><li><a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a></li><li><a href="#CausalityTools.CMIRenyiSarbu"><code>CMIRenyiSarbu</code></a></li><li><a href="#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></li></ul><p>See also: <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_mutual_informations/conditional_mutual_informations.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CMIShannon" href="#CausalityTools.CMIShannon"><code>CausalityTools.CMIShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIShannon &lt;: ConditionalMutualInformation
CMIShannon(; base = 2)</code></pre><p>The Shannon conditional mutual information (CMI) <span>$I^S(X; Y | Z)$</span>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Shannon conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Shannon conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></li><li><a href="#CausalityTools.MIDecomposition"><code>MIDecomposition</code></a></li><li><a href="#CausalityTools.FPVP"><code>FPVP</code></a></li><li><a href="#CausalityTools.MesnerShalizi"><code>MesnerShalizi</code></a></li><li><a href="#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a></li><li><a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li><li><a href="#CausalityTools.GaussianCMI"><code>GaussianCMI</code></a></li></ul><p><strong>Supported definitions</strong></p><p>Consider random variables <span>$X \in \mathbb{R}^{d_X}$</span> and <span>$Y \in \mathbb{R}^{d_Y}$</span>, given <span>$Z \in \mathbb{R}^{d_Z}$</span>. The Shannon conditional mutual information is defined as</p><p class="math-container">\[\begin{align*}
I(X; Y | Z)
&amp;= H^S(X, Z) + H^S(Y, z) - H^S(X, Y, Z) - H^S(Z) \\
&amp;= I^S(X; Y, Z) + I^S(X; Y)
\end{align*},\]</p><p>where <span>$I^S(\cdot; \cdot)$</span> is the Shannon mutual information <a href="#CausalityTools.MIShannon"><code>MIShannon</code></a>, and <span>$H^S(\cdot)$</span> is the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>Differential Shannon CMI is obtained by replacing the entropies by differential entropies.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_CMIShannon_EntropyDecomposition_Kraskov">Example 1</a>:    <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimator.</li><li><a href="@ref CMIShannon_EntropyDecomposition_ValueBinning">Example 2</a>:   <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_CMIShannon_MIDecomposition_KSG1">Example 3</a>:    <a href="#CausalityTools.MIDecomposition"><code>MIDecomposition</code></a> with <a href="@ref"><code>KSG1</code></a> estimator.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_mutual_informations/CMIShannon.jl#L6-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CMIRenyiSarbu" href="#CausalityTools.CMIRenyiSarbu"><code>CausalityTools.CMIRenyiSarbu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiSarbu &lt;: ConditionalMutualInformation
CMIRenyiSarbu(; base = 2, q = 1.5)</code></pre><p>The Rényi conditional mutual information from <a href="../references/#Sarbu2014">Sarbu (2014)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw  Rényi-Sarbu conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Sarbu conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Discrete description</strong></p><p>Assume we observe three discrete random variables <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span>. Sarbu (2014) defines discrete conditional Rényi mutual information as the conditional Rényi <span>$\alpha$</span>-divergence between the conditional joint probability mass function <span>$p(x, y | z)$</span> and the product of the conditional marginals, <span>$p(x |z) \cdot p(y|z)$</span>:</p><p class="math-container">\[I(X, Y; Z)^R_q =
\dfrac{1}{q-1} \sum_{z \in Z} p(Z = z)
\log \left(
    \sum_{x \in X}\sum_{y \in Y}
    \dfrac{p(x, y|z)^q}{\left( p(x|z)\cdot p(y|z) \right)^{q-1}}
\right)\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_mutual_informations/CMIRenyiSarbu.jl#L6-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CMIRenyiJizba" href="#CausalityTools.CMIRenyiJizba"><code>CausalityTools.CMIRenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiJizba &lt;: ConditionalMutualInformation
CMIRenyiJizba(; base = 2, q = 1.5)</code></pre><p>The Rényi conditional mutual information <span>$I_q^{R_{J}}(X; Y | Z)$</span> defined in <a href="../references/#Jizba2012">Jizba <em>et al.</em> (2012)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw  Rényi-Jizba conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Jizba conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></li></ul><p><strong>Definition</strong></p><p class="math-container">\[I_q^{R_{J}}(X; Y | Z) = I_q^{R_{J}}(X; Y, Z) - I_q^{R_{J}}(X; Z),\]</p><p>where <span>$I_q^{R_{J}}(X; Z)$</span> is the <a href="#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a> mutual information.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_CMIRenyiJizba_JointProbabilities_BubbleSortSwaps">Example 1</a>:    <a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.BubbleSortSwaps"><code>BubbleSortSwaps</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_CMIRenyiJizba_EntropyDecomposition_OrdinalPatterns">Example 2</a>:    <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_CMIRenyiJizba_EntropyDecomposition_LeonenkoProzantoSavani">Example 3</a>:    <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with differential entropy estimator <a href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_mutual_informations/CMIRenyiJizba.jl#L5-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CMIRenyiPoczos" href="#CausalityTools.CMIRenyiPoczos"><code>CausalityTools.CMIRenyiPoczos</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIRenyiPoczos &lt;: ConditionalMutualInformation
CMIRenyiPoczos(; base = 2, q = 1.5)</code></pre><p>The differential Rényi conditional mutual information <span>$I_q^{R_{P}}(X; Y | Z)$</span> defined in <a href="../references/#Poczos2012">Póczos and Schneider (2012)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Rényi-Poczos conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Poczos conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li></ul><p><strong>Definition</strong></p><p class="math-container">\[\begin{align*}
I_q^{R_{P}}(X; Y | Z) &amp;= \dfrac{1}{q-1}
\int \int \int \dfrac{p_Z(z) p_{X, Y | Z}^q}{( p_{X|Z}(x|z) p_{Y|Z}(y|z) )^{q-1}} \\
&amp;= \mathbb{E}_{X, Y, Z} \sim p_{X, Y, Z}
\left[ \dfrac{p_{X, Z}^{1-q}(X, Z) p_{Y, Z}^{1-q}(Y, Z) }{p_{X, Y, Z}^{1-q}(X, Y, Z) p_Z^{1-q}(Z)} \right]
\end{align*}\]</p><p><strong>Estimation</strong></p><ul><li><a href="@ref @id CMIRenyiPoczos_PoczosSchneiderCMI">Example 1</a>: Dedicated <a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a> estimator.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_mutual_informations/CMIRenyiPoczos.jl#L5-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CMITsallisPapapetrou" href="#CausalityTools.CMITsallisPapapetrou"><code>CausalityTools.CMITsallisPapapetrou</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMITsallisPapapetrou &lt;: ConditionalMutualInformation
CMITsallisPapapetrou(; base = 2, q = 1.5)</code></pre><p>The Tsallis-Papapetrou conditional mutual information (<a href="../references/#Papapetrou2020">Papapetrou and Kugiumtzis, 2020</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Tsallis-Papapetrou conditional mutual information   using of of the estimators listed below.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise conditional    independence using the Tsallis-Papapetrou conditional mutual information.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.JointProbabilities"><code>JointProbabilities</code></a></li></ul><p><strong>Definition</strong></p><p>Tsallis-Papapetrou conditional mutual information is defined as </p><p class="math-container">\[I_T^q(X, Y \mid Z) = \frac{1}{1 - q} \left( 1 - \sum_{XYZ} \frac{p(x, y, z)^q}{p(x \mid z)^{q-1} p(y \mid z)^{q-1} p(z)^{q-1}} \right).\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/conditional_mutual_informations/CMITsallisPapapetrou.jl#L5-L29">source</a></section></article><h4 id="Conditional-mutual-information-estimators"><a class="docs-heading-anchor" href="#Conditional-mutual-information-estimators">Conditional mutual information estimators</a><a id="Conditional-mutual-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-information-estimators" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ConditionalMutualInformationEstimator" href="#CausalityTools.ConditionalMutualInformationEstimator"><code>CausalityTools.ConditionalMutualInformationEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalMutualInformationEstimator</code></pre><p>The supertype for dedicated <a href="#CausalityTools.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> estimators.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#CausalityTools.FPVP"><code>FPVP</code></a></li><li><a href="#CausalityTools.GaussianCMI"><code>GaussianCMI</code></a></li><li><a href="#CausalityTools.MesnerShalizi"><code>MesnerShalizi</code></a></li><li><a href="#CausalityTools.Rahimzamani"><code>Rahimzamani</code></a></li><li><a href="#CausalityTools.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/core.jl#L67-L79">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.GaussianCMI" href="#CausalityTools.GaussianCMI"><code>CausalityTools.GaussianCMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianCMI &lt;: MutualInformationEstimator
GaussianCMI(definition = CMIShannon(); normalize::Bool = false)</code></pre><p><code>GaussianCMI</code> is a parametric <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>  (<a href="../references/#Vejmelka2008">Vejmelka and Paluš, 2008</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute <a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a> from input data.</li></ul><p><strong>Description</strong></p><p><code>GaussianCMI</code> estimates Shannon CMI through a sum of two mutual information terms that each are estimated using <a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a> (the <code>normalize</code> keyword is the same as for <a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a>):</p><p class="math-container">\[\hat{I}_{Gaussian}(X; Y | Z) = \hat{I}_{Gaussian}(X; Y, Z) - \hat{I}_{Gaussian}(X; Z)\]</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(GaussianCMI(CMIShannon(base = 2)), x, z, y)</code></pre><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/conditional_mutual_info_estimators/GaussianCMI.jl#L4-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.FPVP" href="#CausalityTools.FPVP"><code>CausalityTools.FPVP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FPVP &lt;: ConditionalMutualInformationEstimator
FPVP(definition = CMIShannon(); k = 1, w = 0)</code></pre><p>The Frenzel-Pompe-Vejmelka-Paluš (or <code>FPVP</code> for short) <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a> is used to estimate the conditional mutual information using a <code>k</code>-th nearest neighbor approach that is analogous to that of the <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> mutual information estimator from <a href="../references/#Frenzel2007">Frenzel and Pompe (2007)</a> and <a href="../references/#Vejmelka2008">Vejmelka and Paluš (2008)</a>.</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute <a href="#CausalityTools.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> measure   from input data.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(FPVP(; k = 10), x, z, y) # should be near 0 (and can be negative)</code></pre><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/conditional_mutual_info_estimators/FPVP.jl#L8-L40">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MesnerShalizi" href="#CausalityTools.MesnerShalizi"><code>CausalityTools.MesnerShalizi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MesnerShalizi &lt;: ConditionalMutualInformationEstimator
MesnerShalizi(definition = CMIShannon(); k = 1, w = 0)</code></pre><p>The <code>MesnerShalizi</code> <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a> is designed for data that can be mixtures of discrete and continuous data (<a href="../references/#Mesner2020">Mesner and Shalizi, 2020</a>).</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute <a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a> from input data.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(MesnerShalizi(; k = 10), x, z, y) # should be near 0 (and can be negative)</code></pre><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/conditional_mutual_info_estimators/MesnerShalizi.jl#L3-L31">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.Rahimzamani" href="#CausalityTools.Rahimzamani"><code>CausalityTools.Rahimzamani</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Rahimzamani &lt;: ConditionalMutualInformationEstimator
Rahimzamani(k = 1, w = 0)</code></pre><p>The <code>Rahimzamani</code> <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a> is designed for data that can be mixtures of discrete and continuous data (<a href="../references/#Rahimzamani2018">Rahimzamani <em>et al.</em>, 2018</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute a <a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a> from input data.</li></ul><p><strong>Description</strong></p><p>This estimator is very similar to the <a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a> mutual information estimator, but has been expanded to the conditional mutual information case.</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(Rahimzamani(; k = 10), x, z, y) # should be near 0 (and can be negative)</code></pre><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/conditional_mutual_info_estimators/Rahimzamani.jl#L3-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.PoczosSchneiderCMI" href="#CausalityTools.PoczosSchneiderCMI"><code>CausalityTools.PoczosSchneiderCMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PoczosSchneiderCMI &lt;: ConditionalMutualInformationEstimator
PoczosSchneiderCMI(definition = CMIRenyiPoczos(); k = 1, w = 0)</code></pre><p>The <code>PoczosSchneiderCMI</code> <a href="#CausalityTools.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>  computes conditional mutual informations using a <code>k</code>-th nearest neighbor approach (<a href="../references/#Poczos2012">Póczos and Schneider, 2012</a>).</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute <a href="#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a> from input data.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(PoczosSchneiderCMI(CMIRenyiPoczos(), k = 10), x, z, y) # should be near 0 (and can be negative)</code></pre><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/conditional_mutual_info_estimators/PoczosSchneiderCMI.jl#L5-L34">source</a></section></article><h3 id="Partial-mutual-information"><a class="docs-heading-anchor" href="#Partial-mutual-information">Partial mutual information</a><a id="Partial-mutual-information-1"></a><a class="docs-heading-anchor-permalink" href="#Partial-mutual-information" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.PartialMutualInformation" href="#CausalityTools.PartialMutualInformation"><code>CausalityTools.PartialMutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PartialMutualInformation &lt;: MultivariateInformationMeasure
PartialMutualInformation(; base = 2)</code></pre><p>The partial mutual information (PMI) measure of conditional association (<a href="../references/#Zhao2016">Zhao <em>et al.</em>, 2016</a>).</p><p><strong>Definition</strong></p><p>PMI is defined as for variables <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span> as</p><p class="math-container">\[PMI(X; Y | Z) = D(p(x, y, z) || p^{*}(x|z) p^{*}(y|z) p(z)),\]</p><p>where <span>$p(x, y, z)$</span> is the joint distribution for <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span>, and <span>$D(\cdot, \cdot)$</span> is the extended Kullback-Leibler divergence from <span>$p(x, y, z)$</span> to <span>$p^{*}(x|z) p^{*}(y|z) p(z)$</span>. See <a href="../references/#Zhao2016">Zhao <em>et al.</em> (2016)</a> for details.</p><p><strong>Estimation</strong></p><p>The PMI is estimated by first estimating a 3D probability mass function using  <a href="../probabilities_tutorial/#ComplexityMeasures.probabilities-Tuple{OutcomeSpace}"><code>probabilities</code></a>, then computing <span>$PMI(X; Y | Z)$</span> from those probaiblities.</p><p><strong>Properties</strong></p><p>For the discrete case, the following identities hold in theory (when estimating PMI, they may not).</p><ul><li><code>PMI(X, Y, Z) &gt;= CMI(X, Y, Z)</code> (where CMI is the Shannon CMI). Holds in theory, but   when estimating PartialMutualInformation, the identity may not hold.</li><li><code>PMI(X, Y, Z) &gt;= 0</code>. Holds both in theory and when estimating using discrete estimators.</li><li><code>X ⫫ Y | Z =&gt; PMI(X, Y, Z) = CMI(X, Y, Z) = 0</code> (in theory, but not necessarily for   estimation).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/partial_mutual_information/partial_mutual_information.jl#L3-L36">source</a></section></article><h3 id="Transfer-entropy"><a class="docs-heading-anchor" href="#Transfer-entropy">Transfer entropy</a><a id="Transfer-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.TransferEntropy" href="#CausalityTools.TransferEntropy"><code>CausalityTools.TransferEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TransferEntropy &lt;: AssociationMeasure</code></pre><p>The supertype of all transfer entropy measures. Concrete subtypes are</p><ul><li><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></li><li><a href="#CausalityTools.TERenyiJizba"><code>TERenyiJizba</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/transferentropy/transfer_entropies.jl#L3-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.TEShannon" href="#CausalityTools.TEShannon"><code>CausalityTools.TEShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TEShannon &lt;: TransferEntropy
TEShannon(; base = 2; embedding = EmbeddingTE()) &lt;: TransferEntropy</code></pre><p>The Shannon-type transfer entropy measure.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li><li>Use with <a href="@ref"><code>transferentropy</code></a> to compute the raw transfer entropy.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I^S(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I^S(T^+; S^- | T^-, C^-)
\end{align*}\]</p><p>where <span>$I(T^+; S^- | T^-)$</span> is the Shannon conditional mutual information (<a href="#CausalityTools.CMIShannon"><code>CMIShannon</code></a>). The variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> are described in the docstring for <a href="@ref"><code>transferentropy</code></a>.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_TEShannon_EntropyDecomposition_TransferOperator">Example 1</a>:    <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="@ref"><code>TransferOperator</code></a> outcome space.</li><li><a href="../examples/examples_associations/#example_TEShannon_SymbolicTransferEntropy">Example 2</a>: Estimation using the   <a href="#CausalityTools.SymbolicTransferEntropy"><code>SymbolicTransferEntropy</code></a> estimator.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/transferentropy/TEShannon.jl#L3-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.TERenyiJizba" href="#CausalityTools.TERenyiJizba"><code>CausalityTools.TERenyiJizba</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TERenyiJizba() &lt;: TransferEntropy</code></pre><p>The Rényi transfer entropy from <a href="../references/#Jizba2012">Jizba <em>et al.</em> (2012)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   and conditional dependence.</li><li>Use with <a href="@ref"><code>transferentropy</code></a> to compute the raw transfer entropy.</li></ul><p><strong>Description</strong></p><p>The transfer entropy from source <span>$S$</span> to target <span>$T$</span>, potentially conditioned on <span>$C$</span> is defined as</p><p class="math-container">\[\begin{align*}
TE(S \to T) &amp;:= I_q^{R_J}(T^+; S^- | T^-) \\
TE(S \to T | C) &amp;:= I_q^{R_J}(T^+; S^- | T^-, C^-),
\end{align*},\]</p><p>where <span>$I_q^{R_J}(T^+; S^- | T^-)$</span> is Jizba et al. (2012)&#39;s definition of conditional mutual information (<a href="#CausalityTools.CMIRenyiJizba"><code>CMIRenyiJizba</code></a>). The variables <span>$T^+$</span>, <span>$T^-$</span>, <span>$S^-$</span> and <span>$C^-$</span> are described in the docstring for <a href="@ref"><code>transferentropy</code></a>.</p><p><strong>Estimation</strong></p><p>Estimating Jizba&#39;s Rényi transfer entropy is a bit complicated, since it doesn&#39;t have  a dedicated estimator. Instead, we re-write the Rényi transfer entropy as a  Rényi conditional mutual information, and estimate it using an  <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with a suitable discrete/differential Rényi entropy estimator from the list below as its input.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Sub-estimator</th><th style="text-align: left">Principle</th></tr><tr><td style="text-align: left"><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="../discretization_tutorial/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr><tr><td style="text-align: left"><a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a></td><td style="text-align: left"><a href="@ref"><code>TransferOperator</code></a></td><td style="text-align: left">Four-entropies decomposition</td></tr></table><p>Any of these estimators must be given as input to a <a href="#CausalityTools.CMIDecomposition">`CMIDecomposition</a> estimator.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_TERenyiJizba_EntropyDecomposition_TransferOperator">Example 1</a>: <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> with <a href="@ref"><code>TransferOperator</code></a> outcome space.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/transferentropy/TERenyiJizba.jl#L3-L53">source</a></section></article><h4 id="Transfer-entropy-estimators"><a class="docs-heading-anchor" href="#Transfer-entropy-estimators">Transfer entropy estimators</a><a id="Transfer-entropy-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy-estimators" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.TransferEntropyEstimator" href="#CausalityTools.TransferEntropyEstimator"><code>CausalityTools.TransferEntropyEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The supertype of all dedicated transfer entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/transfer_entropy_estimators/transfer_entropy_estimators.jl#L3-L5">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.Zhu1" href="#CausalityTools.Zhu1"><code>CausalityTools.Zhu1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu1 &lt;: TransferEntropyEstimator
Zhu1(k = 1, w = 0, base = MathConstants.e)</code></pre><p>The <code>Zhu1</code> transfer entropy estimator (<a href="../references/#Zhu2015">Zhu <em>et al.</em>, 2015</a>) for normalized input data  (as described in <a href="../references/#Zhu2015">Zhu <em>et al.</em> (2015)</a>) for both for pairwise and conditional transfer entropy.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute <a href="#CausalityTools.TEShannon"><code>TEShannon</code></a> from input data.</li></ul><p><strong>Description</strong></p><p>This estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in <a href="../references/#Singh2003">Singh <em>et al.</em> (2003)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>For a given points in the joint embedding space <code>jᵢ</code>, this estimator first computes the distance <code>dᵢ</code> from <code>jᵢ</code> to its <code>k</code>-th nearest neighbor. Then, for each point <code>mₖ[i]</code> in the <code>k</code>-th marginal space, it counts the number of points within radius <code>dᵢ</code>.</p><p>The Shannon transfer entropy is then computed as</p><p class="math-container">\[TE_S(X \to Y) =
\psi(k) + \dfrac{1}{N} \sum_{i}^n
\left[
    \sum_{k=1}^3 \left( \psi(m_k[i] + 1) \right)
\right],\]</p><p>where the index <code>k</code> references the three marginal subspaces <code>T</code>, <code>TTf</code> and <code>ST</code> for which neighbor searches are performed. Here this estimator has been modified to allow for  conditioning too (a simple modification to <a href="../references/#Lindner2011">Lindner <em>et al.</em> (2011)</a>&#39;s equation 5 and 6). </p><p><strong>Usage</strong></p><ul><li><a href="@ref"><code>information</code></a><code>(est::Zhu1, x, y, z)</code>.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
est = Zhu1(TEShannon(), k = 10)
association(est, x, z, y) # should be near 0 (and can be negative)</code></pre><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/transfer_entropy_estimators/Zhu1.jl#L10-L72">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.Lindner" href="#CausalityTools.Lindner"><code>CausalityTools.Lindner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lindner &lt;: TransferEntropyEstimator
Lindner(definition = Shannon(); k = 1, w = 0, base = 2)</code></pre><p>The <code>Lindner</code> transfer entropy estimator (<a href="../references/#Lindner2011">Lindner <em>et al.</em>, 2011</a>), which is also used in the Trentool MATLAB toolbox, and is based on nearest neighbor searches.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute <a href="#CausalityTools.TEShannon"><code>TEShannon</code></a> from input data.</li></ul><p><strong>Keyword parameters</strong></p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>The estimator can be used both for pairwise and conditional transfer entropy estimation.</p><p><strong>Description</strong></p><p>For a given points in the joint embedding space <code>jᵢ</code>, this estimator first computes the distance <code>dᵢ</code> from <code>jᵢ</code> to its <code>k</code>-th nearest neighbor. Then, for each point <code>mₖ[i]</code> in the <code>k</code>-th marginal space, it counts the number of points within radius <code>dᵢ</code>.</p><p>The Shannon transfer entropy is then computed as</p><p class="math-container">\[TE_S(X \to Y) =
\psi(k) + \dfrac{1}{N} \sum_{i}^n
\left[
    \sum_{k=1}^3 \left( \psi(m_k[i] + 1) \right)
\right],\]</p><p>where the index <code>k</code> references the three marginal subspaces <code>T</code>, <code>TTf</code> and <code>ST</code> for which neighbor searches are performed. Here this estimator has been modified to allow for  conditioning too (a simple modification to <a href="../references/#Lindner2011">Lindner <em>et al.</em> (2011)</a>&#39;s equation 5 and 6). </p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using CausalityTools
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
est = Lindner(TEShannon(), k = 10)
association(est, x, z, y) # should be near 0 (and can be negative)</code></pre><p><strong>Compatible definitions</strong></p><ul><li><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/transfer_entropy_estimators/Lindner.jl#L7-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.SymbolicTransferEntropy" href="#CausalityTools.SymbolicTransferEntropy"><code>CausalityTools.SymbolicTransferEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicTransferEntropy &lt;: TransferEntropyEstimator
SymbolicTransferEntropy(definition = TEShannon(); m = 3, τ = 1, 
    lt = ComplexityMeasures.isless_rand</code></pre><p>A convenience estimator for symbolic transfer entropy (<a href="../references/#Staniek2008">Staniek and Lehnertz, 2008</a>).</p><p><strong>Compatible measures</strong></p><ul><li><a href="#CausalityTools.TEShannon"><code>TEShannon</code></a></li></ul><p><strong>Description</strong></p><p><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.100.158101">Symbolic transfer entropy</a> consists of two simple steps. First, the input time series are encoded using <a href="../discretization_tutorial/#ComplexityMeasures.codify"><code>codify</code></a> with the <a href="../discretization_tutorial/#CausalityTools.CodifyVariables"><code>CodifyVariables</code></a> discretization and the <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space. This  transforms the input time series into integer time series. Transfer entropy entropy is then  estimated from the encoded time series by applying  </p><p>Transfer entropy is then estimated as usual on the encoded timeseries with the embedding dictated by <code>definition</code> and the <a href="@ref"><code>JointProbababilities</code></a> estimator.</p><p><strong>Examples</strong></p><ul><li><a href="../examples/examples_associations/#example_TEShannon_SymbolicTransferEntropy">Example 1</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/transfer_entropy_estimators/SymbolicTransferEntropy.jl#L4-L29">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.Hilbert" href="#CausalityTools.Hilbert"><code>CausalityTools.Hilbert</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Hilbert(est;
    source::InstantaneousSignalProperty = Phase(),
    target::InstantaneousSignalProperty = Phase(),
    cond::InstantaneousSignalProperty = Phase())
) &lt;: TransferDifferentialEntropyEstimator</code></pre><p>Compute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are obtained by first applying the Hilbert transform to each signal, then extracting the phases/amplitudes of the resulting complex numbers (<a href="../references/#Palus2014">Paluš, 2014</a>). Original time series are thus transformed to instantaneous phase/amplitude time series. Transfer entropy is then estimated using the provided <code>est</code> on those phases/amplitudes (use e.g. <a href="@ref"><code>VisitationFrequency</code></a>, or <a href="../discretization_tutorial/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a>).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Details on estimation of the transfer entropy (conditional mutual information) following the phase/amplitude extraction step is not given in Palus (2014). Here, after instantaneous phases/amplitudes have been obtained, these are treated as regular time series, from which transfer entropy is then computed as usual.</p></div></div><p>See also: <a href="#CausalityTools.Phase"><code>Phase</code></a>, <a href="#CausalityTools.Amplitude"><code>Amplitude</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/transfer_entropy_estimators/Hilbert.jl#L21-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.Phase" href="#CausalityTools.Phase"><code>CausalityTools.Phase</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Phase &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous phases of a signal should be used. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/transfer_entropy_estimators/Hilbert.jl#L14-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.Amplitude" href="#CausalityTools.Amplitude"><code>CausalityTools.Amplitude</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Amplitude &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous amplitudes of a signal should be used. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/estimators/transfer_entropy_estimators/Hilbert.jl#L8-L11">source</a></section></article><h5 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h5><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.optimize_marginals_te" href="#CausalityTools.optimize_marginals_te"><code>CausalityTools.optimize_marginals_te</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">optimize_marginals_te([scheme = OptimiseTraditional()], s, t, [c]) → EmbeddingTE</code></pre><p>Optimize marginal embeddings for transfer entropy computation from source time series <code>s</code> to target time series <code>t</code>, conditioned on <code>c</code> if <code>c</code> is given, using the provided optimization <code>scheme</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/transferentropy/utils/OptimiseTraditional.jl#L22-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.EmbeddingTE" href="#CausalityTools.EmbeddingTE"><code>CausalityTools.EmbeddingTE</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EmbeddingTE(; dS = 1, dT = 1, dTf = 1, dC = 1, τS = -1, τT = -1, ηTf = 1, τC = -1)
EmbeddingTE(opt::OptimiseTraditional, s, t, [c])</code></pre><p><code>EmbeddingTE</code> provide embedding parameters for transfer entropy analysis using either <a href="#CausalityTools.TEShannon"><code>TEShannon</code></a>, <a href="@ref"><code>TERenyi</code></a>, or in general any subtype of <a href="#CausalityTools.TransferEntropy"><code>TransferEntropy</code></a>, which in turns dictates the embedding used with <a href="@ref"><code>transferentropy</code></a>.</p><p>The second method finds parameters using the <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/traditional/">&quot;traditional&quot;</a> optimised embedding techniques from DynamicalSystems.jl</p><p><strong>Convention for generalized delay reconstruction</strong></p><p>We use the following convention. Let <span>$s(i)$</span> be time series for the source variable, <span>$t(i)$</span> be the time series for the target variable and <span>$c(i)$</span> the time series for the conditional variable. To compute transfer entropy, we need the following marginals:</p><p class="math-container">\[\begin{aligned}
T^{+} &amp;= \{t(i+\eta^1), t(i+\eta^2), \ldots, (t(i+\eta^{d_{T^{+}}}) \} \\
T^{-} &amp;= \{ (t(i+\tau^0_{T}), t(i+\tau^1_{T}), t(i+\tau^2_{T}), \ldots, t(t + \tau^{d_{T} - 1}_{T})) \} \\
S^{-} &amp;= \{ (s(i+\tau^0_{S}), s(i+\tau^1_{S}), s(i+\tau^2_{S}), \ldots, s(t + \tau^{d_{S} - 1}_{S})) \} \\
C^{-} &amp;= \{ (c(i+\tau^0_{C}), c(i+\tau^1_{C}), c(i+\tau^2_{C}), \ldots, c(t + \tau^{d_{C} - 1}_{C})) \}
\end{aligned}\]</p><p>Depending on the application, the delay reconstruction lags <span>$\tau^k_{T} \leq 0$</span>, <span>$\tau^k_{S} \leq 0$</span>, and <span>$\tau^k_{C} \leq 0$</span> may be equally spaced, or non-equally spaced. The same applied to the prediction lag(s), but typically only a only a single predictions lag <span>$\eta^k$</span> is used (so that <span>$d_{T^{+}} = 1$</span>).</p><p>For transfer entropy, traditionally at least one <span>$\tau^k_{T}$</span>, one <span>$\tau^k_{S}$</span> and one <span>$\tau^k_{C}$</span> equals zero. This way, the <span>$T^{-}$</span>, <span>$S^{-}$</span> and <span>$C^{-}$</span> marginals always contains present/past states, while the <span>$\mathcal T$</span> marginal contain future states relative to the other marginals. However, this is not a strict requirement, and modern approaches that searches for optimal embeddings can return embeddings without the intantaneous lag.</p><p>Combined, we get the generalized delay reconstruction <span>$\mathbb{E} = (T^{+}_{(d_{T^{+}})}, T^{-}_{(d_{T})}, S^{-}_{(d_{S})}, C^{-}_{(d_{C})})$</span>. Transfer entropy is then computed as</p><p class="math-container">\[\begin{aligned}
TE_{S \rightarrow T | C} = \int_{\mathbb{E}} P(T^{+}, T^-, S^-, C^-)
\log_{b}{\left(\frac{P(T^{+} | T^-, S^-, C^-)}{P(T^{+} | T^-, C^-)}\right)},
\end{aligned}\]</p><p>or, if conditionals are not relevant,</p><p class="math-container">\[\begin{aligned}
TE_{S \rightarrow T} = \int_{\mathbb{E}} P(T^{+}, T^-, S^-)
\log_{b}{\left(\frac{P(T^{+} | T^-, S^-)}{P(T^{+} | T^-)}\right)},
\end{aligned}\]</p><p>Here,</p><ul><li><span>$T^{+}$</span> denotes the <span>$d_{T^{+}}$</span>-dimensional set of vectors furnishing the future   states of <span>$T$</span> (almost always equal to 1 in practical applications),</li><li><span>$T^{-}$</span> denotes the <span>$d_{T}$</span>-dimensional set of vectors furnishing the past and   present states of <span>$T$</span>,</li><li><span>$S^{-}$</span> denotes the <span>$d_{S}$</span>-dimensional set of vectors furnishing the past and   present of <span>$S$</span>, and</li><li><span>$C^{-}$</span> denotes the <span>$d_{C}$</span>-dimensional set of vectors furnishing the past and   present of <span>$C$</span>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>dS</code>, <code>dT</code>, <code>dC</code>, <code>dTf</code> (<code>f</code> for <em>future</em>) are the dimensions of the <span>$S^{-}$</span>,   <span>$T^{-}$</span>, <span>$C^{-}$</span> and <span>$T^{+}$</span> marginals. The parameters <code>dS</code>, <code>dT</code>, <code>dC</code> and <code>dTf</code>   must each be a <em>positive</em> integer number.</li><li><code>τS</code>, <code>τT</code>, <code>τC</code> are the embedding lags for <span>$S^{-}$</span>, <span>$T^{-}$</span>, <span>$C^{-}$</span>.   Each parameter are integers <code>∈ 𝒩⁰⁻</code>, or a vector of integers <code>∈ 𝒩⁰⁻</code>, so   that <span>$S^{-}$</span>, <span>$T^{-}$</span>, <span>$C^{-}$</span> always represents present/past values.   If e.g. <code>τT</code> is an integer, then for the <span>$T^-$</span> marginal is constructed using   lags <span>$\tau_{T} = \{0, \tau, 2\tau, \ldots, (d_{T}- 1)\tau_T \}$</span>.   If is a vector, e.g. <code>τΤ = [-1, -5, -7]</code>, then the dimension <code>dT</code> must match the lags,   and precisely those lags are used: <span>$\tau_{T} = \{-1, -5, -7 \}$</span>.</li><li>The prediction lag(s) <code>ηTf</code> is a positive integer. Combined with the requirement   that the other delay parameters are zero or negative, this ensures that we&#39;re   always predicting from past/present to future. In typical applications,   <code>ηTf = 1</code> is used for transfer entropy.</li></ul><p><strong>Examples</strong></p><p>Say we wanted to compute the Shannon transfer entropy <span>$TE^S(S \to T) = I^S(T^+; S^- | T^-)$</span>. Using some modern procedure for determining optimal embedding parameters using <a href="https://juliadynamics.github.io/DynamicalSystems.jl/dev/embedding/unified/">methods from DynamicalSystems.jl</a>, we find that the optimal embedding of <span>$T^{-}$</span> is three-dimensional and is given by the lags <code>[0, -5, -8]</code>. Using the same procedure, we find that the optimal embedding of <span>$S^{-}$</span> is two-dimensional with lags <span>$[-1, -8]$</span>. We want to predicting a univariate version of the target variable one time step into the future (<code>ηTf = 1</code>). The total embedding is then the set of embedding vectors</p><p><span>$E_{TE} = \{ (T(i+1), S(i-1), S(i-8), T(i), T(i-5), T(i-8)) \}$</span>. Translating this to code, we get:</p><pre><code class="language-julia-repl hljs">using CausalityTools
julia&gt; EmbeddingTE(dT=3, τT=[0, -5, -8], dS=2, τS=[-1, -4], ηTf=1)

# output
EmbeddingTE(dS=2, dT=3, dC=1, dTf=1, τS=[-1, -4], τT=[0, -5, -8], τC=-1, ηTf=1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/information/definitions/transferentropy/embedding.jl#L3-L114">source</a></section></article><h3 id="Single-variable-information-API-(from-ComplexityMeasures.jl)"><a class="docs-heading-anchor" href="#Single-variable-information-API-(from-ComplexityMeasures.jl)">Single-variable information API (from ComplexityMeasures.jl)</a><a id="Single-variable-information-API-(from-ComplexityMeasures.jl)-1"></a><a class="docs-heading-anchor-permalink" href="#Single-variable-information-API-(from-ComplexityMeasures.jl)" title="Permalink"></a></h3><p>Below we list some relevant types from <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl">ComplexityMeasures.jl</a> that  are used for the <a href="#CausalityTools.EntropyDecomposition"><code>EntropyDecomposition</code></a> estimator.</p><h4 id="Entropies"><a class="docs-heading-anchor" href="#Entropies">Entropies</a><a id="Entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Entropies" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Shannon" href="#ComplexityMeasures.Shannon"><code>ComplexityMeasures.Shannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Shannon &lt;: InformationMeasure
Shannon(; base = 2)</code></pre><p>The Shannon (<a href="../references/#Shannon1948">Shannon, 1948</a>) entropy, used with <a href="@ref"><code>information</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$\log$</span> at the given <code>base</code>.</p><p>The maximum value of the Shannon entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="@ref"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/shannon.jl#L3-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Renyi" href="#ComplexityMeasures.Renyi"><code>ComplexityMeasures.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: InformationMeasure
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi generalized order-<code>q</code> entropy (<a href="../references/#Rényi1961">Rényi, 1961</a>), used with <a href="@ref"><code>information</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <a href="../references/#Shannon1948">Shannon (1948)</a>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>The maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="@ref"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/renyi.jl#L3-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Tsallis" href="#ComplexityMeasures.Tsallis"><code>ComplexityMeasures.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: InformationMeasure
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis generalized order-<code>q</code> entropy (<a href="../references/#Tsallis1988">Tsallis, 1988</a>), used with <a href="@ref"><code>information</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>The maximum value of the Tsallis entropy is <span>$k(L^{1 - q} - 1)/(1 - q)$</span>, with <span>$L$</span> the <a href="@ref"><code>total_outcomes</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/tsallis.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Kaniadakis" href="#ComplexityMeasures.Kaniadakis"><code>ComplexityMeasures.Kaniadakis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kaniadakis &lt;: InformationMeasure
Kaniadakis(; κ = 1.0, base = 2.0)</code></pre><p>The Kaniadakis entropy (<a href="../references/#Tsallis2009">Tsallis, 2009</a>), used with <a href="@ref"><code>information</code></a> to compute</p><p class="math-container">\[H_K(p) = -\sum_{i=1}^N p_i f_\kappa(p_i),\]</p><p class="math-container">\[f_\kappa (x) = \dfrac{x^\kappa - x^{-\kappa}}{2\kappa},\]</p><p>where if <span>$\kappa = 0$</span>, regular logarithm to the given <code>base</code> is used, and 0 probabilities are skipped.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/information_measure_definitions/kaniadakis.jl#L3-L18">source</a></section></article><h4 id="Discrete-information-estimators"><a class="docs-heading-anchor" href="#Discrete-information-estimators">Discrete information estimators</a><a id="Discrete-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-information-estimators" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.DiscreteInfoEstimator" href="#ComplexityMeasures.DiscreteInfoEstimator"><code>ComplexityMeasures.DiscreteInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DiscreteInfoEstimator</code></pre><p>The supertype of all discrete information measure estimators, which are used in combination with a <a href="@ref"><code>ProbabilitiesEstimator</code></a> as input to  <a href="@ref"><code>information</code></a> or related functions.</p><p>The first argument to a discrete estimator is always an <a href="@ref"><code>InformationMeasure</code></a> (defaults to <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>).</p><p><strong>Description</strong></p><p>A discrete <a href="@ref"><code>InformationMeasure</code></a> is a functional of a probability mass function. To estimate such a measure from data, we must first estimate a probability mass function using a <a href="@ref"><code>ProbabilitiesEstimator</code></a> from the (encoded/discretized) input data, and then apply the estimator to the estimated probabilities. For example, the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically computed using the <a href="@ref"><code>RelativeAmount</code></a> estimator to compute probabilities, which are then given to the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator. Many other estimators exist, not only for <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy, but other information measures as well.</p><p>We provide a library of both generic estimators such as <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> or <a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a> (which can be applied to any measure), as well as dedicated estimators such as <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>, which computes <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy using the Miller-Madow bias correction. The list below gives a complete overview.</p><p><strong>Implementations</strong></p><p>The following estimators are generic and can compute any <a href="@ref"><code>InformationMeasure</code></a>.</p><ul><li><a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a>. The default, generic plug-in estimator of any information measure.   It computes the measure exactly as stated in the definition, using the computed   probability mass function.</li><li><a href="#ComplexityMeasures.Jackknife"><code>Jackknife</code></a>. Uses the a combination of the plug-in estimator and the jackknife   principle to estimate the information measure.</li></ul><p><strong><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators</strong></p><p>The following estimators are dedicated <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimators, which provide improvements over the naive <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator.</p><ul><li><a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a>.</li><li><a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a>.</li><li><a href="#ComplexityMeasures.Schuermann"><code>Schuermann</code></a>.</li><li><a href="#ComplexityMeasures.GeneralizedSchuermann"><code>GeneralizedSchuermann</code></a>.</li><li><a href="#ComplexityMeasures.ChaoShen"><code>ChaoShen</code></a>.</li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Any of the implemented <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s can be used in combination with <em>any</em> <a href="@ref"><code>ProbabilitiesEstimator</code></a> as input to <a href="@ref"><code>information</code></a>. What this means is that every estimator actually comes in many different variants - one for each <a href="@ref"><code>ProbabilitiesEstimator</code></a>. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is typically calculated with <a href="@ref"><code>RelativeAmount</code></a> probabilities. But here, you can use for example the <a href="@ref"><code>BayesianRegularization</code></a> or the <a href="@ref"><code>Shrinkage</code></a> probabilities estimators instead, i.e. <code>information(MillerMadow(), RelativeAmount(outcome_space), x)</code> and <code>information(MillerMadow(), BayesianRegularization(outcomes_space), x)</code> are distinct estimators. This holds for all <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s. Many of these estimators haven&#39;t been explored in the literature before, so feel free to explore, and please cite this software if you use it to explore some new estimator combination!</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/core/information_measures.jl#L100-L160">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.PlugIn" href="#ComplexityMeasures.PlugIn"><code>ComplexityMeasures.PlugIn</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PlugIn(e::InformationMeasure) &lt;: DiscreteInfoEstimatorGeneric</code></pre><p>The <code>PlugIn</code> estimator is also called the empirical/naive/&quot;maximum likelihood&quot; estimator, and is used with <a href="@ref"><code>information</code></a> to any discrete <a href="@ref"><code>InformationMeasure</code></a>.</p><p>It computes any quantity exactly as given by its formula. When computing an information measure, which here is defined as a probabilities functional, it computes the quantity directly from a probability mass function, which is derived from maximum-likelihood (<a href="@ref"><code>RelativeAmount</code></a> estimates of the probabilities.</p><p><strong>Bias of plug-in estimates</strong></p><p>The plugin-estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy underestimates the true entropy, with a bias that grows with the number of distinct <a href="@ref"><code>outcomes</code></a> (Arora et al., 2022)(<a href="../references/#Arora2022">Arora <em>et al.</em>, 2022</a>),</p><p class="math-container">\[bias(H_S^{plugin}) = -\dfrac{K-1}{2N} + o(N^-1).\]</p><p>where <code>K</code> is the number of distinct outcomes, and <code>N</code> is the sample size. Many authors have tried to remedy this by proposing alternative Shannon entropy estimators. For example, the <a href="#ComplexityMeasures.MillerMadow"><code>MillerMadow</code></a> estimator is a simple correction to the plug-in estimator that adds back the bias term above. Many other estimators exist; see <a href="#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s for an overview.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/plugin.jl#L4-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.MillerMadow" href="#ComplexityMeasures.MillerMadow"><code>ComplexityMeasures.MillerMadow</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MillerMadow &lt;: DiscreteInfoEstimatorShannon
MillerMadow(measure::Shannon = Shannon())</code></pre><p>The <code>MillerMadow</code> estimator is used with <a href="@ref"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../references/#Miller1955">Miller (1955)</a>.</p><p><strong>Description</strong></p><p>The Miller-Madow estimator of Shannon entropy is given by</p><p class="math-container">\[H_S^{MM} = H_S^{plugin} + \dfrac{m - 1}{2N},\]</p><p>where <span>$H_S^{plugin}$</span> is the Shannon entropy estimated using the <a href="#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, <code>m</code> is the number of bins with nonzero probability (as defined in <a href="../references/#Paninski2003">Paninski (2003)</a>), and <code>N</code> is the number of observations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/miller_madow.jl#L3-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Schuermann" href="#ComplexityMeasures.Schuermann"><code>ComplexityMeasures.Schuermann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Schuermann &lt;: DiscreteInfoEstimatorShannon
Schuermann(definition::Shannon; a = 1.0)</code></pre><p>The <code>Schuermann</code> estimator is used with <a href="@ref"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in <a href="../references/#Schurmann2004">Schuermann (2004)</a>.</p><p>See detailed description for <a href="#ComplexityMeasures.GeneralizedSchuermann"><code>GeneralizedSchuermann</code></a> for details.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/schurmann.jl#L6-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.GeneralizedSchuermann" href="#ComplexityMeasures.GeneralizedSchuermann"><code>ComplexityMeasures.GeneralizedSchuermann</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GeneralizedSchuermann &lt;: DiscreteInfoEstimatorShannon
GeneralizedSchuermann(definition = Shannon(); a = 1.0)</code></pre><p>The <code>GeneralizedSchuermann</code> estimator is used with <a href="@ref"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy with the bias-corrected estimator given in <a href="../references/#Grassberger2022">Grassberger (2022)</a>.</p><p>The &quot;generalized&quot; part of the name, as opposed to the <a href="../references/#Schurmann2004">Schuermann (2004)</a> estimator (<a href="#ComplexityMeasures.Schuermann"><code>Schuermann</code></a>), is due to the possibility of picking difference parameters <span>$a_i$</span> for different outcomes. If different parameters are assigned to the different outcomes, <code>a</code> must be a vector of parameters of length <code>length(outcomes)</code>, where the outcomes are obtained using <a href="@ref"><code>outcomes</code></a>. See <a href="../references/#Grassberger2022">Grassberger (2022)</a> for more information. If <code>a</code> is a real number, then <span>$a_i = a \forall i$</span>, and the estimator reduces to the <a href="#ComplexityMeasures.Schuermann"><code>Schuermann</code></a> estimator.</p><p><strong>Description</strong></p><p>For a set of <span>$N$</span> observations over <span>$M$</span> outcomes, the estimator is given by</p><p class="math-container">\[H_S^{opt} = \varphi(N) - \dfrac{1}{N} \sum_{i=1}^M n_i G_{n_i}(a_i),\]</p><p>where <span>$n_i$</span> is the observed frequency of the i-th outcome,</p><p class="math-container">\[G_n(a) = \varphi(n) + (-1)^n \int_0^a \dfrac{x^{n - 1}}{x + 1} dx,\]</p><p><span>$G_n(1) = G_n$</span> and <span>$G_n(0) = \varphi(n)$</span>, and</p><p class="math-container">\[G_n = \varphi(n) + (-1)^n \int_0^1 \dfrac{x^{n - 1}}{x + 1} dx.\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/schurmann_generalized.jl#L3-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Jackknife" href="#ComplexityMeasures.Jackknife"><code>ComplexityMeasures.Jackknife</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Jackknife &lt;: DiscreteInfoEstimatorGeneric
Jackknife(definition::InformationMeasure = Shannon())</code></pre><p>The <code>Jackknife</code> estimator is used with <a href="@ref"><code>information</code></a> to compute any discrete <a href="@ref"><code>InformationMeasure</code></a>.</p><p>The <code>Jackknife</code> estimator uses the generic jackknife principle to reduce bias. <a href="../references/#Zahl1977">Zahl (1977)</a> was the first to apply the jaccknife technique in the context of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy estimation. Here, we&#39;ve generalized his estimator to work with any <a href="@ref"><code>InformationMeasure</code></a>.</p><p><strong>Description</strong></p><p>As an example of the jackknife technique, here is the formula for a jackknife estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy</p><p class="math-container">\[H_S^{J} = N H_S^{plugin} - \dfrac{N-1}{N} \sum_{i=1}^N {H_S^{plugin}}^{-\{i\}},\]</p><p>where <span>$N$</span> is the sample size, <span>$H_S^{plugin}$</span> is the plugin estimate of Shannon entropy, and <span>${H_S^{plugin}}^{-\{i\}}$</span> is the plugin estimate, but computed with the <span>$i$</span>-th sample left out.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/jackknife.jl#L3-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.HorvitzThompson" href="#ComplexityMeasures.HorvitzThompson"><code>ComplexityMeasures.HorvitzThompson</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HorvitzThompson &lt;: DiscreteInfoEstimatorShannon
HorvitzThompson(measure::Shannon = Shannon())</code></pre><p>The <code>HorvitzThompson</code> estimator is used with <a href="@ref"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../references/#Horvitz1952">Horvitz and Thompson (1952)</a>.</p><p><strong>Description</strong></p><p>The Horvitz-Thompson estimator of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy is given by</p><p class="math-container">\[H_S^{HT} = -\sum_{i=1}^M \dfrac{p_i \log(p_i) }{1 - (1 - p_i)^N},\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="@ref"><code>outcomes</code></a>. Given the true probability <span>$p_i$</span> of the <span>$i$</span>-th outcome, <span>$1 - (1 - p_i)^N$</span> is the probability that the outcome appears at least once in a sample of size <span>$N$</span> (<a href="../references/#Arora2022">Arora <em>et al.</em>, 2022</a>). Dividing by this inclusion probability is a form of weighting, and compensates for situations where certain outcomes have so low probabilities that they are not often observed in a sample, for example in power-law distributions.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/horvitz_thompson.jl#L3-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.ChaoShen" href="#ComplexityMeasures.ChaoShen"><code>ComplexityMeasures.ChaoShen</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChaoShen &lt;: DiscreteInfoEstimatorShannon
ChaoShen(definition::Shannon = Shannon())</code></pre><p>The <code>ChaoShen</code> estimator is used with <a href="@ref"><code>information</code></a> to compute the discrete <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy according to <a href="../references/#Chao2003">Chao and Shen (2003)</a>.</p><p><strong>Description</strong></p><p>This estimator is a modification of the <a href="#ComplexityMeasures.HorvitzThompson"><code>HorvitzThompson</code></a> estimator that multiplies each plugin probability estimate by an estimate of sample coverage. If <span>$f_1$</span> is the number of singletons (outcomes that occur only once) in a sample of length <span>$N$</span>, then the sample coverage is <span>$C = 1 - \dfrac{f_1}{N}$</span>. The Chao-Shen estimator of Shannon entropy is then</p><p class="math-container">\[H_S^{CS} = -\sum_{i=1}^M \left( \dfrac{C p_i \log(C p_i)}{1 - (1 - C p_i)^N} \right),\]</p><p>where <span>$N$</span> is the sample size and <span>$M$</span> is the number of <a href="@ref"><code>outcomes</code></a>. If <span>$f_1 = N$</span>, then <span>$f_1$</span> is set to <span>$f_1 = N - 1$</span> to ensure positive entropy (<a href="../references/#Arora2022">Arora <em>et al.</em>, 2022</a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/discrete_info_estimators/chao_shen.jl#L3-L25">source</a></section></article><h4 id="Differential-information-estimators"><a class="docs-heading-anchor" href="#Differential-information-estimators">Differential information estimators</a><a id="Differential-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-information-estimators" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.DifferentialInfoEstimator" href="#ComplexityMeasures.DifferentialInfoEstimator"><code>ComplexityMeasures.DifferentialInfoEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferentialInfoEstimator</code></pre><p>The supertype of all differential information measure estimators. These estimators compute an information measure in various ways that do not involve explicitly estimating a probability distribution.</p><p>Each <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><p>See <a href="@ref"><code>information</code></a> for usage.</p><p><strong>Implementations</strong></p><ul><li><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>.</li><li><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>.</li><li><a href="#ComplexityMeasures.Goria"><code>Goria</code></a>.</li><li><a href="#ComplexityMeasures.Gao"><code>Gao</code></a>.</li><li><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></li><li><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a>.</li><li><a href="#ComplexityMeasures.Lord"><code>Lord</code></a>.</li><li><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>.</li><li><a href="#ComplexityMeasures.Correa"><code>Correa</code></a>.</li><li><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>.</li><li><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>.</li><li><a href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>LeonenkoProzantoSavani</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/core/information_measures.jl#L167-L194">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Kraskov" href="#ComplexityMeasures.Kraskov"><code>ComplexityMeasures.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: DifferentialInfoEstimator
Kraskov(definition = Shannon(); k::Int = 1, w::Int = 0)</code></pre><p>The <code>Kraskov</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> using the <code>k</code>-th nearest neighbor searches method from <a href="../references/#Kraskov2004">Kraskov <em>et al.</em> (2004)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Kraskov</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Kraskov.jl#L3-L29">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.KozachenkoLeonenko" href="#ComplexityMeasures.KozachenkoLeonenko"><code>ComplexityMeasures.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: DifferentialInfoEstimator
KozachenkoLeonenko(definition = Shannon(); w::Int = 0)</code></pre><p>The <code>KozachenkoLeonenko</code> estimator (<a href="../references/#KozachenkoLeonenko1987">Kozachenko and Leonenko, 1987</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>using the nearest neighbor method from <a href="../references/#KozachenkoLeonenko1987">Kozachenko and Leonenko (1987)</a>, as described in <a href="../references/#Charzyńska2015">Charzyńska and Gambin (2016)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/KozachenkoLeonenko.jl#L3-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Zhu" href="#ComplexityMeasures.Zhu"><code>ComplexityMeasures.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: DifferentialInfoEstimator
Zhu(; definition = Shannon(), k = 1, w = 0)</code></pre><p>The <code>Zhu</code> estimator (<a href="../references/#Zhu2015">Zhu <em>et al.</em>, 2015</a>) is an extension to <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, and computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Zhu</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>by approximating densities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Zhu.jl#L3-L30">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.ZhuSingh" href="#ComplexityMeasures.ZhuSingh"><code>ComplexityMeasures.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: DifferentialInfoEstimator
ZhuSingh(definition = Shannon(); k = 1, w = 0)</code></pre><p>The <code>ZhuSingh</code> estimator (<a href="../references/#Zhu2015">Zhu <em>et al.</em>, 2015</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>ZhuSingh</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Like <a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in <a href="../references/#Singh2003">Singh <em>et al.</em> (2003)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/ZhuSingh.jl#L8-L37">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Gao" href="#ComplexityMeasures.Gao"><code>ComplexityMeasures.Gao</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Gao &lt;: DifferentialInfoEstimator
Gao(definition = Shannon(); k = 1, w = 0, corrected = true)</code></pre><p>The <code>Gao</code> estimator (<a href="../references/#Gao2015">Gao <em>et al.</em>, 09–12 May 2015</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a>, using a <code>k</code>-th nearest-neighbor approach based on <a href="../references/#Singh2003">Singh <em>et al.</em> (2003)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><a href="../references/#Gao2015">Gao <em>et al.</em> (09–12 May 2015)</a> give two variants of this estimator. If <code>corrected == false</code>, then the uncorrected version is used. If <code>corrected == true</code>, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Gao.jl#L8-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Goria" href="#ComplexityMeasures.Goria"><code>ComplexityMeasures.Goria</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Goria &lt;: DifferentialInfoEstimator
Goria(measure = Shannon(); k = 1, w = 0)</code></pre><p>The <code>Goria</code> estimator (<a href="../references/#Goria2005">Goria <em>et al.</em>, 2005</a>) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Goria</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Specifically, let <span>$\bf{n}_1, \bf{n}_2, \ldots, \bf{n}_N$</span> be the distance of the samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> to their <code>k</code>-th nearest neighbors. Next, let the geometric mean of the distances be</p><p class="math-container">\[\hat{\rho}_k = \left( \prod_{i=1}^N \right)^{\dfrac{1}{N}}\]</p><p><a href="../references/#Goria2005">Goria <em>et al.</em> (2005)</a>&#39;s estimate of Shannon differential entropy is then</p><p class="math-container">\[\hat{H} = m\hat{\rho}_k + \log(N - 1) - \psi(k) + \log c_1(m),\]</p><p>where <span>$c_1(m) = \dfrac{2\pi^\frac{m}{2}}{m \Gamma(m/2)}$</span> and <span>$\psi$</span> is the digamma function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Goria.jl#L8-L43">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Lord" href="#ComplexityMeasures.Lord"><code>ComplexityMeasures.Lord</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lord &lt;: DifferentialInfoEstimator
Lord(measure = Shannon(); k = 10, w = 0)</code></pre><p>The <code>Lord</code> estimator (<a href="../references/#Lord2018">Lord <em>et al.</em>, 2018</a>) estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> using a nearest neighbor approach with a local nonuniformity correction (LNC), with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function <span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Lord</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))],\]</p><p>by using the resubstitution formula</p><p class="math-container">\[\hat{\bar{X}, k} = -\mathbb{E}[\log(f(X))]
\approx \sum_{i = 1}^N \log(\hat{f}(\bf{x}_i)),\]</p><p>where <span>$\hat{f}(\bf{x}_i)$</span> is an estimate of the density at <span>$\bf{x}_i$</span> constructed in a manner such that <span>$\hat{f}(\bf{x}_i) \propto \dfrac{k(x_i) / N}{V_i}$</span>, where <span>$k(x_i)$</span> is the number of points in the neighborhood of <span>$\bf{x}_i$</span>, and <span>$V_i$</span> is the volume of that neighborhood.</p><p>While most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities <span>$\hat{f}(\bf{x}_i)$</span>, the <code>Lord</code> estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point <code>xᵢ</code>, estimated using singular value decomposition (SVD) on the <code>k</code>-th nearest neighbors of <code>xᵢ</code>. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes <code>Lord</code> a well-suited entropy estimator for a wide range of systems.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/Lord.jl#L25-L67">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.LeonenkoProzantoSavani" href="#ComplexityMeasures.LeonenkoProzantoSavani"><code>ComplexityMeasures.LeonenkoProzantoSavani</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LeonenkoProzantoSavani &lt;: DifferentialInfoEstimator
LeonenkoProzantoSavani(definition = Shannon(); k = 1, w = 0)</code></pre><p>The <code>LeonenkoProzantoSavani</code> estimator (<a href="../references/#LeonenkoProzantoSavani2008">Leonenko <em>et al.</em>, 2008</a>) computes the  <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>, <a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>, or <a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a> differential <a href="@ref"><code>information</code></a> of a multi-dimensional <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p><strong>Description</strong></p><p>The estimator uses <code>k</code>-th nearest-neighbor searches.  <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>For details, see <a href="../references/#LeonenkoProzantoSavani2008">Leonenko <em>et al.</em> (2008)</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/nearest_neighbors/LeonenkoProzantoSavani.jl#L7-L23">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Vasicek" href="#ComplexityMeasures.Vasicek"><code>ComplexityMeasures.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: DifferentialInfoEstimator
Vasicek(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Vasicek</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a timeseries using the method from <a href="../references/#Vasicek1976">Vasicek (1976)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Vasicek</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>, of which <a href="../references/#Vasicek1976">Vasicek (1976)</a> was the first. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Vasicek</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Vasicek</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_V(\bar{X}, m) =
\dfrac{1}{n}
\sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/Vasicek.jl#L3-L53">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.AlizadehArghami" href="#ComplexityMeasures.AlizadehArghami"><code>ComplexityMeasures.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AlizadehArghami &lt;: DifferentialInfoEstimator
AlizadehArghami(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>AlizadehArghami</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a timeseries using the method from <a href="../references/#Alizadeh2010">Alizadeh and Arghami (2010)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>AlizadehArghami</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>AlizadehArghami</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>:</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp.\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>AlizadehArghami</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then the the <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a> estimate <span>$\hat{H}_{V}(\bar{X}, m, n)$</span>, plus a correction factor</p><p class="math-container">\[\hat{H}_{A}(\bar{X}, m, n) = \hat{H}_{V}(\bar{X}, m, n) +
\dfrac{2}{n}\left(m \log(2) \right).\]</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/AlizadehArghami.jl#L3-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Ebrahimi" href="#ComplexityMeasures.Ebrahimi"><code>ComplexityMeasures.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: DifferentialInfoEstimator
Ebrahimi(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Ebrahimi</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> <a href="@ref"><code>information</code></a> of a timeseries using the method from <a href="../references/#Ebrahimi1994">Ebrahimi <em>et al.</em> (1994)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Ebrahimi</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Ebrahimi</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Ebrahimi</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_{E}(\bar{X}, m) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{n}{c_i m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \\
    2,                    &amp; m + 1 \geq i \geq n - m \\
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/Ebrahimi.jl#L3-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.Correa" href="#ComplexityMeasures.Correa"><code>ComplexityMeasures.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: DifferentialInfoEstimator
Correa(definition = Shannon(); m::Int = 1)</code></pre><p>The <code>Correa</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="@ref"><code>information</code></a> of a timeseries using the method from <a href="../references/#Correa1995">Correa (1995)</a>, with logarithms to the <code>base</code> specified in <code>definition</code>.</p><p>The <code>Correa</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Correa</code> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, <code>Correa</code> makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>, ensuring that end points are included. The <code>Correa</code> estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy is then</p><p class="math-container">\[H_C(\bar{X}, m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(\bar{X}_{(j)} -
\tilde{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (\bar{X}_{(j)} - \tilde{X}_{(i)})^2}
\right],\]</p><p>where</p><p class="math-container">\[\tilde{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p><p>See also: <a href="@ref"><code>information</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/ComplexityMeasures.jl/blob/v3.6.5/src/differential_info_estimators/order_statistics/Correa.jl#L3-L55">source</a></section></article><h2 id="correlation_api"><a class="docs-heading-anchor" href="#correlation_api">Correlation measures</a><a id="correlation_api-1"></a><a class="docs-heading-anchor-permalink" href="#correlation_api" title="Permalink"></a></h2><p>This page lists all available <a href="@ref"><code>CorrelationMeasure</code></a>s, as  well as their convenience functions. The <a href="@ref correlation_examples">examples</a> is also useful.</p><h3 id="Pearson-correlation"><a class="docs-heading-anchor" href="#Pearson-correlation">Pearson correlation</a><a id="Pearson-correlation-1"></a><a class="docs-heading-anchor-permalink" href="#Pearson-correlation" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.PearsonCorrelation" href="#CausalityTools.PearsonCorrelation"><code>CausalityTools.PearsonCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PearsonCorrelation</code></pre><p>The Pearson correlation of two variables.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw Pearson correlation coefficient.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise dependence   using the Pearson correlation coefficient.</li></ul><p><strong>Description</strong></p><p>The sample <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation coefficient</a> for real-valued random variables <span>$X$</span> and <span>$Y$</span> with associated samples <span>$\{x_i\}_{i=1}^N$</span> and <span>$\{y_i\}_{i=1}^N$</span> is defined as</p><p class="math-container">\[\rho_{xy} = \dfrac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }{\sqrt{\sum_{i=1}^N (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^N (y_i - \bar{y})^2}},\]</p><p>where <span>$\bar{x}$</span> and <span>$\bar{y}$</span> are the means of the observations <span>$x_k$</span> and <span>$y_k$</span>, respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/correlation/pearson_correlation.jl#L3-L26">source</a></section></article><h3 id="Partial-correlation"><a class="docs-heading-anchor" href="#Partial-correlation">Partial correlation</a><a id="Partial-correlation-1"></a><a class="docs-heading-anchor-permalink" href="#Partial-correlation" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.PartialCorrelation" href="#CausalityTools.PartialCorrelation"><code>CausalityTools.PartialCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PartialCorrelation &lt;: AssociationMeasure</code></pre><p>The correlation of two variables, with the effect of a set of conditioning variables removed.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw partial correlation coefficient.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for   correlated-based conditional independence.</li></ul><p><strong>Description</strong></p><p>There are several ways of estimating the partial correlation. We follow the <a href="https://en.wikipedia.org/wiki/Partial_correlation">matrix inversion method</a>, because for <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s, we can very efficiently compute the required joint covariance matrix <span>$\Sigma$</span> for the random variables.</p><p>Formally, let <span>$X_1, X_2, \ldots, X_n$</span> be a set of <span>$n$</span> real-valued random variables. Consider the joint precision matrix,<span>$P = (p_{ij}) = \Sigma^-1$</span>. The partial correlation of any pair of variables <span>$(X_i, X_j)$</span>, given the remaining variables <span>$\bf{Z} = \{X_k\}_{i=1, i \neq i, j}^n$</span>, is defined as</p><p class="math-container">\[\rho_{X_i X_j | \bf{Z}} = -\dfrac{p_ij}{\sqrt{ p_{ii} p_{jj} }}\]</p><p>In practice, we compute the estimate</p><p class="math-container">\[\hat{\rho}_{X_i X_j | \bf{Z}} =
-\dfrac{\hat{p}_ij}{\sqrt{ \hat{p}_{ii} \hat{p}_{jj} }},\]</p><p>where <span>$\hat{P} = \hat{\Sigma}^{-1}$</span> is the sample precision matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/correlation/partial_correlation.jl#L3-L39">source</a></section></article><h3 id="Distance-correlation"><a class="docs-heading-anchor" href="#Distance-correlation">Distance correlation</a><a id="Distance-correlation-1"></a><a class="docs-heading-anchor-permalink" href="#Distance-correlation" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.DistanceCorrelation" href="#CausalityTools.DistanceCorrelation"><code>CausalityTools.DistanceCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DistanceCorrelation</code></pre><p>The distance correlation (<a href="../references/#Szekely2007">Székely <em>et al.</em>, 2007</a>) measure quantifies potentially nonlinear associations between pairs of variables. If applied to three variables, the partial distance correlation (<a href="../references/#Szekely2014">Székely and Rizzo, 2014</a>) is computed.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw (partial) distance correlation   coefficient.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for   pairwise dependence.</li></ul><p><strong>Description</strong></p><p>The distance correlation can be used to compute the association between two variables, or the conditional association between three variables, like so:</p><pre><code class="nohighlight hljs">association(DistanceCorrelation(), x, y) → dcor ∈ [0, 1]
association(DistanceCorrelation(), x, y, z) → pdcor</code></pre><p>With two variable, we comptue <code>dcor</code>, which is called the empirical/sample distance  correlation (<a href="../references/#Szekely2007">Székely <em>et al.</em>, 2007</a>). With three variables, the  partial distance correlation <code>pdcor</code> is computed (<a href="../references/#Szekely2014">Székely and Rizzo, 2014</a>).</p><div class="admonition is-category-warn"><header class="admonition-header">Warn</header><div class="admonition-body"><p>A partial distance correlation <code>distance_correlation(X, Y, Z) = 0</code> doesn&#39;t always guarantee conditional independence <code>X ⫫ Y | Z</code>. <a href="../references/#Szekely2014">Székely and Rizzo (2014)</a> for an in-depth discussion.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/correlation/distance_correlation.jl#L8-L39">source</a></section></article><h2 id="cross_map_api"><a class="docs-heading-anchor" href="#cross_map_api">Cross-map measures</a><a id="cross_map_api-1"></a><a class="docs-heading-anchor-permalink" href="#cross_map_api" title="Permalink"></a></h2><p>The cross-map measures define different ways of quantifying association based on the  concept of &quot;cross mapping&quot;, which has appeared in many contexts in the literature, and gained huge popularity with  <a href="../references/#Sugihara2012">Sugihara <em>et al.</em> (2012)</a>&#39;s on <em>convergent cross mapping</em>.</p><p>Since their paper, several cross mapping methods and frameworks have emerged in the literature. In CausalityTools.jl, we provide a unified interface for using these cross mapping methods.</p><h3 id="Measures"><a class="docs-heading-anchor" href="#Measures">Measures</a><a id="Measures-1"></a><a class="docs-heading-anchor-permalink" href="#Measures" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CrossmapMeasure" href="#CausalityTools.CrossmapMeasure"><code>CausalityTools.CrossmapMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CrossmapMeasure &lt;: AssociationMeasure</code></pre><p>The supertype for all cross-map measures. Concrete subtypes are</p><ul><li><a href="#CausalityTools.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a>, or <a href="@ref"><code>CCM</code></a> for short.</li><li><a href="#CausalityTools.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a>, or <a href="@ref"><code>PAI</code></a> for short.</li></ul><p>See also: <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/crossmappings.jl#L12-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ConvergentCrossMapping" href="#CausalityTools.ConvergentCrossMapping"><code>CausalityTools.ConvergentCrossMapping</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConvergentCrossMapping &lt;: CrossmapMeasure
ConvergentCrossMapping(; d::Int = 2, τ::Int = -1, w::Int = 0,
    f = Statistics.cor, embed_warn = true)</code></pre><p>The convergent cross mapping measure (<a href="../references/#Sugihara2012">Sugihara <em>et al.</em>, 2012</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> together with a <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a> to compute the    cross-map correlation between input variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.RandomSegment"><code>RandomSegment</code></a></li><li><a href="#CausalityTools.RandomVectors"><code>RandomVectors</code></a></li><li><a href="#CausalityTools.ExpandingSegment"><code>ExpandingSegment</code></a></li></ul><p><strong>Description</strong></p><p>The Theiler window <code>w</code> controls how many temporal neighbors are excluded during neighbor  searches (<code>w = 0</code> means that only the point itself is excluded). <code>f</code> is a function that computes the agreement between observations and predictions (the default, <code>f = Statistics.cor</code>, gives the Pearson correlation coefficient).</p><p><strong>Embedding</strong></p><p>Let <code>S(i)</code> be the source time series variable and <code>T(i)</code> be the target time series variable. This version produces regular embeddings with fixed dimension <code>d</code> and embedding lag <code>τ</code> as follows:</p><p class="math-container">\[( S(i), S(i+\tau), S(i+2\tau), \ldots, S(i+(d-1)\tau, T(i))_{i=1}^{N-(d-1)\tau}.\]</p><p>In this joint embedding, neighbor searches are performed in the subspace spanned by the first <code>D-1</code> variables, while the last (<code>D</code>-th) variable is to be predicted.</p><p>With this convention, <code>τ &lt; 0</code> implies &quot;past/present values of source used to predict target&quot;, and <code>τ &gt; 0</code> implies &quot;future/present values of source used to predict target&quot;. The latter case may not be meaningful for many applications, so by default, a warning will be given if <code>τ &gt; 0</code> (<code>embed_warn = false</code> turns off warnings).</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_ConvergentCrossMapping_RandomVectors">Example 1</a>.    Estimation with <a href="#CausalityTools.RandomVectors"><code>RandomVectors</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_ConvergentCrossMapping_RandomSegment">Example 2</a>.    Estimation with <a href="#CausalityTools.RandomSegment"><code>RandomSegment</code></a> estimator.</li><li><a href="../extended_examples/cross_mapping/#example_ConvergentCrossMapping_reproducing_sugihara">Example 3</a>: Reproducing    figures from <a href="../references/#Sugihara2012">Sugihara <em>et al.</em> (2012)</a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/ccm-like/ConvergentCrossMapping.jl#L6-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.PairwiseAsymmetricInference" href="#CausalityTools.PairwiseAsymmetricInference"><code>CausalityTools.PairwiseAsymmetricInference</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PairwiseAsymmetricInference &lt;: CrossmapMeasure
PairwiseAsymmetricInference(; d::Int = 2, τ::Int = -1, w::Int = 0,
    f = Statistics.cor, embed_warn = true)</code></pre><p>The pairwise asymmetric inference (PAI) measure (<a href="../references/#McCracken2014">McCracken and Weigel, 2014</a>) is a version of <a href="#CausalityTools.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a> that searches for neighbors in <em>mixed</em> embeddings (i.e. both source and target variables included); otherwise, the algorithms are identical.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the pairwise asymmetric inference measure    between variables.</li></ul><p><strong>Compatible estimators</strong></p><ul><li><a href="#CausalityTools.RandomSegment"><code>RandomSegment</code></a></li><li><a href="#CausalityTools.RandomVectors"><code>RandomVectors</code></a></li><li><a href="#CausalityTools.ExpandingSegment"><code>ExpandingSegment</code></a></li></ul><p><strong>Description</strong></p><p>The Theiler window <code>w</code> controls how many temporal neighbors are excluded during neighbor  searches (<code>w = 0</code> means that only the point itself is excluded). <code>f</code> is a function that computes the agreement between observations and predictions (the default, <code>f = Statistics.cor</code>, gives the Pearson correlation coefficient).</p><p><strong>Embedding</strong></p><p>There are many possible ways of defining the embedding for PAI. Currently, we only implement the <em>&quot;add one non-lagged source timeseries to an embedding of the target&quot;</em> approach, which is used as an example in McCracken &amp; Weigel&#39;s paper. Specifically: Let <code>S(i)</code> be the source time series variable and <code>T(i)</code> be the target time series variable. <code>PairwiseAsymmetricInference</code> produces regular embeddings with fixed dimension <code>d</code> and embedding lag <code>τ</code> as follows:</p><p class="math-container">\[(S(i), T(i+(d-1)\tau, \ldots, T(i+2\tau), T(i+\tau), T(i)))_{i=1}^{N-(d-1)\tau}.\]</p><p>In this joint embedding, neighbor searches are performed in the subspace spanned by the first <code>D</code> variables, while the last variable is to be predicted.</p><p>With this convention, <code>τ &lt; 0</code> implies &quot;past/present values of source used to predict target&quot;, and <code>τ &gt; 0</code> implies &quot;future/present values of source used to predict target&quot;. The latter case may not be meaningful for many applications, so by default, a warning will be given if <code>τ &gt; 0</code> (<code>embed_warn = false</code> turns off warnings).</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_PairwiseAsymmetricInference_RandomVectors">Example 1</a>.    Estimation with <a href="#CausalityTools.RandomVectors"><code>RandomVectors</code></a> estimator.</li><li><a href="../examples/examples_associations/#example_PairwiseAsymmetricInference_RandomSegment">Example 2</a>.    Estimation with <a href="#CausalityTools.RandomSegment"><code>RandomSegment</code></a> estimator.</li><li><a href="../extended_examples/pairwise_asymmetric_inference/#example_PairwiseAsymmetricInference_reproduce_mccracken">Example 3</a>. Reproducing    McCracken &amp; Weigel&#39;s results from the original paper.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/ccm-like/PairwiseAsymmetricInference.jl#L6-L64">source</a></section></article><h3 id="Estimators"><a class="docs-heading-anchor" href="#Estimators">Estimators</a><a id="Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.CrossmapEstimator" href="#CausalityTools.CrossmapEstimator"><code>CausalityTools.CrossmapEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CrossmapEstimator{M&lt;:CrossmapMeasure, LIBSIZES, RNG}</code></pre><p>The abstract supertype for all cross-map estimators.</p><p><strong>Concrete subtypes</strong></p><ul><li><a href="#CausalityTools.RandomVectors"><code>RandomVectors</code></a></li><li><a href="#CausalityTools.RandomSegment"><code>RandomSegment</code></a></li><li><a href="#CausalityTools.ExpandingSegment"><code>ExpandingSegment</code></a></li></ul><p><strong>Description</strong></p><p>Because the type of the library may differ between estimators, and because RNGs from different packages may be used, subtypes must implement the <code>LIBSIZES</code> and <code>RNG</code> type parameters.</p><p>For efficiency purposes, subtypes may contain mutable containers that can be re-used for ensemble analysis (see <a href="@ref"><code>Ensemble</code></a>).</p><div class="admonition is-info"><header class="admonition-header">Libraries</header><div class="admonition-body"><p>A cross-map estimator uses the concept of &quot;libraries&quot;. A library is essentially just a reference to a set of points, and usually, a library refers to <em>indices</em> of points, not the actual points themselves.</p><p>For example, for timeseries, <code>RandomVectors(libsizes = 50:25:100)</code> produces three separate libraries, where the first contains 50 randomly selected time indices, the second contains 75 randomly selected time indices, and the third contains 100 randomly selected time indices. This of course assumes that all quantities involved can be indexed using the same time indices, meaning that the concept of &quot;library&quot; only makes sense <em>after</em> relevant quantities have been <em>jointly</em> embedded, so that they can be jointly indexed. For non-instantaneous prediction, the maximum possible library size shrinks with the magnitude of the index/time-offset for the prediction.</p><p>For spatial analyses (not yet implemented), indices could be more complex and involve multi-indices.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/crossmappings.jl#L24-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.RandomVectors" href="#CausalityTools.RandomVectors"><code>CausalityTools.RandomVectors</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomVectors &lt;: CrossmapEstimator
RandomVectors(definition::CrossmapMeasure; libsizes, replace = false, 
    rng = Random.default_rng())</code></pre><p>Cross map <em>once</em> over  <code>N = length(libsizes)</code> different &quot;point libraries&quot;, where  point indices are selected randomly (not considering time ordering). </p><p>This is method 3 from <a href="../references/#Luo2015">Luo <em>et al.</em> (2015)</a>. See <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a> for an in-depth  explanation of what &quot;library&quot; means in this context.</p><p><strong>Description</strong></p><p>The cardinality of the point libraries are given by <code>libsizes</code>. One set of  random point indices is selected per <code>L ∈ libsizes</code>, and the <code>i</code>-th  library has cardinality <code>k = libsizes[i]</code>. </p><p>Point indices within each library are randomly selected, independently of other libraries. A user-specified <code>rng</code> may be specified for reproducibility. The <code>replace</code> argument controls whether sampling is done with or without replacement. If the time series you&#39;re cross mapping between have length <code>M</code>, and <code>Lᵢ &lt; M</code> for any <code>Lᵢ ∈ libsizes</code>, then you must set <code>replace = true</code>.</p><p><strong>Returns</strong></p><p>The return type when used with <a href="#CausalityTools.association"><code>association</code></a> depends on the type of <code>libsizes</code>.</p><ul><li>If <code>libsizes</code> is an <code>Int</code> (a single library), then a single cross-map estimate is returned.</li><li>If <code>libsizes</code> is an <code>AbstractVector{Int}</code> (multiple libraries), then a vector of cross-map   estimates is returned –- one per library.</li></ul><p>See also: <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/estimators/RandomVectors.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.RandomSegment" href="#CausalityTools.RandomSegment"><code>CausalityTools.RandomSegment</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RandomSegment &lt;: CrossmapEstimator
RandomSegment(definition::CrossmapMeasure; libsizes::Int, rng = Random.default_rng())</code></pre><p>Cross map <em>once</em> over <code>N = length(libsizes)</code> different &quot;point libraries&quot;, where  point indices are selected as time-contiguous segments with random starting points.</p><p>This is method 2 from <a href="../references/#Luo2015">Luo <em>et al.</em> (2015)</a>. See <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a> for an in-depth  explanation of what &quot;library&quot; means in this context.</p><p><strong>Description</strong></p><p>The cardinality of the point index segments are given by <code>libsizes</code>. One segment  with a randomly selected starting point is picked per <code>L ∈ libsizes</code>, and the <code>i</code>-th  point index segment has cardinality <code>k = libsizes[i]</code>. </p><p>The starting point for each library is selected independently of other libraries. A user-specified <code>rng</code> may be specified for reproducibility. If the time series you&#39;re cross mapping between have length <code>M</code>, and <code>Lᵢ &lt; M</code> for any <code>Lᵢ ∈ libsizes</code>, then an error will be thrown.</p><p>A user-specified <code>rng</code> may be specified for reproducibility.</p><p><strong>Returns</strong></p><p>The return type when used with <a href="#CausalityTools.association"><code>association</code></a> depends on the type of <code>libsizes</code>.</p><ul><li>If <code>libsizes</code> is an <code>Int</code> (a single library), then a single cross-map estimate is returned.</li><li>If <code>libsizes</code> is an <code>AbstractVector{Int}</code> (multiple libraries), then a vector of cross-map   estimates is returned –- one per library.</li></ul><p>See also: <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/estimators/RandomSegment.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.ExpandingSegment" href="#CausalityTools.ExpandingSegment"><code>CausalityTools.ExpandingSegment</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ExpandingSegment &lt;: CrossmapEstimator
ExpandingSegment(definition::CrossmapMeasure; libsizes, rng = Random.default_rng())</code></pre><p>Cross map <em>once</em> over <code>N = length(libsizes)</code> different &quot;point libraries&quot;, where  point indices are selected as time-contiguous segments/windows.</p><p>This is the method from (<a href="../references/#Sugihara2012">Sugihara <em>et al.</em>, 2012</a>). See <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a> for an in-depth  explanation of what &quot;library&quot; means in this context.</p><p><strong>Description</strong></p><p>Point index segments are selected as first available data point index, up to the <code>L</code>th data point index. This results in one library of contiguous time indices per <code>L ∈ libsizes</code>.</p><p>If used in an ensemble setting, the estimator is applied to time indices <code>Lmin:step:Lmax</code> of the joint embedding.</p><p><strong>Returns</strong></p><p>The return type when used with <a href="#CausalityTools.association"><code>association</code></a> depends on the type of <code>libsizes</code>.</p><ul><li>If <code>libsizes</code> is an <code>Int</code> (a single library), then a single cross-map estimate is returned.</li><li>If <code>libsizes</code> is an <code>AbstractVector{Int}</code> (multiple libraries), then a vector of cross-map   estimates is returned –- one per library.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/estimators/ExpandingSegment.jl#L3-L27">source</a></section></article><h3 id="Advanced-utility-methods"><a class="docs-heading-anchor" href="#Advanced-utility-methods">Advanced utility methods</a><a id="Advanced-utility-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-utility-methods" title="Permalink"></a></h3><p>For most use cases, it is sufficient to provide a <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a> to  <a href="#CausalityTools.association"><code>association</code></a> to compute a cross map measure. However, in some cases it  can be useful to have more fine-grained controls. We offer a few utility functions for this purpose.</p><p>In the example where we <a href="../extended_examples/cross_mapping/#example_sugihara_figs3Cand3D">reproduce Figures 3C and 3D</a> of <a href="@ref">Sugihara2012</a>, these lower-level  functions are used.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.predict" href="#CausalityTools.predict"><code>CausalityTools.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict(measure::CrossmapEstimator, t::AbstractVector, s::AbstractVector) → t̂ₛ, t̄, ρ
predict(measure::CrossmapEstimator, t̄::AbstractVector, S̄::AbstractStateSpaceSet) → t̂ₛ</code></pre><p>Perform point-wise cross mappings between source embeddings and target time series according to the algorithm specified by the given cross-map <code>measure</code> (e.g. <a href="#CausalityTools.ConvergentCrossMapping"><code>ConvergentCrossMapping</code></a> or <a href="#CausalityTools.PairwiseAsymmetricInference"><code>PairwiseAsymmetricInference</code></a>).</p><ul><li><strong>First method</strong>: Jointly embeds the target <code>t</code> and source <code>s</code> time series (according to   <code>measure</code>) to obtain time-index aligned target timeseries <code>t̄</code> and source embedding   <code>S̄</code> (which is now a <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>).   Then calls <code>predict(measure, t̄, S̄)</code> (the first method), and returns both the   predictions <code>t̂ₛ</code>, observations <code>t̄</code> and their correspondence <code>ρ</code> according to <code>measure</code>.</li><li><strong>Second method</strong>: Returns a vector of predictions <code>t̂ₛ</code> (<code>t̂ₛ</code> := &quot;predictions of <code>t̄</code> based   on source embedding <code>S̄</code>&quot;), where <code>t̂ₛ[i]</code> is the prediction for <code>t̄[i]</code>. It assumes   pre-embedded data which have been correctly time-aligned using a joint embedding   (see <a href="@ref"><code>embed</code></a>), i.e. such that <code>t̄[i]</code> and <code>S̄[i]</code> correspond to the same time   index.</li></ul><p><strong>Description</strong></p><p>For each <code>i ∈ {1, 2, …, N}</code> where <code>N = length(t) == length(s)</code>, we make the prediction <code>t̂[i]</code> (an estimate of <code>t[i]</code>) based on a linear combination of <code>D + 1</code> other points in <code>t</code>, where the selection of points and weights for the linear combination are determined by the <code>D+1</code> nearest neighbors of the point <code>S̄[i]</code>. The details of point selection and weights depend on <code>measure</code>.</p><p><em>Note: Some <a href="#CausalityTools.CrossmapMeasure"><code>CrossmapMeasure</code></a>s may define more general mapping procedures. If so, the algorithm is described in their docstring</em>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/crossmappings.jl#L115-L144">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.crossmap" href="#CausalityTools.crossmap"><code>CausalityTools.crossmap</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">crossmap(measure::CrossmapEstimator, t::AbstractVector, s::AbstractVector) → ρ::Real
crossmap(measure::CrossmapEstimator, est, t::AbstractVector, s::AbstractVector) → ρ::Vector
crossmap(measure::CrossmapEstimator, t̄::AbstractVector, S̄::AbstractStateSpaceSet) → ρ</code></pre><p>Compute the cross map estimates between between raw time series <code>t</code> and <code>s</code> (and return the real-valued cross-map statistic <code>ρ</code>). If a <a href="#CausalityTools.CrossmapEstimator"><code>CrossmapEstimator</code></a> <code>est</code> is provided, cross mapping is done on random subsamples of the data, where subsampling is dictated by <code>est</code> (a vector of values for <code>ρ</code> is returned).</p><p>Alternatively, cross-map between time-aligned time series <code>t̄</code> and source embedding <code>S̄</code> that have been constructed by jointly (pre-embedding) some input data.</p><p>This is just a wrapper around <a href="#CausalityTools.predict"><code>predict</code></a> that simply returns the correspondence measure between the source and the target.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/crossmappings/crossmappings.jl#L93-L108">source</a></section></article><h2 id="closeness_api"><a class="docs-heading-anchor" href="#closeness_api">Closeness measures</a><a id="closeness_api-1"></a><a class="docs-heading-anchor-permalink" href="#closeness_api" title="Permalink"></a></h2><h3 id="Joint-distance-distribution"><a class="docs-heading-anchor" href="#Joint-distance-distribution">Joint distance distribution</a><a id="Joint-distance-distribution-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-distance-distribution" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.JointDistanceDistribution" href="#CausalityTools.JointDistanceDistribution"><code>CausalityTools.JointDistanceDistribution</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointDistanceDistribution &lt;: AssociationMeasure end
JointDistanceDistribution(; metric = Euclidean(), B = 10, D = 2, τ = -1, μ = 0.0)</code></pre><p>The joint distance distribution (JDD) measure (<a href="../references/#Amigo2018">Amigó and Hirata, 2018</a>).</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the joint distance distribution measure <code>Δ</code> from   <a href="../references/#Amigo2018">Amigó and Hirata (2018)</a>.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional   dependence.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><strong><code>distance_metric::Metric</code></strong>: An instance of a valid distance metric from <code>Distances.jl</code>.   Defaults to <code>Euclidean()</code>.</li><li><strong><code>B::Int</code></strong>: The number of equidistant subintervals to divide the interval <code>[0, 1]</code> into   when comparing the normalised distances.</li><li><strong><code>D::Int</code></strong>: Embedding dimension.</li><li><strong><code>τ::Int</code></strong>: Embedding delay. By convention, <code>τ</code> is negative.</li><li><strong><code>μ</code></strong>: The hypothetical mean value of the joint distance distribution if there   is no coupling between <code>x</code> and <code>y</code> (default is <code>μ = 0.0</code>).</li></ul><p><strong>Description</strong></p><p>From input time series <span>$x(t)$</span> and <span>$y(t)$</span>, we first construct the delay embeddings (note the positive sign in the embedding lags; therefore the input parameter <code>τ</code> is by convention negative).</p><p class="math-container">\[\begin{align*}
\{\bf{x}_i \} &amp;= \{(x_i, x_{i+\tau}, \ldots, x_{i+(d_x - 1)\tau}) \} \\
\{\bf{y}_i \} &amp;= \{(y_i, y_{i+\tau}, \ldots, y_{i+(d_y - 1)\tau}) \} \\
\end{align*}\]</p><p>The algorithm then proceeds to analyze the distribution of distances between points of these embeddings, as described in <a href="../references/#Amigo2018">Amigó and Hirata (2018)</a>.</p><p><strong>Examples</strong></p><ul><li><a href="@ref quickstart_jdd">Computing the JDD</a></li><li><a href="@ref quickstart_jddtest">Independence testing using JDD</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/closeness/JointDistanceDistribution.jl#L18-L62">source</a></section></article><h3 id="S-measure"><a class="docs-heading-anchor" href="#S-measure">S-measure</a><a id="S-measure-1"></a><a class="docs-heading-anchor-permalink" href="#S-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.SMeasure" href="#CausalityTools.SMeasure"><code>CausalityTools.SMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SMeasure &lt; ClosenessMeasure
SMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p><code>SMeasure</code> is a bivariate association measure from <a href="../references/#Arnhold1999">Arnhold <em>et al.</em> (1999)</a> and <a href="../references/#Quiroga2000">Quiroga <em>et al.</em> (2000)</a> that measure directional dependence between two input (potentially multivariate) time series.</p><p>Note that <code>τx</code> and <code>τy</code> are negative; see explanation below.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw s-measure statistic.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p>The steps of the algorithm are:</p><ol><li>From input time series <span>$x(t)$</span> and <span>$y(t)$</span>, construct the delay embeddings (note  the positive sign in the embedding lags; therefore inputs parameters  <code>τx</code> and <code>τy</code> are by convention negative).</li></ol><p class="math-container">\[\begin{align*}
\{\bf{x}_i \} &amp;= \{(x_i, x_{i+\tau_x}, \ldots, x_{i+(d_x - 1)\tau_x}) \} \\
\{\bf{y}_i \} &amp;= \{(y_i, y_{i+\tau_y}, \ldots, y_{i+(d_y - 1)\tau_y}) \} \\
\end{align*}\]</p><ol><li><p>Let <span>$r_{i,j}$</span> and <span>$s_{i,j}$</span> be the indices of the <code>K</code>-th nearest neighbors  of <span>$\bf{x}_i$</span> and <span>$\bf{y}_i$</span>, respectively. Neighbors closed than <code>w</code> time indices  are excluded during searches (i.e. <code>w</code> is the Theiler window).</p></li><li><p>Compute the the mean squared Euclidean distance to the <span>$K$</span> nearest neighbors  for each <span>$x_i$</span>, using the indices <span>$r_{i, j}$</span>.</p></li></ol><p class="math-container">\[R_i^{(k)}(x) = \dfrac{1}{k} \sum_{i=1}^{k}(\bf{x}_i, \bf{x}_{r_{i,j}})^2\]</p><ul><li>Compute the y-conditioned mean squared Euclidean distance to the <span>$K$</span> nearest   neighbors for each <span>$x_i$</span>, now using the indices <span>$s_{i,j}$</span>.</li></ul><p class="math-container">\[R_i^{(k)}(x|y) = \dfrac{1}{k} \sum_{i=1}^{k}(\bf{x}_i, \bf{x}_{s_{i,j}})^2\]</p><ul><li>Define the following measure of independence, where <span>$0 \leq S \leq 1$</span>, and   low values indicate independence and values close to one occur for   synchronized signals.</li></ul><p class="math-container">\[S^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N} \dfrac{R_i^{(k)}(x)}{R_i^{(k)}(x|y)}\]</p><p><strong>Input data</strong></p><p>The algorithm is slightly modified from (???) to allow univariate timeseries as input.</p><ul><li>If <code>x</code> and <code>y</code> are <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s then use <code>x</code> and <code>y</code> as is and ignore the parameters   <code>dx</code>/<code>τx</code> and <code>dy</code>/<code>τy</code>.</li><li>If <code>x</code> and <code>y</code> are scalar time series, then create <code>dx</code> and <code>dy</code> dimensional embeddings,   respectively, of both <code>x</code> and <code>y</code>, resulting in <code>N</code> different <code>m</code>-dimensional embedding points   <span>$X = \{x_1, x_2, \ldots, x_N \}$</span> and <span>$Y = \{y_1, y_2, \ldots, y_N \}$</span>.   <code>τx</code> and <code>τy</code> control the embedding lags for <code>x</code> and <code>y</code>.</li><li>If <code>x</code> is a scalar-valued vector and <code>y</code> is a <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>, or vice versa,   then create an embedding of the scalar timeseries using parameters <code>dx</code>/<code>τx</code> or <code>dy</code>/<code>τy</code>.</li></ul><p>In all three cases, input StateSpaceSets are length-matched by eliminating points at the end of the longest StateSpaceSet (after the embedding step, if relevant) before analysis.</p><p>See also: <a href="@ref"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/closeness/SMeasure.jl#L9-L82">source</a></section></article><h3 id="H-measure"><a class="docs-heading-anchor" href="#H-measure">H-measure</a><a id="H-measure-1"></a><a class="docs-heading-anchor-permalink" href="#H-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.HMeasure" href="#CausalityTools.HMeasure"><code>CausalityTools.HMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">HMeasure &lt;: AssociationMeasure
HMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>HMeasure</code> (<a href="../references/#Arnhold1999">Arnhold <em>et al.</em>, 1999</a>) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw h-measure statistic.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p>The <code>HMeasure</code> (<a href="../references/#Arnhold1999">Arnhold <em>et al.</em>, 1999</a>) is similar to the <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>, but the numerator of the formula is replaced by <span>$R_i(x)$</span>, the mean squared Euclidean distance to <em>all other points</em>, and there is a <span>$\log$</span>-term inside the sum:</p><p class="math-container">\[H^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{R_i(x)}{R_i^{(k)}(x|y)} \right).\]</p><p>Parameters are the same and <span>$R_i^{(k)}(x|y)$</span> is computed as for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>.</p><p>See also: <a href="@ref"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/closeness/HMeasure.jl#L9-L40">source</a></section></article><h3 id="M-measure"><a class="docs-heading-anchor" href="#M-measure">M-measure</a><a id="M-measure-1"></a><a class="docs-heading-anchor-permalink" href="#M-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MMeasure" href="#CausalityTools.MMeasure"><code>CausalityTools.MMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MMeasure &lt;: ClosenessMeasure
MMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>MMeasure</code> (<a href="../references/#Andrzejak2003">Andrzejak <em>et al.</em>, 2003</a>) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw m-measure statistic.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p>The <code>MMeasure</code> is based on <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> and <a href="#CausalityTools.HMeasure"><code>HMeasure</code></a>. It is given by</p><p class="math-container">\[M^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{R_i(x) - R_i^{(k)}(x|y)}{R_i(x) - R_i^k(x)} \right),\]</p><p>where <span>$R_i(x)$</span> is computed as for <a href="#CausalityTools.HMeasure"><code>HMeasure</code></a>, while <span>$R_i^k(x)$</span> and <span>$R_i^{(k)}(x|y)$</span> is computed as for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>. Parameters also have the same meaning as for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a>/<a href="#CausalityTools.HMeasure"><code>HMeasure</code></a>.</p><p>See also: <a href="@ref"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/closeness/MMeasure.jl#L9-L39">source</a></section></article><h3 id="L-measure"><a class="docs-heading-anchor" href="#L-measure">L-measure</a><a id="L-measure-1"></a><a class="docs-heading-anchor-permalink" href="#L-measure" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.LMeasure" href="#CausalityTools.LMeasure"><code>CausalityTools.LMeasure</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LMeasure &lt;: ClosenessMeasure
LMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)</code></pre><p>The <code>LMeasure</code> (<a href="../references/#Chicharro2009">Chicharro and Andrzejak, 2009</a>) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.</p><p>Note that <code>τx</code> and <code>τy</code> are negative by convention. See docstring for <a href="#CausalityTools.SMeasure"><code>SMeasure</code></a> for an explanation.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw L-measure statistic.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for directional dependence.</li></ul><p><strong>Description</strong></p><p><code>LMeasure</code> is similar to <a href="#CausalityTools.MMeasure"><code>MMeasure</code></a>, but uses distance ranks instead of the raw distances.</p><p>Let <span>$\bf{x_i}$</span> be an embedding vector, and let <span>$g_{i,j}$</span> denote the rank that the distance between <span>$\bf{x_i}$</span> and some other vector <span>$\bf{x_j}$</span> in a sorted ascending list of distances between <span>$\bf{x_i}$</span> and <span>$\bf{x_{i \neq j}}$</span> In other words, <span>$g_{i,j}$</span> this is just the <span>$N-1$</span> nearest neighbor distances sorted )</p><p><code>LMeasure</code> is then defined as</p><p class="math-container">\[L^{(k)}(x|y) = \dfrac{1}{N} \sum_{i=1}^{N}
\log \left( \dfrac{G_i(x) - G_i^{(k)}(x|y)}{G_i(x) - G_i^k(x)} \right),\]</p><p>where <span>$G_i(x) = \frac{N}{2}$</span> and <span>$G_i^K(x) = \frac{k+1}{2}$</span> are the mean and minimal rank, respectively.</p><p>The <span>$y$</span>-conditioned mean rank is defined as</p><p class="math-container">\[G_i^{(k)}(x|y) = \dfrac{1}{K}\sum_{j=1}^{K} g_{i,w_{i, j}},\]</p><p>where <span>$w_{i,j}$</span> is the index of the <span>$j$</span>-th nearest neighbor of <span>$\bf{y_i}$</span>.</p><p>See also: <a href="@ref"><code>ClosenessMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/closeness/LMeasure.jl#L9-L54">source</a></section></article><h2 id="Recurrence-measures"><a class="docs-heading-anchor" href="#Recurrence-measures">Recurrence measures</a><a id="Recurrence-measures-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrence-measures" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.MCR" href="#CausalityTools.MCR"><code>CausalityTools.MCR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MCR &lt;: AssociationMeasure
MCR(; r, metric = Euclidean())</code></pre><p>An association measure based on mean conditional probabilities of recurrence (MCR) introduced by <a href="../references/#Romano2007">Romano <em>et al.</em> (2007)</a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a> to compute the raw MCR for pairwise or conditional association.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise or    conditional association.</li></ul><p><strong>Description</strong></p><p><code>r</code> is  mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of <code>AbstractRecurrenceType</code> from <a href="https://juliadynamics.github.io/RecurrenceAnalysis.jl/stable/">RecurrenceAnalysis.jl</a>. To use any <code>r</code> that is not a real number, you have to do <code>using RecurrenceAnalysis</code> first. The <code>metric</code> is any valid metric from <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a>.</p><p>For input variables <code>X</code> and <code>Y</code>, the conditional probability of recurrence is defined as</p><p class="math-container">\[M(X | Y) = \dfrac{1}{N} \sum_{i=1}^N p(\bf{y_i} | \bf{x_i}) =
\dfrac{1}{N} \sum_{i=1}^N \dfrac{\sum_{i=1}^N J_{R_{i, j}}^{X, Y}}{\sum_{i=1}^N R_{i, j}^X},\]</p><p>where <span>$R_{i, j}^X$</span> is the recurrence matrix and <span>$J_{R_{i, j}}^{X, Y}$</span> is the joint recurrence matrix, constructed using the given <code>metric</code>. The measure <span>$M(Y | X)$</span> is defined analogously.</p><p><a href="../references/#Romano2007">Romano <em>et al.</em> (2007)</a>&#39;s interpretation of this quantity is that if <code>X</code> drives <code>Y</code>, then <code>M(X|Y) &gt; M(Y|X)</code>, if <code>Y</code> drives <code>X</code>, then <code>M(Y|X) &gt; M(X|Y)</code>, and if coupling is symmetric,  then <code>M(Y|X) = M(X|Y)</code>.</p><p><strong>Input data</strong></p><p><code>X</code> and <code>Y</code> can be either both univariate timeseries, or both multivariate <a href="../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a>s.</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_MCR">Example 1</a>. Pairwise versus conditional MCR.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/recurrence/MCR.jl#L7-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CausalityTools.RMCD" href="#CausalityTools.RMCD"><code>CausalityTools.RMCD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMCD &lt;: AssociationMeasure
RMCD(; r, metric = Euclidean(), base = 2)</code></pre><p>The recurrence measure of conditional dependence, or RMCD (<a href="../references/#Ramos2017">Ramos <em>et al.</em>, 2017</a>), is a recurrence-based measure that mimics the conditional mutual information, but uses recurrence probabilities.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="#CausalityTools.association"><code>association</code></a>/<a href="@ref"><code>rmcd</code></a> to compute the raw RMCD for pairwise    or conditional association.</li><li>Use with <a href="../independence/#CausalityTools.independence"><code>independence</code></a> to perform a formal hypothesis test for pairwise   or conditional association.</li></ul><p><strong>Description</strong></p><p><code>r</code> is a mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of <code>AbstractRecurrenceType</code> from <a href="https://juliadynamics.github.io/RecurrenceAnalysis.jl/stable/">RecurrenceAnalysis.jl</a>. To use any <code>r</code> that is not a real number, you have to do <code>using RecurrenceAnalysis</code> first. The <code>metric</code> is any valid metric from <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a>.</p><p>Both the pairwise and conditional RMCD is non-negative, but due to round-off error, negative values may occur. If that happens, an RMCD value of <code>0.0</code> is returned.</p><p><strong>Description</strong></p><p>The RMCD measure is defined by</p><p class="math-container">\[I_{RMCD}(X; Y | Z) = \dfrac{1}{N}
\sum_{i} \left[
\dfrac{1}{N} \sum_{j} R_{ij}^{X, Y, Z}
\log \left(
    \dfrac{\sum_{j} R_{ij}^{X, Y, Z} \sum_{j} R_{ij}^{Z} }{\sum_{j} \sum_{j} R_{ij}^{X, Z} \sum_{j} \sum_{j} R_{ij}^{Y, Z}}
    \right)
\right],\]</p><p>where  <code>base</code> controls the base of the logarithm. <span>$I_{RMCD}(X; Y | Z)$</span> is zero when <span>$Z = X$</span>, <span>$Z = Y$</span> or when <span>$X$</span>, <span>$Y$</span> and <span>$Z$</span> are mutually independent.</p><p>Our implementation allows dropping the third/last argument, in which case the following mutual information-like quantitity is computed (not discussed in <a href="../references/#Ramos2017">Ramos <em>et al.</em> (2017)</a>.</p><p class="math-container">\[I_{RMCD}(X; Y) = \dfrac{1}{N}
\sum_{i} \left[
\dfrac{1}{N} \sum_{j} R_{ij}^{X, Y}
\log \left(
    \dfrac{\sum_{j} R_{ij}^{X}  R_{ij}^{Y} }{\sum_{j} R_{ij}^{X, Y}}
    \right)
\right]\]</p><p><strong>Estimation</strong></p><ul><li><a href="../examples/examples_associations/#example_RMCD">Example 1</a>. Pairwise versus conditional RMCD.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/61736a8c95922f015eb7312a11a091487ecd52d1/src/methods/recurrence/RMCD.jl#L5-L68">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« CausalityTools.jl</a><a class="docs-footer-nextpage" href="../independence/">Independence »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Wednesday 24 July 2024 01:54">Wednesday 24 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
