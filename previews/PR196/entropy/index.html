<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Entropy · CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="CausalityTools.jl logo"/></a><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../correlation_measures/">Correlation measures</a></li><li><span class="tocitem">Information measures</span><ul><li><a class="tocitem" href="../probabilities/">Probability mass functions</a></li><li class="is-active"><a class="tocitem" href>Entropy</a><ul class="internal"><li><a class="tocitem" href="#Entropies-API"><span>Entropies API</span></a></li><li><a class="tocitem" href="#Entropy-definitions"><span>Entropy definitions</span></a></li><li><a class="tocitem" href="#Discrete"><span>Discrete</span></a></li><li><a class="tocitem" href="#Differential/continuous"><span>Differential/continuous</span></a></li></ul></li><li><a class="tocitem" href="../entropy_conditional/">Conditional entropy</a></li><li><a class="tocitem" href="../mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../condmutualinfo/">Conditional mutual information</a></li><li><a class="tocitem" href="../transferentropy/">Transfer entropy</a></li></ul></li><li><a class="tocitem" href="../cross_mappings/">Cross mappings</a></li><li><a class="tocitem" href="../jdd/">Joint distance distribution</a></li><li><a class="tocitem" href="../independence/">Independence testing</a></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-7-1" type="checkbox"/><label class="tocitem" for="menuitem-7-1"><span class="docs-label">Quickstart</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../quickstart/quickstart_mi/">Mutual information</a></li><li><a class="tocitem" href="../quickstart/quickstart_jdd/">Joint distance distribution</a></li><li><a class="tocitem" href="../quickstart/quickstart_independence/">Independence testing</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7-2" type="checkbox"/><label class="tocitem" for="menuitem-7-2"><span class="docs-label">Longer examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../examples/examples_entropy/">Entropy</a></li><li><a class="tocitem" href="../examples/examples_conditional_entropy/">Conditional entropy</a></li><li><a class="tocitem" href="../examples/examples_mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../examples/examples_transferentropy/">Transfer entropy</a></li><li><a class="tocitem" href="../examples/examples_cross_mappings/">Cross mappings</a></li><li><a class="tocitem" href="../examples/examples_independence/">Independence testing</a></li></ul></li></ul></li><li><a class="tocitem" href="../experimental/">Experimental</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Information measures</a></li><li class="is-active"><a href>Entropy</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Entropy</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/entropy.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="entropies"><a class="docs-heading-anchor" href="#entropies">Entropies</a><a id="entropies-1"></a><a class="docs-heading-anchor-permalink" href="#entropies" title="Permalink"></a></h1><h2 id="Entropies-API"><a class="docs-heading-anchor" href="#Entropies-API">Entropies API</a><a id="Entropies-API-1"></a><a class="docs-heading-anchor-permalink" href="#Entropies-API" title="Permalink"></a></h2><p>The entropies API is defined by</p><ul><li><a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a></li><li><a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a></li><li><a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a></li></ul><p>Please be sure you have read the <a href="@ref">Terminology</a> section before going through the API here, to have a good idea of the different &quot;flavors&quot; of entropies and how they all come together over the common interface of the <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> function.</p><h2 id="Entropy-definitions"><a class="docs-heading-anchor" href="#Entropy-definitions">Entropy definitions</a><a id="Entropy-definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Entropy-definitions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.EntropyDefinition" href="#ComplexityMeasures.EntropyDefinition"><code>ComplexityMeasures.EntropyDefinition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EntropyDefinition</code></pre><p><code>EntropyDefinition</code> is the supertype of all types that encapsulate definitions of (generalized) entropies. These also serve as estimators of discrete entropies, see description below.</p><p>Currently implemented entropy definitions are:</p><ul><li><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a>.</li><li><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>.</li><li><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a>, which is a subcase of the above two in the limit <code>q → 1</code>.</li><li><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a>.</li><li><a href="#ComplexityMeasures.Curado"><code>Curado</code></a>.</li><li><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a>.</li></ul><p>These types can be given as inputs to <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> or <a href="#ComplexityMeasures.entropy_normalized"><code>entropy_normalized</code></a>.</p><p><strong>Description</strong></p><p>Mathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amigó et al.&#39;s<sup class="footnote-reference"><a id="citeref-Amigó2018" href="#footnote-Amigó2018">[Amigó2018]</a></sup> summary paper gives a nice overview.</p><p>However, for a software implementation computing entropies <em>in practice</em>, definitions is not really what matters; <strong>estimators matter</strong>. Because in the practical sense, one needs to estimate a definition from finite data, and different ways of estimating a quantity come with their own pros and cons.</p><p>That is why the type <a href="@ref"><code>DiscreteEntropyEstimator</code></a> exists, which is what is actually given to <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>. Some ways to estimate a discrete entropy only apply to a specific entropy definition. For estimators that can be applied to various entropy definitions, this is specified by providing an instance of <code>EntropyDefinition</code> to the estimator.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Shannon" href="#ComplexityMeasures.Shannon"><code>ComplexityMeasures.Shannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Shannon &lt;: EntropyDefinition
Shannon(; base = 2)</code></pre><p>The Shannon<sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup> entropy, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute:</p><p class="math-container">\[H(p) = - \sum_i p[i] \log(p[i])\]</p><p>with the <span>$\log$</span> at the given <code>base</code>.</p><p>The maximum value of the Shannon entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Renyi" href="#ComplexityMeasures.Renyi"><code>ComplexityMeasures.Renyi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Renyi &lt;: EntropyDefinition
Renyi(q, base = 2)
Renyi(; q = 1.0, base = 2)</code></pre><p>The Rényi<sup class="footnote-reference"><a id="citeref-Rényi1960" href="#footnote-Rényi1960">[Rényi1960]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute an entropy with units given by <code>base</code> (typically <code>2</code> or <code>MathConstants.e</code>).</p><p><strong>Description</strong></p><p>Let <span>$p$</span> be an array of probabilities (summing to 1). Then the Rényi generalized entropy is</p><p class="math-container">\[H_q(p) = \frac{1}{1-q} \log \left(\sum_i p[i]^q\right)\]</p><p>and generalizes other known entropies, like e.g. the information entropy (<span>$q = 1$</span>, see <sup class="footnote-reference"><a id="citeref-Shannon1948" href="#footnote-Shannon1948">[Shannon1948]</a></sup>), the maximum entropy (<span>$q=0$</span>, also known as Hartley entropy), or the correlation entropy (<span>$q = 2$</span>, also known as collision entropy).</p><p>The maximum value of the Rényi entropy is <span>$\log_{base}(L)$</span>, which is the entropy of the uniform distribution with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Tsallis" href="#ComplexityMeasures.Tsallis"><code>ComplexityMeasures.Tsallis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Tsallis &lt;: EntropyDefinition
Tsallis(q; k = 1.0, base = 2)
Tsallis(; q = 1.0, k = 1.0, base = 2)</code></pre><p>The Tsallis<sup class="footnote-reference"><a id="citeref-Tsallis1988" href="#footnote-Tsallis1988">[Tsallis1988]</a></sup> generalized order-<code>q</code> entropy, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute an entropy.</p><p><code>base</code> only applies in the limiting case <code>q == 1</code>, in which the Tsallis entropy reduces to Shannon entropy.</p><p><strong>Description</strong></p><p>The Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with <code>k</code> standing for the Boltzmann constant. It is defined as</p><p class="math-container">\[S_q(p) = \frac{k}{q - 1}\left(1 - \sum_{i} p[i]^q\right)\]</p><p>The maximum value of the Tsallis entropy is ``<span>$k(L^{1 - q} - 1)/(1 - q)$</span>, with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kaniadakis" href="#ComplexityMeasures.Kaniadakis"><code>ComplexityMeasures.Kaniadakis</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kaniadakis &lt;: EntropyDefinition
Kaniadakis(; κ = 1.0, base = 2.0)</code></pre><p>The Kaniadakis entropy (Tsallis, 2009)<sup class="footnote-reference"><a id="citeref-Tsallis2009" href="#footnote-Tsallis2009">[Tsallis2009]</a></sup>, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute</p><p class="math-container">\[H_K(p) = -\sum_{i=1}^N p_i f_\kappa(p_i),\]</p><p class="math-container">\[f_\kappa (x) = \dfrac{x^\kappa - x^{-\kappa}}{2\kappa},\]</p><p>where if <span>$\kappa = 0$</span>, regular logarithm to the given <code>base</code> is used, and 0 probabilities are skipped.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Curado" href="#ComplexityMeasures.Curado"><code>ComplexityMeasures.Curado</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Curado &lt;: EntropyDefinition
Curado(; b = 1.0)</code></pre><p>The Curado entropy (Curado &amp; Nobre, 2004)<sup class="footnote-reference"><a id="citeref-Curado2004" href="#footnote-Curado2004">[Curado2004]</a></sup>, used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute</p><p class="math-container">\[H_C(p) = \left( \sum_{i=1}^N e^{-b p_i} \right) + e^{-b} - 1,\]</p><p>with <code>b ∈ ℛ, b &gt; 0</code>, where the terms outside the sum ensures that <span>$H_C(0) = H_C(1) = 0$</span>.</p><p>The maximum entropy for Curado is <span>$L(1 - \exp(-b/L)) + \exp(-b) - 1$</span> with <span>$L$</span> the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.StretchedExponential" href="#ComplexityMeasures.StretchedExponential"><code>ComplexityMeasures.StretchedExponential</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StretchedExponential &lt;: EntropyDefinition
StretchedExponential(; η = 2.0, base = 2)</code></pre><p>The stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo &amp; Plastino, 1999<sup class="footnote-reference"><a id="citeref-Anteneodo1999" href="#footnote-Anteneodo1999">[Anteneodo1999]</a></sup>), used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> to compute</p><p class="math-container">\[S_{\eta}(p) = \sum_{i = 1}^N
\Gamma \left( \dfrac{\eta + 1}{\eta}, - \log_{base}(p_i) \right) -
p_i \Gamma \left( \dfrac{\eta + 1}{\eta} \right),\]</p><p>where <span>$\eta \geq 0$</span>, <span>$\Gamma(\cdot, \cdot)$</span> is the upper incomplete Gamma function, and <span>$\Gamma(\cdot) = \Gamma(\cdot, 0)$</span> is the Gamma function. Reduces to <a href="@ref">Shannon</a> entropy for <code>η = 1.0</code>.</p><p>The maximum entropy for <code>StrechedExponential</code> is a rather complicated expression involving incomplete Gamma functions (see source code).</p></div></section></article><h2 id="Discrete"><a class="docs-heading-anchor" href="#Discrete">Discrete</a><a id="Discrete-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}" href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>ComplexityMeasures.entropy</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">entropy([e::DiscreteEntropyEstimator,] probs::Probabilities)
entropy([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x)</code></pre><p>Compute the <strong>discrete entropy</strong> <code>h::Real ∈ [0, ∞)</code>, using the estimator <code>e</code>, in one of two ways:</p><ol><li>Directly from existing <a href="../probabilities/#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> <code>probs</code>.</li><li>From input data <code>x</code>, by first estimating a probability mass function using the provided <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>, and then computing the entropy from that mass fuction using the provided <a href="@ref"><code>DiscreteEntropyEstimator</code></a>.</li></ol><p>Instead of providing a <a href="@ref"><code>DiscreteEntropyEstimator</code></a>, an <a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a> can be given directly, in which case <a href="@ref"><code>MLEntropy</code></a> is used as the estimator. If <code>e</code> is not provided, <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a><code>()</code> is used by default.</p><p><strong>Maximum entropy and normalized entropy</strong></p><p>All discrete entropies have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the <a href="#ComplexityMeasures.entropy_maximum"><code>entropy_maximum</code></a>. Or, one can use <a href="#ComplexityMeasures.entropy_normalized"><code>entropy_normalized</code></a> to obtain the normalized form of the entropy (divided by the maximum).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = [rand(Bool) for _ in 1:10000] # coin toss
ps = probabilities(x) # gives about [0.5, 0.5] by definition
h = entropy(ps) # gives 1, about 1 bit by definition
h = entropy(Shannon(), ps) # syntactically equivalent to above
h = entropy(Shannon(), CountOccurrences(x), x) # syntactically equivalent to above
h = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition
h = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn&#39;t matter for coin toss</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy_maximum" href="#ComplexityMeasures.entropy_maximum"><code>ComplexityMeasures.entropy_maximum</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_maximum(e::EntropyDefinition, est::ProbabilitiesEstimator, x)</code></pre><p>Return the maximum value of a discrete entropy with the given probabilities estimator and input data <code>x</code>. Like in <a href="../probabilities/#ComplexityMeasures.outcome_space"><code>outcome_space</code></a>, for some estimators the concrete outcome space is known without knowledge of input <code>x</code>, in which case the function dispatches to <code>entropy_maximum(e, est)</code>.</p><pre><code class="nohighlight hljs">entropy_maximum(e::EntropyDefinition, L::Int)</code></pre><p>Same as above, but computed directly from the number of total outcomes <code>L</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.entropy_normalized" href="#ComplexityMeasures.entropy_normalized"><code>ComplexityMeasures.entropy_normalized</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">entropy_normalized([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) → h̃</code></pre><p>Return <code>h̃ ∈ [0, 1]</code>, the normalized discrete entropy of <code>x</code>, i.e. the value of <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> divided by the maximum value for <code>e</code>, according to the given probabilities estimator.</p><p>Instead of a discrete entropy estimator, an <a href="#ComplexityMeasures.EntropyDefinition"><code>EntropyDefinition</code></a> can be given as first argument. If <code>e</code> is not given, it defaults to <code>Shannon()</code>.</p><p>Notice that there is no method <code>entropy_normalized(e::DiscreteEntropyEstimator, probs::Probabilities)</code>, because there is no way to know the amount of <em>possible</em> events (i.e., the <a href="../probabilities/#ComplexityMeasures.total_outcomes"><code>total_outcomes</code></a>) from <code>probs</code>.</p></div></section></article><h2 id="Differential/continuous"><a class="docs-heading-anchor" href="#Differential/continuous">Differential/continuous</a><a id="Differential/continuous-1"></a><a class="docs-heading-anchor-permalink" href="#Differential/continuous" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>entropy(::EntropyDefinition, ::DifferentialEntropyEstimator, ::Any)</code>. Check Documenter&#39;s build log for details.</p></div></div><h3 id="Table-of-differential-entropy-estimators"><a class="docs-heading-anchor" href="#Table-of-differential-entropy-estimators">Table of differential entropy estimators</a><a id="Table-of-differential-entropy-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Table-of-differential-entropy-estimators" title="Permalink"></a></h3><p>The following estimators are <em>differential</em> entropy estimators, and can also be used with <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>.</p><p>Each <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimates the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Principle</th><th style="text-align: left">Input data</th><th style="text-align: center"><a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Renyi"><code>Renyi</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Tsallis"><code>Tsallis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Kaniadakis"><code>Kaniadakis</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.Curado"><code>Curado</code></a></th><th style="text-align: center"><a href="#ComplexityMeasures.StretchedExponential"><code>StretchedExponential</code></a></th></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="@ref"><code>Lord</code></a></td><td style="text-align: left">Nearest neighbors</td><td style="text-align: left"><code>Dataset</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.Correa"><code>Correa</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: left"><a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a></td><td style="text-align: left">Order statistics</td><td style="text-align: left"><code>Vector</code></td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.DifferentialEntropyEstimator" href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>ComplexityMeasures.DifferentialEntropyEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DifferentialEntropyEstimator
DiffEntropyEst # alias</code></pre><p>The supertype of all differential entropy estimators. These estimators compute an entropy value in various ways that do not involve explicitly estimating a probability distribution.</p><p>See the <a href="@ref table_diff_ent_est">table of differential entropy estimators</a> in the docs for all differential entropy estimators.</p><p>See <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> for usage.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Kraskov" href="#ComplexityMeasures.Kraskov"><code>ComplexityMeasures.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov &lt;: DiffEntropyEst
Kraskov(; k::Int = 1, w::Int = 1, base = 2)</code></pre><p>The <code>Kraskov</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> using the <code>k</code>-th nearest neighbor searches method from <sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup> at the given <code>base</code>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Kraskov</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.KozachenkoLeonenko" href="#ComplexityMeasures.KozachenkoLeonenko"><code>ComplexityMeasures.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko &lt;: DiffEntropyEst
KozachenkoLeonenko(; w::Int = 0, base = 2)</code></pre><p>The <code>KozachenkoLeonenko</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>using the nearest neighbor method from Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>In contrast to <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, this estimator uses only the <em>closest</em> neighbor.</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Kraskov"><code>Kraskov</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Zhu" href="#ComplexityMeasures.Zhu"><code>ComplexityMeasures.Zhu</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu &lt;: DiffEntropyEst
Zhu(; k = 1, w = 0, base = 2)</code></pre><p>The <code>Zhu</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> is an extension to <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, and computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Zhu</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p><p>by approximating densities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. <code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.ZhuSingh" href="#ComplexityMeasures.ZhuSingh"><code>ComplexityMeasures.ZhuSingh</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZhuSingh &lt;: DiffEntropyEst
ZhuSingh(; k = 1, w = 0, base = 2)</code></pre><p>The <code>ZhuSingh</code> estimator (Zhu et al., 2015)<sup class="footnote-reference"><a id="citeref-Zhu2015" href="#footnote-Zhu2015">[Zhu2015]</a></sup> computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>ZhuSingh</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Like <a href="#ComplexityMeasures.Zhu"><code>Zhu</code></a>, this estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Gao" href="#ComplexityMeasures.Gao"><code>ComplexityMeasures.Gao</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Gao &lt;: DifferentialEntropyEstimator
Gao(; k = 1, w = 0, base = 2, corrected = true)</code></pre><p>The <code>Gao</code> estimator (Gao et al., 2015) computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, using a <code>k</code>-th nearest-neighbor approach based on Singh et al. (2003)<sup class="footnote-reference"><a id="citeref-Singh2003" href="#footnote-Singh2003">[Singh2003]</a></sup>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>Gao et al., 2015 give two variants of this estimator. If <code>corrected == false</code>, then the uncorrected version is used. If <code>corrected == true</code>, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>KozachenkoLeonenko</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))]\]</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Goria" href="#ComplexityMeasures.Goria"><code>ComplexityMeasures.Goria</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Goria &lt;: DifferentialEntropyEstimator
Goria(; k = 1, w = 0, base = 2)</code></pre><p>The <code>Goria</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> of a multi-dimensional <a href="../#StateSpaceSets.Dataset"><code>Dataset</code></a> in the given <code>base</code>.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}^d$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R}^d \to \mathbb{R}$</span>. <code>Goria</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>Specifically, let <span>$\bf{n}_1, \bf{n}_2, \ldots, \bf{n}_N$</span> be the distance of the samples <span>$\{\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N \}$</span> to their <code>k</code>-th nearest neighbors. Next, let the geometric mean of the distances be</p><p class="math-container">\[\hat{\rho}_k = \left( \prod_{i=1}^N \right)^{\dfrac{1}{N}}\]</p><p>Goria et al. (2005)<sup class="footnote-reference"><a id="citeref-Goria2005" href="#footnote-Goria2005">[Goria2005]</a></sup>&#39;s estimate of Shannon differential entropy is then</p><p class="math-container">\[\hat{H} = m\hat{\rho}_k + \log(N - 1) - \psi(k) + \log c_1(m),\]</p><p>where <span>$c_1(m) = \dfrac{2\pi^\frac{m}{2}}{m \Gamma(m/2)}$</span> and <span>$\psi$</span> is the digamma function.</p></div></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>Lord</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Vasicek" href="#ComplexityMeasures.Vasicek"><code>ComplexityMeasures.Vasicek</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Vasicek &lt;: DiffEntropyEst
Vasicek(; m::Int = 1, base = 2)</code></pre><p>The <code>Vasicek</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given <code>base</code>) of a timeseries using the method from Vasicek (1976)<sup class="footnote-reference"><a id="citeref-Vasicek1976" href="#footnote-Vasicek1976">[Vasicek1976]</a></sup>.</p><p>The <code>Vasicek</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>, of which Vasicek (1976) was the first. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Vasicek</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Vasicek</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_V(\bar{X}, m) =
\dfrac{1}{n}
\sum_{i = 1}^n \log \left[ \dfrac{n}{2m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right]\]</p><p><strong>Usage</strong></p><p>In practice, choice of <code>m</code> influences how fast the entropy converges to the true value. For small value of <code>m</code>, convergence is slow, so we recommend to scale <code>m</code> according to the time series length <code>n</code> and use <code>m &gt;= n/100</code> (this is just a heuristic based on the tests written for this package).</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.AlizadehArghami" href="#ComplexityMeasures.AlizadehArghami"><code>ComplexityMeasures.AlizadehArghami</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AlizadehArghami &lt;: DiffEntropyEst
AlizadehArghami(; m::Int = 1, base = 2)</code></pre><p>The <code>AlizadehArghami</code>estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given <code>base</code>) of a timeseries using the method from Alizadeh &amp; Arghami (2010)<sup class="footnote-reference"><a id="citeref-Alizadeh2010" href="#footnote-Alizadeh2010">[Alizadeh2010]</a></sup>.</p><p>The <code>AlizadehArghami</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>AlizadehArghami</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>:</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp.\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>AlizadehArghami</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then the the <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a> estimate <span>$\hat{H}_{V}(\bar{X}, m, n)$</span>, plus a correction factor</p><p class="math-container">\[\hat{H}_{A}(\bar{X}, m, n) = \hat{H}_{V}(\bar{X}, m, n) +
\dfrac{2}{n}\left(m \log(2) \right).\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Ebrahimi" href="#ComplexityMeasures.Ebrahimi"><code>ComplexityMeasures.Ebrahimi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Ebrahimi &lt;: DiffEntropyEst
Ebrahimi(; m::Int = 1, base = 2)</code></pre><p>The <code>Ebrahimi</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given <code>base</code>) of a timeseries using the method from Ebrahimi (1994)<sup class="footnote-reference"><a id="citeref-Ebrahimi1994" href="#footnote-Ebrahimi1994">[Ebrahimi1994]</a></sup>.</p><p>The <code>Ebrahimi</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Ebrahimi</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, it makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>. The <code>Ebrahimi</code> <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy estimate is then</p><p class="math-container">\[\hat{H}_{E}(\bar{X}, m) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{n}{c_i m} (\bar{X}_{(i+m)} - \bar{X}_{(i-m)}) \right],\]</p><p>where</p><p class="math-container">\[c_i =
\begin{cases}
    1 + \frac{i - 1}{m}, &amp; 1 \geq i \geq m \\
    2,                    &amp; m + 1 \geq i \geq n - m \\
    1 + \frac{n - i}{m} &amp; n - m + 1 \geq i \geq n
\end{cases}.\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.Correa"><code>Correa</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ComplexityMeasures.Correa" href="#ComplexityMeasures.Correa"><code>ComplexityMeasures.Correa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Correa &lt;: DiffEntropyEst
Correa(; m::Int = 1, base = 2)</code></pre><p>The <code>Correa</code> estimator computes the <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a> (in the given `base) of a timeseries using the method from Correa (1995)<sup class="footnote-reference"><a id="citeref-Correa1995" href="#footnote-Correa1995">[Correa1995]</a></sup>.</p><p>The <code>Correa</code> estimator belongs to a class of differential entropy estimators based on <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a>. It only works for <em>timeseries</em> input.</p><p><strong>Description</strong></p><p>Assume we have samples <span>$\bar{X} = \{x_1, x_2, \ldots, x_N \}$</span> from a continuous random variable <span>$X \in \mathbb{R}$</span> with support <span>$\mathcal{X}$</span> and density function<span>$f : \mathbb{R} \to \mathbb{R}$</span>. <code>Correa</code> estimates the <a href="@ref">Shannon</a> differential entropy</p><p class="math-container">\[H(X) = \int_{\mathcal{X}} f(x) \log f(x) dx = \mathbb{E}[-\log(f(X))].\]</p><p>However, instead of estimating the above integral directly, <code>Correa</code> makes use of the equivalent integral, where <span>$F$</span> is the distribution function for <span>$X$</span>,</p><p class="math-container">\[H(X) = \int_0^1 \log \left(\dfrac{d}{dp}F^{-1}(p) \right) dp\]</p><p>This integral is approximated by first computing the <a href="https://en.wikipedia.org/wiki/Order_statistic">order statistics</a> of <span>$\bar{X}$</span> (the input timeseries), i.e. <span>$x_{(1)} \leq x_{(2)} \leq \cdots \leq x_{(n)}$</span>, ensuring that end points are included. The <code>Correa</code> estimate of <a href="#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy is then</p><p class="math-container">\[H_C(\bar{X}, m, n) =
\dfrac{1}{n} \sum_{i = 1}^n \log
\left[ \dfrac{ \sum_{j=i-m}^{i+m}(\bar{X}_{(j)} -
\tilde{X}_{(i)})(j - i)}{n \sum_{j=i-m}^{i+m} (\bar{X}_{(j)} - \tilde{X}_{(i)})^2}
\right],\]</p><p>where</p><p class="math-container">\[\tilde{X}_{(i)} = \dfrac{1}{2m + 1} \sum_{j = i - m}^{i + m} X_{(j)}.\]</p><p>See also: <a href="#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}"><code>entropy</code></a>, <a href="#ComplexityMeasures.AlizadehArghami"><code>AlizadehArghami</code></a>, <a href="#ComplexityMeasures.Ebrahimi"><code>Ebrahimi</code></a>, <a href="#ComplexityMeasures.Vasicek"><code>Vasicek</code></a>, <a href="#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Amigó2018"><a class="tag is-link" href="#citeref-Amigó2018">Amigó2018</a>Amigó, J. M., Balogh, S. G., &amp; Hernández, S. (2018). A brief review of generalized entropies. <a href="https://www.mdpi.com/1099-4300/20/11/813">Entropy, 20(11), 813.</a></li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Shannon1948"><a class="tag is-link" href="#citeref-Shannon1948">Shannon1948</a>C. E. Shannon, Bell Systems Technical Journal <strong>27</strong>, pp 379 (1948)</li><li class="footnote" id="footnote-Tsallis1988"><a class="tag is-link" href="#citeref-Tsallis1988">Tsallis1988</a>Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.</li><li class="footnote" id="footnote-Tsallis2009"><a class="tag is-link" href="#citeref-Tsallis2009">Tsallis2009</a>Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1.</li><li class="footnote" id="footnote-Curado2004"><a class="tag is-link" href="#citeref-Curado2004">Curado2004</a>Curado, E. M., &amp; Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.</li><li class="footnote" id="footnote-Anteneodo1999"><a class="tag is-link" href="#citeref-Anteneodo1999">Anteneodo1999</a>Anteneodo, C., &amp; Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. EntropyDefinition, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Zhu2015"><a class="tag is-link" href="#citeref-Zhu2015">Zhu2015</a>Zhu, J., Bellanger, J. J., Shu, H., &amp; Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Gao2015"><a class="tag is-link" href="#citeref-Gao2015">Gao2015</a>Gao, S., Ver Steeg, G., &amp; Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PMLR.</li><li class="footnote" id="footnote-Singh2003"><a class="tag is-link" href="#citeref-Singh2003">Singh2003</a>Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., &amp; Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.</li><li class="footnote" id="footnote-Goria2005"><a class="tag is-link" href="#citeref-Goria2005">Goria2005</a>Goria, M. N., Leonenko, N. N., Mergel, V. V., &amp; Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297.</li><li class="footnote" id="footnote-Vasicek1976"><a class="tag is-link" href="#citeref-Vasicek1976">Vasicek1976</a>Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.</li><li class="footnote" id="footnote-Alizadeh2010"><a class="tag is-link" href="#citeref-Alizadeh2010">Alizadeh2010</a>Alizadeh, N. H., &amp; Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).</li><li class="footnote" id="footnote-Ebrahimi1994"><a class="tag is-link" href="#citeref-Ebrahimi1994">Ebrahimi1994</a>Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.</li><li class="footnote" id="footnote-Correa1995"><a class="tag is-link" href="#citeref-Correa1995">Correa1995</a>Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../probabilities/">« Probability mass functions</a><a class="docs-footer-nextpage" href="../entropy_conditional/">Conditional entropy »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 6 February 2023 03:17">Monday 6 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
