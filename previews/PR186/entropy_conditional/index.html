<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Conditional entropy · CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CausalityTools.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Information measures</span><ul><li><a class="tocitem" href="../probabilities/">Probability mass functions</a></li><li><a class="tocitem" href="../entropy/">Entropy</a></li><li class="is-active"><a class="tocitem" href>Conditional entropy</a><ul class="internal"><li><a class="tocitem" href="#Conditional-entropy-API"><span>Conditional entropy API</span></a></li><li><a class="tocitem" href="#Conditional-entropy-definitions"><span>Conditional entropy definitions</span></a></li><li><a class="tocitem" href="#Discrete-conditional-entropy"><span>Discrete conditional entropy</span></a></li><li><a class="tocitem" href="#Differential/continuous-conditional-entropy"><span>Differential/continuous conditional entropy</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li></ul></li><li><a class="tocitem" href="../mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../condmutualinfo/">Conditional mutual information</a></li><li><a class="tocitem" href="../transferentropy/">Transfer entropy</a></li></ul></li><li><a class="tocitem" href="../cross_mappings/">Cross mappings</a></li><li><a class="tocitem" href="../jdd/">Joint distance distribution</a></li><li><a class="tocitem" href="../experimental/">Experimental</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Information measures</a></li><li class="is-active"><a href>Conditional entropy</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Conditional entropy</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/entropy_conditional.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Conditional-entropy"><a class="docs-heading-anchor" href="#Conditional-entropy">Conditional entropy</a><a id="Conditional-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropy" title="Permalink"></a></h1><h2 id="Conditional-entropy-API"><a class="docs-heading-anchor" href="#Conditional-entropy-API">Conditional entropy API</a><a id="Conditional-entropy-API-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropy-API" title="Permalink"></a></h2><p>The mutual information API is defined by</p><ul><li><a href="#CausalityTools.ConditionalEntropy"><code>ConditionalEntropy</code></a>,</li><li><a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a>,</li></ul><p>We provide a suite of estimators of various mutual information quantities. Many more variants exist in the literature. Pull requests are welcome!</p><h2 id="Conditional-entropy-definitions"><a class="docs-heading-anchor" href="#Conditional-entropy-definitions">Conditional entropy definitions</a><a id="Conditional-entropy-definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropy-definitions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.ConditionalEntropy" href="#CausalityTools.ConditionalEntropy"><code>CausalityTools.ConditionalEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The supertype for all conditional entropies.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/ce6e03b6806541d4ccdf70511013fa760852bb1e/src/methods/infomeasures/entropy_conditional/entropy_conditional.jl#L5-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CEShannon" href="#CausalityTools.CEShannon"><code>CausalityTools.CEShannon</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CEShannon &lt;: ConditionalEntropy
CEShannon(; base = 2,)</code></pre><p>The<a href="../entropy/#ComplexityMeasures.Shannon"><code>Shannon</code></a> conditional entropy measure.</p><p><strong>Discrete definition</strong></p><p><strong>Sum formulation</strong></p><p>The conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H^{S}(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} = p(x, y) \log(p(x | y)).\]</p><p>This is the definition used when calling <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a> with a <a href="../probabilities/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>.</p><p><strong>Two-entropies formulation</strong></p><p>Equivalently, the following difference of entropies hold</p><p class="math-container">\[H^S(X | Y) = H^S(X, Y) - H^S(Y),\]</p><p>where <span>$H^S(\cdot$</span> and <span>$H^S(\cdot | \cdot)$</span> are the <a href="../entropy/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy and Shannon joint entropy, respectively. This is the definition used when calling <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a> with a <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</p><p><strong>Differential definition</strong></p><p>The differential conditional Shannon entropy is analogously defined as</p><p class="math-container">\[H^S(X | Y) = h^S(X, Y) - h^S(Y),\]</p><p>where <span>$h^S(\cdot$</span> and <span>$h^S(\cdot | \cdot)$</span> are the <a href="../entropy/#ComplexityMeasures.Shannon"><code>Shannon</code></a> differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a> with a <a href="../entropy/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/ce6e03b6806541d4ccdf70511013fa760852bb1e/src/methods/infomeasures/entropy_conditional/CEShannon.jl#L3-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CETsallisFuruichi" href="#CausalityTools.CETsallisFuruichi"><code>CausalityTools.CETsallisFuruichi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CETsallisFuruichi &lt;: ConditionalEntropy
CETsallisFuruichi(; base = 2, q = 1.5)</code></pre><p>Furuichi (2006)&#39;s discrete Tsallis conditional entropy measure.</p><p><strong>Definition</strong></p><p>Furuichi&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}}
p(x, y)^q \log_q(p(x | y)),\]</p><p>when <span>$q \neq 1$</span>. For <span>$q = 1$</span>, <span>$H_q^T(X | Y)$</span> reduces to the Shannon conditional entropy:</p><p class="math-container">\[H_{q=1}^T(X | Y) = -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} =
p(x, y) \log(p(x | y))\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/ce6e03b6806541d4ccdf70511013fa760852bb1e/src/methods/infomeasures/entropy_conditional/CETsallisFuruichi.jl#L3-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.CETsallisAbe" href="#CausalityTools.CETsallisAbe"><code>CausalityTools.CETsallisAbe</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CETsallisAbe &lt;: ConditionalEntropy
CETsallisAbe(; base = 2, q = 1.5)</code></pre><p>Abe &amp; Rajagopal (2001)&#39;s discrete Tsallis conditional entropy measure.</p><p><strong>Definition</strong></p><p>Abe &amp; Rajagopal&#39;s Tsallis conditional entropy between discrete random variables <span>$X$</span> and <span>$Y$</span> with finite ranges <span>$\mathcal{X}$</span> and <span>$\mathcal{Y}$</span> is defined as</p><p class="math-container">\[H_q^{T_A}(X | Y) = \dfrac{H_q^T(X, Y) - H_q^T(Y)}{1 + (1-q)H_q^T(Y)},\]</p><p>where <span>$H_q^T(\cdot)$</span> and <span>$H_q^T(\cdot, \cdot)$</span> is the <a href="../entropy/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a> entropy and the joint Tsallis entropy.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/ce6e03b6806541d4ccdf70511013fa760852bb1e/src/methods/infomeasures/entropy_conditional/CETsallisAbe.jl#L3-L25">source</a></section></article><p>More variants exist in the literature. Pull requests are welcome!</p><h2 id="Discrete-conditional-entropy"><a class="docs-heading-anchor" href="#Discrete-conditional-entropy">Discrete conditional entropy</a><a id="Discrete-conditional-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete-conditional-entropy" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}" href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>CausalityTools.entropy_conditional</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">entropy_conditional(measure::ConditionalEntropy, c::ContingencyMatrix{T, 2}) where T</code></pre><p>Estimate the discrete version of the given <a href="#CausalityTools.ConditionalEntropy"><code>ConditionalEntropy</code></a> <code>measure</code> from its direct (sum) definition, using the probabilities from a pre-computed <a href="../probabilities/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>, constructed from two input variables <code>x</code> and <code>y</code>.</p><p>The convention is to compute the entropy of the variable in the <em>first</em> column of <code>c</code> conditioned on the variable in the <em>second</em> column of <code>c</code>. To do the opposite, call this function with a new contingency matrix where the order of the variables is reversed.</p><p>If <code>measure</code> is not given, then the default is <code>CEShannon()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/ce6e03b6806541d4ccdf70511013fa760852bb1e/src/methods/infomeasures/entropy_conditional/entropy_conditional.jl#L24-L36">source</a></section></article><h3 id="contingency_matrix_ce"><a class="docs-heading-anchor" href="#contingency_matrix_ce">Contingency matrix</a><a id="contingency_matrix_ce-1"></a><a class="docs-heading-anchor-permalink" href="#contingency_matrix_ce" title="Permalink"></a></h3><p>Discrete conditional entropy can be computed directly from its sum-definition by using the probabilities from a <a href="../probabilities/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>. This estimation method works for  both numerical and categorical data, and the following <a href="#CausalityTools.ConditionalEntropy"><code>ConditionalEntropy</code></a> definitions are supported.</p><table><tr><th style="text-align: right"></th><th style="text-align: center"><a href="../probabilities/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a></th></tr><tr><td style="text-align: right"><a href="#CausalityTools.CEShannon"><code>CEShannon</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.CETsallisFuruichi"><code>CETsallisFuruichi</code></a></td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.CETsallisAbe"><code>CETsallisAbe</code></a></td><td style="text-align: center">✓</td></tr></table><h3 id="probabilities_estimators_ce"><a class="docs-heading-anchor" href="#probabilities_estimators_ce">Table of discrete conditional entropy estimators</a><a id="probabilities_estimators_ce-1"></a><a class="docs-heading-anchor-permalink" href="#probabilities_estimators_ce" title="Permalink"></a></h3><p>Here, we list the <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s that are compatible with <a href="#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}"><code>entropy_conditional</code></a>, and which definitions they are valid for.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="#CausalityTools.CEShannon"><code>CEShannon</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisAbe"><code>CETsallisAbe</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisFuruichi"><code>CETsallisFuruichi</code></a></th></tr><tr><td style="text-align: right"><a href="../probabilities/#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../probabilities/#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>SymbolicPermuation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../probabilities/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">x</td></tr></table><h2 id="Differential/continuous-conditional-entropy"><a class="docs-heading-anchor" href="#Differential/continuous-conditional-entropy">Differential/continuous conditional entropy</a><a id="Differential/continuous-conditional-entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Differential/continuous-conditional-entropy" title="Permalink"></a></h2><h3 id="diffentropy_estimators_ce"><a class="docs-heading-anchor" href="#diffentropy_estimators_ce">Table of differential conditional entropy estimators</a><a id="diffentropy_estimators_ce-1"></a><a class="docs-heading-anchor-permalink" href="#diffentropy_estimators_ce" title="Permalink"></a></h3><p>Continuous/differential mutual information may be estimated using any of our <a href="../entropy/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s that support multivariate input data.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="#CausalityTools.CEShannon"><code>CEShannon</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisAbe"><code>CETsallisAbe</code></a></th><th style="text-align: center"><a href="#CausalityTools.CETsallisFuruichi"><code>CETsallisFuruichi</code></a></th></tr><tr><td style="text-align: right"><a href="../entropy/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../entropy/#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../entropy/#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../entropy/#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../entropy/#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>Lord</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><h3 id="Discrete:-example-from-Cover-and-Thomas"><a class="docs-heading-anchor" href="#Discrete:-example-from-Cover-and-Thomas">Discrete: example from Cover &amp; Thomas</a><a id="Discrete:-example-from-Cover-and-Thomas-1"></a><a class="docs-heading-anchor-permalink" href="#Discrete:-example-from-Cover-and-Thomas" title="Permalink"></a></h3><p>This is essentially example 2.2.1 in Cover &amp; Thomas (2006), where they use the following contingency table as an example. We&#39;ll take their example and manually construct a <a href="../probabilities/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a> that we can use to compute the conditional entropy. The <a href="../probabilities/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a> constructor takes the probabilities as the first argument and the raw frequencies as the second argument. Note also that Julia is column-major, so we need to transpose their example. Then their <code>X</code> is in the first dimension of our contingency matrix (along columns) and their <code>Y</code> is our second dimension (rows).</p><pre><code class="language-julia hljs">using CausalityTools
freqs_yx = [1//8 1//16 1//32 1//32;
    1//16 1//8  1//32 1//32;
    1//16 1//16 1//16 1//16;
    1//4  0//1  0//1  0//1];
freqs_xy = transpose(freqs_yx);
probs_xy = freqs_xy ./ sum(freqs_xy)
c_xy = ContingencyMatrix(probs_xy, freqs_xy)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4×4 ContingencyMatrix{Rational{Int64}, 2, Rational{Int64}}:
 1//8   1//16  1//16  1//4
 1//16  1//8   1//16  0//1
 1//32  1//32  1//16  0//1
 1//32  1//32  1//16  0//1</code></pre><p>The marginal distribution for <code>x</code> (first dimension) is</p><pre><code class="language-julia hljs">probabilities(c_xy, 1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Probabilities{Rational{Int64}, 1}:
 1//2
 1//4
 1//8
 1//8</code></pre><p>The marginal distribution for <code>y</code> (second dimension) is</p><pre><code class="language-julia hljs">probabilities(c_xy, 2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Probabilities{Rational{Int64}, 1}:
 1//4
 1//4
 1//4
 1//4</code></pre><p>And the Shannon conditional entropy <span>$H^S(X | Y)$</span></p><pre><code class="language-julia hljs">ce_x_given_y = entropy_conditional(CEShannon(), c_xy) |&gt; Rational</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11//8</code></pre><p>This is the same as in their example. Hooray! To compute <span>$H^S(Y | X)$</span>, we just need to flip the contingency matrix.</p><pre><code class="language-julia hljs">probs_yx = freqs_yx ./ sum(freqs_yx);
c_yx = ContingencyMatrix(probs_yx, freqs_yx);
ce_y_given_x = entropy_conditional(CEShannon(), c_yx) |&gt; Rational</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">13//8</code></pre><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Abe2001"><a class="tag is-link" href="#citeref-Abe2001">Abe2001</a>Abe, S., &amp; Rajagopal, A. K. (2001). Nonadditive conditional entropy and its significance for local realism. Physica A: Statistical Mechanics and its Applications, 289(1-2), 157-164.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../entropy/">« Entropy</a><a class="docs-footer-nextpage" href="../mutualinfo/">Mutual information »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 18 January 2023 10:43">Wednesday 18 January 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
