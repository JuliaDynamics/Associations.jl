var documenterSearchIndex = {"docs":
[{"location":"examples/examples_entropy/#examples_entropy","page":"Entropy","title":"Entropy","text":"","category":"section"},{"location":"examples/examples_entropy/#Differential-entropy:-estimator-comparison","page":"Entropy","title":"Differential entropy: estimator comparison","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Here, we'll test the different nearest-neighbor based differential entropy estimators on a three-dimensional normal distribution mathcalN (mu Sigma) with zero means and covariance matrix Sigma = diag(r_1 r_2 r_3) with r_1 = r_2 = r_3 = 05.  The analytical entropy for multivariate Gaussian is H(mathcalN (mu Sigma)) = dfrac12log(det(2pi e Sigma)). In our case, Sigma is diagonal, so det(Sigma) = (05)^3 and H = 05log(2pi e (05)^3)approx 3217.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Several of these estimators have been shown to convergence to the true entropy with an increasing number of samples. Therefore, we test the  estimators on samples of increasing size N, where N ranges from 1000 to 30000. Since we're estimating entropy from samples of a normal distribution, we don't expect the estimates to perfectly match the analytical entropy every time. On average, however, they should hit the target when the sample size gets large enough.","category":"page"},{"location":"examples/examples_entropy/#Analytical-and-estimated-entropies","page":"Entropy","title":"Analytical and estimated entropies","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"We'll first make two helper functions.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"analytical_entropy(estimators, Ls; d::Int, r, base = 2): Computes the analytical     Shannon differential entropy to the given base of a multivariate normal distribution   with covariance matrix with diagonal elements r and zeros on the off-diagonal.   Does so for each of the given estimators for each   sample size in Ls.\nmvnormal_entropies(; d::Int, r, base = 2, kwargs...): Estimates  the Shannon    entropy to the given base of samples from a multivariate normal distribution as   specified as above.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"using CausalityTools\nusing Distributions: MvNormal\nusing LinearAlgebra\nusing Statistics: quantile\nusing Random; rng = MersenneTwister(12345678)\nusing CairoMakie\n\nanalytical_entropy(; d::Int, r, base = 2) = \n    0.5*log(det(2*pi*â„¯*diagm(repeat([r], d)))) / log(base, â„¯) # convert to desired base\n\nfunction mvnormal_entropies(estimators, Ls; \n        d = 3,\n        base = 2,\n        nreps = 50,\n        r = 0.5,\n    )\n    Î¼ = zeros(d)\n    Î£ = diagm(repeat([r], d))\n    N = MvNormal(Î¼, Î£)    \n    Hs = [[zeros(nreps) for L in Ls] for est in estimators]\n    data = [Dataset([rand(rng, N) for i = 1:maximum(Ls)]) for i = 1:nreps]\n    for (e, est) in enumerate(estimators)\n        for (l, L) in enumerate(Ls)\n            for i = 1:nreps\n                Hs[e][l][i] = entropy(Shannon(; base), est, data[i][1:L])\n            end\n        end\n    end\n    return Hs\nend;","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"We'll also need a function to summarize the estimates.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"# A helper to get the estimator name for plotting.\ngetname(est::DifferentialEntropyEstimator) = typeof(est).name.name  |> string\nfunction medians_and_quantiles(Hs, Ls; q = 0.95)\n    medians = [zeros(length(Ls)) for est in estimators]\n    lb = [zeros(length(Ls)) for est in estimators]\n    ub = [zeros(length(Ls)) for est in estimators]\n\n    for (e, est) in enumerate(estimators)\n        for (l, L) in enumerate(Ls)\n            hÌ‚s = Hs[e][l] # nreps estimates for this combinations of e and l\n            medians[e][l] = quantile(hÌ‚s, 0.5)\n            lb[e][l] = quantile(hÌ‚s, (1 - q) / 2)\n            ub[e][l] = quantile(hÌ‚s, 1 - ((1 - q) / 2))\n        end\n    end\n\n    return medians, lb, ub\nend;","category":"page"},{"location":"examples/examples_entropy/#Plotting-utilities","page":"Entropy","title":"Plotting utilities","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Now, make some plotting helper functions.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"struct Cyclor{T} <: AbstractVector{T}\n    c::Vector{T}\n    n::Int\nend\nCyclor(c) = Cyclor(c, 0)\n\nBase.length(c::Cyclor) = length(c.c)\nBase.size(c::Cyclor) = size(c.c)\nBase.iterate(c::Cyclor, state=1) = Base.iterate(c.c, state)\nBase.getindex(c::Cyclor, i) = c.c[(i-1)%length(c.c) + 1]\nBase.getindex(c::Cyclor, i::AbstractArray) = c.c[i]\nfunction Base.getindex(c::Cyclor)\n    c.n += 1\n    c[c.n]\nend\nBase.iterate(c::Cyclor, i = 1) = iterate(c.c, i)\n\nCOLORSCHEME = [\n    \"#D43F3AFF\", \"#EEA236FF\", \"#5CB85CFF\", \"#46B8DAFF\",\n    \"#357EBDFF\", \"#9632B8FF\", \"#B8B8B8FF\",\n]\n\nCOLORS = Cyclor(COLORSCHEME)\nLINESTYLES = Cyclor(string.([\"--\", \".-\", \".\", \"--.\", \"---...\"]))\nMARKERS = Cyclor(string.([:circle, :rect, :utriangle, :dtriangle, :diamond,\n    :pentagon, :cross, :xcross]))\n\nfunction plot_entropy_estimates(Hs, Ls, Htrue)\n    # Summarize data (medians[e][l]) is the median of the e-th estimator for the \n    # l-th sample size).\n    medians, lbs, ubs = medians_and_quantiles(Hs, Ls);\n\n    fig = Figure(resolution = (800, 1000))\n    ymax = (vcat(Hs...) |> Iterators.flatten |> maximum) * 1.1\n    ymin = (vcat(Hs...) |> Iterators.flatten |> minimum) * 0.9\n\n    # We have 9 estimators, so place them on a 5-by-2 grid\n    positions = (Tuple(c) for c in CartesianIndices((5, 2)))\n    for (i, (est, c)) in enumerate(zip(estimators, positions))\n        ax = Axis(fig[first(c), last(c)],\n            xlabel = \"Sample size (L)\",\n            ylabel = \"HÌ‚ (bits)\",\n            title = getname(est)\n        )\n        ylims!(ax, (ymin, ymax))\n        # Ground truth\n        hlines!(ax, [Htrue], \n            linestyle = :dash, \n            color = :black,\n            linewidth = 2,\n        )\n        # Estimates\n        band!(ax, Ls, lbs[i], ubs[i], color = (COLORS[i], 0.5))\n        lines!(ax, Ls, medians[i], \n            label = getname(est),\n            linestyle = LINESTYLES[i],\n            color = COLORS[i],\n            marker = MARKERS[i],\n            linewidth = 2\n        )\n    end\n    fig\nend;","category":"page"},{"location":"examples/examples_entropy/#Results","page":"Entropy","title":"Results","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Now, we can finally run an ensemble of tests and plot the confidence bands against the ground truth. This","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"k = 4\nestimators = [\n    Kraskov(; k), \n    KozachenkoLeonenko(), \n    Gao(; k),\n    ZhuSingh(; k),\n    Zhu(; k),\n    Goria(; k),\n    LeonenkoProzantoSavani(; k),\n    Lord(; k = k*5)\n]\n\nLs = [100:100:1000 |> collect; 2500:2500:5000 |> collect]\nd = 3\nr = 0.5\nnreps = 30\nHs = mvnormal_entropies(estimators, Ls; d, r, nreps)\nHtrue = analytical_entropy(; d, r)\nplot_entropy_estimates(Hs, Ls, Htrue)","category":"page"},{"location":"quickstart/quickstart_independence/#quickstart_independence","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#quickstart_surrogatetest","page":"Independence testing","title":"SurrogateTest","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Mutual-information","page":"Independence testing","title":"Mutual information","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Categorical","page":"Independence testing","title":"Categorical","text":"","category":"section"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"In this example, we expect the preference and the food variables to be independent.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\n# Simulate \nn = 1000\npreference = rand([\"yes\", \"no\"], n)\nfood = rand([\"veggies\", \"meat\", \"fish\"], n)\ntest = SurrogateTest(MIShannon(), Contingency())\nindependence(test, preference, food)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, there's not enough evidence to reject the null hypothesis that the variables are independent.","category":"page"},{"location":"quickstart/quickstart_independence/#Conditional-mutual-information","page":"Independence testing","title":"Conditional mutual information","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Categorical-2","page":"Independence testing","title":"Categorical","text":"","category":"section"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we simulate a survey at a ski resort. The data are such that the place a person grew up is associated with how many times they fell while going skiing. The control happens through an intermediate variable preferred_equipment, which indicates what type of physical activity the person has engaged with in the past. Some activities like skateboarding leads to better overall balance, so people that are good on a skateboard also don't fall, and people that to less challenging activities fall more often.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"We should be able to reject places â«« experience, but not reject places â«« experience |Â preferred_equipment.  Let's see if we can detect these relationships using (conditional) mutual information.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nn = 10000\n\nplaces = rand([\"city\", \"countryside\", \"under a rock\"], n);\npreferred_equipment = map(places) do place\n    if cmp(place, \"city\") == 1\n        return rand([\"skateboard\", \"bmx bike\"])\n    elseif cmp(place, \"countryside\") == 1\n        return rand([\"sled\", \"snowcarpet\"])\n    else\n        return rand([\"private jet\", \"ferris wheel\"])\n    end\nend;\nexperience = map(preferred_equipment) do equipment\n    if equipment âˆˆ [\"skateboard\", \"bmx bike\"]\n        return \"didn't fall\"\n    elseif equipment âˆˆ [\"sled\", \"snowcarpet\"]\n        return \"fell 3 times or less\"\n    else\n        return \"fell uncontably many times\"\n    end\nend;\n\ntest_mi = independence(SurrogateTest(MIShannon(), Contingency()), places, experience)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the evidence favors the alternative hypothesis that places and  experience are dependent.","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"test_cmi = independence(SurrogateTest(CMIShannon(), Contingency()), places, experience, preferred_equipment)","category":"page"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"Again, as expected, when conditioning on the mediating variable, the dependence disappears, and we can't reject the null hypothesis of independence.","category":"page"},{"location":"quickstart/quickstart_independence/#quickstart_localpermutationtest","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#[CMIShannon](@ref)","page":"Independence testing","title":"CMIShannon","text":"","category":"section"},{"location":"quickstart/quickstart_independence/#Continuous-data","page":"Independence testing","title":"Continuous data","text":"","category":"section"},{"location":"quickstart/quickstart_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nx = randn(1000)\ny = randn(1000) .+ 0.7 .* x\nz = sin.(randn(1000)) .* 0.5 .* y\ntest = LocalPermutationTest(CMIShannon(; base = 2), FPVP(k = 10), nshuffles = 30)\nindependence(test, x, z, y)","category":"page"},{"location":"correlation_measures/#Correlation-measures","page":"Correlation measures","title":"Correlation measures","text":"","category":"section"},{"location":"correlation_measures/#Pearson-correlation","page":"Correlation measures","title":"Pearson correlation","text":"","category":"section"},{"location":"correlation_measures/","page":"Correlation measures","title":"Correlation measures","text":"PearsonCorrelation\npearson_correlation","category":"page"},{"location":"correlation_measures/#CausalityTools.PearsonCorrelation","page":"Correlation measures","title":"CausalityTools.PearsonCorrelation","text":"PearsonCorrelation\n\nThe Pearson correlation of two variables.\n\nThis type exists to be used with independence testing. If you only need the correlation coefficient, do pearson_correlation(x, y).\n\nDescription\n\nThe sample Pearson correlation coefficient for real-valued random variables X and Y with associated samples x_i_i=1^N and y_i_i=1^N is defined as\n\nrho_xy = dfracsum_i=1^n (x_i - barx)(y_i - bary) sqrtsum_i=1^N (x_i - barx)^2sqrtsum_i=1^N (y_i - bary)^2\n\nwhere barx and bary are the means of the observations x_k and y_k, respectively.\n\n\n\n\n\n","category":"type"},{"location":"correlation_measures/#CausalityTools.pearson_correlation","page":"Correlation measures","title":"CausalityTools.pearson_correlation","text":"pearson_correlation(x::VectorOrDataset, y::VectorOrDataset)\n\nCompute the PearsonCorrelation between x and y, which must each be 1-dimensional.\n\n\n\n\n\n","category":"function"},{"location":"correlation_measures/#Partial-correlation","page":"Correlation measures","title":"Partial correlation","text":"","category":"section"},{"location":"correlation_measures/","page":"Correlation measures","title":"Correlation measures","text":"PartialCorrelation\npartial_correlation","category":"page"},{"location":"correlation_measures/#CausalityTools.PartialCorrelation","page":"Correlation measures","title":"CausalityTools.PartialCorrelation","text":"PartialCorrelation\n\nThe correlation of two variables, with the effect of a set of conditioning variables removed.\n\nThis type exists to be used with independence testing. If you only need the partial correlation coefficient, do partial_correlation(x, y, z).\n\nDescription\n\nThere are several ways of estimating the partial correlation. We follow the matrix inversion method, because for Datasets, we can very efficiently compute the required joint covariance matrix Sigma for the random variables.\n\nFormally, let X_1 X_2 ldots X_n be a set of n real-valued random variables. Consider the joint precision matrix,P = (p_ij) = Sigma^-1. The partial correlation of any pair of variables (X_i X_j), given the remaining variables bfZ = X_k_i=1 i neq i j^n, is defined as\n\nrho_X_i X_j  bfZ = -dfracp_ijsqrt p_ii p_jj \n\nIn practice, we compute the estimate\n\nhatrho_X_i X_j  bfZ =\n-dfrachatp_ijsqrt hatp_ii hatp_jj \n\nwhere hatP = hatSigma^-1 is the sample precision matrix.\n\n\n\n\n\n","category":"type"},{"location":"correlation_measures/#CausalityTools.partial_correlation","page":"Correlation measures","title":"CausalityTools.partial_correlation","text":"partial_correlation(x::VectorOrDataset, y::VectorOrDataset,\n    z::VectorOrDataset...)\n\nCompute the PartialCorrelation between x and y, given z.\n\n\n\n\n\n","category":"function"},{"location":"correlation_measures/#Distance-correlation","page":"Correlation measures","title":"Distance correlation","text":"","category":"section"},{"location":"correlation_measures/","page":"Correlation measures","title":"Correlation measures","text":"DistanceCorrelation\ndistance_correlation","category":"page"},{"location":"correlation_measures/#CausalityTools.DistanceCorrelation","page":"Correlation measures","title":"CausalityTools.DistanceCorrelation","text":"DistanceCorrelation\n\nThe distance correlation of two variables.\n\nThis type exists to be used with independence testing. If you only need the distance correlation, do distance_correlation(x, y).\n\n\n\n\n\n","category":"type"},{"location":"correlation_measures/#CausalityTools.distance_correlation","page":"Correlation measures","title":"CausalityTools.distance_correlation","text":"distance_correlation(x, y) â†’ dcor âˆˆ [0, 1]\n\nCompute the empirical/sample distance correlation (SzÃ©kely et al., 2007)[SzÃ©kely2007], here called dcor, between datasets x and y.\n\n[SzÃ©kely2007]: SzÃ©kely, G. J., Rizzo, M. L., & Bakirov, N. K. (2007). Measuring and testing dependence by correlation of distances. The annals of statistics, 35(6), 2769-2794.\n\n\n\n\n\n","category":"function"},{"location":"entropy_conditional/#Conditional-entropy","page":"Conditional entropy","title":"Conditional entropy","text":"","category":"section"},{"location":"entropy_conditional/#Conditional-entropy-API","page":"Conditional entropy","title":"Conditional entropy API","text":"","category":"section"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"The mutual information API is defined by","category":"page"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"ConditionalEntropy,\nentropy_conditional,","category":"page"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"We provide a suite of estimators of various mutual information quantities. Many more variants exist in the literature. Pull requests are welcome!","category":"page"},{"location":"entropy_conditional/#Conditional-entropy-definitions","page":"Conditional entropy","title":"Conditional entropy definitions","text":"","category":"section"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"ConditionalEntropy\nCEShannon\nCETsallisFuruichi\nCETsallisAbe","category":"page"},{"location":"entropy_conditional/#CausalityTools.ConditionalEntropy","page":"Conditional entropy","title":"CausalityTools.ConditionalEntropy","text":"The supertype for all conditional entropies.\n\n\n\n\n\n","category":"type"},{"location":"entropy_conditional/#CausalityTools.CEShannon","page":"Conditional entropy","title":"CausalityTools.CEShannon","text":"CEShannon <: ConditionalEntropy\nCEShannon(; base = 2,)\n\nTheShannon conditional entropy measure.\n\nDiscrete definition\n\nSum formulation\n\nThe conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH^S(X  Y) = -sum_x in mathcalX y in mathcalY = p(x y) log(p(x  y))\n\nThis is the definition used when calling entropy_conditional with a ContingencyMatrix.\n\nTwo-entropies formulation\n\nEquivalently, the following difference of entropies hold\n\nH^S(X  Y) = H^S(X Y) - H^S(Y)\n\nwhere H^S(cdot and H^S(cdot  cdot) are the Shannon entropy and Shannon joint entropy, respectively. This is the definition used when calling entropy_conditional with a ProbabilitiesEstimator.\n\nDifferential definition\n\nThe differential conditional Shannon entropy is analogously defined as\n\nH^S(X  Y) = h^S(X Y) - h^S(Y)\n\nwhere h^S(cdot and h^S(cdot  cdot) are the Shannon differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling entropy_conditional with a DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropy_conditional/#CausalityTools.CETsallisFuruichi","page":"Conditional entropy","title":"CausalityTools.CETsallisFuruichi","text":"CETsallisFuruichi <: ConditionalEntropy\nCETsallisFuruichi(; base = 2, q = 1.5)\n\nFuruichi (2006)'s discrete Tsallis conditional entropy measure.\n\nDefinition\n\nFuruichi's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T(X  Y) = -sum_x in mathcalX y in mathcalY\np(x y)^q log_q(p(x  y))\n\nwhen q neq 1. For q = 1, H_q^T(X  Y) reduces to the Shannon conditional entropy:\n\nH_q=1^T(X  Y) = -sum_x in mathcalX y in mathcalY =\np(x y) log(p(x  y))\n\n\n\n\n\n","category":"type"},{"location":"entropy_conditional/#CausalityTools.CETsallisAbe","page":"Conditional entropy","title":"CausalityTools.CETsallisAbe","text":"CETsallisAbe <: ConditionalEntropy\nCETsallisAbe(; base = 2, q = 1.5)\n\nAbe & Rajagopal (2001)'s discrete Tsallis conditional entropy measure.\n\nDefinition\n\nAbe & Rajagopal's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T_A(X  Y) = dfracH_q^T(X Y) - H_q^T(Y)1 + (1-q)H_q^T(Y)\n\nwhere H_q^T(cdot) and H_q^T(cdot cdot) is the Tsallis entropy and the joint Tsallis entropy.\n\n[Abe2001]: Abe, S., & Rajagopal, A. K. (2001). Nonadditive conditional entropy and its significance for local realism. Physica A: Statistical Mechanics and its Applications, 289(1-2), 157-164.\n\n\n\n\n\n","category":"type"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"More variants exist in the literature. Pull requests are welcome!","category":"page"},{"location":"entropy_conditional/#Discrete-conditional-entropy","page":"Conditional entropy","title":"Discrete conditional entropy","text":"","category":"section"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"entropy_conditional(::ConditionalEntropy, ::ContingencyMatrix)","category":"page"},{"location":"entropy_conditional/#CausalityTools.entropy_conditional-Tuple{ConditionalEntropy, ContingencyMatrix}","page":"Conditional entropy","title":"CausalityTools.entropy_conditional","text":"entropy_conditional(measure::ConditionalEntropy, c::ContingencyMatrix{T, 2}) where T\n\nEstimate the discrete version of the given ConditionalEntropy measure from its direct (sum) definition, using the probabilities from a pre-computed ContingencyMatrix, constructed from two input variables x and y.\n\nThe convention is to compute the entropy of the variable in the first column of c conditioned on the variable in the second column of c. To do the opposite, call this function with a new contingency matrix where the order of the variables is reversed.\n\nIf measure is not given, then the default is CEShannon().\n\n\n\n\n\n","category":"method"},{"location":"entropy_conditional/#contingency_matrix_ce","page":"Conditional entropy","title":"Contingency matrix","text":"","category":"section"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"Discrete conditional entropy can be computed directly from its sum-definition by using the probabilities from a ContingencyMatrix. This estimation method works for  both numerical and categorical data, and the following ConditionalEntropy definitions are supported.","category":"page"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":" ContingencyMatrix\nCEShannon âœ“\nCETsallisFuruichi âœ“\nCETsallisAbe âœ“","category":"page"},{"location":"entropy_conditional/#probabilities_estimators_ce","page":"Conditional entropy","title":"Table of discrete conditional entropy estimators","text":"","category":"section"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"Here, we list the ProbabilitiesEstimators that are compatible with entropy_conditional, and which definitions they are valid for.","category":"page"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"Estimator Principle CEShannon CETsallisAbe CETsallisFuruichi\nCountOccurrences Frequencies âœ“ âœ“ x\nValueHistogram Binning (histogram) âœ“ âœ“ x\nSymbolicPermuation Ordinal patterns âœ“ âœ“ x\nDispersion Dispersion patterns âœ“ âœ“ x","category":"page"},{"location":"entropy_conditional/#Differential/continuous-conditional-entropy","page":"Conditional entropy","title":"Differential/continuous conditional entropy","text":"","category":"section"},{"location":"entropy_conditional/#diffentropy_estimators_ce","page":"Conditional entropy","title":"Table of differential conditional entropy estimators","text":"","category":"section"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"Continuous/differential mutual information may be estimated using any of our DifferentialEntropyEstimators that support multivariate input data.","category":"page"},{"location":"entropy_conditional/","page":"Conditional entropy","title":"Conditional entropy","text":"Estimator Principle CEShannon CETsallisAbe CETsallisFuruichi\nKraskov Nearest neighbors âœ“ x x\nZhu Nearest neighbors âœ“ x x\nZhuSingh Nearest neighbors âœ“ x x\nGao Nearest neighbors âœ“ x x\nGoria Nearest neighbors âœ“ x x\nLord Nearest neighbors âœ“ x x\nLeonenkoProzantoSavani Nearest neighbors âœ“ x x","category":"page"},{"location":"transferentropy/#Transfer-entropy","page":"Transfer entropy","title":"Transfer entropy","text":"","category":"section"},{"location":"transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The transfer entropy API is made up of the following functions and types, which are listed below:","category":"page"},{"location":"transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"transferentropy.\nTransferEntropy, and its subtypes.\nEmbeddingTE, which exists to provide embedding instructions to   subtypes of TransferEntropy.\nTransferEntropyEstimator, and its subtypes.","category":"page"},{"location":"transferentropy/#API","page":"Transfer entropy","title":"API","text":"","category":"section"},{"location":"transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"transferentropy\nEmbeddingTE\noptimize_marginals_te","category":"page"},{"location":"transferentropy/#CausalityTools.transferentropy","page":"Transfer entropy","title":"CausalityTools.transferentropy","text":"transferentropy([measure::TEShannon], est, s, t, [c])\ntransferentropy(measure::TERenyiJizba, est, s, t, [c])\n\nEstimate the transfer entropy TE^*(S to T) or TE^*(S to T  C) if c is given, using the provided estimator est, where * indicates the given measure. If measure is not given, then TEShannon(; base = 2) is the default.\n\nArguments\n\nmeasure: The transfer entropy measure, e.g. TEShannon or   TERenyi, which dictates which formula is computed.   Embedding parameters are stored in measure.embedding, and   is represented by an EmbeddingTE instance. If calling transferentropy   without giving measure, then the embedding is optimized by finding   suitable delay embedding parameters using the \"traditional\"   approach from DynamicalSystems.jl.\ns: The source timeseries.\nt: The target timeseries.\nc: Optional. A conditional timeseries.\n\nDescription\n\nThe Shannon transfer entropy is defined as TE^S(S to T  C) = I^S(T^+ S^-  T^- C^-), where I^S(T^+ S^-  T^- C^-) is CMIShannon, and marginals for the CMI are constructed as described in EmbeddingTE. The definition is analogous for TERenyiJizba.\n\nIf s, t, and c are univariate timeseries, then the the marginal embedding variables T^+ (target future), T^- (target present/past), S^- (source present/past) and C^- (present/past of conditioning variables) are constructed by first jointly embedding  s, t and c with relevant delay embedding parameters, then subsetting relevant columns of the embedding.\n\nSince estimates of TE^*(S to T) and TE^*(S to T  C) are just a special cases of conditional mutual information where input data are marginals of a particular form of delay embedding, any combination of variables, e.g. S = (A B), T = (C D), C = (D E F) are valid inputs (given as Datasets). In practice, however, s, t and c are most often timeseries, and if  s, t and c are Datasets, it is assumed that the data are pre-embedded and the embedding step is skipped.\n\nCompatible estimators\n\ntransferentropy is just a simple wrapper around condmutualinfo that constructs an appropriate delay embedding from the input data before CMI is estimated. Consequently, any estimator that can be used for ConditionalMutualInformation is, in principle, also a valid transfer entropy estimator. Documentation strings for TEShannon and TERenyiJizba list compatible estimators, and an overview table can be found in the online documentation.\n\n\n\n\n\n","category":"function"},{"location":"transferentropy/#CausalityTools.EmbeddingTE","page":"Transfer entropy","title":"CausalityTools.EmbeddingTE","text":"EmbeddingTE(; dS = 1, dT = 1, dTf = 1, dC = 1, Ï„S = -1, Ï„T = -1, Î·Tf = 1, Ï„C = -1)\nEmbeddingTE(opt::OptimiseTraditional, s, t, [c])\n\nEmbeddingTE provide embedding parameters for transfer entropy analysis using either TEShannon, TERenyi, or in general any subtype of TransferEntropy, which in turns dictates the embedding used with transferentropy.\n\nThe second method finds parameters using the \"traditional\" optimised embedding techniques from DynamicalSystems.jl\n\nConvention for generalized delay reconstruction\n\nWe use the following convention. Let s(i) be time series for the source variable, t(i) be the time series for the target variable and c(i) the time series for the conditional variable. To compute transfer entropy, we need the following marginals:\n\nbeginaligned\nT^+ = t(i+eta^1) t(i+eta^2) ldots (t(i+eta^d_T^+)  \nT^- =  (t(i+tau^0_T) t(i+tau^1_T) t(i+tau^2_T) ldots t(t + tau^d_T - 1_T))  \nS^- =  (s(i+tau^0_S) s(i+tau^1_S) s(i+tau^2_S) ldots s(t + tau^d_S - 1_S))  \nC^- =  (c(i+tau^0_C) c(i+tau^1_C) c(i+tau^2_C) ldots c(t + tau^d_C - 1_C)) \nendaligned\n\nDepending on the application, the delay reconstruction lags tau^k_T leq 0, tau^k_S leq 0, and tau^k_C leq 0 may be equally spaced, or non-equally spaced. The same applied to the prediction lag(s), but typically only a only a single predictions lag eta^k is used (so that d_T^+ = 1).\n\nFor transfer entropy, traditionally at least one tau^k_T, one tau^k_S and one tau^k_C equals zero. This way, the T^-, S^- and C^- marginals always contains present/past states, while the mathcal T marginal contain future states relative to the other marginals. However, this is not a strict requirement, and modern approaches that searches for optimal embeddings can return embeddings without the intantaneous lag.\n\nCombined, we get the generalized delay reconstruction mathbbE = (T^+_(d_T^+) T^-_(d_T) S^-_(d_S) C^-_(d_C)). Transfer entropy is then computed as\n\nbeginaligned\nTE_S rightarrow T  C = int_mathbbE P(T^+ T^- S^- C^-)\nlog_bleft(fracP(T^+  T^- S^- C^-)P(T^+  T^- C^-)right)\nendaligned\n\nor, if conditionals are not relevant,\n\nbeginaligned\nTE_S rightarrow T = int_mathbbE P(T^+ T^- S^-)\nlog_bleft(fracP(T^+  T^- S^-)P(T^+  T^-)right)\nendaligned\n\nHere,\n\nT^+ denotes the d_T^+-dimensional set of vectors furnishing the future   states of T (almost always equal to 1 in practical applications),\nT^- denotes the d_T-dimensional set of vectors furnishing the past and   present states of T,\nS^- denotes the d_S-dimensional set of vectors furnishing the past and   present of S, and\nC^- denotes the d_C-dimensional set of vectors furnishing the past and   present of C.\n\nKeyword arguments\n\ndS, dT, dC, dTf (f for future) are the dimensions of the S^-,   T^-, C^- and T^+ marginals. The parameters dS, dT, dC and dTf   must each be a positive integer number.\nÏ„S, Ï„T, Ï„C are the embedding lags for S^-, T^-, C^-.   Each parameter are integers âˆˆ ð’©â°â», or a vector of integers âˆˆ ð’©â°â», so   that S^-, T^-, C^- always represents present/past values.   If e.g. Ï„T is an integer, then for the T^- marginal is constructed using   lags tau_T = 0 tau 2tau ldots (d_T- 1)tau_T .   If is a vector, e.g. Ï„Î¤ = [-1, -5, -7], then the dimension dT must match the lags,   and precisely those lags are used: tau_T = -1 -5 -7 .\nThe prediction lag(s) Î·Tf is a positive integer. Combined with the requirement   that the other delay parameters are zero or negative, this ensures that we're   always predicting from past/present to future. In typical applications,   Î·Tf = 1 is used for transfer entropy.\n\nExamples\n\nSay we wanted to compute the Shannon transfer entropy TE^S(S to T) = I^S(T^+ S^-  T^-). Using some modern procedure for determining optimal embedding parameters using methods from DynamicalSystems.jl, we find that the optimal embedding of T^- is three-dimensional and is given by the lags [0, -5, -8]. Using the same procedure, we find that the optimal embedding of S^- is two-dimensional with lags -1 -8. We want to predicting a univariate version of the target variable one time step into the future (Î·Tf = 1). The total embedding is then the set of embedding vectors\n\nE_TE =  (T(i+1) S(i-1) S(i-8) T(i) T(i-5) T(i-8)) . Translating this to code, we get:\n\nusing CausalityTools\njulia> EmbeddingTE(dT=3, Ï„T=[0, -5, -8], dS=2, Ï„S=[-1, -4], Î·Tf=1)\n\n# output\nEmbeddingTE(dS=2, dT=3, dC=1, dTf=1, Ï„S=[-1, -4], Ï„T=[0, -5, -8], Ï„C=-1, Î·Tf=1)\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#CausalityTools.optimize_marginals_te","page":"Transfer entropy","title":"CausalityTools.optimize_marginals_te","text":"optimize_marginals_te([scheme = OptimiseTraditional()], s, t, [c]) â†’ EmbeddingTE\n\nOptimize marginal embeddings for transfer entropy computation from source time series s to target time series t, conditioned on c if c is given, using the provided optimization scheme.\n\n\n\n\n\n","category":"function"},{"location":"transferentropy/#Definitions","page":"Transfer entropy","title":"Definitions","text":"","category":"section"},{"location":"transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"TransferEntropy\nTEShannon\nTERenyiJizba","category":"page"},{"location":"transferentropy/#CausalityTools.TransferEntropy","page":"Transfer entropy","title":"CausalityTools.TransferEntropy","text":"The supertype of all transfer entropy measures.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#CausalityTools.TEShannon","page":"Transfer entropy","title":"CausalityTools.TEShannon","text":"TEShannon <: TransferEntropy\nTEShannon(; base = 2; embedding = EmbeddingTE()) <: TransferEntropy\n\nThe Shannon-type transfer entropy measure.\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I^S(T^+ S^-  T^-) \nTE(S to T  C) = I^S(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I(T^+ S^-  T^-) is the Shannon conditional mutual information (CMIShannon). The variables T^+, T^-, S^- and C^- are described in the docstring for transferentropy.\n\nCompatible estimators\n\nProbabilitiesEstimator: Any probabilities estimator that accepts   multivariate input data or has an implementation for marginal_encodings.   Transfer entropy is computed a sum of marginal (discrete) entropy estimates.   Example: ValueHistogram.\nDifferentialEntropyEstimator. Any differential entropy   estimator that accepts multivariate input data.   Transfer entropy is computed a sum of marginal differential entropy estimates.   Example: Kraskov.\nMutualInformationEstimator. Any mutual information estimator.   Formulates the transfer entropy as a sum of mutual information terms, which are   estimated separately using mutualinfo. Example: KraskovStÃ¶gbauerGrassberger2.\nConditionalMutualInformationEstimator. Dedicated CMI estimators.   Example: FPVP.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#CausalityTools.TERenyiJizba","page":"Transfer entropy","title":"CausalityTools.TERenyiJizba","text":"TERenyiJizba() <: TransferEntropy\n\nThe RÃ©nyi transfer entropy from Jizba et al. (2012)[Jizba2012].\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I_q^R_J(T^+ S^-  T^-) \nTE(S to T  C) = I_q^R_J(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I_q^R_J(T^+ S^-  T^-) is Jizba et al. (2012)'s definition of conditional mutual information (CMIRenyiJizba). The variables T^+, T^-, S^- and C^- are described in the docstring for transferentropy.\n\nCompatible estimators\n\nProbabilitiesEstimator: Any probabilities estimator that accepts   multivariate input data or has an implementation for marginal_encodings.   Transfer entropy is computed a sum of marginal (discrete) entropy estimates.   Example: ValueHistogram.\nDifferentialEntropyEstimator. Any differential entropy   estimator that accepts multivariate input data.   Transfer entropy is computed a sum of marginal differential entropy estimates.   Example: Kraskov.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). RÃ©nyiâ€™s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#Estimators","page":"Transfer entropy","title":"Estimators","text":"","category":"section"},{"location":"transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"TransferEntropyEstimator\nZhu1\nLindner","category":"page"},{"location":"transferentropy/#CausalityTools.TransferEntropyEstimator","page":"Transfer entropy","title":"CausalityTools.TransferEntropyEstimator","text":"The supertype of all dedicated transfer entropy estimators.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#CausalityTools.Zhu1","page":"Transfer entropy","title":"CausalityTools.Zhu1","text":"Zhu1 <: DifferentialEntropyEstimator\nZhu1(k = 1, w = 0, base = MathConstants.e)\n\nThe Zhu1 transfer entropy estimator (Zhu et al., 2015)[Zhu2015].\n\nAssumes that the input data have been normalized as described in (Zhu et al., 2015).\n\nThis estimator approximates probabilities within hyperrectangles surrounding each point xáµ¢ âˆˆ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin JeannÃ¨s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#CausalityTools.Lindner","page":"Transfer entropy","title":"CausalityTools.Lindner","text":"Lindner <: TransferEntropyEstimator\nLindner(k = 1, w = 0, base = 2)\n\nThe Lindner transfer entropy estimator (Lindner et al., 2011)[Lindner2011], which is also used in the Trentool MATLAB toolbox, and is based on nearest neighbor searches.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nFor a given points in the joint embedding space jáµ¢, this estimator first computes the distance dáµ¢ from jáµ¢ to its k-th nearest neighbor. Then, for each point mâ‚–[i] in the k-th marginal space, it counts the number of points within radius dáµ¢.\n\nThe transfer entropy is then computed as\n\nTE(X to Y) =\npsi(k) + dfrac1N sum_i^n\nleft\n    sum_k=1^3 left( psi(m_ki + 1) right)\nright\n\nwhere the index k references the three marginal subspaces T, TTf and ST for which neighbor searches are performed.\n\n[Lindner2011]: Lindner, M., Vicente, R., Priesemann, V., & Wibral, M. (2011). TRENTOOL:     A Matlab open source toolbox to analyse information flow in time series data with     transfer entropy. BMC neuroscience, 12(1), 1-22.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#Convenience","page":"Transfer entropy","title":"Convenience","text":"","category":"section"},{"location":"transferentropy/#Symbolic-transfer-entropy","page":"Transfer entropy","title":"Symbolic transfer entropy","text":"","category":"section"},{"location":"transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"SymbolicTransferEntropy","category":"page"},{"location":"transferentropy/#CausalityTools.SymbolicTransferEntropy","page":"Transfer entropy","title":"CausalityTools.SymbolicTransferEntropy","text":"SymbolicTransferEntropy <: TransferEntropyEstimator\nSymbolicTransferEntropy(; m = 3, Ï„ = 1, lt = ComplexityMeasures.isless_rand\n\nA convenience estimator for symbolic transfer entropy (Stanieck & Lenertz, 2008)[Stanieck2008].\n\nDescription\n\nSymbolic transfer entropy consists of two simple steps. First, the input time series are embedded with embedding lag m and delay Ï„. The ordinal patterns of the embedding vectors are then encoded using SymbolicPermutation with marginal_encodings. This transforms the input time series into integer time series using OrdinalPatternEncoding.\n\nTransfer entropy is then estimated as usual on the encoded timeseries with transferentropy and the CountOccurrences naive frequency estimator.\n\n[Stanieck2008]: Staniek, M., & Lehnertz, K. (2008). Symbolic transfer entropy. Physical review letters, 100(15), 158101.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#Phase/amplitude-transfer-entropy","page":"Transfer entropy","title":"Phase/amplitude transfer entropy","text":"","category":"section"},{"location":"transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Hilbert\nPhase\nAmplitude","category":"page"},{"location":"transferentropy/#CausalityTools.Hilbert","page":"Transfer entropy","title":"CausalityTools.Hilbert","text":"Hilbert(est;\n    source::InstantaneousSignalProperty = Phase(),\n    target::InstantaneousSignalProperty = Phase(),\n    cond::InstantaneousSignalProperty = Phase())\n) <: TransferDifferentialEntropyEstimator\n\nCompute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are obtained by first applying the Hilbert transform to each signal, then extracting the phases/amplitudes of the resulting complex numbers[Palus2014]. Original time series are thus transformed to instantaneous phase/amplitude time series. Transfer entropy is then estimated using the provided est on those phases/amplitudes (use e.g. VisitationFrequency, or SymbolicPermutation).\n\ninfo: Info\nDetails on estimation of the transfer entropy (conditional mutual information) following the phase/amplitude extraction step is not given in Palus (2014). Here, after instantaneous phases/amplitudes have been obtained, these are treated as regular time series, from which transfer entropy is then computed as usual.\n\nSee also: Phase, Amplitude.\n\n[Palus2014]: PaluÅ¡, M. (2014). Cross-scale interactions and information transfer. Entropy, 16(10), 5263-5289.\n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#CausalityTools.Phase","page":"Transfer entropy","title":"CausalityTools.Phase","text":"Phase <: InstantaneousSignalProperty\n\nIndicates that the instantaneous phases of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"transferentropy/#CausalityTools.Amplitude","page":"Transfer entropy","title":"CausalityTools.Amplitude","text":"Amplitude <: InstantaneousSignalProperty\n\nIndicates that the instantaneous amplitudes of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#Mutual-information","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"mutualinfo/#Mutual-information-API","page":"Mutual information","title":"Mutual information API","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"The mutual information API is defined by","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"MutualInformation,\nmutualinfo,\nMutualInformationEstimator.","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"We provide a suite of estimators of various mutual information quantities. Many more variants exist in the literature. Pull requests are welcome!","category":"page"},{"location":"mutualinfo/#Mutual-information-definitions","page":"Mutual information","title":"Mutual information definitions","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"MutualInformation\nMIShannon\nMITsallisFuruichi\nMITsallisMartin\nMIRenyiSarbu\nMIRenyiJizba","category":"page"},{"location":"mutualinfo/#CausalityTools.MutualInformation","page":"Mutual information","title":"CausalityTools.MutualInformation","text":"The supertype of all mutual information measures \n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.MIShannon","page":"Mutual information","title":"CausalityTools.MIShannon","text":"MIShannon <: MutualInformation\nMIShannon(; base = 2)\n\nThe Shannon mutual information I^S(X Y).\n\nDiscrete definition\n\nThere are many equivalent formulations of discrete Shannon mutual information. In this package, we currently use the double-sum and the three-entropies formulations.\n\nDouble sum formulation\n\nAssume we observe samples barbfX_1N_y = barbfX_1 ldots barbfX_n  and barbfY_1N_x = barbfY_1 ldots barbfY_n  from two discrete random variables X and Y with finite supports mathcalX =  x_1 x_2 ldots x_M_x  and mathcalY = y_1 y_2 ldots x_M_y. The double-sum estimate is obtained by replacing the double sum\n\nhatI_DS(X Y) =\n sum_x_i in mathcalX y_i in mathcalY p(x_i y_j) log left( dfracp(x_i y_i)p(x_i)p(y_j) right)\n\nwhere  hatp(x_i) = fracn(x_i)N_x, hatp(y_i) = fracn(y_j)N_y, and hatp(x_i x_j) = fracn(x_i)N, and N = N_x N_y. This definition is used by mutualinfo when called with a ContingencyMatrix.\n\nThree-entropies formulation\n\nAn equivalent formulation of discrete Shannon mutual information is\n\nI^S(X Y) = H^S(X) + H_q^S(Y) - H^S(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint discrete Shannon entropies. This definition is used by mutualinfo when called with a ProbabilitiesEstimator.\n\nDifferential mutual information\n\nOne possible formulation of differential Shannon mutual information is\n\nI^S(X Y) = h^S(X) + h_q^S(Y) - h^S(X Y)\n\nwhere h^S(cdot) and h^S(cdot cdot) are the marginal and joint differential Shannon entropies. This definition is used by mutualinfo when called with a DifferentialEntropyEstimator.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.MITsallisFuruichi","page":"Mutual information","title":"CausalityTools.MITsallisFuruichi","text":"MITsallisFuruichi <: MutualInformation\nMITsallisFuruichi(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Furuichi (2006)[Furuichi2006], which in that paper is called the mutual entropy.\n\nDescription\n\nFuruichi's Tsallis mutual entropy between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_q^T(X Y) = H_q^T(X) - H_q^T(X  Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X Y)\n\nwhere H^T(cdot) and H^T(cdot cdot) are the marginal and joint Tsallis entropies, and q is the Tsallis-parameter. ```\n\n[Furuichi2006]: Furuichi, S. (2006). Information theoretical properties of Tsallis entropies. Journal of Mathematical Physics, 47(2), 023302.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.MITsallisMartin","page":"Mutual information","title":"CausalityTools.MITsallisMartin","text":"MITsallisMartin <: MutualInformation\nMITsallisMartin(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Martin et al. (2005)[Martin2004].\n\nDescription\n\nMartin et al.'s Tsallis mutual information between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_textMartin^T(X Y q) = H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint Shannon entropies, and q is the Tsallis-parameter.\n\n[Martin2004]: Martin, S., Morison, G., Nailon, W., & Durrani, T. (2004). Fast and accurate image registration using Tsallis entropy and simultaneous perturbation stochastic approximation. Electronics Letters, 40(10), 1.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.MIRenyiSarbu","page":"Mutual information","title":"CausalityTools.MIRenyiSarbu","text":"MIRenyiSarbu <: MutualInformation\nMIRenyiSarbu(; base = 2, q = 1.5)\n\nThe discrete RÃ©nyi mutual information from Sarbu (2014)[Sarbu2014].\n\nDescription\n\nSarbu (2014) defines discrete RÃ©nyi mutual information as the RÃ©nyi alpha-divergence between the conditional joint probability mass function p(x y) and the product of the conditional marginals, p(x) cdot p(y):\n\nI(X Y)^R_q =\ndfrac1q-1\nlog left(\n    sum_x in X y in Y\n    dfracp(x y)^qleft( p(x)cdot p(y) right)^q-1\nright)\n\n[Sarbu2014]: Sarbu, S. (2014, May). RÃ©nyi information transfer: Partial RÃ©nyi transfer entropy and partial RÃ©nyi mutual information. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5666-5670). IEEE.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.MIRenyiJizba","page":"Mutual information","title":"CausalityTools.MIRenyiJizba","text":"MIRenyiJizba <: MutualInformation\n\nThe RÃ©nyi mutual information I_q^R_J(X Y) defined in Jizba et al. (2012)[Jizba2012].\n\nDefinition\n\nI_q^R_J(X Y) = S_q^R(X) + S_q^R(Y) - S_q^R(X Y)\n\nwhere S_q^R(cdot) and S_q^R(cdot cdot) the RÃ©nyi entropy and the joint RÃ©nyi entropy.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). RÃ©nyi's information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#Dedicated-estimators","page":"Mutual information","title":"Dedicated estimators","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"mutualinfo(est::MutualInformationEstimator, ::Any, ::Any) ","category":"page"},{"location":"mutualinfo/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}","page":"Mutual information","title":"CausalityTools.mutualinfo","text":"mutualinfo([measure::MutualInformation], est::MutualInformationEstimator, x, y)\n\nEstimate the mutual information measure between x and y using the dedicated MutualInformationEstimator est, which can be either discrete, continuous, or a mixture of both, and typically involve some bias correction. If measure is not given, then the default is MIShannon().\n\nSee the online documentation for a list of compatible measures.\n\n\n\n\n\n","category":"method"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"MutualInformationEstimator\nKraskovStÃ¶gbauerGrassberger1\nKraskovStÃ¶gbauerGrassberger2\nGaoKannanOhViswanath\nGaoOhViswanath","category":"page"},{"location":"mutualinfo/#CausalityTools.MutualInformationEstimator","page":"Mutual information","title":"CausalityTools.MutualInformationEstimator","text":"MutualInformationEstimator\n\nThe supertype of all dedicated mutual information estimators.\n\nMutualInformationEstimators can be either mixed, discrete or a combination of both. Each estimator uses a specialized technique to approximate relevant densities/integrals and/or probabilities, and is typically tailored to a specific type of MutualInformation (mostly MIShannon).\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.KraskovStÃ¶gbauerGrassberger1","page":"Mutual information","title":"CausalityTools.KraskovStÃ¶gbauerGrassberger1","text":"KSG1 <: MutualInformationEstimator\nKraskovStÃ¶gbauerGrassberger1 <: MutualInformationEstimator\nKraskovStÃ¶gbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovStÃ¶gbauerGrassberger1 mutual information estimator (you can use KSG1 for short) is the I^(1) k-th nearest neighbor estimator from Kraskov et al. (2004)[Kraskov2004].\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nDescription\n\nLet the joint dataset X = bfX_1 bfX_2 ldots bfX_m  be defined by the concatenation of the marginal datasets  bfX_k _k=1^m, where each bfX_k is potentially multivariate. Let bfx_1 bfx_2 ldots bfx_N be the points in the joint space X.\n\n[Kraskov2004]: Kraskov, A., StÃ¶gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.KraskovStÃ¶gbauerGrassberger2","page":"Mutual information","title":"CausalityTools.KraskovStÃ¶gbauerGrassberger2","text":"KSG2 <: MutualInformationEstimator\nKraskovStÃ¶gbauerGrassberger2 <: MutualInformationEstimator\nKraskovStÃ¶gbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovStÃ¶gbauerGrassberger2 mutual information estimator (you can use KSG2 for short) is the I^(2) k-th nearest neighbor estimator from Kraskov et al. (2004)[Kraskov2004].\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nDescription\n\nLet the joint dataset X = bfX_1 bfX_2 ldots bfX_m  be defined by the concatenation of the marginal datasets  bfX_k _k=1^m, where each bfX_k is potentially multivariate. Let bfx_1 bfx_2 ldots bfx_N be the points in the joint space X.\n\nThe KraskovStÃ¶gbauerGrassberger2 estimator first locates, for each bfx_i in X, the point bfn_i in X, the k-th nearest neighbor to bfx_i, according to the maximum norm (Chebyshev metric). Let epsilon_i be the distance d(bfx_i bfn_i).\n\nConsider x_i^m in bfX_m, the i-th point in the marginal space bfX_m. For each bfx_i^m, we determine theta_i^m := the number of points bfx_k^m in bfX_m that are a distance less than epsilon_i away from bfx_i^m. That is, we use the distance from a query point bfx_i in X (in the joint space) to count neighbors of x_i^m in bfX_m (in the marginal space).\n\nMutual information between the variables bfX_1 bfX_2 ldots bfX_m is then estimated as\n\nhatI_KSG2(bfX) =\n    psi(k) -\n    dfracm - 1k +\n    (m - 1)psi(N) -\n    dfrac1N sum_i = 1^N sum_j = 1^m psi(theta_i^j + 1)\n\n[Kraskov2004]: Kraskov, A., StÃ¶gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.GaoKannanOhViswanath","page":"Mutual information","title":"CausalityTools.GaoKannanOhViswanath","text":"GaoKannanOhViswanath <: MutualInformationEstimator\nGaoKannanOhViswanath(; k = 1, w = 0)\n\nThe GaoKannanOhViswanath (Shannon) estimator is designed for estimating mutual information between variables that may be either discrete, continuous or a mixture of both (Gao et al., 2017).\n\nnote: Explicitly convert your discrete data to floats\nEven though the GaoKannanOhViswanath estimator is designed to handle discrete data, our implementation demands that all input data are Datasets whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to mutualinfo.\n\nDescription\n\nThe estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using k-nearest neighbor distances from empirical samples.\n\nThe estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator \"strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.\"\n\nwarn: Implementation note\nIn Gao et al., (2017), they claim (roughly speaking) that the estimator reduces to the KraskovStÃ¶gbauerGrassberger1 estimator for continuous-valued data. However, KraskovStÃ¶gbauerGrassberger1 uses the digamma function, while GaoKannanOhViswanath uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the KSG1 approach. The KSG1 estimator uses k-th neighbor distances in the joint space, while the GaoKannanOhViswanath algorithm selects the maximum k-th nearest distances among the two marginal spaces, which are in general not the same as the k-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in GaoKannanOhViswanath. We have modified it in a way that mimics KraskovStÃ¶gbauerGrassberger1 for continous data. Note that because of using the log function instead of digamma, there will be slight differences between the methods. See the source code for more details.\n\nSee also: mutualinfo.\n\n[GaoKannanOhViswanath2017]: Gao, W., Kannan, S., Oh, S., & Viswanath, P. (2017). Estimating mutual information for discrete-continuous mixtures. Advances in neural information processing systems, 30.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#CausalityTools.GaoOhViswanath","page":"Mutual information","title":"CausalityTools.GaoOhViswanath","text":"GaoOhViswanath <: MutualInformationEstimator\n\nThe GaoOhViswanath mutual information estimator, also called the bias-improved-KSG estimator, or BI-KSG, by Gao et al. (2018)[Gao2018], is given by\n\nbeginalign*\nhatH_GAO(X Y)\n= hatH_KSG(X) + hatH_KSG(Y) - hatH_KZL(X Y) \n= psi(k) +\n    log(N) +\n    log\n        left(\n            dfracc_d_x 2 c_d_y 2c_d_x + d_y 2\n        right)\n     - \n     dfrac1N sum_i=1^N left( log(n_x i 2) + log(n_y i 2) right)\nendalign*\n\nwhere c_d 2 = dfracpi^fracd2Gamma(dfracd2 + 1) is the volume of a d-dimensional unit mathcall_2-ball.\n\n[Gao2018]: Gao, W., Oh, S., & Viswanath, P. (2018). Demystifying fixed k-nearest neighbor information estimators. IEEE Transactions on Information Theory, 64(8), 5629-5661.\n\n\n\n\n\n","category":"type"},{"location":"mutualinfo/#dedicated_estimators_mi","page":"Mutual information","title":"Table of dedicated estimators","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Estimator Type Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiSarbu MIRenyiJizba\nKraskovStÃ¶gbauerGrassberger1 Continuous Nearest neighbors âœ“ x x x x\nKraskovStÃ¶gbauerGrassberger2 Continuous Nearest neighbors âœ“ x x x x\nGaoKannanOhViswanath Mixed Nearest neighbors âœ“ x x x x\nGaoOhViswanath Continuous Nearest neighbors âœ“ x x x x","category":"page"},{"location":"mutualinfo/#Discrete-mutual-information","page":"Mutual information","title":"Discrete mutual information","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"mutualinfo(::ProbabilitiesEstimator, ::Any, ::Any)","category":"page"},{"location":"mutualinfo/#CausalityTools.mutualinfo-Tuple{ProbabilitiesEstimator, Any, Any}","page":"Mutual information","title":"CausalityTools.mutualinfo","text":"mutualinfo([measure::MutualInformation], est::ProbabilitiesEstimator, x, y)\n\nEstimate the mutual information measure between x and y by a sum of three entropy terms, without any bias correction, using the provided ProbabilitiesEstimator est. If measure is not given, then the default is MIShannon().\n\nJoint and marginal probabilities are computed by jointly discretizing x and y using the approach given by est, and obtaining marginal distributions from the joint distribution.\n\nThis only works for estimators that have an implementation for marginal_encodings. See the online documentation for a list of compatible measures.\n\n\n\n\n\n","category":"method"},{"location":"mutualinfo/#@id-dedicated_probabilities_estimators_mi","page":"Mutual information","title":"Table of discrete mutual information estimators","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here, we list the ProbabilitiesEstimators that can be used to compute discrete mutualinformation.","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Estimator Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiJizba MIRenyiSarbu\nCountOccurrences Frequencies âœ“ âœ“ âœ“ âœ“ x\nValueHistogram Binning (histogram) âœ“ âœ“ âœ“ âœ“ x\nSymbolicPermuation Ordinal patterns âœ“ âœ“ âœ“ âœ“ x\nDispersion Dispersion patterns âœ“ âœ“ âœ“ âœ“ x","category":"page"},{"location":"mutualinfo/#contingency_matrix_mi","page":"Mutual information","title":"Contingency matrix","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"mutualinfo(::MutualInformation, ::ContingencyMatrix)","category":"page"},{"location":"mutualinfo/#CausalityTools.mutualinfo-Tuple{MutualInformation, ContingencyMatrix}","page":"Mutual information","title":"CausalityTools.mutualinfo","text":"mutualinfo(measure::MutualInformation, est::MutualInformationEstimator, x, y)\nmutualinfo(measure::MutualInformation, est::DifferentialEntropyEstimator, x, y)\nmutualinfo(measure::MutualInformation, est::ProbabilitiesEstimator, x, y)\nmutualinfo(measure::MutualInformation, c::ContingencyMatrix)\n\nEstimate the mutual information measure (either MIShannon or MITsallis, ) between x and y using the provided estimator est. Alternatively, compute mutual information from a pre-computed ContingencyMatrix.\n\nCompatible measures/definitions and estimators are listed in the online documentation.\n\n\n\n\n\n","category":"method"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Discrete mutual information can be computed directly from its double-sum definition by using the probabilities from a ContingencyMatrix. This estimation method works for    both numerical and categorical data, and the following MutualInformations are supported.","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":" ContingencyMatrix\nMIShannon âœ“\nMITsallisFuruichi âœ“\nMITsallisMartin âœ“\nMIRenyiSarbu âœ“\nMIRenyiJizba âœ“","category":"page"},{"location":"mutualinfo/#Differential/continuous-mutual-information","page":"Mutual information","title":"Differential/continuous mutual information","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"mutualinfo(::DifferentialEntropyEstimator, ::Any, ::Any)","category":"page"},{"location":"mutualinfo/#CausalityTools.mutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any}","page":"Mutual information","title":"CausalityTools.mutualinfo","text":"mutualinfo([measure::MutualInformation], est::DifferentialEntropyEstimator, x, y)\n\nEstimate the mutual information measure between x and y by a sum of three entropy terms, without any bias correction, using any DifferentialEntropyEstimator compatible with multivariate data. If measure is not given, then the default is MIShannon().\n\nSee the online documentation for a list of compatible measures.\n\n\n\n\n\n","category":"method"},{"location":"mutualinfo/#dedicated_diffentropy_estimators_mi","page":"Mutual information","title":"Table of differential mutual information estimators","text":"","category":"section"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In addition to the dedicated differential mutual information estimators listed above, continuous/differential mutual information may also be estimated using any of our DifferentialEntropyEstimator that support multivariate input data. When using these estimators, mutual information is computed as a sum of entropy terms (with different dimensions), and no bias correction is applied.","category":"page"},{"location":"mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Estimator Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiJizba MIRenyiSurbu\nKraskov Nearest neighbors âœ“ x x x x\nZhu Nearest neighbors âœ“ x x x x\nZhuSingh Nearest neighbors âœ“ x x x x\nGao Nearest neighbors âœ“ x x x x\nGoria Nearest neighbors âœ“ x x x x\nLord Nearest neighbors âœ“ x x x x\nLeonenkoProzantoSavani Nearest neighbors âœ“ x x x x","category":"page"},{"location":"examples/examples_transferentropy/#examples_transferentropy","page":"Transfer entropy","title":"Transfer entropy","text":"","category":"section"},{"location":"examples/examples_transferentropy/#Schreiber's-original-example","page":"Transfer entropy","title":"Schreiber's original example","text":"","category":"section"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Let's try to reproduce the results from Schreiber's original paper[Schreiber2000] where he introduced the transfer entropy. We'll use the ValueHistogram estimator, which is visitation frequency based and computes entropies by counting visits of the system's orbit to discrete portions of its reconstructed state space.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"using CausalityTools\nusing DynamicalSystemsBase\nusing CairoMakie\nusing Statistics\nusing Random; Random.seed!(12234);\n\nfunction ulam_system(dx, x, p, t)\n    f(x) = 2 - x^2\n    Îµ = p[1]\n    dx[1] = f(Îµ*x[length(dx)] + (1-Îµ)*x[1])\n    for i in 2:length(dx)\n        dx[i] = f(Îµ*x[i-1] + (1-Îµ)*x[i])\n    end\nend\n\nds = DiscreteDynamicalSystem(ulam_system, rand(100) .- 0.5, [0.04])\ntrajectory(ds, 1000; Ttr = 1000);\n\nÎµs = 0.02:0.02:1.0\nbase = 2\nte_x1x2 = zeros(length(Îµs)); te_x2x1 = zeros(length(Îµs))\n# Guess an appropriate bin width of 0.2 for the histogram\nest = ValueHistogram(0.2)\n\nfor (i, Îµ) in enumerate(Îµs)\n    set_parameter!(ds, 1, Îµ)\n    tr = trajectory(ds, 2000; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = transferentropy(TEShannon(; base), est, X1, X2)\n    te_x2x1[i] = transferentropy(TEShannon(; base), est, X2, X1)\nend\n\nfig = with_theme(theme_minimal(), markersize = 2) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\n    scatterlines!(ax, Îµs, te_x1x2, label = \"X1 to X2\", color = :black, lw = 1.5)\n    scatterlines!(ax, Îµs, te_x2x1, label = \"X2 to X1\", color = :red, lw = 1.5)\n    axislegend(ax, position = :lt)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"As expected, transfer entropy from X1 to X2 is higher than from X2 to X1 across parameter values for Îµ. But, by our definition of the ulam system, dynamical coupling only occurs from X1 to X2. The results, however, show nonzero transfer entropy in both directions. What does this mean?","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Computing transfer entropy from finite time series introduces bias, and so does any particular choice of entropy estimator used to calculate it. To determine whether a transfer entropy estimate should be trusted, we can employ surrogate testing. We'll generate surrogate using TimeseriesSurrogates.jl.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"In the example below, we continue with the same time series generated above. However, at each value of Îµ, we also compute transfer entropy for nsurr = 50 different randomly shuffled (permuted) versions of the source process. If the original transfer entropy exceeds that of some percentile the transfer entropy estimates of the surrogate ensemble, we will take that as \"significant\" transfer entropy.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"nsurr = 25 # in real applications, you should use more surrogates\nbase = 2\nte_x1x2 = zeros(length(Îµs)); te_x2x1 = zeros(length(Îµs))\nte_x1x2_surr = zeros(length(Îµs), nsurr); te_x2x1_surr = zeros(length(Îµs), nsurr)\nest = ValueHistogram(0.2) # use same bin-width as before\n\nfor (i, Îµ) in enumerate(Îµs)\n    set_parameter!(ds, 1, Îµ)\n    tr = trajectory(ds, 500; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = transferentropy(TEShannon(; base), est, X1, X2)\n    te_x2x1[i] = transferentropy(TEShannon(; base), est, X2, X1)\n    s1 = surrogenerator(X1, RandomShuffle()); s2 = surrogenerator(X2, RandomShuffle())\n\n    for j = 1:nsurr\n        te_x1x2_surr[i, j] =  transferentropy(TEShannon(; base), est, s1(), X2)\n        te_x2x1_surr[i, j] =  transferentropy(TEShannon(; base), est, s2(), X1)\n    end\nend\n\n# Compute 95th percentiles of the surrogates for each Îµ\nqs_x1x2 = [quantile(te_x1x2_surr[i, :], 0.95) for i = 1:length(Îµs)]\nqs_x2x1 = [quantile(te_x2x1_surr[i, :], 0.95) for i = 1:length(Îµs)]\n\nfig = with_theme(theme_minimal(), markersize = 2) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\n    scatterlines!(ax, Îµs, te_x1x2, label = \"X1 to X2\", color = :black, lw = 1.5)\n    scatterlines!(ax, Îµs, qs_x1x2, color = :black, linestyle = :dot, lw = 1.5)\n    scatterlines!(ax, Îµs, te_x2x1, label = \"X2 to X1\", color = :red)\n    scatterlines!(ax, Îµs, qs_x2x1, color = :red, linestyle = :dot)\n    axislegend(ax, position = :lt)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The plot above shows the original transfer entropies (solid lines) and the 95th percentile transfer entropies of the surrogate ensembles (dotted lines). As expected, using the surrogate test, the transfer entropies from X1 to X2 are mostly significant (solid black line is above dashed black line). The transfer entropies from X2 to X1, on the other hand, are mostly not significant (red solid line is below red dotted line).","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"[Schreiber2000]: Schreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.","category":"page"},{"location":"examples/examples_transferentropy/#Estimator-comparison","page":"Transfer entropy","title":"Estimator comparison","text":"","category":"section"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Let's reproduce Figure 4 from Zhu et al (2015)[Zhu2015], where they test some dedicated transfer entropy estimators on a bivariate autoregressive system. We will test","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The Lindner and Zhu1 dedicated transfer entropy estimators,   which try to eliminate bias.\nThe KSG1 estimator, which computes TE naively as a sum of mutual information   terms (without guaranteed cancellation of biases for the total sum).\nThe Kraskov estimator, which computes TE naively as a sum of entropy    terms (without guaranteed cancellation of biases for the total sum).","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin JeannÃ¨s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"using CausalityTools\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\n\nfunction model2(n::Int)\n    ð’©x = Normal(0, 0.1)\n    ð’©y = Normal(0, 0.1)\n    x = zeros(n+2)\n    y = zeros(n+2)\n    x[1] = rand(ð’©x)\n    x[2] = rand(ð’©x)\n    y[1] = rand(ð’©y)\n    y[2] = rand(ð’©y)\n\n    for i = 3:n+2\n        x[i] = 0.45*sqrt(2)*x[i-1] - 0.9*x[i-2] - 0.6*y[i-2] + rand(ð’©x)\n        y[i] = 0.6*x[i-2] - 0.175*sqrt(2)*y[i-1] + 0.55*sqrt(2)*y[i-2] + rand(ð’©y)\n    end\n    return x[3:end], y[3:end]\nend\nte_true = 0.42 # eyeball the theoretical value from their Figure 4.\n\nm = TEShannon(embedding = EmbeddingTE(dT = 2, dS = 2), base = â„¯)\nestimators = [Zhu1(k = 8), Lindner(k = 8), KSG1(k = 8), Kraskov(k = 8)]\nLs = [floor(Int, 2^i) for i in 8.0:0.5:11]\nnreps = 8\ntes_xy = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\ntes_yx = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\nfor (k, est) in enumerate(estimators)\n    for (i, L) in enumerate(Ls)\n        for j = 1:nreps\n            x, y = model2(L);\n            tes_xy[k][i][j] = transferentropy(m, est, x, y)\n            tes_yx[k][i][j] = transferentropy(m, est, y, x)\n        end\n    end\nend\n\nymin = minimum(map(x -> minimum(Iterators.flatten(Iterators.flatten(x))), (tes_xy, tes_yx)))\nestimator_names = [\"Zhu1\", \"Lindner\", \"KSG1\", \"Kraskov\"]\nls = [:dash, :dot, :dash, :dot]\nmr = [:rect, :hexagon, :xcross, :pentagon]\n\nfig = Figure(resolution = (800, 350))\nax_xy = Axis(fig[1,1], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"x â†’ y\")\nax_yx = Axis(fig[1,2], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"y â†’ x\")\nfor (k, e) in enumerate(estimators)\n    label = estimator_names[k]\n    marker = mr[k]\n    scatterlines!(ax_xy, Ls, mean.(tes_xy[k]); label, marker)\n    scatterlines!(ax_yx, Ls, mean.(tes_yx[k]); label, marker)\n    hlines!(ax_xy, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black) \n    hlines!(ax_yx, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black)\n    linkaxes!(ax_xy, ax_yx)\nend\naxislegend(ax_xy, position = :rb)\n\nfig","category":"page"},{"location":"quickstart/quickstart_mi/#quickstart_mutualinfo","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"quickstart/quickstart_mi/#[MIShannon](@ref)-(differential)","page":"Mutual information","title":"MIShannon (differential)","text":"","category":"section"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"The differential Shannon mutual information (MIShannon) can be estimated using a dedicated mutual information estimator like KraskovStÃ¶gbauerGrassberger2. These estimators typically apply some form of bias correction.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(KSG2(k = 5), x, y)","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"We can also estimate MIShannon by naively applying a DifferentialEntropyEstimator, which doesn't apply any bias correction.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(Kraskov(k = 3), x, y)","category":"page"},{"location":"quickstart/quickstart_mi/#[MIShannon](@ref)-(discrete,-numerical)","page":"Mutual information","title":"MIShannon (discrete, numerical)","text":"","category":"section"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"A ValueHistogram estimator can be used to bin the data and compute discrete Shannon mutual information.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\n\n# Use the H3-estimation method with a discrete visitation frequency based \n# probabilities estimator over a fixed grid covering the range of the data,\n# which is on [0, 1].\nest = ValueHistogram(FixedRectangularBinning(0, 1, 5))\nmutualinfo(est, x, y)","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"If you need access to the estimated joint probability mass function, use a ContingencyMatrix. This is slower, but convenient if you need to investigate the probabilities manually.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nc = contingency_matrix(est, x, y)\nmutualinfo(c)","category":"page"},{"location":"quickstart/quickstart_mi/#[MIShannon](@ref)-(discrete,-categorical)","page":"Mutual information","title":"MIShannon (discrete, categorical)","text":"","category":"section"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"The ContingencyMatrix approach can also be used with categorical data. For example, let's compare the Shannon mutual information between the preferences of a population sample with regards to different foods.","category":"page"},{"location":"quickstart/quickstart_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nn = 1000\npreferences = rand([\"neutral\", \"like it\", \"hate it\"], n);\nrandom_foods = rand([\"water\", \"flour\", \"bananas\", \"booze\", \"potatoes\", \"beans\", \"soup\"], n)\nbiased_foods = map(preferences) do preference\n    if cmp(preference, \"neutral\") == 1\n        return rand([\"water\", \"flour\"])\n    elseif cmp(preference, \"like it\") == 1\n        return rand([\"bananas\", \"booze\"])\n    else\n        return rand([\"potatoes\", \"beans\", \"soup\"])\n    end\nend\n\nc_biased = contingency_matrix(preferences, biased_foods) \nc_random = contingency_matrix(preferences, random_foods) \nmutualinfo(c_biased), mutualinfo(c_random)","category":"page"},{"location":"condmutualinfo/#Conditional-mutual-information-(CMI)","page":"Conditional mutual information","title":"Conditional mutual information (CMI)","text":"","category":"section"},{"location":"condmutualinfo/#API","page":"Conditional mutual information","title":"API","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"The condition mutual information API is defined by","category":"page"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"ConditionalMutualInformation,\nmutualinfo,\nConditionalMutualInformationEstimator.","category":"page"},{"location":"condmutualinfo/#Definitions","page":"Conditional mutual information","title":"Definitions","text":"","category":"section"},{"location":"condmutualinfo/#Shannon-CMI","page":"Conditional mutual information","title":"Shannon CMI","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"ConditionalMutualInformation\nCMIShannon\nCMIRenyiJizba\nCMIRenyiPoczos","category":"page"},{"location":"condmutualinfo/#CausalityTools.ConditionalMutualInformation","page":"Conditional mutual information","title":"CausalityTools.ConditionalMutualInformation","text":"ConditionalMutualInformation <: InformationMeasure\nCMI # alias\n\nThe supertype of all conditional mutual informations.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#CausalityTools.CMIShannon","page":"Conditional mutual information","title":"CausalityTools.CMIShannon","text":"CMIShannon <: ConditionalMutualInformation\nCMIShannon(; base = 2)\n\nThe Shannon conditional mutual information (CMI) I^S(X Y  Z).\n\nSupported definitions\n\nConsider random variables X in mathbbR^d_X and Y in mathbbR^d_Y, given Z in mathbbR^d_Z. The Shannon conditional mutual information is defined as\n\nbeginalign*\nI(X Y  Z)\n= H^S(X Z) + H^S(Y z) - H^S(X Y Z) - H^S(Z) \n= I^S(X Y Z) + I^S(X Y)\nendalign*\n\nwhere I^S(cdot cdot) is the Shannon mutual information MIShannon, and H^S(cdot) is the Shannon entropy.\n\nDifferential Shannon CMI is obtained by replacing the entropies by differential entropies.\n\nSee also: condmutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#CausalityTools.CMIRenyiJizba","page":"Conditional mutual information","title":"CausalityTools.CMIRenyiJizba","text":"CMIRenyiJizba <: ConditionalMutualInformation\n\nThe RÃ©nyi conditional mutual information I_q^R_J(X Y  Z defined in Jizba et al. (2012)[Jizba2012].\n\nDefinition\n\nI_q^R_J(X Y  Z) = I_q^R_J(X Y Z) - I_q^R_J(X Z)\n\nwhere I_q^R_J(X Z) is the MIRenyiJizba mutual information.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). RÃ©nyiâ€™s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#CausalityTools.CMIRenyiPoczos","page":"Conditional mutual information","title":"CausalityTools.CMIRenyiPoczos","text":"CMIRenyiPoczos <: ConditionalMutualInformation\n\nThe differential RÃ©nyi conditional mutual information I_q^R_P(X Y  Z) defined in (PÃ³czos & Schneider, 2012)[PÃ³czos2012].\n\nDefinition\n\nbeginalign*\nI_q^R_P(X Y  Z) = dfrac1q-1\nint int int dfracp_Z(z) p_X Y  Z^q( p_XZ(xz) p_YZ(yz) )^q-1 \nmathbbE_X Y Z sim p_X Y Z\nleft dfracp_X Z^1-q(X Z) p_Y Z^1-q(Y Z) p_X Y Z^1-q(X Y Z) p_Z^1-q(Z) right\nendalign*\n\n[PÃ³czos2012]: PÃ³czos, B., & Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#Dedicated-estimators","page":"Conditional mutual information","title":"Dedicated estimators","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"condmutualinfo(::ConditionalMutualInformationEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"condmutualinfo/#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}","page":"Conditional mutual information","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::CMIEstimator, x, y, z) â†’ cmi::Real\n\nEstimate a conditional mutual information (CMI) of some kind (specified by measure), between x and y, given z, using the given dedicated ConditionalMutualInformationEstimator, which may be discrete, continuous or mixed.\n\n\n\n\n\n","category":"method"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"ConditionalMutualInformationEstimator\nFPVP\nMesnerShalisi\nPoczosSchneiderCMI\nRahimzamani","category":"page"},{"location":"condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator","page":"Conditional mutual information","title":"CausalityTools.ConditionalMutualInformationEstimator","text":"ConditionalMutualInformationEstimator <: InformationEstimator\nCMIEstimator # alias\n\nThe supertype of all conditional mutual information estimators.\n\nSubtypes\n\nFPVP.\nPoczosSchneiderCMI.\nRahimzamani.\nMesnerShalisi.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#CausalityTools.FPVP","page":"Conditional mutual information","title":"CausalityTools.FPVP","text":"FPVP <: ConditionalMutualInformationEstimator\nFPVP(k = 1, w = 0)\n\nThe Frenzel-Pompe-Vejmelka-PaluÅ¡ (or FPVP for short) estimator is used to estimate the differential conditional mutual information using a k-th nearest neighbor approach that is analogous to that of the KraskovStÃ¶gbauerGrassberger1 mutual information estimator (Frenzel & Pompe, 2007[Frenzel2007]; Vejmelka & PaluÅ¡, 2008[Vejmelka2008]).\n\nw is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.\n\n[Frenzel2007]: Frenzel, S., & Pompe, B. (2007). Partial mutual information for coupling analysis of multivariate time series. Physical review letters, 99(20), 204101. w is the Theiler window.\n\n[Vejmelka2008]: Vejmelka, M., & PaluÅ¡, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E, 77(2), 026214.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#CausalityTools.MesnerShalisi","page":"Conditional mutual information","title":"CausalityTools.MesnerShalisi","text":"MesnerShalisi <: ConditionalMutualInformationEstimator\nMesnerShalisi(k = 1, w = 0)\n\nThe MesnerShalisi estimator is an estimator for conditional mutual information for data that can be mixtures of discrete and continuous data (Mesner & Shalisi et al., 2020)[MesnerShalisi2020].\n\n[MesnerShalisi2020]: Mesner, O. C., & Shalizi, C. R. (2020). Conditional mutual information estimation for mixed, discrete and continuous data. IEEE Transactions on Information Theory, 67(1), 464-484.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#CausalityTools.PoczosSchneiderCMI","page":"Conditional mutual information","title":"CausalityTools.PoczosSchneiderCMI","text":"PoczosSchneiderCMI <: ConditionalMutualInformationEstimator\nPoczosSchneiderCMI(k = 1, w = 0)\n\nThe PoczosSchneiderCMI estimator computes various (differential) conditional mutual informations, using a k-th nearest neighbor approach (PÃ³czos & Schneider, 2012)[PÃ³czos2012].\n\n[PÃ³czos2012]: PÃ³czos, B., & Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#CausalityTools.Rahimzamani","page":"Conditional mutual information","title":"CausalityTools.Rahimzamani","text":"Rahimzamani <: ConditionalMutualInformationEstimator\nRahimzamani(k = 1, w = 0)\n\nThe Rahimzamani estimator, short for Rahimzamani-Asnani-Viswanath-Kannan, is an estimator for Shannon conditional mutual information for data that can be mixtures of discrete and continuous data (Rahimzamani et al., 2018)[Rahimzamani2018].\n\nThis is very similar to the GaoKannanOhViswanath mutual information estimator, but has been expanded to the conditional case.\n\n[Rahimzamani2018]: Rahimzamani, A., Asnani, H., Viswanath, P., & Kannan, S. (2018). Estimators for multivariate information measures in general probability spaces. Advances in Neural Information Processing Systems, 31.\n\n\n\n\n\n","category":"type"},{"location":"condmutualinfo/#condmutualinfo_dedicated_estimators","page":"Conditional mutual information","title":"Table of dedicated CMI estimators","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Estimator Principle CMIShannon CMIRenyiPoczos\nFPVP Nearest neighbors âœ“ x\nMesnerShalisi Nearest neighbors âœ“ x\nRahimzamani Nearest neighbors âœ“ x\nPoczosSchneiderCMI Nearest neighbors x âœ“\nGaussianCMI Parametric âœ“ x","category":"page"},{"location":"condmutualinfo/#Estimation-through-mutual-information","page":"Conditional mutual information","title":"Estimation through mutual information","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"condmutualinfo(::MutualInformationEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"condmutualinfo/#CausalityTools.condmutualinfo-Tuple{MutualInformationEstimator, Any, Any, Any}","page":"Conditional mutual information","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::MutualInformationEstimator, x, y, z) â†’ cmi::Real\n\nEstimate the conditional mutual information (CMI) measure between x and y using a difference of mutual information terms, without any bias correction, using the provided MutualInformationEstimator est, which may be continuous/differential, discrete or mixed. If measure is not given, then the default is CMIShannon().\n\n\n\n\n\n","category":"method"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Estimator Type Principle CMIShannon\nKraskovStÃ¶gbauerGrassberger1 Continuous Nearest neighbors âœ“\nKraskovStÃ¶gbauerGrassberger2 Continuous Nearest neighbors âœ“\nGaoKannanOhViswanath Mixed Nearest neighbors âœ“\nGaoOhViswanath Continuous Nearest neighbors âœ“\nGaussianMI  Parametric âœ“","category":"page"},{"location":"condmutualinfo/#Discrete-CMI","page":"Conditional mutual information","title":"Discrete CMI","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"condmutualinfo(::ProbabilitiesEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"condmutualinfo/#CausalityTools.condmutualinfo-Tuple{ProbabilitiesEstimator, Any, Any, Any}","page":"Conditional mutual information","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::ProbabilitiesEstimator, x, y, z) â†’ cmi::Real âˆˆ [0, a)\n\nEstimate the conditional mutual information (CMI) measure between x and y given z using a sum of entropy terms, without any bias correction, using the provided ProbabilitiesEstimator est. If measure is not given, then the default is CMIShannon().\n\nWith a ProbabilitiesEstimator, the returned cmi is guaranteed to be non-negative.\n\n\n\n\n\n","category":"method"},{"location":"condmutualinfo/#mutualinfo_overview","page":"Conditional mutual information","title":"Table of discrete mutual information estimators","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Here, we list the ProbabilitiesEstimators that are compatible with condmutualinfo, and which definitions they are valid for.","category":"page"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Estimator Principle CMIShannon CMIRenyiSarbu\nCountOccurrences Frequencies âœ“ âœ“\nValueHistogram Binning (histogram) âœ“ âœ“\nSymbolicPermuation Ordinal patterns âœ“ âœ“\nDispersion Dispersion patterns âœ“ âœ“","category":"page"},{"location":"condmutualinfo/#Differential-CMI","page":"Conditional mutual information","title":"Differential CMI","text":"","category":"section"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"condmutualinfo(::DifferentialEntropyEstimator, ::Any, ::Any, ::Any)","category":"page"},{"location":"condmutualinfo/#CausalityTools.condmutualinfo-Tuple{DifferentialEntropyEstimator, Any, Any, Any}","page":"Conditional mutual information","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::DifferentialEntropyEstimator, x, y, z) â†’ cmi\n\nEstimate the conditional mutual information (CMI) measure between x and y using a sum of entropy terms, without any bias correction, using the provided DifferentialEntropyEstimator est (which must support multivariate data). If measure is not given, then the default is CMIShannon().\n\n\n\n\n\n","category":"method"},{"location":"condmutualinfo/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Estimator Principle Input data CMIShannon\nKraskov Nearest neighbors Dataset âœ“\nZhu Nearest neighbors Dataset âœ“\nGao Nearest neighbors Dataset âœ“\nGoria Nearest neighbors Dataset âœ“\nLord Nearest neighbors Dataset âœ“\nLeonenkoProzantoSavani Nearest neighbors Dataset âœ“","category":"page"},{"location":"independence/#Independence-testing","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"A common application of information theoretic methods such as conditional mutual information (condmutualinfo) is in the context of null hypothesis testing for the conditional independence of variables.","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"Depending on the context, the input data and the method used, there are many considerations to be made about how to perform this conditional independence testing. Luckily, many excellent frameworks for doing so exist in the literature.","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"Here, we present some commonly used independence tests from the scientific literature, which can all be seamlessly used with the function independence, with any measure that quantifies conditional independence, in combination with any compatible estimator.","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"For example, in just a few lines of code, you can perform Runge's local permutation (LocalPermutationTest test on your data with over 20 different estimators for the conditional mutual information. If your application rather calls for the use of traditional surrogate data, the SurrogateTest test seamlessly integrates with any time series surrogate method from TimeseriesSurrogates.jl.","category":"page"},{"location":"independence/#Independence-test-API","page":"Independence testing","title":"Independence test API","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"The independence test API is defined by","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"independence\nIndependenceTest\nConditionalIndependenceTest","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"independence","category":"page"},{"location":"independence/#CausalityTools.independence","page":"Independence testing","title":"CausalityTools.independence","text":"independence(test::IndependenceTest, x, y, [z]) â†’ summary\n\nPerform the given IndependenceTest test on data x, y and z. If only x and y are given, test must provide a bivariate association measure. If z is given too, then test must provide a conditional association measure.\n\nReturns a test summary, whose type depends on test.\n\n\n\n\n\n","category":"function"},{"location":"independence/#Independence-tests","page":"Independence testing","title":"Independence tests","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"ConditionalIndependenceTest","category":"page"},{"location":"independence/#Surrogate-test-(global-permutation)","page":"Independence testing","title":"Surrogate test (global permutation)","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"SurrogateTest","category":"page"},{"location":"independence/#CausalityTools.SurrogateTest","page":"Independence testing","title":"CausalityTools.SurrogateTest","text":"SurrogateTest <: IndependenceTest\nSurrogateTest(measure, [est];\n    nshuffles::Int = 100,\n    surrogate = RandomShuffle(),\n    rng = Random.default_rng(),\n)\n\nA generic (conditional) independence test for assessing whether two variables X and Y are independendent, potentially conditioned on a third variable Z, based on surrogate data. Used with independence.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether x and y are independent (given z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between the input variables.\n\nThere are different ways of shuffling, dictated by surrogate, each representing a distinct null hypothesis. For each shuffle, the provided measure is computed (using est, if relevant). This procedure is repeated nshuffles times, and a test summary is returned.\n\nFor bivariate measures, the default is to shuffle both input variables. For conditional measures accepting three input variables, the default is to shuffle the first input. Exceptions are:\n\nIf TransferEntropy measure such as TEShannon,   then the source variable is always shuffled, and the target and conditional   variable are left unshuffled.\n\nExamples\n\nQuickstart examples.\n\n\n\n\n\n","category":"type"},{"location":"independence/#Local-permutation","page":"Independence testing","title":"Local permutation","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"LocalPermutationTest","category":"page"},{"location":"independence/#CausalityTools.LocalPermutationTest","page":"Independence testing","title":"CausalityTools.LocalPermutationTest","text":"LocalPermutationTest <: IndependenceTest\nLocalPermutationTest(measure, [est];\n    kperm::Int = 5,\n    nshuffles::Int = 100,\n    rng = Random.default_rng())\n\nLocalPermutationTest is a generic conditional independence test (Runge, 2018)[Runge2018] for assessing whether two variables X and Y are conditionally independendent given a third variable Z (all of which may be multivariate).\n\nAny association measure (with a compatible estimator est, if relevant) with ordering hatM(X Y  Z) (conditional variable is the third) can be used. To obtain the nearest-neighbor approach in Runge, 2018, use the CMIShannon measure with the FPVP estimator.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether x and y are independent (given z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between the input variables.\n\nFor each shuffle, the provided measure is computed (using est, if relevant) while keeping Y and Z fixed, but permuting X, i.e. hatM(hatX Y  Z). Each shuffle of X is done conditional on Z, such that xáµ¢ is replaced with xâ±¼ only if záµ¢ â‰ˆ zâ±¼, i.e. záµ¢ and zâ±¼ are close. Closeness is determined by a kperm-th nearest neighbor search among the points in Z, and permuted points are constructed as (x_i^* y_i z_i)_i=1^N, where the goal is that x_i^* are drawn without replacement, and x_i is replaced by x_j only if z_i approx z_j. This procedure is repeated nshuffles times, and a test summary is returned.\n\nExamples\n\nSee quickstart examples.\n\n[Runge2018]: Runge, J. (2018, March). Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information. In International Conference on Artificial Intelligence and Statistics (pp. 938-947). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Probability-mass-functions-(pmf)","page":"Probability mass functions","title":"Probability mass functions (pmf)","text":"","category":"section"},{"location":"probabilities/#API","page":"Probability mass functions","title":"API","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"The probabilities API is defined by","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities_and_outcomes\nContingencyMatrix\ncontingency_matrix","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"and related functions that you will find in the following documentation blocks:","category":"page"},{"location":"probabilities/#Probabilitities","page":"Probability mass functions","title":"Probabilitities","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities!\nProbabilities","category":"page"},{"location":"probabilities/#ComplexityMeasures.ProbabilitiesEstimator","page":"Probability mass functions","title":"ComplexityMeasures.ProbabilitiesEstimator","text":"ProbabilitiesEstimator\n\nThe supertype for all probabilities estimators.\n\nIn ComplexityMeasures.jl, probability distributions are estimated from data by defining a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L , and assigning to each outcome omega_i a probability p(omega_i), such that sum_i=1^N p(omega_i) = 1. It is the role of a ProbabilitiesEstimator to\n\nDefine Omega, the \"outcome space\", which is the set of all possible outcomes over  which probabilities are estimated. The cardinality of this set can be obtained using  total_outcomes.\nDefine how probabilities p_i = p(omega_i) are assigned to outcomes omega_i.\n\nIn practice, probability estimation is done by calling probabilities with some input data and one of the following probabilities estimators. The result is a Probabilities p (Vector-like), where each element p[i] is the probability of the outcome Ï‰[i]. Use probabilities_and_outcomes if you need both the probabilities and the outcomes, and use outcome_space to obtain Omega alone. The element type of Omega varies between estimators, but it is guaranteed to be hashable. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\nSome estimators can deduce Omega without knowledge of the input, such as SymbolicPermutation. For others, knowledge of input is necessary for concretely specifying Omega, such as ValueHistogram with RectangularBinning. This only matters for the functions outcome_space and total_outcomes.\n\nAll currently implemented probability estimators are listed in a nice table in the probabilities estimators section of the online documentation.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.probabilities","page":"Probability mass functions","title":"ComplexityMeasures.probabilities","text":"probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) â†’ p::Probabilities\n\nCompute a probability distribution over the set of possible outcomes defined by the probabilities estimator est, given input data x, which is typically an Array or a Dataset; see Input data for ComplexityMeasures.jl. Configuration options are always given as arguments to the chosen estimator.\n\nTo obtain the outcomes corresponding to these probabilities, use outcomes.\n\nDue to performance optimizations, whether the returned probablities contain 0s as entries or not depends on the estimator. E.g., in ValueHistogram 0s are skipped, while in SymbolicPermutation 0 are not, because we get them for free.\n\nprobabilities(x::Vector_or_Dataset) â†’ p::Probabilities\n\nEstimate probabilities by directly counting the elements of x, assuming that Î© = sort(unique(x)), i.e. that the outcome space is the unique elements of x. This is mostly useful when x contains categorical data.\n\nSee also: Probabilities, ProbabilitiesEstimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.probabilities!","page":"Probability mass functions","title":"ComplexityMeasures.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.Probabilities","page":"Probability mass functions","title":"ComplexityMeasures.Probabilities","text":"Probabilities <: AbstractArray\nProbabilities(x) â†’ p\n\nProbabilities is a simple wrapper around x::AbstractArray{<:Real, N} that ensures its values sum to 1, so that p can be interpreted as N-dimensional probability mass function. In most use cases, p will be a vector.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Outcomes","page":"Probability mass functions","title":"Outcomes","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"probabilities_and_outcomes\noutcomes\noutcome_space\ntotal_outcomes\nmissing_outcomes","category":"page"},{"location":"probabilities/#ComplexityMeasures.probabilities_and_outcomes","page":"Probability mass functions","title":"ComplexityMeasures.probabilities_and_outcomes","text":"probabilities_and_outcomes(est, x)\n\nReturn probs, outs, where probs = probabilities(x, est) and outs[i] is the outcome with probability probs[i]. The element type of outs depends on the estimator. outs is a subset of the outcome_space of est.\n\nSee also outcomes, total_outcomes.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.outcomes","page":"Probability mass functions","title":"ComplexityMeasures.outcomes","text":"outcomes(est::ProbabilitiesEstimator, x)\n\nReturn all (unique) outcomes contained in x according to the given estimator. Equivalent with probabilities_and_outcomes(x, est)[2], but for some estimators it may be explicitly extended for better performance.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.outcome_space","page":"Probability mass functions","title":"ComplexityMeasures.outcome_space","text":"outcome_space(est::ProbabilitiesEstimator, x) â†’ Î©\n\nReturn a container containing all possible outcomes of est for input x.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to outcome_space(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.total_outcomes","page":"Probability mass functions","title":"ComplexityMeasures.total_outcomes","text":"total_outcomes(est::ProbabilitiesEstimator, x)\n\nReturn the length (cardinality) of the outcome space Omega of est.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to total_outcomes(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.missing_outcomes","page":"Probability mass functions","title":"ComplexityMeasures.missing_outcomes","text":"missing_outcomes(est::ProbabilitiesEstimator, x) â†’ n_missing::Int\n\nEstimate a probability distribution for x using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.\n\nSee also: MissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Estimators","page":"Probability mass functions","title":"Estimators","text":"","category":"section"},{"location":"probabilities/#probabilities_estimators","page":"Probability mass functions","title":"Overview of probabilities estimators","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Any of the following estimators can be used with probabilities (in the column \"input data\"  it is assumed that the eltype of the input is <: Real). Some estimators can also be used with contingency_matrix to estimate multivariate pmfs.","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Estimator Principle Input data\nContingency Count frequencies, optionally discretize first Any\nCountOccurrences Count of unique elements Any\nValueHistogram Binning (histogram) Vector, Dataset\nTransferOperator Binning (transfer operator) Vector, Dataset\nNaiveKernel Kernel density estimation Dataset\nSymbolicPermutation Ordinal patterns Vector, Dataset\nSymbolicWeightedPermutation Ordinal patterns Vector, Dataset\nSymbolicAmplitudeAwarePermutation Ordinal patterns Vector, Dataset\nSpatialSymbolicPermutation Ordinal patterns in space Array\nDispersion Dispersion patterns Vector\nSpatialDispersion Dispersion patterns in space Array\nDiversity Cosine similarity Vector\nWaveletOverlap Wavelet transform Vector\nPowerSpectrum Fourier transform Vector","category":"page"},{"location":"probabilities/#Contingency","page":"Probability mass functions","title":"Contingency","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Contingency","category":"page"},{"location":"probabilities/#CausalityTools.Contingency","page":"Probability mass functions","title":"CausalityTools.Contingency","text":"Contingency <: ProbabilitiesEstimator\nContingency(est::Union{ProbabilitiesEstimator, Nothing} = nothing)\n\nContingency is a probabilities estimator that transforms input data to a multidimensional probability mass function (internally represented as ContingencyMatrix.\n\nIt works directly on raw discrete/categorical data. Alternatively, if a ProbabilitiesEstimator est for which marginal_encodings is implemented is given, then input data are first discretized before creating the contingency matrix.\n\nnote: Note\nContingency estimator differs from other ProbabilitiesEstimators in that it's not compatible with probabilities and other methods. Instead, it is used to construct ContingencyMatrix, from which probabilities can be computed.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Count-occurrences","page":"Probability mass functions","title":"Count occurrences","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"CountOccurrences","category":"page"},{"location":"probabilities/#ComplexityMeasures.CountOccurrences","page":"Probability mass functions","title":"ComplexityMeasures.CountOccurrences","text":"CountOccurrences()\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input. Hence, input x is needed for a well-defined outcome_space.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Histograms","page":"Probability mass functions","title":"Histograms","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"ValueHistogram\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"probabilities/#ComplexityMeasures.ValueHistogram","page":"Probability mass functions","title":"ComplexityMeasures.ValueHistogram","text":"ValueHistogram(b::AbstractBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are:\n\nRectangularBinning\nFixedRectangularBinning\n\nThe ValueHistogram estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes Îµ without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueHistogram(Ïµ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueHistogram is the unique bins constructed from b. Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals [a, b). The bins are in data units, not integer (cartesian indices units), and are returned as SVectors, i.e., same type as input data.\n\nFor convenience, outcome_space returns the outcomes in the same array format as the underlying binning (e.g., Matrix for 2D input).\n\nFor FixedRectangularBinning the outcome_space is well-defined from the binning, but for RectangularBinning input x is needed as well.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RectangularBinning","page":"Probability mass functions","title":"ComplexityMeasures.RectangularBinning","text":"RectangularBinning(Ïµ, precise = false) <: AbstractBinning\n\nRectangular box partition of state space using the scheme Ïµ, deducing the histogram extent and bin width from the input data.\n\nRectangularBinning is a convenience struct. It is re-cast into FixedRectangularBinning once the data are provided, so see that docstring for info on the bin calculation and the meaning of precise.\n\nBinning instructions are deduced from the type of Ïµ as follows:\n\nÏµ::Int divides each coordinate axis into Ïµ equal-length intervals  that cover all data.\nÏµ::Float64 divides each coordinate axis into intervals of fixed size Ïµ, starting  from the axis minima until the data is completely covered by boxes.\nÏµ::Vector{Int} divides the i-th coordinate axis into Ïµ[i] equal-length  intervals that cover all data.\nÏµ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  Ïµ[i], starting from the axis minima until the data is completely covered by boxes.\n\nRectangularBinning ensures all input data are covered by extending the created ranges if need be.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.FixedRectangularBinning","page":"Probability mass functions","title":"ComplexityMeasures.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false)\n\nRectangular box partition of state space where the partition along each dimension is explicitly given by each range ranges, which is a tuple of AbstractRange subtypes. Typically, each range is the output of the range Base function, e.g., ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)). All ranges must be sorted.\n\nThe optional second argument precise dictates whether Julia Base's TwicePrecision is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is false.\n\nPoints falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open: [a, b). This means that the last value of each of the ranges dictates the last right-closing value. This value does not belong to the histogram! E.g., if given a range r = range(0, 1; length = 11), with r[end] = 1, the value 1 is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here [0.9, 1))!\n\nEquivalently, the size of the histogram is histsize = map(r -> length(r)-1, ranges)!\n\nFixedRectangularBinning leads to a well-defined outcome space without knowledge of input data, see ValueHistogram.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Symbolic-permutations","page":"Probability mass functions","title":"Symbolic permutations","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"SymbolicPermutation\nSymbolicWeightedPermutation\nSymbolicAmplitudeAwarePermutation","category":"page"},{"location":"probabilities/#ComplexityMeasures.SymbolicPermutation","page":"Probability mass functions","title":"ComplexityMeasures.SymbolicPermutation","text":"SymbolicPermutation <: ProbabilitiesEstimator\nSymbolicPermutation(; m = 3, Ï„ = 1, lt::Function = ComplexityMeasures.isless_rand)\n\nA probabilities estimator based on ordinal permutation patterns.\n\nWhen passed to probabilities the output depends on the input data type:\n\nUnivariate data. If applied to a univariate timeseries (AbstractVector), then the timeseries   is first embedded using embedding delay Ï„ and dimension m, resulting in embedding   vectors  bfx_i _i=1^N-(m-1)tau. Then, for each bfx_i,   we find its permutation pattern pi_i. Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using CountOccurrences. When giving the resulting probabilities to   entropy, the original permutation entropy is computed [BandtPompe2002].\nMultivariate data. If applied to a an D-dimensional Dataset,   then no embedding is constructed, m must be equal to D and Ï„ is ignored.   Each vector bfx_i of the dataset is mapped   directly to its permutation pattern pi_i by comparing the   relative magnitudes of the elements of bfx_i.   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy[He2016], although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of[He2016]).\n\nInternally, SymbolicPermutation uses the OrdinalPatternEncoding to represent ordinal patterns as integers for efficient computations.\n\nSee SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see SpatialSymbolicPermutation.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution [Zunino2017]. Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = ComplexityMeasures.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless.\n\nOutcome space\n\nThe outcome space Î© for SymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, â€¦, m. There are factorial(m) such patterns.\n\nFor example, the outcome [2, 3, 1] corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also [OrdinalPatternEncoding(@ref).\n\nIn-place symbolization\n\nSymbolicPermutation also implements the in-place probabilities! for Dataset input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example\n\nusing ComplexityMeasures\nm, N = 2, 100\nest = SymbolicPermutation(; m, Ï„)\nx = Dataset(rand(N, m)) # some input dataset\nÏ€s_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(Ï€s_ts, est, x)\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for timeseries.\" Physical review letters 88.17 (2002): 174102.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based timeseries analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n[He2016]: He, S., Sun, K., & Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.SymbolicWeightedPermutation","page":"Probability mass functions","title":"ComplexityMeasures.SymbolicWeightedPermutation","text":"SymbolicWeightedPermutation <: ProbabilitiesEstimator\nSymbolicWeightedPermutation(; Ï„ = 1, m = 3, lt::Function = ComplexityMeasures.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the weighted permutation entropy[Fadlallah2013]. The outcome space and keywords are the same as in SymbolicPermutation.\n\nDescription\n\nFor each ordinal pattern extracted from each state (or delay) vector, a weight is attached to it which is the variance of the vector. Probabilities are then estimated by summing the weights corresponding to the same pattern, instead of just counting the occurrence of the same pattern.\n\nnote: An implementation note\nNote: in equation 7, section III, of the original paper, the authors writew_j = dfrac1msum_k=1^m (x_j-(k-1)tau - mathbfhatx_j^m tau)^2*But given the formula they give for the arithmetic mean, this is not the variance of the delay vector mathbfx_i, because the indices are mixed: x_j+(k-1)tau in the weights formula, vs. x_j+(k+1)tau in the arithmetic mean formula. Here, delay embedding and computation of the patterns and their weights are completely separated processes, ensuring that we compute the arithmetic mean correctly for each vector of the input dataset (which may be a delay-embedded timeseries).\n\n[Fadlallah2013]: Fadlallah, et al. \"Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.\" Physical Review E 87.2 (2013): 022911.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.SymbolicAmplitudeAwarePermutation","page":"Probability mass functions","title":"ComplexityMeasures.SymbolicAmplitudeAwarePermutation","text":"SymbolicAmplitudeAwarePermutation <: ProbabilitiesEstimator\nSymbolicAmplitudeAwarePermutation(; Ï„ = 1, m = 3, A = 0.5, lt = ComplexityMeasures.isless_rand)\n\nA variant of SymbolicPermutation that also incorporates amplitude information, based on the amplitude-aware permutation entropy[Azami2016]. The outcome space and keywords are the same as in SymbolicPermutation.\n\nDescription\n\nSimilarly to SymbolicWeightedPermutation, a weight w_i is attached to each ordinal pattern extracted from each state (or delay) vector mathbfx_i = (x_1^i x_2^i ldots x_m^i) as\n\nw_i = dfracAm sum_k=1^m x_k^i  + dfrac1-Ad-1\nsum_k=2^d x_k^i - x_k-1^i\n\nwith 0 leq A leq 1. When A=0 , only internal differences between the elements of mathbfx_i are weighted. Only mean amplitude of the state vector elements are weighted when A=1. With, 0A1, a combined weighting is used.\n\n[Azami2016]: Azami, H., & Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Dispersion-patterns","page":"Probability mass functions","title":"Dispersion patterns","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Dispersion","category":"page"},{"location":"probabilities/#ComplexityMeasures.Dispersion","page":"Probability mass functions","title":"ComplexityMeasures.Dispersion","text":"Dispersion(; c = 5, m = 2, Ï„ = 1, check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values[Li2018] are m âˆˆ [2, 3], Ï„ = 1 for the embedding, and c âˆˆ [3, 4, â€¦, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is encoded into a symbol timeseries S using the Gaussian encoding GaussianCDFEncoding with empirical mean Î¼ and empirical standard deviation Ïƒ (both determined from X), and c as given to Dispersion.\n\nThen, S is embedded into an m-dimensional time series, using an embedding lag of tau, which yields a total of N - (m - 1)tau delay vectors z_i, or \"dispersion patterns\". Since each element of z_i can take on c different values, and each delay vector has m entries, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nThe returned probabilities are simply the frequencies of the unique dispersion patterns present in S (i.e., the CountOccurences of S).\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of S.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\nFor a version of this estimator that can be used on high-dimensional arrays, see SpatialDispersion.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. EntropyDefinition, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#TransferOperator-(binning)","page":"Probability mass functions","title":"TransferOperator (binning)","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"TransferOperator","category":"page"},{"location":"probabilities/#ComplexityMeasures.TransferOperator","page":"Probability mass functions","title":"ComplexityMeasures.TransferOperator","text":"TransferOperator <: ProbabilitiesEstimator\nTransferOperator(b::RectangularBinning)\n\nA probability estimator based on binning data into rectangular boxes dictated by the given binning scheme b, then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nOutcome space\n\nThe outcome space for TransferOperator is the set of unique bins constructed from b. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as SVectors.\n\nBin ordering\n\nBins returned by probabilities_and_outcomes are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if\n\nb = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries\n\nthen probs[i] is the invariant measure (probability) of the bin outcomes[i], which is the i-th bin visited by the timeseries with nonzero measure.\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by Ïµ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to binning, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"For explicit estimation of the transfer operator, see ComplexityMeasures.jl.","category":"page"},{"location":"probabilities/#Utility-methods/types","page":"Probability mass functions","title":"Utility methods/types","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"probabilities/#ComplexityMeasures.InvariantMeasure","page":"Probability mass functions","title":"ComplexityMeasures.InvariantMeasure","text":"InvariantMeasure(to, Ï)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure Ï, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.invariantmeasure","page":"Probability mass functions","title":"ComplexityMeasures.invariantmeasure","text":"invariantmeasure(x::AbstractDataset, binning::RectangularBinning) â†’ iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems, Plots, ComplexityMeasures\nD = 4\nds = Systems.lorenz96(D; F = 32.0)\nN, dt = 20000, 0.1\norbit = trajectory(ds, N*dt; dt = dt, Ttr = 10.0)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) â†’ (Ï::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element Ï[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.transfermatrix","page":"Probability mass functions","title":"ComplexityMeasures.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) â†’ (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Kernel-density","page":"Probability mass functions","title":"Kernel density","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"NaiveKernel","category":"page"},{"location":"probabilities/#ComplexityMeasures.NaiveKernel","page":"Probability mass functions","title":"ComplexityMeasures.NaiveKernel","text":"NaiveKernel(Ïµ::Real; method = KDTree, w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius Ïµ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nKeyword arguments\n\nmethod = KDTree: the search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\nw = 0: the Theiler window, which excludes indices s that are within i - s  w from the given point x_i.\nmetric = Euclidean(): the distance metric.\n\nOutcome space\n\nThe outcome space Î© for NaiveKernel are the indices of the input data, eachindex(x). Hence, input x is needed for a well-defined outcome_space. The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors).\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Local-likelihood","page":"Probability mass functions","title":"Local likelihood","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"LocalLikelihood","category":"page"},{"location":"probabilities/#CausalityTools.LocalLikelihood","page":"Probability mass functions","title":"CausalityTools.LocalLikelihood","text":"LocalLikelihood <: ProbabilitiesEstimator\nLocalLikelihood(k = 5, w = 0, metric = Euclidean())\n\nThe LocalLikelihood estimator estimates the density around a given query point by a Gaussian kernel informed by the local mean and covariance.\n\nTo form probabilities from the pointwise density estimates, the densities are simply sum-normalized to 1.\n\nOutcome space\n\nThe outcome_space for LocalLikelihood is the set of input points.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Timescales","page":"Probability mass functions","title":"Timescales","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"probabilities/#ComplexityMeasures.WaveletOverlap","page":"Probability mass functions","title":"ComplexityMeasures.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001]. Input timeseries x is needed for a well-defined outcome space.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\nOutcome space\n\nThe outcome space for WaveletOverlap are the integers 1, 2, â€¦, N enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input and input x is needed for a well-defined outcome_space.\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.PowerSpectrum","page":"Probability mass functions","title":"ComplexityMeasures.PowerSpectrum","text":"PowerSpectrum() <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nOutcome space\n\nThe outcome space Î© for PowerSpectrum is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1. Input x is needed for a well-defined outcome_space.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral EntropyDefinition Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Diversity","page":"Probability mass functions","title":"Diversity","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Diversity","category":"page"},{"location":"probabilities/#ComplexityMeasures.Diversity","page":"Probability mass functions","title":"ComplexityMeasures.Diversity","text":"Diversity(; m::Int, Ï„::Int, nbins::Int)\n\nA ProbabilitiesEstimator based on the cosine similarity. It can be used with entropy to compute the diversity entropy of an input timeseries[Wang2020].\n\nThe implementation here allows for Ï„ != 1, which was not considered in the original paper.\n\nDescription\n\nDiversity probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag Ï„ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mÏ„.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mÏ„-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals (including the value +1).\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for Diversity is the bins of the [-1, 1] interval, and the return configuration is the same as in ValueHistogram (left bin edge).\n\n[Wang2020]: Wang, X., Si, S., & Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Spatial-estimators","page":"Probability mass functions","title":"Spatial estimators","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"SpatialSymbolicPermutation\nSpatialDispersion","category":"page"},{"location":"probabilities/#ComplexityMeasures.SpatialSymbolicPermutation","page":"Probability mass functions","title":"ComplexityMeasures.SpatialSymbolicPermutation","text":"SpatialSymbolicPermutation <: ProbabilitiesEstimator\nSpatialSymbolicPermutation(stencil, x; periodic = true)\n\nA symbolic, permutation-based probabilities estimator for spatiotemporal systems that generalises SymbolicPermutation to high-dimensional arrays. The order m of the permutation pattern is extracted from the stencil, see below.\n\nSpatialSymbolicPermutation is based on the 2D and 3D spatiotemporal permutation entropy estimators by by Ribeiro et al. (2012)[Ribeiro2012] and Schlemmer et al. (2018)[Schlemmer2018]), respectively, but is here implemented as a pure probabilities probabilities estimator that is generalized for D-dimensional input array x, with arbitrary regions (stencils) to get patterns form and (possibly) periodic boundary conditions.\n\nSee below for ways to specify the stencil. If periodic = true, then the stencil wraps around at the ends of the array. If false, then collected regions with indices which exceed the array bounds are skipped.\n\nIn combination with entropy and entropy_normalized, this probabilities estimator can be used to compute generalized spatiotemporal permutation EntropyDefinition of any type.\n\nOutcome space\n\nThe outcome space Î© for SpatialSymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, â€¦, m, ordered lexicographically. There are factorial(m) such patterns. Here m refers to the number of points included in stencil.\n\nStencils\n\nThe stencil defines what local area to use to group hypervoxels. Each grouping of hypervoxels is mapped to an order-m permutation pattern, which is then mapped to an integer as in SymbolicPermutation. The stencil is moved around the input array, in a sense \"scanning\" the input array, to collect all possible groupings allowed by the boundary condition (periodic or not).\n\nStencils are passed in one of the following three ways:\n\nAs vectors of CartesianIndex which encode the offset of indices to include in the  stencil, with respect to the current array index when scanning over the array.  For example stencil = CartesianIndex.([(0,0), (0,1), (1,1), (1,0)]).  Don't forget to include the zero offset index if you want to include the hypervoxel  itself, which is almost always the case.  Here the stencil creates a 2x2 square extending to the bottom and right of the pixel  (directions here correspond to the way Julia prints matrices by default).  When passing a stencil as a vector of CartesianIndex, m = length(stencil).\nAs a D-dimensional array (where D matches the dimensionality of the input data)  containing 0s and 1s, where if stencil[index] == 1, the corresponding pixel is  included, and if stencil[index] == 0, it is not included.  To generate the same estimator as in 1., use stencil = [1 1; 1 1].  When passing a stencil as a D-dimensional array, m = sum(stencil)\nAs a Tuple containing two Tuples, both of length D, for D-dimensional data.  The first tuple specifies the extent of the stencil, where extent[i]  dictates the number of hypervoxels to be included along the ith axis and lag[i]  the separation of hypervoxels along the same axis.  This method can only generate (hyper)rectangular stencils. To create the same estimator as  in the previous examples, use here stencil = ((2, 2), (1, 1)).  When passing a stencil using extent and lag, m = prod(extent).\n\n[Ribeiro2012]: Ribeiro et al. (2012). Complexity-entropy causality plane as a complexity measure for two-dimensional patterns. https://doi.org/10.1371/journal.pone.0040689\n\n[Schlemmer2018]: Schlemmer et al. (2018). Spatiotemporal Permutation EntropyDefinition as a Measure for Complexity of Cardiac Arrhythmia. https://doi.org/10.3389/fphy.2018.00039\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.SpatialDispersion","page":"Probability mass functions","title":"ComplexityMeasures.SpatialDispersion","text":"SpatialDispersion <: ProbabilitiesEstimator\nSpatialDispersion(stencil, x::AbstractArray;\n    periodic = true,\n    c = 5,\n    skip_encoding = false,\n    L = nothing,\n)\n\nA dispersion-based probabilities estimator that generalises Dispersion for input data that are high-dimensional arrays.\n\nSpatialDispersion is based on Azami et al. (2019)[Azami2019]'s 2D square dispersion (Shannon) entropy estimator, but is here implemented as a pure probabilities probabilities estimator that is generalized for N-dimensional input data x, with arbitrary neighborhood regions (stencils) and (optionally) periodic boundary conditions.\n\nIn combination with entropy and entropy_normalized, this probabilities estimator can be used to compute (normalized) generalized spatiotemporal dispersion EntropyDefinition of any type.\n\nArguments\n\nstencil. Defines what local area (hyperrectangle), or which points within this area,   to include around each hypervoxel (i.e. pixel in 2D). The examples below demonstrate   different ways of specifying stencils. For details, see   SpatialSymbolicPermutation. See SpatialSymbolicPermutation for   more information about stencils.\nx::AbstractArray. The input data. Must be provided because we need to know its size   for optimization and bound checking.\n\nKeyword arguments\n\nperiodic::Bool. If periodic == true, then the stencil should wrap around at the   end of the array. If periodic = false, then pixels whose stencil exceeds the array   bounds are skipped.\nc::Int. Determines how many discrete categories to use for the Gaussian encoding.\nskip_encoding. If skip_encoding == true, encoding is ignored, and dispersion   patterns are computed directly from x, under the assumption that L is the alphabet   length for x (useful for categorical or integer data). Thus, if   skip_encoding == true, then L must also be specified. This is useful for   categorical or integer-valued data.\nL. If L == nothing (default), then the number of total outcomes is inferred from   stencil and encoding. If L is set to an integer, then the data is considered   pre-encoded and the number of total outcomes is set to L.\n\nOutcome space\n\nThe outcome space for SpatialDispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF. Hence, the outcome space is all m-dimensional delay vectors whose elements are all possible values in 1:c. There are c^m such vectors.\n\nDescription\n\nEstimating probabilities/entropies from higher-dimensional data is conceptually simple.\n\nDiscretize each value (hypervoxel) in x relative to all other values xáµ¢ âˆˆ x using the  provided encoding scheme.\nUse stencil to extract relevant (discretized) points around each hypervoxel.\nConstruct a symbol these points.\nTake the sum-normalized histogram of the symbol as a probability distribution.\nOptionally, compute entropy or entropy_normalized from this  probability distribution.\n\nUsage\n\nHere's how to compute spatial dispersion entropy using the three different ways of specifying stencils.\n\nx = rand(50, 50) # first \"time slice\" of a spatial system evolution\n\n# Cartesian stencil\nstencil_cartesian = CartesianIndex.([(0,0), (1,0), (1,1), (0,1)])\nest = SpatialDispersion(stencil_cartesian, x)\nentropy_normalized(est, x)\n\n# Extent/lag stencil\nextent = (2, 2); lag = (1, 1); stencil_ext_lag = (extent, lag)\nest = SpatialDispersion(stencil_ext_lag, x)\nentropy_normalized(est, x)\n\n# Matrix stencil\nstencil_matrix = [1 1; 1 1]\nest = SpatialDispersion(stencil_matrix, x)\nentropy_normalized(est, x)\n\nTo apply this to timeseries of spatial data, simply loop over the call (broadcast), e.g.:\n\nimgs = [rand(50, 50) for i = 1:100]; # one image per second over 100 seconds\nstencil = ((2, 2), (1, 1)) # a 2x2 stencil (i.e. dispersion patterns of length 4)\nest = SpatialDispersion(stencil, first(imgs))\nh_vs_t = entropy_normalized.(Ref(est), imgs)\n\nComputing generalized spatiotemporal dispersion entropy is trivial, e.g. with Renyi:\n\nx = reshape(repeat(1:5, 500) .+ 0.1*rand(500*5), 50, 50)\nest = SpatialDispersion(stencil, x)\nentropy(Renyi(q = 2), est, x)\n\nSee also: SpatialSymbolicPermutation, GaussianCDFEncoding, symbolize.\n\n[Azami2019]: Azami, H., da Silva, L. E. V., Omoto, A. C. M., & Humeau-Heurtier, A. (2019). Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images. Signal Processing: Image Communication, 75, 178-187.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Encodings","page":"Probability mass functions","title":"Encodings","text":"","category":"section"},{"location":"probabilities/#Encodings-API","page":"Probability mass functions","title":"Encodings API","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Some probability estimators first \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\".","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"The encodings API is defined by:","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Encoding\nencode\ndecode","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"Encoding\nencode\ndecode","category":"page"},{"location":"probabilities/#ComplexityMeasures.Encoding","page":"Probability mass functions","title":"ComplexityMeasures.Encoding","text":"Encoding\n\nThe supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions encode and decode. Some probability estimators utilize encodings internally.\n\nCurrent available encodings are:\n\nOrdinalPatternEncoding.\nGaussianCDFEncoding.\nRectangularBinEncoding.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.encode","page":"Probability mass functions","title":"ComplexityMeasures.encode","text":"encode(c::Encoding, Ï‡) -> i::Int\n\nEncode an element Ï‡ âˆˆ x of input data x (those given to probabilities) using encoding c.\n\nThe special value of -1 is reserved as a return value for inappropriate elements Ï‡ that cannot be encoded according to c.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#ComplexityMeasures.decode","page":"Probability mass functions","title":"ComplexityMeasures.decode","text":"decode(c::Encoding, i::Int) -> Ï‰\n\nDecode an encoded element i into the outcome Ï‰ âˆˆ Î© it corresponds to.\n\nÎ© is the outcome_space of a probabilities estimator that uses encoding c.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Available-encodings","page":"Probability mass functions","title":"Available encodings","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"OrdinalPatternEncoding\nGaussianCDFEncoding\nRectangularBinEncoding","category":"page"},{"location":"probabilities/#ComplexityMeasures.OrdinalPatternEncoding","page":"Probability mass functions","title":"ComplexityMeasures.OrdinalPatternEncoding","text":"OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding(m::Int, lt = ComplexityMeasures.isless_rand)\n\nAn encoding scheme that encodes length-m vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by SymbolicPermutation and similar estimators, see that for a description of the outcome space.\n\nThe ordinal/permutation pattern of a vector Ï‡ is simply sortperm(Ï‡), which gives the indices that would sort Ï‡ in ascending order.\n\nDescription\n\nThe Lehmer code, as implemented here, is a bijection between the set of factorial(m) possible permutations for a length-m sequence, and the integers 1, 2, â€¦, factorial(m). The encoding step uses algorithm 1 in Berger et al. (2019)[Berger2019], which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!).\n\nExample\n\njulia> using ComplexityMeasures\n\njulia> Ï‡ = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, Ï‡)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3\n\nIf you want to encode something that is already a permutation pattern, then you can use the non-exported permutation_to_integer function.\n\n[Berger2019]: Berger et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.GaussianCDFEncoding","page":"Probability mass functions","title":"ComplexityMeasures.GaussianCDFEncoding","text":"GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding(; Î¼, Ïƒ, c::Int = 3)\n\nAn encoding scheme that encodes a scalar value into one of the integers sáµ¢ âˆˆ [1, 2, â€¦, c] based on the normal cumulative distribution function (NCDF), and decodes the sáµ¢ into subintervals of [0, 1] (with some loss of information).\n\nNotice that the decoding step does not yield an element of any outcome space of the estimators that use GaussianCDFEncoding internally, such as Dispersion. That is because these estimators additionally delay embed the encoded data.\n\nDescription\n\nGaussianCDFEncoding first maps an input point x  (scalar) to a new real number y_ in 0 1 by using the normal cumulative distribution function (CDF) with the given mean Î¼ and standard deviation Ïƒ, according to the map\n\nx to y  y = dfrac1 sigma\n    sqrt2 pi int_-infty^x e^(-(x - mu)^2)(2 sigma^2) dx\n\nNext, the interval [0, 1] is equidistantly binned and enumerated 1 2 ldots c,  and y is linearly mapped to one of these integers using the linear map  y to z  z = textfloor(y(c-1)) + 1.\n\nBecause of the floor operation, some information is lost, so when used with decode, each decoded sáµ¢ is mapped to a subinterval of [0, 1].\n\nExamples\n\njulia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> Î¼, Ïƒ = mean(x), std(x); encoding = GaussianCDFEncoding(; Î¼, Ïƒ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#ComplexityMeasures.RectangularBinEncoding","page":"Probability mass functions","title":"ComplexityMeasures.RectangularBinEncoding","text":"RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning)\n\nAn encoding scheme that encodes points Ï‡ âˆˆ x into their histogram bins.\n\nThe first call signature simply initializes a FixedRectangularBinning and then calls the second call signature.\n\nSee FixedRectangularBinning for info on mapping points to bins.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#Contingency-tables","page":"Probability mass functions","title":"Contingency tables","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"To estimate discrete information theoretic quantities that are functions of more than one variable, we must estimate empirical joint probability mass functions (pmf). The function contingency_matrix accepts an arbitrary number of equal-length input data and returns the corresponding multidimensional contingency table as a ContingencyMatrix. From this table, we can extract the necessary joint and marginal pmfs for computing any discrete function of multivariate discrete probability distributions. This is essentially the multivariate analogue of Probabilities.","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"But why would I use a ContingencyMatrix instead of some other indirect estimation method, you may ask. The answer is that ContingencyMatrix allows you to compute any of the information theoretic quantities offered in this package for any type of input data. You input data can literally be any hashable type, for example String, Tuple{Int, String, Int}, or YourCustomHashableDataType.","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"In the case of numeric data, using a ContingencyMatrix is typically a bit slower than other dedicated estimation procedures. For example, quantities like discrete Shannon-type condmutualinfo are faster to estimate using a formulation based on sums of four entropies (the H4-principle). This is faster because we can both utilize the blazingly fast Dataset structure directly, and we can avoid explicitly estimating the entire joint pmf, which demands many extra calculation steps. Whatever you use in practice depends on your use case and available estimation methods, but you can always fall back to contingency matrices for any discrete measure.","category":"page"},{"location":"probabilities/#Contingency-matrix-API","page":"Probability mass functions","title":"Contingency matrix API","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"ContingencyMatrix\ncontingency_matrix\nmarginal_encodings","category":"page"},{"location":"probabilities/#CausalityTools.ContingencyMatrix","page":"Probability mass functions","title":"CausalityTools.ContingencyMatrix","text":"ContingencyMatrix{T, N} <: Probabilities{T, N}\nContingencyMatrix(frequencies::AbstractArray{Int, N})\n\nA contingency matrix is essentially a multivariate analogue of Probabilities that also keep track of raw frequencies.\n\nThe contingency matrix can be constructed directyly from an N-dimensional frequencies array. Alternatively, the contingency_matrix function performs counting for you; this works on both raw categorical data, or by first discretizing data using a a ProbabilitiesEstimator.\n\nDescription\n\nA ContingencyMatrix c is just a simple wrapper around around AbstractArray{T, N}. Indexing c with multiple indices i, j, â€¦ returns the (i, j, â€¦)th element of the empirical probability mass function (pmf). The following convencience methods are defined:\n\nfrequencies(c; dims) returns the multivariate raw counts along the given `dims   (default to all available dimensions).\nprobabilities(c; dims) returns a multidimensional empirical   probability mass function (pmf) along the given dims (defaults to all available   dimensions), i.e. the normalized counts.\nprobabilities(c, i::Int) returns the marginal probabilities for the i-th dimension.\noutcomes(c, i::Int) returns the marginal outcomes for the i-th dimension.\n\nOrdering\n\nThe ordering of outcomes are internally consistent, but we make no promise on the ordering of outcomes relative to the input data. This means that if your input data are x = rand([\"yes\", \"no\"], 100); y = rand([\"small\", \"medium\", \"large\"], 100), you'll get a 2-by-3 contingency matrix, but there currently no easy way to determine which outcome the i-j-th row/column of this matrix corresponds to.\n\nSince ContingencyMatrix is intended for use in information theoretic methods that don't care about ordering, as long as the ordering is internally consistent, this is not an issue for practical applications in this package. This may change in future releases.\n\nUsage\n\nContingency matrices is used in the computation of discrete versions of the following quantities:\n\nentropy_joint.\nmutualinfo.\ncondmutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"probabilities/#CausalityTools.contingency_matrix","page":"Probability mass functions","title":"CausalityTools.contingency_matrix","text":"contingency_matrix(x, y, [z, ...]) â†’ c::ContingencyMatrix\ncontingency_matrix(est::ProbabilitiesEstimator, x, y, [z, ...]) â†’ c::ContingencyMatrix\n\nEstimate a multidimensional contingency matrix c from input data x, y, â€¦, where the input data can be of any and different types, as long as length(x) == length(y) == â€¦.\n\nFor already discretized data, use the first method. For continuous data, you want to discretize the data before computing the contingency table. You can do this manually and then use the first method. Alternatively, you can provide a ProbabilitiesEstimator as the first argument to the constructor. Then the input variables x, y, â€¦ are discretized separately according to est (enforcing the same outcome space for all variables), by calling marginal_encodings.\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#CausalityTools.marginal_encodings","page":"Probability mass functions","title":"CausalityTools.marginal_encodings","text":"marginal_encodings(est::ProbabilitiesEstimator, x::VectorOrDataset...)\n\nEncode/discretize each input vector xáµ¢ âˆˆ x according to a procedure determined by est. Any xáµ¢ âˆˆ X that are multidimensional (Datasets) will be encoded column-wise, i.e. each column of xáµ¢ is treated as a timeseries and is encoded separately.\n\nThis is useful for computing any discrete information theoretic quantity, and is used internally by contingency_matrix.\n\nSupported estimators\n\nValueHistogram. Bin visitation frequencies are counted in the joint space XY,   then marginal visitations are obtained from the joint bin visits.   This behaviour is the same for both FixedRectangularBinning and   RectangularBinning (which adapts the grid to the data).   When using FixedRectangularBinning, the range along the first dimension   is used as a template for all other dimensions.\nSymbolicPermutation. Each timeseries is separately encoded according   to its ordinal pattern.\nDispersion. Each timeseries is separately encoded according to its   dispersion pattern.\n\nMany more implementations are possible. Each new implementation gives one new way of estimating the ContingencyMatrix\n\n\n\n\n\n","category":"function"},{"location":"probabilities/#Examples","page":"Probability mass functions","title":"Examples","text":"","category":"section"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"The following two mutual information estimates on the integer (\"categorical\") vectors x, y are equivalent (up to rounding errors), but the latter is much faster to compute.","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"using CausalityTools\nn = 1000\nx = rand(1:3, n)\ny = rand(1:4, n)\nmcont = mutualinfo(MIShannon(), contingency_matrix(x, y))\nmfast = mutualinfo(MIShannon(), CountOccurrences(), x, y)\nmcont, mfast","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"On purely categorical data, you have to use the contingency matrix approach.","category":"page"},{"location":"probabilities/","page":"Probability mass functions","title":"Probability mass functions","text":"using CausalityTools\nn = 100\nlikeit = rand([\"yes\", \"no\"], n)\nfood = rand([\"veggies\", \"meat\", \"fish\"], n)\nmutualinfo(MIShannon(), contingency_matrix(likeit, food))","category":"page"},{"location":"examples/examples_conditional_entropy/#examples_condentropy","page":"Conditional entropy","title":"Entropy","text":"","category":"section"},{"location":"examples/examples_conditional_entropy/#Discrete:-example-from-Cover-and-Thomas","page":"Conditional entropy","title":"Discrete: example from Cover & Thomas","text":"","category":"section"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"This is essentially example 2.2.1 in Cover & Thomas (2006), where they use the following contingency table as an example. We'll take their example and manually construct a ContingencyMatrix that we can use to compute the conditional entropy. The ContingencyMatrix constructor takes the probabilities as the first argument and the raw frequencies as the second argument. Note also that Julia is column-major, so we need to transpose their example. Then their X is in the first dimension of our contingency matrix (along columns) and their Y is our second dimension (rows).","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"using CausalityTools\nfreqs_yx = [1//8 1//16 1//32 1//32; \n    1//16 1//8  1//32 1//32;\n    1//16 1//16 1//16 1//16; \n    1//4  0//1  0//1  0//1];\nfreqs_xy = transpose(freqs_yx);\nprobs_xy = freqs_xy ./ sum(freqs_xy)\nc_xy = ContingencyMatrix(probs_xy, freqs_xy)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"The marginal distribution for x (first dimension) is","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probabilities(c_xy, 1)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"The marginal distribution for y (second dimension) is","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probabilities(c_xy, 2)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"And the Shannon conditional entropy H^S(X  Y)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"ce_x_given_y = entropy_conditional(CEShannon(), c_xy) |> Rational","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"This is the same as in their example. Hooray! To compute H^S(Y  X), we just need to flip the contingency matrix.","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probs_yx = freqs_yx ./ sum(freqs_yx);\nc_yx = ContingencyMatrix(probs_yx, freqs_yx);\nce_y_given_x = entropy_conditional(CEShannon(), c_yx) |> Rational","category":"page"},{"location":"entropy/#entropies","page":"Entropy","title":"Entropies","text":"","category":"section"},{"location":"entropy/#Entropies-API","page":"Entropy","title":"Entropies API","text":"","category":"section"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"The entropies API is defined by","category":"page"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"EntropyDefinition\nentropy\nDifferentialEntropyEstimator","category":"page"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"Please be sure you have read the Terminology section before going through the API here, to have a good idea of the different \"flavors\" of entropies and how they all come together over the common interface of the entropy function.","category":"page"},{"location":"entropy/#Entropy-definitions","page":"Entropy","title":"Entropy definitions","text":"","category":"section"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"EntropyDefinition\nShannon\nRenyi\nTsallis\nKaniadakis\nCurado\nStretchedExponential","category":"page"},{"location":"entropy/#ComplexityMeasures.EntropyDefinition","page":"Entropy","title":"ComplexityMeasures.EntropyDefinition","text":"EntropyDefinition\n\nEntropyDefinition is the supertype of all types that encapsulate definitions of (generalized) entropies. These also serve as estimators of discrete entropies, see description below.\n\nCurrently implemented entropy definitions are:\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q â†’ 1.\nKaniadakis.\nCurado.\nStretchedExponential.\n\nThese types can be given as inputs to entropy or entropy_normalized.\n\nDescription\n\nMathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. AmigÃ³ et al.'s[AmigÃ³2018] summary paper gives a nice overview.\n\nHowever, for a software implementation computing entropies in practice, definitions is not really what matters; estimators matter. Because in the practical sense, one needs to estimate a definition from finite data, and different ways of estimating a quantity come with their own pros and cons.\n\nThat is why the type DiscreteEntropyEstimator exists, which is what is actually given to entropy. Some ways to estimate a discrete entropy only apply to a specific entropy definition. For estimators that can be applied to various entropy definitions, this is specified by providing an instance of EntropyDefinition to the estimator.\n\n[AmigÃ³2018]: AmigÃ³, J. M., Balogh, S. G., & HernÃ¡ndez, S. (2018). A brief review of generalized entropies. Entropy, 20(11), 813.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Shannon","page":"Entropy","title":"ComplexityMeasures.Shannon","text":"Shannon <: EntropyDefinition\nShannon(; base = 2)\n\nThe Shannon[Shannon1948] entropy, used with entropy to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nThe maximum value of the Shannon entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Renyi","page":"Entropy","title":"ComplexityMeasures.Renyi","text":"Renyi <: EntropyDefinition\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe RÃ©nyi[RÃ©nyi1960] generalized order-q entropy, used with entropy to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the RÃ©nyi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nThe maximum value of the RÃ©nyi entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[RÃ©nyi1960]: A. RÃ©nyi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Tsallis","page":"Entropy","title":"ComplexityMeasures.Tsallis","text":"Tsallis <: EntropyDefinition\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis[Tsallis1988] generalized order-q entropy, used with entropy to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nThe maximum value of the Tsallis entropy is ``k(L^1 - q - 1)(1 - q), with L the total_outcomes.\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Kaniadakis","page":"Entropy","title":"ComplexityMeasures.Kaniadakis","text":"Kaniadakis <: EntropyDefinition\nKaniadakis(; Îº = 1.0, base = 2.0)\n\nThe Kaniadakis entropy (Tsallis, 2009)[Tsallis2009], used with entropy to compute\n\nH_K(p) = -sum_i=1^N p_i f_kappa(p_i)\n\nf_kappa (x) = dfracx^kappa - x^-kappa2kappa\n\nwhere if kappa = 0, regular logarithm to the given base is used, and 0 probabilities are skipped.\n\n[Tsallis2009]: Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Curado","page":"Entropy","title":"ComplexityMeasures.Curado","text":"Curado <: EntropyDefinition\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado & Nobre, 2004)[Curado2004], used with entropy to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b âˆˆ â„›, b > 0, where the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\nThe maximum entropy for Curado is L(1 - exp(-bL)) + exp(-b) - 1 with L the total_outcomes.\n\n[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.StretchedExponential","page":"Entropy","title":"ComplexityMeasures.StretchedExponential","text":"StretchedExponential <: EntropyDefinition\nStretchedExponential(; Î· = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999[Anteneodo1999]), used with entropy to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for Î· = 1.0.\n\nThe maximum entropy for StrechedExponential is a rather complicated expression involving incomplete Gamma functions (see source code).\n\n[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#Discrete","page":"Entropy","title":"Discrete","text":"","category":"section"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"entropy(::EntropyDefinition, ::ProbabilitiesEstimator, ::Any)\nentropy_maximum\nentropy_normalized","category":"page"},{"location":"entropy/#ComplexityMeasures.entropy-Tuple{EntropyDefinition, ProbabilitiesEstimator, Any}","page":"Entropy","title":"ComplexityMeasures.entropy","text":"entropy([e::DiscreteEntropyEstimator,] probs::Probabilities)\nentropy([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x)\n\nCompute the discrete entropy h::Real âˆˆ [0, âˆž), using the estimator e, in one of two ways:\n\nDirectly from existing Probabilities probs.\nFrom input data x, by first estimating a probability mass function using the provided ProbabilitiesEstimator, and then computing the entropy from that mass fuction using the provided DiscreteEntropyEstimator.\n\nInstead of providing a DiscreteEntropyEstimator, an EntropyDefinition can be given directly, in which case MLEntropy is used as the estimator. If e is not provided, Shannon() is used by default.\n\nMaximum entropy and normalized entropy\n\nAll discrete entropies have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the entropy_maximum. Or, one can use entropy_normalized to obtain the normalized form of the entropy (divided by the maximum).\n\nExamples\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), CountOccurrences(x), x) # syntactically equivalent to above\nh = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\n\n\n\n\n\n","category":"method"},{"location":"entropy/#ComplexityMeasures.entropy_maximum","page":"Entropy","title":"ComplexityMeasures.entropy_maximum","text":"entropy_maximum(e::EntropyDefinition, est::ProbabilitiesEstimator, x)\n\nReturn the maximum value of a discrete entropy with the given probabilities estimator and input data x. Like in outcome_space, for some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to entropy_maximum(e, est).\n\nentropy_maximum(e::EntropyDefinition, L::Int)\n\nSame as above, but computed directly from the number of total outcomes L.\n\n\n\n\n\n","category":"function"},{"location":"entropy/#ComplexityMeasures.entropy_normalized","page":"Entropy","title":"ComplexityMeasures.entropy_normalized","text":"entropy_normalized([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) â†’ hÌƒ\n\nReturn hÌƒ âˆˆ [0, 1], the normalized discrete entropy of x, i.e. the value of entropy divided by the maximum value for e, according to the given probabilities estimator.\n\nInstead of a discrete entropy estimator, an EntropyDefinition can be given as first argument. If e is not given, it defaults to Shannon().\n\nNotice that there is no method entropy_normalized(e::DiscreteEntropyEstimator, probs::Probabilities), because there is no way to know the amount of possible events (i.e., the total_outcomes) from probs.\n\n\n\n\n\n","category":"function"},{"location":"entropy/#Differential/continuous","page":"Entropy","title":"Differential/continuous","text":"","category":"section"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"entropy(::EntropyDefinition, ::DifferentialEntropyEstimator, ::Any)","category":"page"},{"location":"entropy/#Table-of-differential-entropy-estimators","page":"Entropy","title":"Table of differential entropy estimators","text":"","category":"section"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"The following estimators are differential entropy estimators, and can also be used with entropy.","category":"page"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"Each DifferentialEntropyEstimators uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, Kraskov estimates the Shannon entropy.","category":"page"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"Estimator Principle Input data Shannon Renyi Tsallis Kaniadakis Curado StretchedExponential\nKozachenkoLeonenko Nearest neighbors Dataset âœ“ x x x x x\nKraskov Nearest neighbors Dataset âœ“ x x x x x\nZhu Nearest neighbors Dataset âœ“ x x x x x\nZhuSingh Nearest neighbors Dataset âœ“ x x x x x\nGao Nearest neighbors Dataset âœ“ x x x x x\nGoria Nearest neighbors Dataset âœ“ x x x x x\nLord Nearest neighbors Dataset âœ“ x x x x x\nVasicek Order statistics Vector âœ“ x x x x x\nEbrahimi Order statistics Vector âœ“ x x x x x\nCorrea Order statistics Vector âœ“ x x x x x\nAlizadehArghami Order statistics Vector âœ“ x x x x x","category":"page"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"DifferentialEntropyEstimator","category":"page"},{"location":"entropy/#ComplexityMeasures.DifferentialEntropyEstimator","page":"Entropy","title":"ComplexityMeasures.DifferentialEntropyEstimator","text":"DifferentialEntropyEstimator\nDiffEntropyEst # alias\n\nThe supertype of all differential entropy estimators. These estimators compute an entropy value in various ways that do not involve explicitly estimating a probability distribution.\n\nSee the table of differential entropy estimators in the docs for all differential entropy estimators.\n\nSee entropy for usage.\n\n\n\n\n\n","category":"type"},{"location":"entropy/","page":"Entropy","title":"Entropy","text":"Kraskov\nKozachenkoLeonenko\nZhu\nZhuSingh\nGao\nGoria\nLord\nVasicek\nAlizadehArghami\nEbrahimi\nCorrea","category":"page"},{"location":"entropy/#ComplexityMeasures.Kraskov","page":"Entropy","title":"ComplexityMeasures.Kraskov","text":"Kraskov <: DiffEntropyEst\nKraskov(; k::Int = 1, w::Int = 1, base = 2)\n\nThe Kraskov estimator computes the Shannon differential entropy of a multi-dimensional Dataset using the k-th nearest neighbor searches method from [Kraskov2004] at the given base.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Kraskov estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Kraskov2004]: Kraskov, A., StÃ¶gbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.KozachenkoLeonenko","page":"Entropy","title":"ComplexityMeasures.KozachenkoLeonenko","text":"KozachenkoLeonenko <: DiffEntropyEst\nKozachenkoLeonenko(; w::Int = 0, base = 2)\n\nThe KozachenkoLeonenko estimator computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nusing the nearest neighbor method from Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in CharzyÅ„ska and Gambin[CharzyÅ„ska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\nSee also: entropy, Kraskov, DifferentialEntropyEstimator.\n\n[CharzyÅ„ska2016]: CharzyÅ„ska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. EntropyDefinition, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Zhu","page":"Entropy","title":"ComplexityMeasures.Zhu","text":"Zhu <: DiffEntropyEst\nZhu(; k = 1, w = 0, base = 2)\n\nThe Zhu estimator (Zhu et al., 2015)[Zhu2015] is an extension to KozachenkoLeonenko, and computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Zhu estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby approximating densities within hyperrectangles surrounding each point xáµ¢ âˆˆ x using using k nearest neighbor searches. w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin JeannÃ¨s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.ZhuSingh","page":"Entropy","title":"ComplexityMeasures.ZhuSingh","text":"ZhuSingh <: DiffEntropyEst\nZhuSingh(; k = 1, w = 0, base = 2)\n\nThe ZhuSingh estimator (Zhu et al., 2015)[Zhu2015] computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. ZhuSingh estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point xáµ¢ âˆˆ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin JeannÃ¨s, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Gao","page":"Entropy","title":"ComplexityMeasures.Gao","text":"Gao <: DifferentialEntropyEstimator\nGao(; k = 1, w = 0, base = 2, corrected = true)\n\nThe Gao estimator (Gao et al., 2015) computes the Shannon differential entropy, using a k-th nearest-neighbor approach based on Singh et al. (2003)[Singh2003].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nGao et al., 2015 give two variants of this estimator. If corrected == false, then the uncorrected version is used. If corrected == true, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\n[Gao2015]: Gao, S., Ver Steeg, G., & Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PMLR.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Goria","page":"Entropy","title":"ComplexityMeasures.Goria","text":"Goria <: DifferentialEntropyEstimator\nGoria(; k = 1, w = 0, base = 2)\n\nThe Goria estimator computes the Shannon differential entropy of a multi-dimensional Dataset in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Goria estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSpecifically, let bfn_1 bfn_2 ldots bfn_N be the distance of the samples bfx_1 bfx_2 ldots bfx_N  to their k-th nearest neighbors. Next, let the geometric mean of the distances be\n\nhatrho_k = left( prod_i=1^N right)^dfrac1N\n\nGoria et al. (2005)[Goria2005]'s estimate of Shannon differential entropy is then\n\nhatH = mhatrho_k + log(N - 1) - psi(k) + log c_1(m)\n\nwhere c_1(m) = dfrac2pi^fracm2m Gamma(m2) and psi is the digamma function.\n\n[Goria2005]: Goria, M. N., Leonenko, N. N., Mergel, V. V., & Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Vasicek","page":"Entropy","title":"ComplexityMeasures.Vasicek","text":"Vasicek <: DiffEntropyEst\nVasicek(; m::Int = 1, base = 2)\n\nThe Vasicek estimator computes the Shannon differential entropy (in the given base) of a timeseries using the method from Vasicek (1976)[Vasicek1976].\n\nThe Vasicek estimator belongs to a class of differential entropy estimators based on order statistics, of which Vasicek (1976) was the first. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Vasicek estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Vasicek Shannon differential entropy estimate is then\n\nhatH_V(barX m) =\ndfrac1n\nsum_i = 1^n log left dfracn2m (barX_(i+m) - barX_(i-m)) right\n\nUsage\n\nIn practice, choice of m influences how fast the entropy converges to the true value. For small value of m, convergence is slow, so we recommend to scale m according to the time series length n and use m >= n/100 (this is just a heuristic based on the tests written for this package).\n\n[Vasicek1976]: Vasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society: Series B (Methodological), 38(1), 54-59.\n\nSee also: entropy, Correa, AlizadehArghami, Ebrahimi, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.AlizadehArghami","page":"Entropy","title":"ComplexityMeasures.AlizadehArghami","text":"AlizadehArghami <: DiffEntropyEst\nAlizadehArghami(; m::Int = 1, base = 2)\n\nThe AlizadehArghamiestimator computes the Shannon differential entropy (in the given base) of a timeseries using the method from Alizadeh & Arghami (2010)[Alizadeh2010].\n\nThe AlizadehArghami estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. AlizadehArghami estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X:\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The AlizadehArghami Shannon differential entropy estimate is then the the Vasicek estimate hatH_V(barX m n), plus a correction factor\n\nhatH_A(barX m n) = hatH_V(barX m n) +\ndfrac2nleft(m log(2) right)\n\n[Alizadeh2010]: Alizadeh, N. H., & Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).\n\nSee also: entropy, Correa, Ebrahimi, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Ebrahimi","page":"Entropy","title":"ComplexityMeasures.Ebrahimi","text":"Ebrahimi <: DiffEntropyEst\nEbrahimi(; m::Int = 1, base = 2)\n\nThe Ebrahimi estimator computes the Shannon entropy (in the given base) of a timeseries using the method from Ebrahimi (1994)[Ebrahimi1994].\n\nThe Ebrahimi estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Ebrahimi estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Ebrahimi Shannon differential entropy estimate is then\n\nhatH_E(barX m) =\ndfrac1n sum_i = 1^n log\nleft dfracnc_i m (barX_(i+m) - barX_(i-m)) right\n\nwhere\n\nc_i =\nbegincases\n    1 + fraci - 1m  1 geq i geq m \n    2                     m + 1 geq i geq n - m \n    1 + fracn - im  n - m + 1 geq i geq n\nendcases\n\n[Ebrahimi1994]: Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n\nSee also: entropy, Correa, AlizadehArghami, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"entropy/#ComplexityMeasures.Correa","page":"Entropy","title":"ComplexityMeasures.Correa","text":"Correa <: DiffEntropyEst\nCorrea(; m::Int = 1, base = 2)\n\nThe Correa estimator computes the Shannon differential entropy (in the given `base) of a timeseries using the method from Correa (1995)[Correa1995].\n\nThe Correa estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Correa estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, Correa makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n), ensuring that end points are included. The Correa estimate of Shannon differential entropy is then\n\nH_C(barX m n) =\ndfrac1n sum_i = 1^n log\nleft dfrac sum_j=i-m^i+m(barX_(j) -\ntildeX_(i))(j - i)n sum_j=i-m^i+m (barX_(j) - tildeX_(i))^2\nright\n\nwhere\n\ntildeX_(i) = dfrac12m + 1 sum_j = i - m^i + m X_(j)\n\n[Correa1995]: Correa, J. C. (1995). A new estimator of entropy. Communications in Statistics-Theory and Methods, 24(10), 2439-2449.\n\nSee also: entropy, AlizadehArghami, Ebrahimi, Vasicek, DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"experimental/#[Experimental](@ref-experimental_methods)","page":"Experimental","title":"Experimental","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"Here we list implemented methods that do not yet appear in peer-reviewed journals, but are found, for example, in pre-print servers like arXiv.","category":"page"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"The API for these methods, and their return values, may change at any time until they appear as part of the public API. Use them wisely.","category":"page"},{"location":"experimental/#Predictive-asymmetry","page":"Experimental","title":"Predictive asymmetry","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"predictive_asymmetry","category":"page"},{"location":"experimental/#CausalityTools.predictive_asymmetry","page":"Experimental","title":"CausalityTools.predictive_asymmetry","text":"predictive_asymmetry(estimator::TransferEntropyEstimator, Î·s; s, t, [c],\n    dTf = 1, dT = 1, dS = 1, Ï„T = -1, Ï„S = -1, [dC = 1, Ï„C = -1],\n    normalize::Bool = false, f::Real = 1.0, base = 2) â†’ Vector{Float64}\n\nCompute the predictive asymmetry[Haaga2020] ð”¸(s â†’ t) for source time series s and target time series t over prediction lags Î·s, using the given estimator and embedding parameters dTf, dT, dS, Ï„T, Ï„S (see also EmbeddingTE)\n\nIf a conditional time series c is provided, compute ð”¸(s â†’ t | c). Then, dC and Ï„C controls the embedding dimension and embedding lag for the conditional variable.\n\nReturns\n\nReturns a vector containing the predictive asymmetry for each value of Î·s.\n\nNormalization (hypothesis test)\n\nIf normalize == true (the default), then compute the normalized predictive asymmetry ð’œ. In this case, for each eta in Î·s, compute ð’œ(Î·) by normalizing ð”¸(Î·) to some fraction f of the mean transfer entropy over prediction lags -eta  eta (exluding lag 0). Haaga et al. (2020)[Haaga2020] uses a normalization with f=1.0 as a built-in hypothesis test, avoiding more computationally costly surrogate testing.\n\nEstimators\n\nAny estimator that works for transferentropy will also work with predictive_asymmetry. Check the online documentation for compatiable estimators.\n\nExamples\n\nusing CausalityTools\n# Some example time series\nx, y = rand(100), rand(100)\n# ð”¸(x â†’ y) over prediction lags 1:5\nð”¸reg  = predictive_asymmetry(x, y, VisitationFrequency(RectangularBinning(5)), 1:5)\n\ninfo: Experimental!\nThis is a method that does not yet appear in a peer-reviewed scientific journal. Feel free to use, but consider it experimental for now. It will reappear in a 2.X release in new form once published in a peer-reviewed journal.\n\n[Haaga2020]: Haaga, Kristian AgasÃ¸ster, David Diego, Jo Brendryen, and Bjarte Hannisdal. \"A simple test for causality in complex systems.\" arXiv preprint arXiv:2005.01860 (2020).\n\n\n\n\n\n","category":"function"},{"location":"experimental/#Automated-embedding-for-transfer-entropy","page":"Experimental","title":"Automated embedding for transfer entropy","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"bbnue","category":"page"},{"location":"cross_mappings/#cross_mapping_api","page":"Cross mappings","title":"Cross mapping","text":"","category":"section"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Several cross mapping methods have emerged in the literature Following Sugihara et al. (2012)'s paper on the convergent cross mapping. In CausalityTools.jl, we provide a unified interface for using these cross mapping methods. We indicate the different types of cross mappings by passing an CrossmapMeasure instance as the first argument to crossmap or predict.","category":"page"},{"location":"cross_mappings/#API","page":"Cross mappings","title":"API","text":"","category":"section"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"The cross mapping API consists of the following functions.","category":"page"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"predict\ncrossmap","category":"page"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"These functions can dispatch on a CrossmapMeasure, and we currently implement","category":"page"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"ConvergentCrossMapping.\nPairwiseAsymmetricEmbedding.","category":"page"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"crossmap\npredict","category":"page"},{"location":"cross_mappings/#CausalityTools.crossmap","page":"Cross mappings","title":"CausalityTools.crossmap","text":"crossmap(measure::CrossmapMeasure, tÌ„::AbstractVector, SÌ„::AbstractDataset) â†’ Ï\ncrossmap(measure::CrossmapMeasure, target::AbstractVector, source::AbstractVector) â†’ Ï\n\nCompute the cross map estimates between time-aligned time series tÌ„ and source embedding SÌ„, or between raw time series t and s.\n\nThis is just a wrapper around predict that simply returns the correspondence measure between the source and the target.\n\n\n\n\n\n","category":"function"},{"location":"cross_mappings/#CausalityTools.predict","page":"Cross mappings","title":"CausalityTools.predict","text":"predict(measure::CrossmapMeasure, tÌ„::AbstractVector, SÌ„::AbstractDataset) â†’ tÌ‚â‚›\npredict(measure::CrossmapMeasure, target::AbstractVector, source::AbstractVector) â†’ tÌ‚â‚›, tÌ„, Ï\n\nPerform point-wise cross mappings between source embeddings and target time series according to the algorithm specified by the given cross-map measure (e.g. ConvergentCrossMapping or PairwiseAsymmetricInference).\n\nFirst method: Returns a vector of predictions tÌ‚â‚› (tÌ‚â‚› := \"predictions of tÌ„ based   on source embedding SÌ„\"), where tÌ‚â‚›[i] is the prediction for tÌ„[i]. It assumes   pre-embedded data which have been correctly time-aligned using a joint embedding   (see embed), i.e. such that tÌ„[i] and SÌ„[i] correspond to the same time   index.\nSecond method: Jointly embeds the target and source time series (according to   measure) to obtain time-index aligned target timeseries tÌ„ and source embedding   SÌ„ (which is now a Dataset).   Then calls predict(measure, tÌ„, SÌ„) (the first method), and returns both the   predictions tÌ‚â‚›, observations tÌ„ and their correspondence Ï according to measure.\n\nDescription\n\nFor each i âˆˆ {1, 2, â€¦, N} where N = length(t) == length(s), we make the prediction tÌ‚[i] (an estimate of t[i]) based on a linear combination of D + 1 other points in t, where the selection of points and weights for the linear combination are determined by the D+1 nearest neighbors of the point SÌ„[i]. The details of point selection and weights depend on measure.\n\nNote: Some CrossmapMeasures may define more general mapping procedures. If so, the algorithm is described in their docstring.\n\n\n\n\n\n","category":"function"},{"location":"cross_mappings/#Measures","page":"Cross mappings","title":"Measures","text":"","category":"section"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"CrossmapMeasure\nConvergentCrossMapping\nPairwiseAsymmetricInference","category":"page"},{"location":"cross_mappings/#CausalityTools.CrossmapMeasure","page":"Cross mappings","title":"CausalityTools.CrossmapMeasure","text":"The supertype for all cross-map measures\n\nCurrently implemented measures are:\n\nConvergentCrossMapping, or CCM for short.\nPairwiseAsymmetricInference, or PAI for short.\n\n\n\n\n\n","category":"type"},{"location":"cross_mappings/#CausalityTools.ConvergentCrossMapping","page":"Cross mappings","title":"CausalityTools.ConvergentCrossMapping","text":"ConvergentCrossMapping <: CrossmapMeasure\nConvergentCrossMapping(; d::Int = 2, Ï„::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe convergent cross mapping measure (Sugihara et al., 2012)[Sugihara2012]).\n\nSpecifies embedding dimension d, embedding lag Ï„ to be used, as described below, with predict or crossmap. The Theiler window w controls how many temporal neighbors are excluded during neighbor searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nLet S(i) be the source time series variable and T(i) be the target time series variable. This version produces regular embeddings with fixed dimension d and embedding lag Ï„ as follows:\n\n( S(i) S(i+tau) S(i+2tau) ldots S(i+(d-1)tau T(i))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D-1 variables, while the last (D-th) variable is to be predicted.\n\nWith this convention, Ï„ < 0 implies \"past/present values of source used to predict target\", and Ï„ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if Ï„ > 0 (embed_warn = false turns off warnings).\n\n[Sugihara2012]: Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.\n\n\n\n\n\n","category":"type"},{"location":"cross_mappings/#CausalityTools.PairwiseAsymmetricInference","page":"Cross mappings","title":"CausalityTools.PairwiseAsymmetricInference","text":"PairwiseAsymmetricInference <: CrossmapMeasure\nPairwiseAsymmetricInference(; d::Int = 2, Ï„::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe pairwise asymmetric inference (PAI) measure (McCracken & Weigel (2014)[McCracken2014]) is a version of ConvergentCrossMapping that searches for neighbors in mixed embeddings (i.e. both source and target variables included); otherwise, the algorithms are identical.\n\nSpecifies embedding dimension d, embedding lag Ï„ to be used, as described below, with predict or crossmap. The Theiler window w controls how many temporal neighbors are excluded during neighbor searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nThere are many possible ways of defining the embedding for PAI. Currently, we only implement the \"add one non-lagged source timeseries to an embedding of the target\" approach, which is used as an example in McCracken & Weigel's paper. Specifically: Let S(i) be the source time series variable and T(i) be the target time series variable. PairwiseAsymmetricInference produces regular embeddings with fixed dimension d and embedding lag Ï„ as follows:\n\n(S(i) T(i+(d-1)tau ldots T(i+2tau) T(i+tau) T(i)))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D variables, while the last variable is to be predicted.\n\nWith this convention, Ï„ < 0 implies \"past/present values of source used to predict target\", and Ï„ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if Ï„ > 0 (embed_warn = false turns off warnings).\n\n[McCracken2014]: McCracken, J. M., & Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E, 90(6), 062903.\n\n\n\n\n\n","category":"type"},{"location":"cross_mappings/#Estimators","page":"Cross mappings","title":"Estimators","text":"","category":"section"},{"location":"cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"CrossmapEstimator\nRandomVectors\nRandomSegment\nExpandingSegment","category":"page"},{"location":"cross_mappings/#CausalityTools.CrossmapEstimator","page":"Cross mappings","title":"CausalityTools.CrossmapEstimator","text":"CrossmapEstimator{LIBSIZES, RNG}\n\nA parametric supertype for all cross-map estimators, which are used with predict and crossmap.\n\nBecause the type of the library may differ between estimators, and because RNGs from different packages may be used, subtypes must implement the LIBSIZES and RNG type parameters.\n\nFor efficiency purposes, subtypes may contain mutable containers that can be re-used for ensemble analysis (see Ensemble).\n\nLibraries\n\nA cross-map estimator uses the concept of \"libraries\". A library is essentially just a reference to a set of points, and usually, a library refers to indices of points, not the actual points themselves.\n\nFor example, for timeseries, RandomVectors(libsizes = 50:25:100) produces three separate libraries, where the first contains 50 randomly selected time indices, the second contains 75 randomly selected time indices, and the third contains 100 randomly selected time indices. This of course assumes that all quantities involved can be indexed using the same time indices, meaning that the concept of \"library\" only makes sense after relevant quantities have been jointly embedded, so that they can be jointly indexed. For non-instantaneous prediction, the maximum possible library size shrinks with the magnitude of the index/time-offset for the prediction.\n\nFor spatial analyses (not yet implemented), indices could be more complex and involve multi-indices.\n\n\n\n\n\n","category":"type"},{"location":"cross_mappings/#CausalityTools.RandomVectors","page":"Cross mappings","title":"CausalityTools.RandomVectors","text":"RandomVectors <: CrossmapEstimator\nRandomVectors(; libsizes, replace = false, rng = Random.default_rng())\n\nCross-map over N different libraries, where N = length(libsizes), and the i-th library has cardinality k = libsizes[i]. Points within each library are randomly selected, independently of other libraries, and replace controls whether or not to sample with replacement. A user-specified rng may be specified for reproducibility.\n\nThis is method 3 from Luo et al. (2015)[Luo2015].\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou.\n\nSee also: CrossmapEstimator.\n\n\n\n\n\n","category":"type"},{"location":"cross_mappings/#CausalityTools.RandomSegment","page":"Cross mappings","title":"CausalityTools.RandomSegment","text":"RandomSegment <: CrossmapEstimator\nRandomSegment(; libsizes::Int, rng = Random.default_rng())\n\nIndicatates that cross mapping is performed on contiguous time series segments/windows of length L with a randomly selected starting point.\n\nThis is method 2 from Luo et al. (2015)[Luo2015].\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou.\n\n\n\n\n\n","category":"type"},{"location":"cross_mappings/#CausalityTools.ExpandingSegment","page":"Cross mappings","title":"CausalityTools.ExpandingSegment","text":"ExpandingSegment <: CrossmapEstimator\nExpandingSegment(; libsizes::Int, rng = Random.default_rng())\n\nIndicatates that cross mapping is performed on a contiguous time series segment/window, starting from the first available data point up to the Lth data point.\n\nIf used in an ensemble setting, the estimator is applied to time indices Lmin:step:Lmax of the joint embedding.\n\n\n\n\n\n","category":"type"},{"location":"examples/examples_independence/#examples_independence","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"examples/examples_independence/#[LocalPermutationTest](@ref)","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we'll create a three-variable scenario where X and Z are connected through Y, so that I(X Z  Y) = 0 and I(X Y  Z)  0. We'll test for conditional independence using Shannon conditional mutual information (CMIShannon). To estimate CMI, we'll use the Kraskov differential entropy estimator, which naively computes CMI as a sum of entropy terms without guaranteed bias cancellation.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\n\nX = randn(1000)\nY = X .+ randn(1000) .* 0.4\nZ = randn(1000) .+ Y\nx, y, z = Dataset.((X, Y, Z))\ntest = LocalPermutationTest(CMIShannon(base = 2), Kraskov(k = 10), nsurr = 30)\ntest_result = independence(test, x, y, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We expect there to be a detectable influence from X to Y, if we condition on Z or not, because Z doesn't influence neither X nor Y. The null hypothesis is that the first two variables are conditionally independent given the third, which we reject with a very low p-value. Hence, we accept the alternative hypothesis that the first two variables X and Y. are conditionally dependent given Z.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test_result = independence(test, x, z, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we cannot reject the null hypothesis that X and Z are conditionally independent given Y, because Y is the variable that transmits information from X to Z.","category":"page"},{"location":"examples/examples_independence/#[SurrogateTest](@ref)","page":"Independence testing","title":"SurrogateTest","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"To demonstrate the SurrogateTest test, we use the transfer entropy measure, which accepts either two input timeseries, or three timeseries when computing the partial/conditional transfer entropy.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nsys = logistic2_unidir(c_xy = 0.5) # x affects y, but not the other way around.\nx, y = columns(trajectory(sys, 1000, Ttr = 10000))\n\ntest = SurrogateTest(TEShannon(), KSG1(k = 4))\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we can reject the null hypothesis that the future of y is independent of  x, because x does actually influence y. This doesn't change if we compute  partial transfer entropy with respect to some random extra time series, because it doesn't influence either variables.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, x, y, rand(length(x)))","category":"page"},{"location":"examples/examples_mutualinfo/#examples_mutualinfo","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(discrete):-synthetic-system","page":"Mutual information","title":"MIShannon (discrete): synthetic system","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In this example we generate realizations of two different systems where we know the strength of coupling between the variables. Our aim is to compute Shannon mutual information I^S(X Y) (MIShannon) between time series of each variable and assess how the magnitude of I^S(X Y) changes as we change the strength of coupling between X and Y.","category":"page"},{"location":"examples/examples_mutualinfo/#Defining-the-systems","page":"Mutual information","title":"Defining the systems","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here we implement two of the example systems that come with the CausalityTools.jl:","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"A stochastic system consisting of two unidirectionally coupled first-order autoregressive processes (ar1_unidir)\nA deterministic, chaotic system consisting of two unidirectionally coupled logistic maps (logistic2_unidir)","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"We use the default input parameter values (see ar1_unidir and logistic2_unidir for details) and below we toggle only the random initial conditions and the coupling strength parameter c_xy. For each value of c_xy we generate 1,000 unique realizations of the system and obtain 500-point time series of the coupled variables.","category":"page"},{"location":"examples/examples_mutualinfo/#Estimating-mutual-information","page":"Mutual information","title":"Estimating mutual information","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here we use the binning-based ValueHistogram estimator. We summarize the distribution of I(X Y) values across all realizations using the median and quantiles encompassing 95 % of the values.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Statistics\nusing CairoMakie\n\n# Span a range of x-y coupling strengths\nc = 0.0:0.1:1.0\n\n# Number of observations in each time series\nnpts = 500\n\n# Number of unique realizations of each system\nn_realizations = 1000\n\n# Get MI for multiple realizations of two systems, \n# saving three quantiles for each c value\nmi = zeros(length(c), 3, 2)\n\n# Define an estimator for MI\nb = RectangularBinning(4)\nestimator = VisitationFrequency(b)\n\nfor i in 1 : length(c)\n    \n    tmp = zeros(n_realizations, 2)\n    \n    for k in 1 : n_realizations\n        \n        # Obtain time series realizations of the two 2D systems \n        # for a given coupling strength and random initial conditions\n        lmap = trajectory(logistic2_unidir(uâ‚€ = rand(2), c_xy = c[i]), npts - 1, Ttr = 1000)\n        ar1 = trajectory(ar1_unidir(uâ‚€ = rand(2), c_xy = c[i]), npts - 1)\n        \n        # Compute the MI between the two coupled components of each system\n        tmp[k, 1] = mutualinfo(MIShannon(), estimator, lmap[:, 1], lmap[:, 2])\n        tmp[k, 2] = mutualinfo(MIShannon(), estimator, ar1[:, 1], ar1[:, 2])\n    end\n    \n    # Compute lower, middle, and upper quantiles of MI for each coupling strength\n    mi[i, :, 1] = quantile(tmp[:, 1], [0.025, 0.5, 0.975])\n    mi[i, :, 2] = quantile(tmp[:, 2], [0.025, 0.5, 0.975])\nend\n\n# Plot distribution of MI values as a function of coupling strength for both systems\nfig =with_theme(theme_minimal()) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"Coupling strength\", ylabel = \"Mutual information\")\n    band!(ax, c, mi[:, 1, 1], mi[:, 3, 1], color = (:black, 0.3))\n    lines!(ax, c, mi[:, 2, 1], label = \"2D chaotic logistic maps\", color = :black)\n    band!(ax, c, mi[:, 1, 2], mi[:, 3, 2], color = (:red, 0.3))\n    lines!(ax, c, mi[:, 2, 2],  label = \"2D order-1 autoregressive\", color = :red)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"As expected, I(X Y) increases with coupling strength in a system-specific manner.","category":"page"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(differential):-Reproducing-Kraskov-et-al.-(2004)","page":"Mutual information","title":"MIShannon (differential): Reproducing Kraskov et al. (2004)","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here, we'll reproduce Figure 4 from Kraskov et al. (2004)'s seminal paper on the nearest-neighbor based mutual information estimator. We'll estimate the mutual information between marginals of a bivariate Gaussian for a fixed time series length of 2000, varying the number of neighbors. Note: in the original paper, they show multiple curves corresponding to different time series length. We only show two single curves: one for the KSG1 estimator and one for the KSG2 estimator.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing LinearAlgebra: det\nusing Distributions: MvNormal\nusing StateSpaceSets: Dataset\nusing CairoMakie\nusing Statistics\n\nN = 2000\nc = 0.9\nÎ£ = [1 c; c 1]\nN2 = MvNormal([0, 0], Î£)\nmitrue = -0.5*log(det(Î£)) # in nats\nks = [2; 5; 7; 10:10:70] .* 2\n\nnreps = 30\nmis_ksg1 = zeros(nreps, length(ks))\nmis_ksg2 = zeros(nreps, length(ks))\nfor i = 1:nreps\n    D2 = Dataset([rand(N2) for i = 1:N])\n    X = D2[:, 1] |> Dataset\n    Y = D2[:, 2] |> Dataset\n    measure = MIShannon(; base = â„¯)\n    mis_ksg1[i, :] = map(k -> mutualinfo(measure, KSG1(; k), X, Y), ks)\n    mis_ksg2[i, :] = map(k -> mutualinfo(measure, KSG2(; k), X, Y), ks)\nend\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"k / N\", ylabel = \"Mutual infomation (nats)\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg1, dims = 1) |> vec, label = \"KSG1\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg2, dims = 1) |> vec, label = \"KSG2\")\nhlines!(ax, [mitrue], color = :black, linewidth = 3, label = \"I (true)\")\naxislegend()\nfig","category":"page"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(differential):-estimator-comparison","page":"Mutual information","title":"MIShannon (differential): estimator comparison","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Let's compare the performance of a subset of the implemented mutual information estimators. We'll use example data from Lord et al., where the analytical mutual information is known.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing LinearAlgebra: det\nusing StateSpaceSets: Dataset\nusing Distributions: MvNormal\nusing LaTeXStrings\nusing CairoMakie\n\n# adapted from https://juliadatascience.io/makie_colors\nfunction new_cycle_theme()\n    # https://nanx.me/ggsci/reference/pal_locuszoom.html\n    my_colors = [\"#D43F3AFF\", \"#EEA236FF\", \"#5CB85CFF\", \"#46B8DAFF\",\n        \"#357EBDFF\", \"#9632B8FF\", \"#B8B8B8FF\"]\n    cycle = Cycle([:color, :linestyle, :marker], covary=true) # alltogether\n    my_markers = [:circle, :rect, :utriangle, :dtriangle, :diamond,\n        :pentagon, :cross, :xcross]\n    my_linestyle = [nothing, :dash, :dot, :dashdot, :dashdotdot]\n    return Theme(\n        fontsize = 22, font=\"CMU Serif\",\n        colormap = :linear_bmy_10_95_c78_n256,\n        palette = (\n            color = my_colors, \n            marker = my_markers, \n            linestyle = my_linestyle,\n        ),\n        Axis = (\n            backgroundcolor= (:white, 0.2), \n            xgridstyle = :dash, \n            ygridstyle = :dash\n        ),\n        Lines = (\n            cycle= cycle,\n        ), \n        ScatterLines = (\n            cycle = cycle,\n        ),\n        Scatter = (\n            cycle = cycle,\n        ),\n        Legend = (\n            bgcolor = (:grey, 0.05), \n            framecolor = (:white, 0.2),\n            labelsize = 13,\n        )\n    )\nend\n\nrun(est; f::Function, # function that generates data\n        base::Real = â„¯, \n        nreps::Int = 10, \n        Î±s = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], \n        n::Int = 1000) =\n    map(Î± -> mutualinfo(MIShannon(; base), est, f(Î±, n)...), Î±s)\n\nfunction compute_results(f::Function; estimators, k = 5, k_lord = 20,\n        n = 1000, base = â„¯, nreps = 10,\n        as = 7:-1:0,\n        Î±s = [1/10^(a) for a in as])\n    \n    is = [zeros(length(Î±s)) for est in estimators]\n    for (k, est) in enumerate(estimators)\n        tmp = zeros(length(Î±s))\n        for i = 1:nreps\n            tmp .+= run(est; f = f, Î±s, base, n)\n        end\n        is[k] .= tmp ./ nreps\n    end\n\n    return is\nend\n\nfunction plot_results(f::Function, ftrue::Function; \n        base, estimators, k_lord, k, \n        as = 7:-1:0, Î±s = [1/10^(a) for a in as], kwargs...\n    )\n    is = compute_results(f; \n        base, estimators, k_lord, k, as, Î±s, kwargs...)\n    itrue = [ftrue(Î±; base) for Î± in Î±s]\n\n    xmin, xmax = minimum(Î±s), maximum(Î±s)\n    \n    ymin = floor(Int, min(minimum(itrue), minimum(Iterators.flatten(is))))\n    ymax = ceil(Int, max(maximum(itrue), maximum(Iterators.flatten(is))))\n    f = Figure()\n    ax = Axis(f[1, 1],\n        xlabel = \"Î±\", ylabel = \"I (nats)\",\n        xscale = log10, aspect = 1,\n        xticks = (Î±s, [latexstring(\"10^{$(-a)}\") for a in as]),\n        yticks = (ymin:ymax)\n        )\n    xlims!(ax, (1/10^first(as), 1/10^last(as)))\n    ylims!(ax, (ymin, ymax))\n    lines!(ax, Î±s, itrue, \n        label = \"I (true)\", linewidth = 4, color = :black)\n    for (i, est) in enumerate(estimators)\n        es = string(typeof(est).name.name)\n        lbl = occursin(\"Lord\", es) ? \"$es (k = $k_lord)\" : \"$es (k = $k)\"\n        scatter!(ax, Î±s, is[i], label = lbl)\n        lines!(ax, Î±s, is[i])\n\n    end\n    axislegend()\n    return f\nend\n\nset_theme!(new_cycle_theme())\nk_lord = 20\nk = 5\nbase = â„¯\n\nestimators = [\n    Kraskov(; k), \n    KozachenkoLeonenko(),\n    Zhu(; k), \n    ZhuSingh(; k),\n    Gao(; k),\n    Lord(; k = k_lord),\n    KSG1(; k), \n    KSG2(; k),\n    GaoOhViswanath(; k),\n    GaoKannanOhViswanath(; k),\n    GaussianMI(),\n]","category":"page"},{"location":"examples/examples_mutualinfo/#Family-2","page":"Mutual information","title":"Family 2","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"function family2(Î±, n::Int)\n    Î£ = [1 Î±; Î± 1]\n    N2 = MvNormal(zeros(2), Î£)\n    D2 = Dataset([rand(N2) for i = 1:n])\n    X = Dataset(D2[:, 1])\n    Y = Dataset(D2[:, 2])\n    return X, Y\nend\n\nfunction ifamily2(Î±; base = â„¯)\n    return (-0.5 * log(1 - Î±^2)) / log(base, â„¯)\nend\n\nÎ±s = 0.05:0.05:0.95\nestimators = estimators\nwith_theme(new_cycle_theme()) do\n    f = Figure();\n    ax = Axis(f[1, 1], xlabel = \"Î±\", ylabel = \"I (nats)\")\n    is_true = map(Î± -> ifamily2(Î±), Î±s)\n    is_est = map(est -> run(est; f = family2, Î±s, nreps = 20), estimators)\n    lines!(ax, Î±s, is_true, \n        label = \"I (true)\", color = :black, linewidth = 3)\n    for (i, est) in enumerate(estimators)\n        estname = typeof(est).name.name |> String\n        scatterlines!(ax, Î±s, is_est[i], label = estname)\n    end\n    axislegend(position = :lt)\n    return f\nend","category":"page"},{"location":"examples/examples_mutualinfo/#Family-1","page":"Mutual information","title":"Family 1","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In this system, samples are concentrated around the diagonal X = Y, and the strip of samples gets thinner as alpha to 0.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"function family1(Î±, n::Int)\n    x = rand(n)\n    v = rand(n)\n    y = x + Î± * v\n    return Dataset(x), Dataset(y)\nend\n\n# True mutual information values for these data\nfunction ifamily1(Î±; base = â„¯)\n    mi = -log(Î±) - Î± - log(2)\n    return mi / log(base, â„¯)\nend\n\nfig = plot_results(family1, ifamily1; \n    k_lord = k_lord, k = k, nreps = 10,\n    estimators = estimators,\n    base = base)","category":"page"},{"location":"examples/examples_mutualinfo/#Family-3","page":"Mutual information","title":"Family 3","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"In this system, we draw samples from a 4D Gaussian distribution distributed as specified in the ifamily3 function below. We let X be the two first variables, and Y be the two last variables.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"function ifamily3(Î±; base = â„¯)\n    Î£ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+Î±]\n    Î£x = Î£[1:2, 1:2]; Î£y = Î£[3:4, 3:4]\n    mi = 0.5*log(det(Î£x) * det(Î£y) / det(Î£))\n    return mi / log(base, â„¯)\nend\n\nfunction family3(Î±, n::Int)\n    Î£ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+Î±]\n    N4 = MvNormal(zeros(4), Î£)\n    D4 = Dataset([rand(N4) for i = 1:n])\n    X = D4[:, 1:2]\n    Y = D4[:, 3:4]\n    return X, Y\nend\n\nfig = plot_results(family3, ifamily3; \n    k_lord = k_lord, k = k, nreps = 10,\n    n = 2000,\n    estimators = estimators, base = base)","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"We see that the Lord estimator, which estimates local volume elements using a singular-value decomposition (SVD) of local neighborhoods, outperforms the other estimators by a large margin.","category":"page"},{"location":"examples/examples_mutualinfo/#[MIShannon](@ref)-(continuous/discrete):-estimator-comparison","page":"Mutual information","title":"MIShannon (continuous/discrete): estimator comparison","text":"","category":"section"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Most estimators suffer from significant bias when applied to discrete data. One possible resolution is to add a small amount of noise to discrete variables, so that the data becomes continuous in practice.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Instead of adding noise to your data, you can consider using an estimator that is specifically designed to deal with continuous-discrete mixture data. One example is the GaoKannanOhViswanath estimator.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"Here, we compare its performance to KSG1 on uniformly  distributed discrete multivariate data. The true mutual information is zero.","category":"page"},{"location":"examples/examples_mutualinfo/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Statistics\nusing StateSpaceSets: Dataset\nusing Statistics: mean\nusing CairoMakie\n\nfunction compare_ksg_gkov(;\n        k = 5,\n        base = 2,\n        nreps = 15,\n        Ls = [500:100:1000; 1500; 2000; 3000; 4000; 5000; 1000])\n\n    est_gkov = GaoKannanOhViswanath(; k)\n    est_ksg1 = KSG1(; k)\n\n    mis_ksg1_mix = zeros(nreps, length(Ls))\n    mis_ksg1_discrete = zeros(nreps, length(Ls))\n    mis_ksg1_cont = zeros(nreps, length(Ls))\n    mis_gkov_mix = zeros(nreps, length(Ls))\n    mis_gkov_discrete = zeros(nreps, length(Ls))\n    mis_gkov_cont = zeros(nreps, length(Ls))\n\n    for (j, L) in enumerate(Ls)\n        for i = 1:nreps\n            X = Dataset(float.(rand(1:8, L, 2)))\n            Y = Dataset(float.(rand(1:8, L, 2)))\n            Z = Dataset(rand(L, 2))\n            W = Dataset(rand(L, 2))\n            measure = MIShannon(; base = â„¯)\n            mis_ksg1_discrete[i, j] = mutualinfo(measure, est_ksg1, X, Y)\n            mis_gkov_discrete[i, j] = mutualinfo(measure, est_gkov, X, Y)\n            mis_ksg1_mix[i, j] = mutualinfo(measure, est_ksg1, X, Z)\n            mis_gkov_mix[i, j] = mutualinfo(measure, est_gkov, X, Z)\n            mis_ksg1_cont[i, j] = mutualinfo(measure, est_ksg1, Z, W)\n            mis_gkov_cont[i, j] = mutualinfo(measure, est_gkov, Z, W)\n        end\n    end\n    return mis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n        mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont\nend\n\nfig = Figure()\nax = Axis(fig[1, 1], \n    xlabel = \"Sample size\", \n    ylabel = \"Mutual information (bits)\")\nLs = [100; 200; 500; 1000; 2500; 5000; 10000]\nnreps = 5\nk = 3\nmis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n    mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont = \n    compare_ksg_gkov(; nreps, k, Ls)\n\nscatterlines!(ax, Ls, mean(mis_ksg1_mix, dims = 1) |> vec, \n    label = \"KSG1 (mixed)\", color = :black, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_ksg1_discrete, dims = 1) |> vec, \n    label = \"KSG1 (discrete)\", color = :black, \n    linestyle = :dash, marker = 'â–²')\nscatterlines!(ax, Ls, mean(mis_ksg1_cont, dims = 1) |> vec, \n    label = \"KSG1 (continuous)\", color = :black, \n    linestyle = :dot, marker = 'â—')\nscatterlines!(ax, Ls, mean(mis_gkov_mix, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (mixed)\", color = :red, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_gkov_discrete, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (discrete)\", color = :red, \n    linestyle = :dash, marker = 'â–²')\nscatterlines!(ax, Ls, mean(mis_gkov_cont, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (continuous)\", color = :red, \n    linestyle = :dot, marker = 'â—')\naxislegend(position = :rb)\nfig","category":"page"},{"location":"estimator_list/#Continuous/differential-entropy","page":"-","title":"Continuous/differential entropy","text":"","category":"section"},{"location":"estimator_list/","page":"-","title":"-","text":"Continuous (differential) entropies are defined by an integral, and are  related to but don't share all the same properties as discrete entropies. For example, Shannon differential entropy may even be negative for  some distributions.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Continuous entropies must be estimated using some form of \"plug-in\" estimator. For example, the Shannon differential entropy for a random variable X with support mathcalX is defined as","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"h(x) = mathbbE-log(f(X)) = -int_mathcalXf(x) log f(x) dx","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"There are several ways of estimating this integral from observed data, using what is called \"plug-in\" estimators. A common plug-in estimator is the resubstitution estimator","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"hatH(x) = -frac1Nsum_i=1^N log(hatp(X_i))","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"where hatp is estimated using the samples X_1 X_2 ldots X_N, is a plug-in estimator for Shannon differential entropy.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Subtypes of DifferentialEntropyEstimators use various forms of plug-in estimators to estimate differential entropy. For example, Kraskov estimates Shannon differential entropy. LeonenkoProzantoSavani, on the other hand, estimates both Shannon, Renyi and  Tsallis differential entropy.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"note: Plug-in estimators for differential entropy\nWhen using entropy with a ProbabilitiesEstimator, it is always the discrete entropy that is computed. When using entropy with an DifferentialEntropyEstimator, it is the differential entropy that is computed.","category":"page"},{"location":"estimator_list/#Generalized-entropies","page":"-","title":"Generalized entropies","text":"","category":"section"},{"location":"estimator_list/","page":"-","title":"-","text":"There exists a multitude of various entropy measures in the scientific literature, which appear either in discrete form, differential/continuous form, or both.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Discrete entropies are simply functions of sums over","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"probability mass functions (pmf). Hence, every ProbabilitiesEstimator yields a naive plug-in estimator for generalized entropy (i.e. just plug the probabilities into the relevant entropy formulas). No bias correction is currently performed for discrete estimators.","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"Continuous (differential) entropies are functions of","category":"page"},{"location":"estimator_list/","page":"-","title":"-","text":"probability density functions (pdf). Therefore, continuous entropy estimators approximate integrals instead of sums, as in the discrete case, and boils down to density function estimation. Every DifferentialEntropyEstimators has a slightly different way of estimating densities,  hence yielding slightly different differential entropy estimates.","category":"page"},{"location":"information_measures_api/#information_measures","page":"API & design","title":"API & design","text":"","category":"section"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"Information measures are build on probabilities/densities and entropies. We implement estimators of these quantities in ComplexityMeasures.jl. ComplexityMeasures.jl was built with modularity in mind, and provides a plethora of estimators of probabilities and generalized entropies, both discrete and continuous. These estimators are used frequently throughout CausalityTools.jl, relyin on the fact that any \"high-level\" information measure, in some way or another, can be expressed in terms of probabilities or entropies.","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"Information measures are computed in their discrete form by using   ProbabilitiesEstimator.\nInformation measures are computed in their differential/continuous   form by using DifferentialEntropyEstimators. Many measures also   have dedicated estimators (like MutualInformationEstimator for   mutualinfo, some of which are designed to compute continuous quantities.","category":"page"},{"location":"information_measures_api/#Naming-convention:-The-same-name-for-different-things","page":"API & design","title":"Naming convention: The same name for different things","text":"","category":"section"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"In contrast to generalized entropies, which each have one definition, it gets a bit more complicated when it comes to the \"higher-level\" measures we provide here.","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"Upon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use the same name for different concepts. For novices in the field of information theory, this can be very confusing. This package is designed to alleviate any confusion regarding the names of information theoretic quantities and their estimation.","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"We first consider the straight-forward case of multiple definitions. The Shannon mutual information (MI) has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral over a joint probability mass function (pmf), as a sum of three entropy terms, and as a Kullback-Leibler divergence between the joint pmf and the product of the marginal distributions. Since these are all equivalent, we only need once type ([MIShannon`](@ref)) to represent them.","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"But Shannon MI variant this is not the only type of mutual information! Taking as a starting point some generalized entropy definition like Tsallis, several authors have proposed variants of \"Tsallis mutual information\". Like Shannon MI, the Tsallis variant also has many definitions in the scientific literature, and not all of them are equivalent, even though they are referred to by the same name! Naming ambiguities like these are likely to cause confusion.","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"To alleviate any confusion, we group equivalent definitions of a information measure in a single type. Nonequivalent definitions are assigned separate types. Every measure starts with an abbrevation of the quantity it measures, followed by the name of the measure: CERenyi measures conditional RÃ©nyi entropy, and CEShannon measures conditional Shannon entropy. If there are multiple definitions for the same name, the author name is appended to the type: MITsallisFuruichi and MITsallisMartin are separate measures, because they are defined by nonequivalent mathematical formulas, whereas MIShannon has many equivalent definitions.","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"To estimate some information measure, an instance of the measure type (e.g. MIShannon) is combined with an estimator, which control how the quantity is computed, given som input data. The most basic estimators are ProbabilitiesEstimators for discrete measures, and DifferentialEntropyEstimators for continuous/differential measures. Some measures have dedicated estimators, that may be discrete, continuous or try to estimate a mixture of discrete and continuous data.","category":"page"},{"location":"information_measures_api/#Examples","page":"API & design","title":"Examples","text":"","category":"section"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"Here)'s an example of computing Shannon mutual information using various estimators on various kinds of data.","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"Other measure like condmutualinfo also have multiple estimation routes. To compute your favorite measure, simply find a suitable estimator in one of the overview tables, and apply it to some input data! Follow one of the examples for inspiration.","category":"page"},{"location":"information_measures_api/#Summary","page":"API & design","title":"Summary","text":"","category":"section"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"With this modular API, one could in principle estimate any information measure using any estimator. Although the current interface doesn't allow every combination of measure and estimator (and it's probably not theoretically meaningful to do so), you can already do a lot!","category":"page"},{"location":"information_measures_api/","page":"API & design","title":"API & design","text":"If you're interested in a deeper understanding, we try to give mathematical formulas and implementation details as best we can in the docstrings of the various measures and definitions.","category":"page"},{"location":"jdd/#Joint-distance-distribution","page":"Joint distance distribution","title":"Joint distance distribution","text":"","category":"section"},{"location":"jdd/","page":"Joint distance distribution","title":"Joint distance distribution","text":"jdd","category":"page"},{"location":"jdd/#CausalityTools.jdd","page":"Joint distance distribution","title":"CausalityTools.jdd","text":"jdd(source, target; distance_metric = SqEuclidean(),\n    B::Int = 10, D::Int = 2, Ï„::Int = 1) â†’ Vector{Float64}\n\nCompute the joint distance distribution (AmigÃ³ & Hirata, 2018[Amigo2018]) from source to target using the provided distance_metric, with B controlling the number of subintervals, D the embedding dimension and Ï„ the embedding lag.\n\nExample\n\nusing CausalityTools\nx, y = rand(1000), rand(1000)\n\njdd(x, y)\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.   Defaults to SqEuclidean().\nB: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances.\nD: Embedding dimension.\nÏ„: Embedding delay.\n\n[Amigo2018]: AmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\njdd(test::OneSampleTTest, source, target;\n    distance_metric = SqEuclidean(), B::Int = 10, D::Int = 2, Ï„::Int = 1,\n    Î¼0 = 0.0) â†’ OneSampleTTest\n\nPerform a one sample t-test to check that the joint distance distribution[Amigo] computed from source to target is biased towards positive values, using the null hypothesis that the mean of the distribution is Î¼0.\n\nThe interpretation of the t-test is that if we can reject the null, then the joint distance distribution is biased towards positive values, and then there exists an underlying coupling from source to target.\n\nExample\n\nusing CausalityTools, HypothesisTests\nx, y = rand(1000), rand(1000)\n\njdd(OneSampleTTest, x, y)\n\nwhich gives\n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         0.0\n    point estimate:          0.06361857324022721\n    95% confidence interval: (0.0185, 0.1087)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0082\n\nDetails:\n    number of observations:   20\n    t-statistic:              2.9517208721082873\n    degrees of freedom:       19\n    empirical standard error: 0.0215530451545668\n\nThe lower bound of the confidence interval for the mean of the joint distance distribution is 0.0185 at confidence level Î± = 0.05. The meaning that the test falsely detected causality from x to y between these two random time series.\n\nTo get the confidence intervals at confidence level Î±, run\n\nconfinf(jdd(OneSampleTTest, x, y), Î±).\n\nIf you just want the p-value at 95% confidence, run\n\npvalue(jdd(OneSampleTTest, x, y), tail = :right)\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.   Defaults to SqEuclidean().\nB: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances.\nD: Embedding dimension.\nÏ„: Embedding delay.\nÎ¼0: The hypothetical mean value of the joint distance distribution if there   is no coupling between x and y (default is Î¼0 = 0.0).\n\nReferences\n\n[Amigo]: AmigÃ³, JosÃ© M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Overview","title":"Overview","text":"(Image: CausalityTools.jl static logo)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"CausalityTools is a Julia package that provides algorithms for detecting dynamical influences and causal inference based on time series data, and other commonly used measures of dependence and association.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"info: Info\nYou are reading the development version of the documentation of CausalityTools.jl that will become version 2.0.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"info: Info\nThis package has been and is under heavy development. Don't hesitate to submit an issue if you find something that doesn't work or doesn't make sense, or if there's some functionality that you're missing. Pull requests are also very welcome!","category":"page"},{"location":"#Content","page":"Overview","title":"Content","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Probability mass function estimation, either in one-dimensional   (Probabilities  or multi-dimensional (ContingencyMatrix) form.\nGeneralized entropy estimation.\nInformation measures, such as   mutualinfo, condmutualinfo and transferentropy.   along with a plethora of estimators for computation of discrete and   continuous variants of these measures.\nA generic cross-map interface for causal inference methods   based on state space prediction methods. This includes   measures such as ConvergentCrossMapping and   PairwiseAsymmetricInference.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Other measures are listed in the menu.","category":"page"},{"location":"#Goals","page":"Overview","title":"Goals","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Causal inference, and quantification of association in general, is fundamental to most scientific disciplines. There exists a multitude of bivariate and multivariate association measures in the scientific literature. However, beyond the most basic measures, most methods aren't readily available for practical use. Most scientific papers don't provide code, which makes reproducing them difficult or impossible, without investing significant time and resources into deciphering and understanding the original papers to the point where an implementation is possible.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Our main goal with this package is to provide an easily extendible library of association measures, and an as-complete-as-possible set of their estimators. We also want to lower the entry-point to the field of causal inference and association quantification, by providing well-documented implementations of literature methods with runnable code examples.","category":"page"},{"location":"#API-design","page":"Overview","title":"API design","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Central to the API design is clarifying naming convention/confusion that inevitably arises when diving into the literature. Our API clearly distinguishes between methods that are conceptually the same but named differently in the literature due to differing estimation strategies, from methods that actually have different definitions.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"We have taken great care to make sure that estimators are reusable and modular. The power of this design really shines when computing things like conditional mutual information, which can be estimated in more than 20 different ways. Functions have the the general form","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"f([measure], estimator, input_data...)\n\n# Some examples\ncondmutualinfo(CMIShannon(base = 2), ValueHistogram(3), x, y, z)\ncondmutualinfo(CMIRenyiJizba(base = 2), KSG2(k = 5), x, y, z)\ncondmutualinfo(CMIRenyiJizba(base = 2), KSG2(k = 5), x, y, z)\ncondmutualinfo(CMIRenyiPoczos(base = 2), PoczosSchneiderCMI(k = 10), x, y, z)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"This modular design really shines when it comes to independence testing and causal graph inference. You can essentially test the performance of any independence measure with any estimator, as long as its implemented (and if it's not, submit a PR or issue!). We hope that this will both ease reproduction of existing literature results, and spawn new research. Please let us know if you use the package for something useful, or publish something based on it!","category":"page"},{"location":"#Input-data","page":"Overview","title":"Input data","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Input data for CausalityTools are given as:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Univariate timeseries, which are given as standard Julia Vectors.\nMultivariate timeseries, datasets, or state space sets, which are given as   Datasets. Many methods convert timeseries inputs to Dataset   for faster internal computations.\nCategorical data can be used with ContingencyMatrix to compute various   information theoretic measures and is represented using any iterable whose elements   can be any arbitrarily complex data type (as long as it's hashable), for example   Vector{String}, {Vector{Int}}, or Vector{Tuple{Int, String}}.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Dataset","category":"page"},{"location":"#StateSpaceSets.Dataset","page":"Overview","title":"StateSpaceSets.Dataset","text":"Dataset{D, T} <: AbstractDataset{D,T}\n\nA dedicated interface for datasets. It contains equally-sized datapoints of length D, represented by SVector{D, T}. These data are a standard Julia Vector{SVector}, and can be obtained with vec(dataset).\n\nWhen indexed with 1 index, a dataset is like a vector of datapoints. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables.\n\nDataset also supports most sensible operations like append!, push!, hcat, eachrow, among others, and when iterated over, it iterates over its contained points.\n\nDescription of indexing\n\nIn the following let i, j be integers,  typeof(data) <: AbstractDataset and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges, and for massive performance benefits make v2 an SVector{X, Int}).\n\ndata[i] == data[i, :] gives the ith datapoint (returns an SVector)\ndata[v1] == data[v1, :], returns a Dataset with the points in those indices.\ndata[:, j] gives the jth variable timeseries, as Vector\ndata[v1, v2], data[:, v2] returns a Dataset with the appropriate entries (first indices being \"time\"/point index, while second being variables)\ndata[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(dataset) or Dataset(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like Dataset(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"#Maintainers-and-contributors","page":"Overview","title":"Maintainers and contributors","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The CausalityTools.jl software is maintained by Kristian AgasÃ¸ster Haaga, who also curates and writes this documentation. Significant contributions to the API and documentation design has been made by George Datseris, which also co-authors ComplexityMeasures.jl, which we develop in tandem with this package.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"A complete list of contributors to this repo are listed on the main Github page. Some important contributions are:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Norbert Genera contributed bug reports and   investigations that led to subsequent improvements for the pairwise asymmetric   inference algorithm and an improved cross mapping API.\nDavid Diego's contributions were   invaluable in the initial stages of development. His MATLAB code provided the basis   for several transfer entropy methods and binning-related code.\nGeorge Datseris also ported KSG1 and KSG2 mutual   information estimators to Neighborhood.jl.\nBjarte Hannisdal provided tutorials for mutual information.\nTor Einar MÃ¸ller contributed to cross-mapping methods in initial stages of development.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Many individuals has contributed code to other packages in the JuliaDynamics ecosystem which we use here. Contributors are listed in the respective GitHub repos and webpages.","category":"page"},{"location":"#Related-packages","page":"Overview","title":"Related packages","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"TransferEntropy.jl previously   provided mutual infromation and transfer entropy estimators. These have been   re-implemented from scratch and moved here.","category":"page"},{"location":"examples/examples_cross_mappings/#examples_crossmappings","page":"Cross mappings","title":"Cross mappings","text":"","category":"section"},{"location":"examples/examples_cross_mappings/#Convergent-cross-mapping-(reproducing-Sugihara-et-al.,-2012)","page":"Cross mappings","title":"Convergent cross mapping (reproducing Sugihara et al., 2012)","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"note: Run blocks consecutively\nIf copying these examples and running them locally, make sure the relevant packages (given in the first block) are loaded first.","category":"page"},{"location":"examples/examples_cross_mappings/#Figures-3C-and-3D","page":"Cross mappings","title":"Figures 3C and 3D","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's reproduce figures 3C and 3D in Sugihara et al. (2012)[Sugihara2012], which introduced the ConvergentCrossMapping measure. Equations and parameters can be found in their supplementary material. Simulatenously, we also compute the PairwiseAsymmetricInference measure from McCracken & Weigel (2014)[McCracken2014], which is a related method, but uses a slightly different embedding.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"[Sugihara2012]: Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"[McCracken2014]: McCracken, J. M., & Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E, 90(6), 062903.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n\n# -----------------------------------------------------------------------------------------\n# Create 400-point long time series for Sugihara et al. (2012)'s example for figure 3.\n# -----------------------------------------------------------------------------------------\nfunction eom_logistic_sugi(u, p, t)\n    (; rx, ry, Î²xy, Î²yx) = p\n    (; x, y) = u\n\n    dx = x*(rx - rx*x - Î²xy*y)\n    dy = y*(ry - ry*y - Î²yx*x)\n    return SVector{2}(dx, dy)\nend\n\n# Î²xy := effect on x of y\n# Î²yx := effect on y of x\nfunction logistic_sugi(; u0 = rand(2), rx, ry, Î²xy, Î²yx)\n    p = @LArray [rx, ry, Î²xy, Î²yx] (:rx, :ry, :Î²xy, :Î²yx)\n    DiscreteDynamicalSystem(eom_logistic_sugi, u0, p)\nend\n\nsys_unidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, Î²xy = 0.00, Î²yx = 0.32);\nx, y = columns(trajectory(sys_unidir, 1000, Ttr = 10000));\n\n# -----------------------------------------------------------------------------------------\n# Cross map.\n# -----------------------------------------------------------------------------------------\nm_ccm = ConvergentCrossMapping(d = 2)\nm_pai = PairwiseAsymmetricInference(d = 2)\n# Make predictions xÌ‚y, i.e. predictions `xÌ‚` made from embedding of y (AND x, if PAI)\ntÌ‚ccm_xÌ‚y, tccm_xÌ‚y, Ïccm_xÌ‚y = predict(m_ccm, x, y)\ntÌ‚pai_xÌ‚y, tpai_xÌ‚y, Ïpai_xÌ‚y = predict(m_pai, x, y);\n# Make predictions yÌ‚x, i.e. predictions `yÌ‚` made from embedding of x (AND y, if PAI)\ntÌ‚ccm_yÌ‚x, tccm_yÌ‚x, Ïccm_yÌ‚x = predict(m_ccm, y, x)\ntÌ‚pai_yÌ‚x, tpai_yÌ‚x, Ïpai_yÌ‚x = predict(m_pai, y, x);\n\n# -----------------------------------------------------------------------------------------\n# Plot results\n# -----------------------------------------------------------------------------------------\nÏs = (Ïccm_xÌ‚y, Ïpai_xÌ‚y, Ïccm_yÌ‚x, Ïpai_yÌ‚x)\nsccm_xÌ‚y, spai_xÌ‚y, sccm_yÌ‚x, spai_yÌ‚x = (map(Ï -> (@sprintf \"%.3f\" Ï), Ïs)...,)\n\nÏs = (Ïccm_xÌ‚y, Ïpai_xÌ‚y, Ïccm_yÌ‚x, Ïpai_yÌ‚x)\nsccm_xÌ‚y, spai_xÌ‚y, sccm_yÌ‚x, spai_yÌ‚x = (map(Ï -> (@sprintf \"%.3f\" Ï), Ïs)...,)\n\nwith_theme(theme_minimal(),\n    markersize = 5) do\n    fig = Figure();\n    ax_yÌ‚x = Axis(fig[2,1], aspect = 1, xlabel = \"y(t) (observed)\", ylabel = \"yÌ‚(t) | x (predicted)\")\n    ax_xÌ‚y = Axis(fig[2,2], aspect = 1, xlabel = \"x(t) (observed)\", ylabel = \"xÌ‚(t) | y (predicted)\")\n    xlims!(ax_yÌ‚x, (0, 1)), ylims!(ax_yÌ‚x, (0, 1))\n    xlims!(ax_xÌ‚y, (0, 1)), ylims!(ax_xÌ‚y, (0, 1))\n    ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n    scatterlines!(ax_ts, x[1:300], label = \"x\")\n    scatterlines!(ax_ts, y[1:300], label = \"y\")\n    axislegend()\n    scatter!(ax_yÌ‚x, tccm_yÌ‚x, tÌ‚ccm_yÌ‚x, label = \"CCM (Ï = $sccm_yÌ‚x)\", color = :black)\n    scatter!(ax_yÌ‚x, tpai_yÌ‚x, tÌ‚pai_yÌ‚x, label = \"PAI (Ï = $spai_yÌ‚x)\", color = :red)\n    axislegend(ax_yÌ‚x, position = :lt)\n    scatter!(ax_xÌ‚y, tccm_xÌ‚y, tÌ‚ccm_xÌ‚y, label = \"CCM (Ï = $sccm_xÌ‚y)\", color = :black)\n    scatter!(ax_xÌ‚y, tpai_xÌ‚y, tÌ‚pai_xÌ‚y, label = \"PAI (Ï = $spai_xÌ‚y)\", color = :red)\n    axislegend(ax_xÌ‚y, position = :lt)\n    fig\nend","category":"page"},{"location":"examples/examples_cross_mappings/#Figure-3A","page":"Cross mappings","title":"Figure 3A","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's reproduce figure 3A too, focusing only on ConvergentCrossMapping this time. In this figure, they compute the cross mapping for libraries of increasing size, always starting at time index 1. This approach - which we here call the ExpandingSegment estimator - is one of many ways of estimating the correspondence between observed and predicted value.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"For this example, they use a bidirectional system with asymmetrical coupling strength.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n# Used in `reproduce_figure_3A_naive`, and `reproduce_figure_3A_ensemble` below.\nfunction add_to_fig!(fig_pos, libsizes, Ïs_xÌ‚y, Ïs_yÌ‚x; title = \"\", quantiles = false)\n    ax = Axis(fig_pos; title, aspect = 1,\n        xlabel = \"Library size\", ylabel = \"Correlation (Ï)\")\n    ylims!(ax, (-1, 1))\n    hlines!([0], linestyle = :dash, alpha = 0.5, color = :grey)\n    scatterlines!(libsizes, median.(Ïs_xÌ‚y), label = \"xÌ‚|y\", color = :blue)\n    scatterlines!(libsizes, median.(Ïs_yÌ‚x), label = \"yÌ‚|x\", color = :red)\n    if quantiles\n        band!(libsizes, quantile.(Ïs_xÌ‚y, 0.05), quantile.(Ïs_xÌ‚y, 0.95), color = (:blue, 0.5))\n        band!(libsizes, quantile.(Ïs_yÌ‚x, 0.05), quantile.(Ïs_yÌ‚x, 0.95), color = (:red, 0.5))\n    end\n    axislegend(ax, position = :rb)\nend\n\nfunction reproduce_figure_3A_naive(measure::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, Î²xy = 0.02, Î²yx = 0.32);\n    x, y = columns(trajectory(sys_bidir, 3100, Ttr = 10000));\n    libsizes = [20:2:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:3000]\n    est = ExpandingSegment(; libsizes);\n    Ïs_xÌ‚y = crossmap(measure, est, x, y)\n    Ïs_yÌ‚x = crossmap(measure, est, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, Ïs_xÌ‚y, Ïs_yÌ‚x; title = \"`ExpandingSegment`\")\n        fig\n    end\nend\n\nreproduce_figure_3A_naive(ConvergentCrossMapping(d = 3))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Hm. This looks a bit like the paper, but the curve is not smooth. We can do better!","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"It is not clear from the paper exactly what they plot in their Figure 3A, if they plot an average of some kind, or precisely what parameters and initial conditions they use. However, we can get a smoother plot by using a Ensemble. Combined with a CrossmapEstimator, it uses Monte Carlo resampling on subsets of the input data to compute an ensemble of Ïs that we here use to compute the median and 90-th percentile range for each library size.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"function reproduce_figure_3A_ensemble(measure::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.4, 0.2], rx = 3.8, ry = 3.5, Î²xy = 0.02, Î²yx = 0.1);\n    x, y = columns(trajectory(sys_bidir, 10000, Ttr = 10000));\n    # Note: our time series are 1000 points long. When embedding, some points are\n    # lost, so we must use slightly less points for the segments than \n    # there are points in the original time series.\n    libsizes = [20:5:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:3000]\n    # No point in doing more than one rep, because there data are always the same\n    # for `ExpandingSegment.`\n    ensemble_ev = Ensemble(measure, ExpandingSegment(; libsizes); nreps = 1)\n    ensemble_rs = Ensemble(measure, RandomSegment(; libsizes); nreps = 50)\n    ensemble_rv = Ensemble(measure, RandomVectors(; libsizes); nreps = 50)\n    Ïs_xÌ‚y_es = crossmap(ensemble_ev, x, y)\n    Ïs_yÌ‚x_es = crossmap(ensemble_ev, y, x)\n    Ïs_xÌ‚y_rs = crossmap(ensemble_rs, x, y)\n    Ïs_yÌ‚x_rs = crossmap(ensemble_rs, y, x)\n    Ïs_xÌ‚y_rv = crossmap(ensemble_rv, x, y)\n    Ïs_yÌ‚x_rv = crossmap(ensemble_rv, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, Ïs_xÌ‚y_es, Ïs_yÌ‚x_es; title = \"`ExpandingSegment`\", quantiles = false) # quantiles make no sense for `ExpandingSegment`\n        add_to_fig!(fig[1, 2], libsizes, Ïs_xÌ‚y_rs, Ïs_yÌ‚x_rs; title = \"`RandomSegment`\", quantiles = true)\n        add_to_fig!(fig[1, 3], libsizes, Ïs_xÌ‚y_rv, Ïs_yÌ‚x_rv; title = \"`RandomVector`\", quantiles = true)\n        fig\n    end\nend\n\nreproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, Ï„ = -1))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"With the RandomVectors estimator, the mean of our ensemble Ïs seem to look pretty much identical to Figure 3A in Sugihara et al. The RandomSegment estimator also performs pretty well, but since subsampled segments are contiguous, there are probably some autocorrelation effects at play.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"We can avoid the autocorrelation issue by tuning the w parameter of the ConvergentCrossMapping measure, which is the  Theiler window. Setting the Theiler window to w > 0, we can exclude neighbors of a query point p that are close to p in time, and thus deal with autocorrelation issues that way (the default w = 0 excludes only the point itself). Let's re-do the analysis with w = 5, just for fun.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"reproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, Ï„ = -1, w = 5))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"There wasn't really that much of a difference, since for the logistic map, the autocorrelation function flips sign for every lag increase. However, for examples from other systems, tuning w may be important.","category":"page"},{"location":"examples/examples_cross_mappings/#Figure-3B","page":"Cross mappings","title":"Figure 3B","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"What about figure 3B? Here they generate time series of length 400 for a range of values for both coupling parameters, and plot the dominant direction Delta = rho(hatx  y) - rho(haty  x).","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"In the paper, they use a 1000 different parameterizations for the logistic map parameters, but don't state what is summarized in the plot. For simplicity, we'll therefore just stick to rx = ry = 3.7, as in the examples above, and just loop over the coupling strengths in either direction.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"function reproduce_figure_3B()\n    Î²xys = 0.0:0.02:0.4\n    Î²yxs = 0.0:0.02:0.4\n    ÏxÌ‚ys = zeros(length(Î²xys), length(Î²yxs))\n    ÏyÌ‚xs = zeros(length(Î²xys), length(Î²yxs))\n\n    for (i, Î²xy) in enumerate(Î²xys)\n        for (j, Î²yx) in enumerate(Î²yxs)\n            sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.7, Î²xy, Î²yx);\n            # Generate 1000 points. Randomly select a 400-pt long segment.\n            x, y = columns(trajectory(sys_bidir, 1300, Ttr = 10000));\n            ensemble = Ensemble(CCM(d = 3, w = 5, Ï„ = -1), RandomVectors(libsizes = 400), nreps = 10)\n            ÏxÌ‚ys[i, j] = mean(crossmap(ensemble, x, y))\n            ÏyÌ‚xs[i, j] = mean(crossmap(ensemble, y, x))\n        end\n    end\n    Î” = ÏyÌ‚xs .- ÏxÌ‚ys\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure();\n        ax = Axis(fig[1, 1], xlabel = \"Î²xy\", ylabel = \"Î²yx\")\n        cont = contourf!(ax, Î”, levels = range(-1, 1, length = 10),\n            colormap = :curl)\n        ax.xticks = 1:length(Î²xys), string.([i % 2 == 0 ? Î²xys[i] : \"\" for i in 1:length(Î²xys)])\n        ax.yticks = 1:length(Î²yxs), string.([i % 2 == 0 ? Î²yxs[i] : \"\" for i in 1:length(Î²yxs)])\n        Colorbar(fig[1 ,2], cont, label = \"Î” (Ï(yÌ‚|x) - Ï(xÌ‚|y))\")\n        tightlimits!(ax)\n        fig\n    end\nend\n\nreproduce_figure_3B()","category":"page"},{"location":"examples/examples_cross_mappings/#Pairwise-asymmetric-inference-(reproducing-McCracken-and-Weigel,-2014)","page":"Cross mappings","title":"Pairwise asymmetric inference (reproducing McCracken & Weigel, 2014)","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's try to reproduce figure 8 from McCracken & Weigel (2014)'s[McCracken2014] paper on PairwiseAsymmetricInference (PAI). We'll start by defining the their example B (equations 6-7). This system consists of two variables X and Y, where X drives Y.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"After we have computed the PAI in both directions, we define a measure of directionality as the difference between PAI in the X to Y direction and in the Y to X direction, so that if X drives Y, then Delta  0.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\nusing Distributions: Normal\nusing Statistics: mean, std\n\nfunction eom_nonlinear_sindriver(dx, x, p, n)\n    a, b, c, t, Î”t = (p...,)\n    x, y = x[1], x[2]\n    ð’© = Normal(0, 1)\n    \n    dx[1] = sin(t)\n    dx[2] = a*x * (1 - b*x) + c* rand(ð’©)\n    p[end-1] += 1 # update t\n\n    return\nend\n\nfunction nonlinear_sindriver(;uâ‚€ = rand(2), a = 1.0, b = 1.0, c = 2.0, Î”t = 1)\n    DiscreteDynamicalSystem(eom_nonlinear_sindriver, uâ‚€, [a, b, c, 0, Î”t])\nend\n\nfunction reproduce_figure_8_mccraken(; \n        c = 2.0, Î”t = 0.2,\n        as = 0.25:0.25:5.0,\n        bs = 0.25:0.25:5.0)\n    # -----------------------------------------------------------------------------------------\n    # Generate many time series for many different values of the parameters `a` and `b`,\n    # and compute PAI. This will replicate the upper right panel of \n    # figure 8 in McCracken & Weigel (2014).\n    # -----------------------------------------------------------------------------------------\n    \n    measure = PairwiseAsymmetricInference(d = 3)\n\n    # Manually resample `nreps` length-`L` time series and use mean Ï(xÌ‚|XÌ„y) - Ï(yÌ‚|YÌ„x)\n    # for each parameter combination.\n    nreps = 50\n    L = 300 # length of timeseries\n    Î” = zeros(length(as), length(bs))\n    for (i, a) in enumerate(as)\n        for (j, b) in enumerate(bs)\n            s = nonlinear_sindriver(; a, b, c,  Î”t)\n            x, y = columns(trajectory(s, 1000, Ttr = 10000))\n            Î”reps = zeros(nreps)\n            for i = 1:nreps\n                # Ensure we're subsampling at the same time indices. \n                ind_start = rand(1:(1000-L))\n                r = ind_start:(ind_start + L)\n                Î”reps[i] = @views crossmap(measure, y[r], x[r]) - \n                    crossmap(measure, x[r], y[r])\n            end\n            Î”[i, j] = mean(Î”reps)\n        end\n    end\n\n    # -----------------------------------------------------------------------------------------\n    # An example time series for plotting.\n    # -----------------------------------------------------------------------------------------\n    sys = nonlinear_sindriver(; a = 1.0, b = 1.0, c, Î”t)\n    npts = 500\n    orbit = trajectory(sys, npts, Ttr = 10000)\n    x, y = columns(orbit)\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        \n        X = x[1:300]\n        Y = y[1:300]\n        fig = Figure();\n        ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n        scatterlines!(ax_ts, (X .- mean(X)) ./ std(X), label = \"x\")\n        scatterlines!(ax_ts, (Y .- mean(Y)) ./ std(Y), label = \"y\")\n        axislegend()\n\n        ax_hm = Axis(fig[2, 1:2], xlabel = \"a\", ylabel = \"b\")\n        ax_hm.yticks = (1:length(as), string.([i % 2 == 0 ? as[i] : \"\" for i = 1:length(as)]))\n        ax_hm.xticks = (1:length(bs), string.([i % 2 == 0 ? bs[i] : \"\" for i = 1:length(bs)]))\n        hm = heatmap!(ax_hm, Î”,  colormap = :viridis)\n        Colorbar(fig[2, 3], hm; label = \"Î”' = Ï(yÌ‚ | yx) - Ï(xÌ‚ | xy)\")\n        fig\n    end\nend\n\nreproduce_figure_8_mccraken()","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"As expected, Delta  0 for all parameter combinations, implying that X \"PAI drives\" Y.","category":"page"}]
}
