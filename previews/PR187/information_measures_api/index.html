<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API &amp; design · CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CausalityTools.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Information measures</span><ul><li><a class="tocitem" href="../probabilities/">Probability mass functions</a></li><li><a class="tocitem" href="../entropy/">Entropy</a></li><li><a class="tocitem" href="../entropy_conditional/">Conditional entropy</a></li><li><a class="tocitem" href="../mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../condmutualinfo/">Conditional mutual information</a></li><li><a class="tocitem" href="../transferentropy/">Transfer entropy</a></li></ul></li><li><a class="tocitem" href="../cross_mappings/">Cross mappings</a></li><li><a class="tocitem" href="../jdd/">Joint distance distribution</a></li><li><a class="tocitem" href="../experimental/">Experimental</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API &amp; design</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API &amp; design</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/information_measures_api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="information_measures"><a class="docs-heading-anchor" href="#information_measures">API &amp; design</a><a id="information_measures-1"></a><a class="docs-heading-anchor-permalink" href="#information_measures" title="Permalink"></a></h1><p>Information measures are build on <a href="@ref probabilities_header">probabilities</a>/densities and <a href="@ref entropy_header">entropies</a>. We implement estimators of these quantities in <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl">ComplexityMeasures.jl</a>. ComplexityMeasures.jl was built with modularity in mind, and provides a plethora of estimators of probabilities and generalized entropies, both discrete and continuous. These estimators are used frequently throughout CausalityTools.jl, relyin on the fact that any &quot;high-level&quot; information measure, in some way or another, can be expressed in terms of probabilities or entropies.</p><ul><li>Information measures are computed in their discrete form by using   <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>.</li><li>Information measures are computed in their differential/continuous   form by using <a href="../entropy/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s. Many measures also   have dedicated estimators (like <a href="../mutualinfo/#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> for   <a href="../mutualinfo/#CausalityTools.mutualinfo-Tuple{MutualInformationEstimator, Any, Any}"><code>mutualinfo</code></a>, some of which are designed to compute continuous quantities.</li></ul><h2 id="Naming-convention:-The-same-name-for-different-things"><a class="docs-heading-anchor" href="#Naming-convention:-The-same-name-for-different-things">Naming convention: The same name for different things</a><a id="Naming-convention:-The-same-name-for-different-things-1"></a><a class="docs-heading-anchor-permalink" href="#Naming-convention:-The-same-name-for-different-things" title="Permalink"></a></h2><p>In contrast to generalized entropies, which each have <em>one</em> definition, it gets a bit more complicated when it comes to the &quot;higher-level&quot; measures we provide here.</p><p>Upon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use <em>the same name for different concepts</em>. For novices in the field of information theory, this can be very confusing. This package is designed to alleviate any confusion regarding the names of information theoretic quantities and their estimation.</p><p>We first consider the straight-forward case of multiple definitions. The Shannon mutual information (MI) has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral over a joint probability mass function (pmf), as a sum of three entropy terms, and as a Kullback-Leibler divergence between the joint pmf and the product of the marginal distributions. Since these are all equivalent, we only need once type (<code>[</code>MIShannon`](@ref)) to represent them.</p><p>But Shannon MI variant this is not the only type of mutual information! Taking as a starting point some generalized entropy definition like <a href="../entropy/#ComplexityMeasures.Tsallis"><code>Tsallis</code></a>, several authors have proposed variants of &quot;Tsallis mutual information&quot;. Like Shannon MI, the Tsallis variant also has many definitions in the scientific literature, and <em>not all of them are equivalent, even though they are referred to by the same name</em>! Naming ambiguities like these are likely to cause confusion.</p><p>To alleviate any confusion, we group <em>equivalent</em> definitions of a information measure in a single type. Nonequivalent definitions are assigned separate types. Every measure starts with an abbrevation of the quantity it measures, followed by the name of the measure: <a href="@ref"><code>CERenyi</code></a> measures conditional Rényi entropy, and <a href="../entropy_conditional/#CausalityTools.CEShannon"><code>CEShannon</code></a> measures conditional Shannon entropy. If there are multiple definitions for the same name, the author name is appended to the type: <a href="../mutualinfo/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a> and <a href="../mutualinfo/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a> are separate measures, because they are defined by <em>nonequivalent</em> mathematical formulas, whereas <a href="../mutualinfo/#CausalityTools.MIShannon"><code>MIShannon</code></a> has many <em>equivalent</em> definitions.</p><p>To estimate some information measure, an instance of the measure type (e.g. <a href="../mutualinfo/#CausalityTools.MIShannon"><code>MIShannon</code></a>) is combined with an <em>estimator</em>, which control <em>how</em> the quantity is computed, given som input data. The most basic estimators are <a href="../probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>s for discrete measures, and <a href="../entropy/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s for continuous/differential measures. Some measures have dedicated estimators, that may be discrete, continuous or try to estimate a mixture of discrete and continuous data.</p><h3 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h3><p><a href="../mutualinfo/#example_mi_quickstart">Here</a>)&#39;s an example of computing Shannon mutual information using various estimators on various kinds of data.</p><p>Other measure like <a href="../condmutualinfo/#CausalityTools.condmutualinfo-Tuple{ConditionalMutualInformationEstimator, Any, Any, Any}"><code>condmutualinfo</code></a> also have multiple estimation routes. To compute your favorite measure, simply find a suitable estimator in one of the overview tables, and apply it to some input data! Follow one of the examples for inspiration.</p><h3 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h3><p>With this modular API, one could in principle estimate <em>any</em> information measure using <em>any</em> estimator. Although the current interface doesn&#39;t allow <em>every</em> combination of measure and estimator (and it&#39;s probably not theoretically meaningful to do so), you can already do a lot!</p><p>If you&#39;re interested in a deeper understanding, we try to give mathematical formulas and implementation details as best we can in the docstrings of the various measures and definitions.</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 19 January 2023 00:38">Thursday 19 January 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
