<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Estimators · CausalityTools.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CausalityTools.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../surrogate/">Surrogate data</a></li><li><span class="tocitem">Distance based</span><ul><li><a class="tocitem" href="../joint_distance_distribution/">Joint distance distribution</a></li><li><a class="tocitem" href="../s_measure/">S-measure</a></li><li><a class="tocitem" href="../cross_mapping/">Cross mapping</a></li></ul></li><li><span class="tocitem">Information/entropy based</span><ul><li><a class="tocitem" href="../mutualinfo/">Mutual information</a></li><li><a class="tocitem" href="../TransferEntropy/">Transfer entropy</a></li><li><a class="tocitem" href="../predictive_asymmetry/">Predictive asymmetry</a></li><li><a class="tocitem" href="../generalized_entropy/">Generalized entropy</a></li><li class="is-active"><a class="tocitem" href>Estimators</a><ul class="internal"><li><a class="tocitem" href="#Binning-based"><span>Binning based</span></a></li><li><a class="tocitem" href="#Kernel-density-based"><span>Kernel density based</span></a></li><li><a class="tocitem" href="#Nearest-neighbor-based"><span>Nearest neighbor based</span></a></li><li><a class="tocitem" href="#symbolic"><span>Permutation based</span></a></li><li><a class="tocitem" href="#Wavelet-based"><span>Wavelet based</span></a></li><li><a class="tocitem" href="#Hilbert"><span>Hilbert</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../example_systems/">Example systems</a></li><li><span class="tocitem">Utilities</span><ul><li><a class="tocitem" href="../invariant_measure/">Invariant measures and transfer operators</a></li><li><a class="tocitem" href="../dataset/">Multivariate <code>Dataset</code>s</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Information/entropy based</a></li><li class="is-active"><a href>Estimators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Estimators</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/info_estimators.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="estimators"><a class="docs-heading-anchor" href="#estimators">Estimators</a><a id="estimators-1"></a><a class="docs-heading-anchor-permalink" href="#estimators" title="Permalink"></a></h1><p>Information theoretic causality measures in this package are calculated using entropy estimation. To do so, it uses estimators and <a href="../generalized_entropy/#Entropies.genentropy"><code>genentropy</code></a> from the <a href="https://github.com/JuliaDynamics/Entropies.jl">Entropies.jl</a> package. However, additional estimators are also available for some of the higher-level methods.</p><p>The following method-estimator combinations are possible.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: center"><a href="../generalized_entropy/#Entropies.genentropy"><code>genentropy</code></a></th><th style="text-align: center"><a href="../mutualinfo/#TransferEntropy.mutualinfo"><code>mutualinfo</code></a></th><th style="text-align: center"><a href="../TransferEntropy/#TransferEntropy.transferentropy"><code>transferentropy</code></a></th><th style="text-align: center"><a href="../predictive_asymmetry/#CausalityTools.PredictiveAsymmetry.predictive_asymmetry"><code>predictive_asymmetry</code></a></th></tr><tr><td style="text-align: left"><a href="#Entropies.VisitationFrequency"><code>VisitationFrequency</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: left"><a href="#Entropies.TransferOperator"><code>TransferOperator</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: left"><a href="#Entropies.NaiveKernel"><code>NaiveKernel</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: left"><a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: left"><a href="#symbolic"><code>SymbolicAmplitudeAwarePermutation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center"></td><td style="text-align: center"></td><td style="text-align: center"></td></tr><tr><td style="text-align: left"><a href="#symbolic"><code>SymbolicWeightedPermutation</code></a></td><td style="text-align: center">✓</td><td style="text-align: center"></td><td style="text-align: center"></td><td style="text-align: center"></td></tr><tr><td style="text-align: left"><a href="#Entropies.KozachenkoLeonenko"><code>KozachenkoLeonenko</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: left"><a href="#Entropies.Kraskov"><code>Kraskov</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: left"><a href="#TransferEntropy.Kraskov1"><code>Kraskov1</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center"></td><td style="text-align: center"></td></tr><tr><td style="text-align: left"><a href="#TransferEntropy.Kraskov2"><code>Kraskov2</code></a></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center"></td><td style="text-align: center"></td></tr><tr><td style="text-align: left"><a href="#TransferEntropy.Hilbert"><code>Hilbert</code></a></td><td style="text-align: center">✓</td><td style="text-align: center"></td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: left"><a href="#Entropies.TimeScaleMODWT"><code>TimeScaleMODWT</code></a></td><td style="text-align: center">✓</td><td style="text-align: center"></td><td style="text-align: center"></td><td style="text-align: center"></td></tr></table><h2 id="Binning-based"><a class="docs-heading-anchor" href="#Binning-based">Binning based</a><a id="Binning-based-1"></a><a class="docs-heading-anchor-permalink" href="#Binning-based" title="Permalink"></a></h2><h3 id="Visitation-frequency"><a class="docs-heading-anchor" href="#Visitation-frequency">Visitation frequency</a><a id="Visitation-frequency-1"></a><a class="docs-heading-anchor-permalink" href="#Visitation-frequency" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.VisitationFrequency" href="#Entropies.VisitationFrequency"><code>Entropies.VisitationFrequency</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">VisitationFrequency(r::RectangularBinning) &lt;: BinningProbabilitiesEstimator</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by the binning scheme <code>r</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Construct boxes by dividing each coordinate axis into 5 equal-length chunks.
b = RectangularBinning(5)

# A probabilities estimator that, when applied a dataset, computes visitation frequencies
# over the boxes of the binning
est = VisitationFrequency(b)</code></pre><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.RectangularBinning" href="#Entropies.RectangularBinning"><code>Entropies.RectangularBinning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RectangularBinning(ϵ) &lt;: RectangularBinningScheme</code></pre><p>Instructions for creating a rectangular box partition using the binning scheme <code>ϵ</code>.  Binning instructions are deduced from the type of <code>ϵ</code>.</p><p>Rectangular binnings may be automatically adjusted to the data in which the <code>RectangularBinning</code>  is applied, as follows:</p><ol><li><p><code>ϵ::Int</code> divides each coordinate axis into <code>ϵ</code> equal-length intervals,   extending the upper bound 1/100th of a bin size to ensure all points are covered.</p></li><li><p><code>ϵ::Float64</code> divides each coordinate axis into intervals of fixed size <code>ϵ</code>, starting   from the axis minima until the data is completely covered by boxes.</p></li><li><p><code>ϵ::Vector{Int}</code> divides the i-th coordinate axis into <code>ϵ[i]</code> equal-length   intervals, extending the upper bound 1/100th of a bin size to ensure all points are   covered.</p></li><li><p><code>ϵ::Vector{Float64}</code> divides the i-th coordinate axis into intervals of fixed size <code>ϵ[i]</code>, starting   from the axis minima until the data is completely covered by boxes.</p></li></ol><p>Rectangular binnings may also be specified on arbitrary min-max ranges. </p><ol><li><code>ϵ::Tuple{Vector{Tuple{Float64,Float64}},Int64}</code> creates intervals   along each coordinate axis from ranges indicated by a vector of <code>(min, max)</code> tuples, then divides   each coordinate axis into an integer number of equal-length intervals. <em>Note: this does not ensure   that all points are covered by the data (points outside the binning are ignored)</em>.</li></ol><p><strong>Example 1: Grid deduced automatically from data (partition guaranteed to cover data points)</strong></p><p><strong>Flexible box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis, then  split each of those data ranges (with some tiny padding on the edges) into <code>10</code> equal-length  intervals. This gives (hyper-)rectangular boxes, and works for data of any dimension.</p><pre><code class="language-julia hljs">using Entropies
RectangularBinning(10)</code></pre><p>Now, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then  splits the range along the first coordinate axis (with some tiny padding on the edges)  into <code>10</code> equal-length intervals, and the range along the second coordinate axis (with some  tiny padding on the edges) into <code>5</code> equal-length intervals. This gives (hyper-)rectangular boxes.</p><pre><code class="language-julia hljs">using Entropies
RectangularBinning([10, 5])</code></pre><p><strong>Fixed box sizes</strong></p><p>The following binning specification finds the minima/maxima along each coordinate axis,  then split the axis ranges into equal-length intervals of fixed size <code>0.5</code> until the all data  points are covered by boxes. This approach yields (hyper-)cubic boxes, and works for  data of any dimension.</p><pre><code class="language-julia hljs">using Entropies
RectangularBinning(0.5)</code></pre><p>Again, assume the data consists of 2-dimensional points, and that we want a finer grid along one of the dimensions than over the other dimension.</p><p>The following binning specification finds the minima/maxima along each coordinate axis, then splits the range along the first coordinate axis into equal-length intervals of size <code>0.3</code>, and the range along the second axis into equal-length intervals of size <code>0.1</code> (in both cases,  making sure the data are completely covered by the boxes). This approach gives a (hyper-)rectangular boxes. </p><pre><code class="language-julia hljs">using Entropies
RectangularBinning([0.3, 0.1])</code></pre><p><strong>Example 2: Custom grids (partition not guaranteed to cover data points):</strong></p><p>Assume the data consists of 3-dimensional points <code>(x, y, z)</code>, and that we want a grid  that is fixed over the intervals <code>[x₁, x₂]</code> for the first dimension, over <code>[y₁, y₂]</code> for the second dimension, and over <code>[z₁, z₂]</code> for the third dimension. We when want to split each of those ranges into 4 equal-length pieces. <em>Beware: some points may fall  outside the partition if the intervals are not chosen properly (these points are  simply discarded)</em>. </p><p>The following binning specification produces the desired (hyper-)rectangular boxes. </p><pre><code class="language-julia hljs">using Entropies, DelayEmbeddings

D = Dataset(rand(100, 3));

x₁, x₂ = 0.5, 1 # not completely covering the data, which are on [0, 1]
y₁, y₂ = -2, 1.5 # covering the data, which are on [0, 1]
z₁, z₂ = 0, 0.5 # not completely covering the data, which are on [0, 1]

ϵ = [(x₁, x₂), (y₁, y₂), (z₁, z₂)], 4 # [interval 1, interval 2, ...], n_subdivisions

RectangularBinning(ϵ)</code></pre></div></section></article><h3 id="Transfer-operator"><a class="docs-heading-anchor" href="#Transfer-operator">Transfer operator</a><a id="Transfer-operator-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-operator" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="Entropies.TransferOperator" href="#Entropies.TransferOperator"><code>Entropies.TransferOperator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TransferOperator(ϵ::RectangularBinning) &lt;: BinningProbabilitiesEstimator</code></pre><p>A probability estimator based on binning data into rectangular boxes dictated by  the binning scheme <code>ϵ</code>, then approxmating the transfer (Perron-Frobenius) operator  over the bins, then taking the invariant measure associated with that transfer operator  as the bin probabilities. Assumes that the input data are sequential (time-ordered).</p><p>This implementation follows the grid estimator approach in Diego et al. (2019)<sup class="footnote-reference"><a id="citeref-Diego2019" href="#footnote-Diego2019">[Diego2019]</a></sup>.</p><p><strong>Description</strong></p><p>The transfer operator <span>$P^{N}$</span>is computed as an <code>N</code>-by-<code>N</code> matrix of transition  probabilities between the states defined by the partition elements, where <code>N</code> is the  number of boxes in the partition that is visited by the orbit/points. </p><p>If  <span>$\{x_t^{(D)} \}_{n=1}^L$</span> are the <span>$L$</span> different <span>$D$</span>-dimensional points over  which the transfer operator is approximated, <span>$\{ C_{k=1}^N \}$</span> are the <span>$N$</span> different  partition elements (as dictated by <code>ϵ</code>) that gets visited by the points, and  <span>$\phi(x_t) = x_{t+1}$</span>, then</p><p class="math-container">\[P_{ij} = \dfrac
{\#\{ x_n | \phi(x_n) \in C_j \cap x_n \in C_i \}}
{\#\{ x_m | x_m \in C_i \}},\]</p><p>where <span>$\#$</span> denotes the cardinal. The element <span>$P_{ij}$</span> thus indicates how many points  that are initially in box <span>$C_i$</span> end up in box <span>$C_j$</span> when the points in <span>$C_i$</span> are  projected one step forward in time. Thus, the row <span>$P_{ik}^N$</span> where  <span>$k \in \{1, 2, \ldots, N \}$</span> gives the probability  of jumping from the state defined by box <span>$C_i$</span> to any of the other <span>$N$</span> states. It  follows that <span>$\sum_{k=1}^{N} P_{ik} = 1$</span> for all <span>$i$</span>. Thus, <span>$P^N$</span> is a row/right  stochastic matrix.</p><p><strong>Invariant measure estimation from transfer operator</strong></p><p>The left invariant distribution <span>$\mathbf{\rho}^N$</span> is a row vector, where  <span>$\mathbf{\rho}^N P^{N} = \mathbf{\rho}^N$</span>. Hence, <span>$\mathbf{\rho}^N$</span> is a row  eigenvector of the transfer matrix <span>$P^{N}$</span> associated with eigenvalue 1. The distribution  <span>$\mathbf{\rho}^N$</span> approximates the invariant density of the system subject to the  partition <code>ϵ</code>, and can be taken as a probability distribution over the partition elements.</p><p>In practice, the invariant measure <span>$\mathbf{\rho}^N$</span> is computed using  <a href="../invariant_measure/#Entropies.invariantmeasure"><code>invariantmeasure</code></a>, which also approximates the transfer matrix. The invariant distribution is initialized as a length-<code>N</code> random distribution which is then applied to <span>$P^{N}$</span>.  The resulting length-<code>N</code> distribution is then applied to <span>$P^{N}$</span> again. This process  repeats until the difference between the distributions over consecutive iterations is  below some threshold. </p><p><strong>Probability and entropy estimation</strong></p><ul><li><code>probabilities(x::AbstractDataset, est::TransferOperator{RectangularBinning})</code> estimates    probabilities for the bins defined by the provided binning (<code>est.ϵ</code>)</li><li><code>genentropy(x::AbstractDataset, est::TransferOperator{RectangularBinning})</code> does the same,    but computes generalized entropy using the probabilities.</li></ul><p>See also: <a href="#Entropies.RectangularBinning"><code>RectangularBinning</code></a>, <a href="../invariant_measure/#Entropies.invariantmeasure"><code>invariantmeasure</code></a>.</p></div></section></article><h2 id="Kernel-density-based"><a class="docs-heading-anchor" href="#Kernel-density-based">Kernel density based</a><a id="Kernel-density-based-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-density-based" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.NaiveKernel" href="#Entropies.NaiveKernel"><code>Entropies.NaiveKernel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NaiveKernel(ϵ::Real, ss = KDTree; w = 0, metric = Euclidean()) &lt;: ProbabilitiesEstimator</code></pre><p>Estimate probabilities/entropy using a &quot;naive&quot; kernel density estimation approach (KDE), as  discussed in Prichard and Theiler (1995) <sup class="footnote-reference"><a id="citeref-PrichardTheiler1995" href="#footnote-PrichardTheiler1995">[PrichardTheiler1995]</a></sup>.</p><p>Probabilities <span>$P(\mathbf{x}, \epsilon)$</span> are assigned to every point <span>$\mathbf{x}$</span> by  counting how many other points occupy the space spanned by  a hypersphere of radius <code>ϵ</code> around <span>$\mathbf{x}$</span>, according to:</p><p class="math-container">\[P_i( X, \epsilon) \approx \dfrac{1}{N} \sum_{s} B(||X_i - X_j|| &lt; \epsilon),\]</p><p>where <span>$B$</span> gives 1 if the argument is <code>true</code>. Probabilities are then normalized.</p><p>The search structure <code>ss</code> is any search structure supported by Neighborhood.jl. Specifically, use <code>KDTree</code> to use a tree-based neighbor search, or <code>BruteForce</code> for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.</p><p>The keyword <code>w</code> stands for the <a href="@ref">Theiler window</a>, and excludes indices <span>$s$</span> that are within <span>$|i - s| ≤ w$</span> from the given point <span>$X_i$</span>.</p></div></section></article><h2 id="Nearest-neighbor-based"><a class="docs-heading-anchor" href="#Nearest-neighbor-based">Nearest neighbor based</a><a id="Nearest-neighbor-based-1"></a><a class="docs-heading-anchor-permalink" href="#Nearest-neighbor-based" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.KozachenkoLeonenko" href="#Entropies.KozachenkoLeonenko"><code>Entropies.KozachenkoLeonenko</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KozachenkoLeonenko(; k::Int = 1, w::Int = 0) &lt;: EntropyEstimator</code></pre><p>Entropy estimator based on nearest neighbors. This implementation is based on Kozachenko &amp; Leonenko (1987)<sup class="footnote-reference"><a id="citeref-KozachenkoLeonenko1987" href="#footnote-KozachenkoLeonenko1987">[KozachenkoLeonenko1987]</a></sup>, as described in Charzyńska and Gambin (2016)<sup class="footnote-reference"><a id="citeref-Charzyńska2016" href="#footnote-Charzyńska2016">[Charzyńska2016]</a></sup>.</p><p><code>w</code> is the Theiler window (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>This estimator is only available for entropy estimation. Probabilities cannot be obtained directly.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Entropies.Kraskov" href="#Entropies.Kraskov"><code>Entropies.Kraskov</code></a> — <span class="docstring-category">Type</span></header><section><div><p><strong>k-th nearest neighbour(kNN) based</strong></p><pre><code class="nohighlight hljs">Kraskov(; k::Int = 1, w::Int = 0) &lt;: EntropyEstimator</code></pre><p>Entropy estimator based on <code>k</code>-th nearest neighbor searches<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>. <code>w</code> is the <a href="@ref">Theiler window</a>.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>This estimator is only available for entropy estimation.  Probabilities cannot be obtained directly.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.Kraskov1" href="#TransferEntropy.Kraskov1"><code>TransferEntropy.Kraskov1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov1(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) &lt;: MutualInformationEstimator</code></pre><p>The <span>$I^{(1)}$</span> nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using <code>k</code> nearest neighbors. The distance metric for  the marginals <span>$x$</span> and <span>$y$</span> can be chosen separately, while the <code>Chebyshev</code> metric  is always used for the <code>z = (x, y)</code> joint space.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.Kraskov2" href="#TransferEntropy.Kraskov2"><code>TransferEntropy.Kraskov2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Kraskov2(k::Int = 1; metric_x = Chebyshev(), metric_y = Chebyshev()) &lt;: MutualInformationEstimator</code></pre><p>The <span>$I^{(2)}(x, y)$</span> nearest neighbor based mutual information estimator from  Kraskov et al. (2004), using <code>k</code> nearest neighbors. The distance metric for  the marginals <span>$x$</span> and <span>$y$</span> can be chosen separately, while the <code>Chebyshev</code> metric  is always used for the <code>z = (x, y)</code> joint space.</p></div></section></article><h2 id="symbolic"><a class="docs-heading-anchor" href="#symbolic">Permutation based</a><a id="symbolic-1"></a><a class="docs-heading-anchor-permalink" href="#symbolic" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.SymbolicPermutation" href="#Entropies.SymbolicPermutation"><code>Entropies.SymbolicPermutation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) &lt;: ProbabilityEstimator
SymbolicWeightedPermutation(; τ = 1, m = 3, lt = Entropies.isless_rand) &lt;: ProbabilityEstimator
SymbolicAmplitudeAwarePermutation(; τ = 1, m = 3, A = 0.5, lt = Entropies.isless_rand) &lt;: ProbabilityEstimator</code></pre><p>Symbolic, permutation-based probabilities/entropy estimators. <code>m</code> is the permutation order (or the symbol size or the embedding dimension) and  <code>τ</code> is the delay time (or lag).</p><p><strong>Repeated values during symbolization</strong></p><p>In the original implementation of permutation entropy <sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup>, equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low-amplitude resolution <sup class="footnote-reference"><a id="citeref-Zunino2017" href="#footnote-Zunino2017">[Zunino2017]</a></sup>. Here, we resolve this issue by letting the user provide a custom &quot;less-than&quot; function. The keyword <code>lt</code> accepts a function that decides which of two state vector elements are smaller. If two elements are equal, the default behaviour is to randomly assign one of them as the largest (<code>lt = Entropies.isless_rand</code>). For data with low amplitude resolution, computing probabilities multiple times using the random approach may reduce these erroneous effects.</p><p>To get the behaviour described in Bandt and Pompe (2002), use <code>lt = Base.isless</code>).</p><p><strong>Properties of original signal preserved</strong></p><ul><li><strong><code>SymbolicPermutation</code></strong>: Preserves ordinal patterns of state vectors (sorting information). This   implementation is based on Bandt &amp; Pompe et al. (2002)<sup class="footnote-reference"><a id="citeref-BandtPompe2002" href="#footnote-BandtPompe2002">[BandtPompe2002]</a></sup> and   Berger et al. (2019) <sup class="footnote-reference"><a id="citeref-Berger2019" href="#footnote-Berger2019">[Berger2019]</a></sup>.</li><li><strong><code>SymbolicWeightedPermutation</code></strong>: Like <code>SymbolicPermutation</code>, but also encodes amplitude   information by tracking the variance of the state vectors. This implementation is based   on Fadlallah et al. (2013)<sup class="footnote-reference"><a id="citeref-Fadlallah2013" href="#footnote-Fadlallah2013">[Fadlallah2013]</a></sup>.</li><li><strong><code>SymbolicAmplitudeAwarePermutation</code></strong>: Like <code>SymbolicPermutation</code>, but also encodes   amplitude information by considering a weighted combination of <em>absolute amplitudes</em>   of state vectors, and <em>relative differences between elements</em> of state vectors. See   description below for explanation of the weighting parameter <code>A</code>. This implementation   is based on Azami &amp; Escudero (2016) <sup class="footnote-reference"><a id="citeref-Azami2016" href="#footnote-Azami2016">[Azami2016]</a></sup>.</li></ul><p><strong>Probability estimation</strong></p><p><strong>Univariate time series</strong></p><p>To estimate probabilities or entropies from univariate time series, use the following methods:</p><ul><li><code>probabilities(x::AbstractVector, est::SymbolicProbabilityEstimator)</code>. Constructs state vectors   from <code>x</code> using embedding lag <code>τ</code> and embedding dimension <code>m</code>, symbolizes state vectors,   and computes probabilities as (weighted) relative frequency of symbols.</li><li><code>genentropy(x::AbstractVector, est::SymbolicProbabilityEstimator; α=1, base = 2)</code> computes   probabilities by calling <code>probabilities(x::AbstractVector, est)</code>,   then computer the order-<code>α</code> generalized entropy to the given base.</li></ul><p><strong>Speeding up repeated computations</strong></p><p>A pre-allocated integer symbol array <code>s</code> can be provided to save some memory allocations if the probabilities are to be computed for multiple data sets.</p><p><em>Note: it is not the array that will hold the final probabilities that is pre-allocated, but the temporary integer array containing the symbolized data points. Thus, if provided, it is required that <code>length(x) == length(s)</code> if <code>x</code> is a Dataset, or <code>length(s) == length(x) - (m-1)τ</code> if <code>x</code> is a univariate signal that is to be embedded first</em>.</p><p>Use the following signatures (only works for <code>SymbolicPermutation</code>).</p><pre><code class="language-julia hljs">probabilities!(s::Vector{Int}, x::AbstractVector, est::SymbolicPermutation) → ps::Probabilities
probabilities!(s::Vector{Int}, x::AbstractDataset, est::SymbolicPermutation) → ps::Probabilities</code></pre><p><strong>Multivariate datasets</strong></p><p>Although not dealt with in the original paper describing the estimators, numerically speaking, permutation entropies can also be computed for multivariate datasets with dimension ≥ 2 (but see caveat below). Such datasets may be, for example, preembedded time series. Then, just skip the delay reconstruction step, compute and symbols directly from the <span>$L$</span> existing state vectors <span>$\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x_L}\}$</span>.</p><ul><li><code>probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)</code>. Compute ordinal patterns of the   state vectors of <code>x</code> directly (without doing any embedding), symbolize those patterns,   and compute probabilities as (weighted) relative frequencies of symbols.</li><li><code>genentropy(x::AbstractDataset, est::SymbolicProbabilityEstimator)</code>. Computes probabilities from   symbol frequencies using <code>probabilities(x::AbstractDataset, est::SymbolicProbabilityEstimator)</code>,   then computes the order-<code>α</code> generalized (permutation) entropy to the given base.</li></ul><p><em>Caveat: A dynamical interpretation of the permutation entropy does not necessarily hold if computing it on generic multivariate datasets. Method signatures for <code>Dataset</code>s are provided for convenience, and should only be applied if you understand the relation between your input data, the numerical value for the permutation entropy, and its interpretation.</em></p><p><strong>Description</strong></p><p>All symbolic estimators use the same underlying approach to estimating probabilities.</p><p><strong>Embedding, ordinal patterns and symbolization</strong></p><p>Consider the <span>$n$</span>-element univariate time series <span>$\{x(t) = x_1, x_2, \ldots, x_n\}$</span>. Let <span>$\mathbf{x_i}^{m, \tau} = \{x_j, x_{j+\tau}, \ldots, x_{j+(m-1)\tau}\}$</span> for <span>$j = 1, 2, \ldots n - (m-1)\tau$</span> be the <span>$i$</span>-th state vector in a delay reconstruction with embedding dimension <span>$m$</span> and reconstruction lag <span>$\tau$</span>. There are then <span>$N = n - (m-1)\tau$</span> state vectors.</p><p>For an <span>$m$</span>-dimensional vector, there are <span>$m!$</span> possible ways of sorting it in ascending order of magnitude. Each such possible sorting ordering is called a <em>motif</em>. Let <span>$\pi_i^{m, \tau}$</span> denote the motif associated with the <span>$m$</span>-dimensional state vector <span>$\mathbf{x_i}^{m, \tau}$</span>, and let <span>$R$</span> be the number of distinct motifs that can be constructed from the <span>$N$</span> state vectors. Then there are at most <span>$R$</span> motifs; <span>$R = N$</span> precisely when all motifs are unique, and <span>$R = 1$</span> when all motifs are the same.</p><p>Each unique motif <span>$\pi_i^{m, \tau}$</span> can be mapped to a unique integer symbol <span>$0 \leq s_i \leq M!-1$</span>. Let <span>$S(\pi) : \mathbb{R}^m \to \mathbb{N}_0$</span> be the function that maps the motif <span>$\pi$</span> to its symbol <span>$s$</span>, and let <span>$\Pi$</span> denote the set of symbols <span>$\Pi = \{ s_i \}_{i\in \{ 1, \ldots, R\}}$</span>.</p><p><strong>Probability computation</strong></p><p><strong><code>SymbolicPermutation</code></strong></p><p>The probability of a given motif is its frequency of occurrence, normalized by the total number of motifs (with notation from <sup class="footnote-reference"><a id="citeref-Fadlallah2013" href="#footnote-Fadlallah2013">[Fadlallah2013]</a></sup>),</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left(\mathbf{x}_k^{m, \tau} \right) }{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi} \left(\mathbf{x}_k^{m, \tau} \right)} = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left(\mathbf{x}_k^{m, \tau} \right) }{N},\]</p><p>where the function <span>$\mathbf{1}_A(u)$</span> is the indicator function of a set <span>$A$</span>. That     is, <span>$\mathbf{1}_A(u) = 1$</span> if <span>$u \in A$</span>, and <span>$\mathbf{1}_A(u) = 0$</span> otherwise.</p><p><strong><code>SymbolicAmplitudeAwarePermutation</code></strong></p><p>Amplitude-aware permutation entropy is computed analogously to regular permutation entropy but probabilities are weighted by amplitude information as follows.</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi} \left( \mathbf{x}_k^{m, \tau} \right) \,a_k} = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i} \left( \mathbf{x}_k^{m, \tau} \right) \, a_k}{\sum_{k=1}^N a_k}.\]</p><p>The weights encoding amplitude information about state vector <span>$\mathbf{x}_i = (x_1^i, x_2^i, \ldots, x_m^i)$</span> are</p><p class="math-container">\[a_i = \dfrac{A}{m} \sum_{k=1}^m |x_k^i | + \dfrac{1-A}{d-1} \sum_{k=2}^d |x_{k}^i - x_{k-1}^i|,\]</p><p>with <span>$0 \leq A \leq 1$</span>. When <span>$A=0$</span> , only internal differences between the elements of <span>$\mathbf{x}_i$</span> are weighted. Only mean amplitude of the state vector elements are weighted when <span>$A=1$</span>. With, <span>$0&lt;A&lt;1$</span>, a combined weighting is used.</p><p><strong><code>SymbolicWeightedPermutation</code></strong></p><p>Weighted permutation entropy is also computed analogously to regular permutation entropy, but adds weights that encode amplitude information too:</p><p class="math-container">\[p(\pi_i^{m, \tau}) = \dfrac{\sum_{k=1}^N \mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right)
\, w_k}{\sum_{k=1}^N \mathbf{1}_{u:S(u) \in \Pi}
\left( \mathbf{x}_k^{m, \tau} \right) \,w_k} = \dfrac{\sum_{k=1}^N
\mathbf{1}_{u:S(u) = s_i}
\left( \mathbf{x}_k^{m, \tau} \right) \, w_k}{\sum_{k=1}^N w_k}.\]</p><p>The weighted permutation entropy is equivalent to regular permutation entropy when weights are positive and identical (<span>$w_j = \beta \,\,\, \forall \,\,\, j \leq N$</span> and <span>$\beta &gt; 0)$</span>. Weights are dictated by the variance of the state vectors.</p><p>Let the aritmetic mean of state vector <span>$\mathbf{x}_i$</span> be denoted by</p><p class="math-container">\[\mathbf{\hat{x}}_j^{m, \tau} = \frac{1}{m} \sum_{k=1}^m x_{j + (k+1)\tau}.\]</p><p>Weights are then computed as</p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j+(k+1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</p><p><em>Note: in equation 7, section III, of the original paper, the authors write</em></p><p class="math-container">\[w_j = \dfrac{1}{m}\sum_{k=1}^m (x_{j-(k-1)\tau} - \mathbf{\hat{x}}_j^{m, \tau})^2.\]</p><p><em>But given the formula they give for the arithmetic mean, this is <strong>not</strong> the variance of <span>$\mathbf{x}_i$</span>, because the indices are mixed: <span>$x_{j+(k-1)\tau}$</span> in the weights formula, vs. <span>$x_{j+(k+1)\tau}$</span> in the arithmetic mean formula. This seems to imply that amplitude information about previous delay vectors are mixed with mean amplitude information about current vectors. The authors also mix the terms &quot;vector&quot; and &quot;neighboring vector&quot; (but uses the same notation for both), making it hard to interpret whether the sign switch is a typo or intended. Here, we use the notation above, which actually computes the variance for <span>$\mathbf{x}_i$</span></em>.</p></div></section></article><h2 id="Wavelet-based"><a class="docs-heading-anchor" href="#Wavelet-based">Wavelet based</a><a id="Wavelet-based-1"></a><a class="docs-heading-anchor-permalink" href="#Wavelet-based" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Entropies.TimeScaleMODWT" href="#Entropies.TimeScaleMODWT"><code>Entropies.TimeScaleMODWT</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TimeScaleMODWT &lt;: WaveletProbabilitiesEstimator
TimeScaleMODWT(wl::Wavelets.WT.OrthoWaveletClass = Wavelets.WT.Daubechies{12}())</code></pre><p>Apply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities/entropy from the energies at different wavelet scales. This implementation is based on Rosso et al. (2001)<sup class="footnote-reference"><a id="citeref-Rosso2001" href="#footnote-Rosso2001">[Rosso2001]</a></sup>. Optionally specify a wavelet to be used.</p><p>The probability <code>p[i]</code> is the relative/total energy for the i-th wavelet scale.</p><p><strong>Example</strong></p><p>Manually picking a wavelet is done as follows.</p><pre><code class="language-julia hljs">using Entropies, Wavelets
N = 200
a = 10
t = LinRange(0, 2*a*π, N)
x = sin.(t .+  cos.(t/0.1)) .- 0.1;

# Pick a wavelet (if no wavelet provided, defaults to Wavelets.WL.Daubechies{12}())
wl = Wavelets.WT.Daubechies{12}()

# Compute the probabilities (relative energies) at the different wavelet scales
probabilities(x, TimeScaleMODWT(wl))</code></pre></div></section></article><h2 id="Hilbert"><a class="docs-heading-anchor" href="#Hilbert">Hilbert</a><a id="Hilbert-1"></a><a class="docs-heading-anchor-permalink" href="#Hilbert" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.Hilbert" href="#TransferEntropy.Hilbert"><code>TransferEntropy.Hilbert</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Hilbert(est; 
    source::InstantaneousSignalProperty = Phase(),
    target::InstantaneousSignalProperty = Phase(),
    cond::InstantaneousSignalProperty = Phase())
) &lt;: TransferEntropyEstimator</code></pre><p>Compute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are  obtained by first applying the Hilbert transform to each signal, then extracting the  phases/amplitudes of the resulting complex numbers<sup class="footnote-reference"><a id="citeref-Palus2014" href="#footnote-Palus2014">[Palus2014]</a></sup>. Original time series are  thus transformed to instantaneous phase/amplitude time series. Transfer  entropy is then estimated using the provided <code>est</code> on those phases/amplitudes (use e.g.  <a href="#Entropies.VisitationFrequency"><code>VisitationFrequency</code></a>, or <a href="#Entropies.SymbolicPermutation"><code>SymbolicPermutation</code></a>).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Details on estimation of the transfer entropy (conditional mutual information)  following the phase/amplitude extraction step is not given in Palus (2014). Here,  after instantaneous phases/amplitudes have been obtained, these are treated as regular  time series, from which transfer entropy is then computed as usual.</p></div></div><p>See also: <a href="#TransferEntropy.Phase"><code>Phase</code></a>, <a href="#TransferEntropy.Amplitude"><code>Amplitude</code></a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.Amplitude" href="#TransferEntropy.Amplitude"><code>TransferEntropy.Amplitude</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Amplitude &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous amplitudes of a signal should be used. </p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="TransferEntropy.Phase" href="#TransferEntropy.Phase"><code>TransferEntropy.Phase</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Phase &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous phases of a signal should be used. </p></div></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Diego2019"><a class="tag is-link" href="#citeref-Diego2019">Diego2019</a>Diego, D., Haaga, K. A., &amp; Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.</li><li class="footnote" id="footnote-PrichardTheiler1995"><a class="tag is-link" href="#citeref-PrichardTheiler1995">PrichardTheiler1995</a>Prichard, D., &amp; Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.</li><li class="footnote" id="footnote-Charzyńska2016"><a class="tag is-link" href="#citeref-Charzyńska2016">Charzyńska2016</a>Charzyńska, A., &amp; Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. Entropy, 18(1), 13.</li><li class="footnote" id="footnote-KozachenkoLeonenko1987"><a class="tag is-link" href="#citeref-KozachenkoLeonenko1987">KozachenkoLeonenko1987</a>Kozachenko, L. F., &amp; Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-BandtPompe2002"><a class="tag is-link" href="#citeref-BandtPompe2002">BandtPompe2002</a>Bandt, Christoph, and Bernd Pompe. &quot;Permutation entropy: a natural complexity measure for time series.&quot; Physical review letters 88.17 (2002): 174102.</li><li class="footnote" id="footnote-Berger2019"><a class="tag is-link" href="#citeref-Berger2019">Berger2019</a>Berger, Sebastian, et al. &quot;Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.&quot; Entropy 21.10 (2019): 1023.</li><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, Bilal, et al. &quot;Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.&quot; Physical Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Rényi1960"><a class="tag is-link" href="#citeref-Rényi1960">Rényi1960</a>A. Rényi, <em>Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability</em>, pp 547 (1960)</li><li class="footnote" id="footnote-Azami2016"><a class="tag is-link" href="#citeref-Azami2016">Azami2016</a>Azami, H., &amp; Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer methods and programs in biomedicine, 128, 40-51.</li><li class="footnote" id="footnote-Fadlallah2013"><a class="tag is-link" href="#citeref-Fadlallah2013">Fadlallah2013</a>Fadlallah, Bilal, et al. &quot;Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information.&quot; Physical Review E 87.2 (2013): 022911.</li><li class="footnote" id="footnote-Zunino2017"><a class="tag is-link" href="#citeref-Zunino2017">Zunino2017</a>Zunino, L., Olivares, F., Scholkmann, F., &amp; Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.</li><li class="footnote" id="footnote-Rosso2001"><a class="tag is-link" href="#citeref-Rosso2001">Rosso2001</a>Rosso, O. A., Blanco, S., Yordanova, J., Kolev, V., Figliola, A., Schürmann, M., &amp; Başar, E. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.</li><li class="footnote" id="footnote-Palus2014"><a class="tag is-link" href="#citeref-Palus2014">Palus2014</a>Paluš, M. (2014). Cross-scale interactions and information transfer. Entropy, 16(10), 5263-5289.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../generalized_entropy/">« Generalized entropy</a><a class="docs-footer-nextpage" href="../example_systems/">Example systems »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Friday 26 November 2021 14:24">Friday 26 November 2021</span>. Using Julia version 1.6.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
