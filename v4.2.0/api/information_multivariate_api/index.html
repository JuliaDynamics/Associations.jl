<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multivariate information API · Associations.jl</title><meta name="title" content="Multivariate information API · Associations.jl"/><meta property="og:title" content="Multivariate information API · Associations.jl"/><meta property="twitter:title" content="Multivariate information API · Associations.jl"/><meta name="description" content="Documentation for Associations.jl."/><meta property="og:description" content="Documentation for Associations.jl."/><meta property="twitter:description" content="Documentation for Associations.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Associations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Associations.jl</a></li><li><span class="tocitem">Core API reference</span><ul><li><a class="tocitem" href="../../associations/">Association measures</a></li><li><a class="tocitem" href="../../independence/">Independence</a></li><li><a class="tocitem" href="../../causal_graphs/">Network/graph inference</a></li></ul></li><li><span class="tocitem">Extended API reference</span><ul><li><a class="tocitem" href="../discretization_counts_probs_api/">Discretization API</a></li><li><a class="tocitem" href="../counts_and_probabilities_api/">Multivariate counts and probabilities API</a></li><li><a class="tocitem" href="../information_single_variable_api/">Single-variable information API</a></li><li class="is-active"><a class="tocitem" href>Multivariate information API</a><ul class="internal"><li><a class="tocitem" href="#Estimators"><span>Estimators</span></a></li><li><a class="tocitem" href="#tutorial_infomeasures"><span>A small tutorial</span></a></li></ul></li><li><a class="tocitem" href="../cross_map_api/">Cross-map API</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/examples_associations/">Associations</a></li><li><a class="tocitem" href="../../examples/examples_independence/">Independence testing</a></li><li><a class="tocitem" href="../../examples/examples_infer_graphs/">Causal graph inference</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Extended API reference</a></li><li class="is-active"><a href>Multivariate information API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multivariate information API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Associations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Associations.jl/blob/main/docs/src/api/information_multivariate_api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="information_api"><a class="docs-heading-anchor" href="#information_api">Multivariate information API</a><a id="information_api-1"></a><a class="docs-heading-anchor-permalink" href="#information_api" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ComplexityMeasures.information-Tuple{MultivariateInformationMeasureEstimator}" href="#ComplexityMeasures.information-Tuple{MultivariateInformationMeasureEstimator}"><code>ComplexityMeasures.information</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">information(est::MultivariateInformationMeasureEstimator, x...)</code></pre><p>Estimate some <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a> on input data <code>x...</code>, using the given <a href="#Associations.MultivariateInformationMeasureEstimator"><code>MultivariateInformationMeasureEstimator</code></a>.</p><p>This is just a convenience wrapper around <a href="../../associations/#Associations.association"><code>association</code></a><code>(est, x...)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/core.jl#L158-L165">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MultivariateInformationMeasureEstimator" href="#Associations.MultivariateInformationMeasureEstimator"><code>Associations.MultivariateInformationMeasureEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MultivariateInformationMeasureEstimator</code></pre><p>The supertype for all estimators of multivariate information measures.</p><p><strong>Generic implementations</strong></p><ul><li><a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a></li><li><a href="#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a></li><li><a href="#Associations.MIDecomposition"><code>MIDecomposition</code></a></li><li><a href="#Associations.CMIDecomposition"><code>CMIDecomposition</code></a></li></ul><p><strong>Dedicated implementations</strong></p><p><a href="#Associations.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s:</p><ul><li><a href="#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a></li><li><a href="#Associations.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a></li><li><a href="#Associations.GaoOhViswanath"><code>GaoOhViswanath</code></a></li><li><a href="#Associations.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></li><li><a href="#Associations.GaussianMI"><code>GaussianMI</code></a></li></ul><p><a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>s:</p><ul><li><a href="#Associations.FPVP"><code>FPVP</code></a></li><li><a href="#Associations.MesnerShalizi"><code>MesnerShalizi</code></a></li><li><a href="#Associations.Rahimzamani"><code>Rahimzamani</code></a></li><li><a href="#Associations.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li><li><a href="#Associations.GaussianCMI"><code>GaussianCMI</code></a></li></ul><p><a href="#Associations.TransferEntropyEstimator"><code>TransferEntropyEstimator</code></a>s:</p><ul><li><a href="#Associations.Zhu1"><code>Zhu1</code></a></li><li><a href="#Associations.Lindner"><code>Lindner</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/core.jl#L14-L48">source</a></section></article><h2 id="Estimators"><a class="docs-heading-anchor" href="#Estimators">Estimators</a><a id="Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Estimators" title="Permalink"></a></h2><h3 id="Generic-estimators"><a class="docs-heading-anchor" href="#Generic-estimators">Generic estimators</a><a id="Generic-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Generic-estimators" title="Permalink"></a></h3><p>We provide a set of generic estimators that can be used to calculate  potentially several types of information measures.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.JointProbabilities" href="#Associations.JointProbabilities"><code>Associations.JointProbabilities</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">JointProbabilities &lt;: InformationMeasureEstimator
JointProbabilities(
    definition::MultivariateInformationMeasure,
    discretization::Discretization
)</code></pre><p><code>JointProbabilities</code> is a generic estimator for multivariate discrete information measures.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute an information measure from input data.</li></ul><p><strong>Description</strong></p><p>It first encodes the input data according to the given <code>discretization</code>, then constructs  <code>probs</code>, a multidimensional <a href="../counts_and_probabilities_api/#ComplexityMeasures.Probabilities"><code>Probabilities</code></a> instance. Finally, <code>probs</code> are  forwarded to a <a href="../information_single_variable_api/#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator, which computes the measure according to  <code>definition</code>.</p><p><strong>Compatible encoding schemes</strong></p><ul><li><a href="../discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> (encode each <em>variable</em>/column of the input data independently by    applying an encoding in a sliding window over each input variable).  </li><li><a href="../discretization_counts_probs_api/#Associations.CodifyPoints"><code>CodifyPoints</code></a> (encode each <em>point</em>/column of the input data)</li></ul><p>Works for any <a href="../discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a> that implements <a href="../discretization_counts_probs_api/#ComplexityMeasures.codify"><code>codify</code></a>.</p><div class="admonition is-info"><header class="admonition-header">Joint probabilities vs decomposition methods</header><div class="admonition-body"><p>Using <a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a> to compute an information measure,  e.g. conditional mutual estimation, is typically slower than other dedicated estimation procedures like <a href="#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a>. The reason is that measures such as <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a> can be formulated as a sum of four entropies, which can be estimated individually and summed afterwards.  This decomposition is fast because because we avoid <em>explicitly</em> estimating the entire joint pmf,  which demands many extra calculation steps, However, the decomposition is biased,  because it fails to fully take into consideration the joint relationships between the variables. Pick your estimator according to your needs.</p></div></div><p>See also: <a href="../counts_and_probabilities_api/#ComplexityMeasures.Counts"><code>Counts</code></a>, <a href="../counts_and_probabilities_api/#ComplexityMeasures.Probabilities"><code>Probabilities</code></a>, <a href="https://juliadynamics.github.io/DynamicalSystemsDocs.jl/complexitymeasures/stable/probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a>, <a href="../discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>, <a href="../information_single_variable_api/#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/JointProbabilities.jl#L4-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.EntropyDecomposition" href="#Associations.EntropyDecomposition"><code>Associations.EntropyDecomposition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EntropyDecomposition(definition::MultivariateInformationMeasure, 
    est::DifferentialInfoEstimator)
EntropyDecomposition(definition::MultivariateInformationMeasure,
    est::DiscreteInfoEstimator,
    discretization::CodifyVariables{&lt;:OutcomeSpace},
    pest::ProbabilitiesEstimator = RelativeAmount())</code></pre><p>Estimate the multivariate information measure specified by <code>definition</code> by rewriting its formula into some combination of entropy terms. </p><p>If calling the second method (discrete variant), then discretization is always done  per variable/column and each column is encoded into integers using <a href="../discretization_counts_probs_api/#ComplexityMeasures.codify"><code>codify</code></a>.</p><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute a <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>   from input data: <a href="../../associations/#Associations.association"><code>association</code></a><code>(est::EntropyDecomposition, x...)</code>.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>The entropy terms are estimated using <code>est</code>, and then combined to form the final  estimate of <code>definition</code>. No bias correction is applied. If <code>est</code> is a <a href="../information_single_variable_api/#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>, then <code>discretization</code> and <code>pest</code>  are ignored. If <code>est</code> is a <a href="../information_single_variable_api/#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>, then <code>discretization</code> and a probabilities estimator <code>pest</code> must also be provided (default to <code>RelativeAmount</code>,  which uses naive plug-in probabilities).</p><p><strong>Compatible differential information estimators</strong></p><p>If using the first signature, any compatible <a href="../information_single_variable_api/#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a> can be  used.</p><p><strong>Compatible outcome spaces for discrete estimation</strong></p><p>If using the second signature, the outcome spaces can be used for discretisation.  Note that not all outcome spaces will work with all measures.</p><table><tr><th style="text-align: left">Estimator</th><th style="text-align: left">Principle</th></tr><tr><td style="text-align: left"><a href="../discretization_counts_probs_api/#ComplexityMeasures.UniqueElements"><code>UniqueElements</code></a></td><td style="text-align: left">Count of unique elements</td></tr><tr><td style="text-align: left"><a href="../discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a></td><td style="text-align: left">Binning (histogram)</td></tr><tr><td style="text-align: left"><a href="../discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a></td><td style="text-align: left">Ordinal patterns</td></tr><tr><td style="text-align: left"><a href="../discretization_counts_probs_api/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: left">Dispersion patterns</td></tr><tr><td style="text-align: left"><a href="../discretization_counts_probs_api/#ComplexityMeasures.BubbleSortSwaps"><code>BubbleSortSwaps</code></a></td><td style="text-align: left">Sorting complexity</td></tr><tr><td style="text-align: left"><a href="../discretization_counts_probs_api/#ComplexityMeasures.CosineSimilarityBinning"><code>CosineSimilarityBinning</code></a></td><td style="text-align: left">Cosine similarities histogram</td></tr></table><p><strong>Bias</strong></p><p>Estimating the <code>definition</code> by decomposition into a combination of entropy terms, which are estimated independently, will in general be more biased than when using a dedicated estimator. One reason is that this decomposition may miss out on crucial information in the joint space. To remedy this, dedicated information measure  estimators typically derive the marginal estimates by first considering the joint space, and then does some clever trick to eliminate the bias that is introduced through a naive decomposition. Unless specified below, no bias correction is  applied for <code>EntropyDecomposition</code>.</p><p><strong>Handling of overlapping parameters</strong></p><p>If there are overlapping parameters between the measure to be estimated, and the lower-level decomposed measures, then the top-level measure parameter takes precedence. For example, if we want to estimate <code>CMIShannon(base = 2)</code> through a decomposition  of entropies using the <code>Kraskov(Shannon(base = ℯ))</code> Shannon entropy estimator, then <code>base = 2</code> is used.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Not all measures have the property that they can be decomposed into more fundamental information theoretic quantities. For example, <a href="../../associations/#Associations.MITsallisMartin"><code>MITsallisMartin</code></a> <em>can</em> be  decomposed into a combination of marginal entropies, while <a href="../../associations/#Associations.MIRenyiSarbu"><code>MIRenyiSarbu</code></a> cannot. An error will be thrown if decomposition is not possible.</p></div></div><p><strong>Discrete entropy decomposition</strong></p><p>The second signature is for discrete estimation using <a href="../information_single_variable_api/#ComplexityMeasures.DiscreteInfoEstimator"><code>DiscreteInfoEstimator</code></a>s, for example <a href="../information_single_variable_api/#ComplexityMeasures.PlugIn"><code>PlugIn</code></a>. The given <code>discretization</code> scheme (typically an  <a href="../discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>) controls how the joint/marginals are discretized, and the probabilities estimator <code>pest</code> controls how probabilities are estimated from counts.</p><div class="admonition is-info"><header class="admonition-header">Bias</header><div class="admonition-body"><p>Like for <a href="../information_single_variable_api/#ComplexityMeasures.DifferentialInfoEstimator"><code>DifferentialInfoEstimator</code></a>, using a dedicated estimator  for the measure in question will be more reliable than using a decomposition estimate. Here&#39;s how different <code>discretization</code>s are applied:</p><ul><li><a href="../discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a>. Bin visitation frequencies are counted in the joint space   <code>XY</code>, then marginal visitations are obtained from the joint bin visits.   This behaviour is the same for both <a href="../discretization_counts_probs_api/#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a> and   <a href="../discretization_counts_probs_api/#ComplexityMeasures.RectangularBinning"><code>RectangularBinning</code></a> (which adapts the grid to the data).   When using <a href="../discretization_counts_probs_api/#ComplexityMeasures.FixedRectangularBinning"><code>FixedRectangularBinning</code></a>, the range along the first dimension   is used as a template for all other dimensions. This is a bit slower than naively    binning each marginal, but lessens bias.</li><li><a href="../discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a>. Each timeseries is separately <a href="../discretization_counts_probs_api/#ComplexityMeasures.codify"><code>codify</code></a>-ed   according to its ordinal pattern (no bias correction).</li><li><a href="../discretization_counts_probs_api/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a>. Each timeseries is separately <a href="../discretization_counts_probs_api/#ComplexityMeasures.codify"><code>codify</code></a>-ed according   to its dispersion pattern  (no bias correction).</li></ul></div></div><p><strong>Examples</strong></p><ul><li><a href="../../examples/examples_associations/#example_MIShannon_EntropyDecomposition_Jackknife_ValueBinning">Example 1</a>:   <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a> estimation using decomposition into discrete <a href="../information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a>   entropy estimated using <a href="../discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> with <a href="../discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a>.</li><li><a href="../../examples/examples_associations/#example_MIShannon_EntropyDecomposition_BubbleSortSwaps">Example 2</a>:   <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a> estimation using decomposition into discrete <a href="../information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a>   entropy estimated using <a href="../discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> with <a href="../discretization_counts_probs_api/#ComplexityMeasures.BubbleSortSwaps"><code>BubbleSortSwaps</code></a>.</li><li><a href="../../examples/examples_associations/#example_MIShannon_EntropyDecomposition_Kraskov">Example 3</a>:   <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a> estimation using decomposition into differental <a href="../information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a>   entropy estimated using the <a href="../information_single_variable_api/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimator.</li></ul><p>See also: <a href="#Associations.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>, <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/decomposition/EntropyDecomposition.jl#L4-L115">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MIDecomposition" href="#Associations.MIDecomposition"><code>Associations.MIDecomposition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MIDecomposition(definition::MultivariateInformationMeasure, 
    est::MutualInformationEstimator)</code></pre><p>Estimate the <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a> specified by <code>definition</code> by by decomposing, the measure, if possible, into a combination of mutual information terms. These terms are individually estimated using the given <a href="#Associations.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> <code>est</code>, and finally combined to form the final  value of the measure. </p><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute a <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>   from input data: <a href="../../associations/#Associations.association"><code>association</code></a><code>(est::MIDecomposition, x...)</code>.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Examples</strong></p><ul><li><a href="../../examples/examples_associations/#example_CMIShannon_MIDecomposition">Example 1</a>: Estimating <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a>   using a decomposition into <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a> terms using    the <a href="#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> mutual information estimator.</li></ul><p>See also: <a href="#Associations.MultivariateInformationMeasureEstimator"><code>MultivariateInformationMeasureEstimator</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/decomposition/MIDecomposition.jl#L4-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.CMIDecomposition" href="#Associations.CMIDecomposition"><code>Associations.CMIDecomposition</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CMIDecomposition(definition::MultivariateInformationMeasure, 
    est::ConditionalMutualInformationEstimator)</code></pre><p>Estimate some multivariate information measure specified by <code>definition</code>, by decomposing it into a combination of conditional mutual information terms. </p><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute a <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>   from input data: <a href="../../associations/#Associations.association"><code>association</code></a><code>(est::CMIDecomposition, x...)</code>.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>Each of the conditional mutual information terms are estimated using <code>est</code>, which  can be any <a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>. Finally, these estimates  are combined according to the relevant decomposition formula.</p><p>This estimator is similar to <a href="#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a>, but <code>definition</code> is expressed as  conditional mutual information terms instead of entropy terms.</p><p><strong>Examples</strong></p><ul><li><a href="../../examples/examples_associations/#example_TEShannon_CMIDecomposition">Example 1</a>: Estimating <a href="../../associations/#Associations.TEShannon"><code>TEShannon</code></a>   by decomposing it into <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a> which is estimated using the   <a href="#Associations.FPVP"><code>FPVP</code></a> estimator.</li></ul><p>See also: <a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>,  <a href="#Associations.MultivariateInformationMeasureEstimator"><code>MultivariateInformationMeasureEstimator</code></a>, <a href="../../associations/#Associations.MultivariateInformationMeasure"><code>MultivariateInformationMeasure</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/decomposition/CMIDecomposition.jl#L3-L35">source</a></section></article><h3 id="Mutual-information-estimators"><a class="docs-heading-anchor" href="#Mutual-information-estimators">Mutual information estimators</a><a id="Mutual-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-information-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MutualInformationEstimator" href="#Associations.MutualInformationEstimator"><code>Associations.MutualInformationEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MutualInformationEstimator</code></pre><p>The supertype for dedicated <a href="../../associations/#Associations.MutualInformation"><code>MutualInformation</code></a> estimators.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a></li><li><a href="#Associations.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a></li><li><a href="#Associations.GaoOhViswanath"><code>GaoOhViswanath</code></a></li><li><a href="#Associations.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></li><li><a href="#Associations.GaussianMI"><code>GaussianMI</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/core.jl#L52-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.KraskovStögbauerGrassberger1" href="#Associations.KraskovStögbauerGrassberger1"><code>Associations.KraskovStögbauerGrassberger1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger1</code> mutual information estimator (you can use <code>KSG1</code> for short) is the <span>$I^{(1)}$</span> <code>k</code>-th nearest neighbor estimator from <a href="../../references/#Kraskov2004">Kraskov <em>et al.</em> (2004)</a>.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute Shannon mutual information from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(KSG1(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/mutual_info_estimators/KSG1.jl#L16-L52">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.KraskovStögbauerGrassberger2" href="#Associations.KraskovStögbauerGrassberger2"><code>Associations.KraskovStögbauerGrassberger2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger2</code> mutual information estimator (you can use <code>KSG2</code> for short) is the <span>$I^{(2)}$</span> <code>k</code>-th nearest neighbor estimator from (<a href="../../references/#Kraskov2004">Kraskov <em>et al.</em>, 2004</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute Shannon mutual information from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Description</strong></p><p>Let the joint StateSpaceSet <span>$X := \{\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m \}$</span> be defined by the concatenation of the marginal StateSpaceSets <span>$\{ \bf{X}_k \}_{k=1}^m$</span>, where each <span>$\bf{X}_k$</span> is potentially multivariate. Let <span>$\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N$</span> be the points in the joint space <span>$X$</span>.</p><p>The <code>KraskovStögbauerGrassberger2</code> estimator first locates, for each <span>$\bf{x}_i \in X$</span>, the point <span>$\bf{n}_i \in X$</span>, the <code>k</code>-th nearest neighbor to <span>$\bf{x}_i$</span>, according to the maximum norm (<code>Chebyshev</code> metric). Let <span>$\epsilon_i$</span> be the distance <span>$d(\bf{x}_i, \bf{n}_i)$</span>.</p><p>Consider <span>$x_i^m \in \bf{X}_m$</span>, the <span>$i$</span>-th point in the marginal space <span>$\bf{X}_m$</span>. For each <span>$\bf{x}_i^m$</span>, we determine <span>$\theta_i^m$</span> := the number of points <span>$\bf{x}_k^m \in \bf{X}_m$</span> that are a distance less than <span>$\epsilon_i$</span> away from <span>$\bf{x}_i^m$</span>. That is, we use the distance from a query point <span>$\bf{x}_i \in X$</span> (in the <em>joint</em> space) to count neighbors of <span>$x_i^m \in \bf{X}_m$</span> (in the marginal space).</p><p>Shannon mutual information between the variables <span>$\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m$</span> is then estimated as</p><p class="math-container">\[\hat{I}_{KSG2}(\bf{X}) =
    \psi{(k)} -
    \dfrac{m - 1}{k} +
    (m - 1)\psi{(N)} -
    \dfrac{1}{N} \sum_{i = 1}^N \sum_{j = 1}^m \psi{(\theta_i^j + 1)}\]</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(KSG2(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/mutual_info_estimators/KSG2.jl#L9-L74">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.GaoKannanOhViswanath" href="#Associations.GaoKannanOhViswanath"><code>Associations.GaoKannanOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoKannanOhViswanath &lt;: MutualInformationEstimator
GaoKannanOhViswanath(; k = 1, w = 0)</code></pre><p>The <code>GaoKannanOhViswanath</code> (Shannon) estimator is designed for estimating Shannon mutual information between variables that may be either discrete, continuous or a mixture of both (<a href="../../references/#GaoKannanOhViswanath2017">Gao <em>et al.</em>, 2017</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute Shannon mutual information from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>The estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using <code>k</code>-nearest neighbor distances from empirical samples.</p><p>The estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator <em>&quot;strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.&quot;</em></p><div class="admonition is-category-warn"><header class="admonition-header">Implementation note</header><div class="admonition-body"><p>In <a href="../../references/#GaoKannanOhViswanath2017">Gao <em>et al.</em> (2017)</a>, they claim (roughly speaking) that the estimator reduces to the <a href="#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> estimator for continuous-valued data. However, <a href="#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> uses the digamma function, while <code>GaoKannanOhViswanath</code> uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.</p><p>Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the <code>KraskovStögbauerGrassberger1</code> approach. The <code>KraskovStögbauerGrassberger1</code> estimator uses <code>k</code>-th neighbor distances in the <em>joint</em> space, while the <code>GaoKannanOhViswanath</code> algorithm selects the maximum <code>k</code>-th nearest distances among the two marginal spaces, which are in general not the same as the <code>k</code>-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in <code>GaoKannanOhViswanath</code>. We have modified it in a way that mimics <a href="#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> for continous data. Note that because of using the <code>log</code> function instead of <code>digamma</code>, there will be slight differences between the methods. See the source code for more details.</p></div></div><div class="admonition is-info"><header class="admonition-header">Explicitly convert your discrete data to floats</header><div class="admonition-body"><p>Even though the <code>GaoKannanOhViswanath</code> estimator is designed to handle discrete data, our implementation demands that all input data are <code>StateSpaceSet</code>s whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to <a href="../../associations/#Associations.association"><code>association</code></a>.</p></div></div><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(GaoKannanOhViswanath(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/mutual_info_estimators/GaoKannanOhViswanath.jl#L5-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.GaoOhViswanath" href="#Associations.GaoOhViswanath"><code>Associations.GaoOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoOhViswanath &lt;: MutualInformationEstimator</code></pre><p>The <code>GaoOhViswanath</code> is a mutual information estimator based on nearest neighbors, and is also called the bias-improved-KSG estimator, or BI-KSG, by (<a href="../../references/#Gao2018">Gao <em>et al.</em>, 2018</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute Shannon mutual information from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>The estimator is given by</p><p class="math-container">\[\begin{align*}
\hat{H}_{GAO}(X, Y)
&amp;= \hat{H}_{KSG}(X) + \hat{H}_{KSG}(Y) - \hat{H}_{KZL}(X, Y) \\
&amp;= \psi{(k)} +
    \log{(N)} +
    \log{
        \left(
            \dfrac{c_{d_{x}, 2} c_{d_{y}, 2}}{c_{d_{x} + d_{y}, 2}}
        \right)
    } - \\
    &amp; \dfrac{1}{N} \sum_{i=1}^N \left( \log{(n_{x, i, 2})} + \log{(n_{y, i, 2})} \right)
\end{align*},\]</p><p>where <span>$c_{d, 2} = \dfrac{\pi^{\frac{d}{2}}}{\Gamma{(\dfrac{d}{2} + 1)}}$</span> is the volume of a <span>$d$</span>-dimensional unit <span>$\mathcal{l}_2$</span>-ball.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(GaoOhViswanath(; k = 10), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/mutual_info_estimators/GaoOhViswanath.jl#L4-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.GaussianMI" href="#Associations.GaussianMI"><code>Associations.GaussianMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianMI &lt;: MutualInformationEstimator
GaussianMI(; normalize::Bool = false)</code></pre><p><code>GaussianMI</code> is a parametric estimator for Shannon mutual information.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute Shannon mutual information from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>Given <span>$d_x$</span>-dimensional and <span>$d_y$</span>-dimensional input data <code>X</code> and <code>Y</code>, <code>GaussianMI</code> first constructs the <span>$d_x + d_y$</span>-dimensional joint <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> <code>XY</code>. If <code>normalize == true</code>, then we follow the approach in Vejmelka &amp; Palus (2008)(<a href="../../references/#Vejmelka2008">Vejmelka and Paluš, 2008</a>) and transform each column in <code>XY</code> to have zero mean and unit standard deviation. If <code>normalize == false</code>, then the algorithm proceeds without normalization.</p><p>Next, the <code>C_{XY}</code>, the correlation matrix for the (normalized) joint data <code>XY</code> is computed. The mutual information estimate <code>GaussianMI</code> assumes the input variables are distributed according to normal distributions with zero means and unit standard deviations. Therefore, given <span>$d_x$</span>-dimensional and <span>$d_y$</span>-dimensional input data <code>X</code> and <code>Y</code>, <code>GaussianMI</code> first constructs the joint <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> <code>XY</code>, then transforms each column in <code>XY</code> to have zero mean and unit standard deviation, and finally computes the <code>\Sigma</code>, the correlation matrix for <code>XY</code>.</p><p>The mutual information estimated (for <code>normalize == false</code>) is then estimated as</p><p class="math-container">\[\hat{I}^S_{Gaussian}(X; Y) = \dfrac{1}{2}
\dfrac{ \det(\Sigma_X) \det(\Sigma_Y)) }{\det(\Sigma))},\]</p><p>where we <span>$\Sigma_X$</span> and <span>$\Sigma_Y$</span> appear in <span>$\Sigma$</span> as</p><p class="math-container">\[\Sigma = \begin{bmatrix}
\Sigma_{X} &amp; \Sigma^{&#39;}\\
\Sigma^{&#39;} &amp; \Sigma_{Y}
\end{bmatrix}.\]</p><p>If <code>normalize == true</code>, then the mutual information is estimated as</p><p class="math-container">\[\hat{I}^S_{Gaussian}(X; Y) = -\dfrac{1}{2} \sum_{i = 1}^{d_x + d_y} \sigma_i,\]</p><p>where <span>$\sigma_i$</span> are the eigenvalues for <span>$\Sigma$</span>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000); y = rand(rng, 10000)
association(GaussianMI(), x, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/mutual_info_estimators/GaussianMI.jl#L6-L70">source</a></section></article><h3 id="Conditional-mutual-information-estimators"><a class="docs-heading-anchor" href="#Conditional-mutual-information-estimators">Conditional mutual information estimators</a><a id="Conditional-mutual-information-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-mutual-information-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.ConditionalMutualInformationEstimator" href="#Associations.ConditionalMutualInformationEstimator"><code>Associations.ConditionalMutualInformationEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConditionalMutualInformationEstimator</code></pre><p>The supertype for dedicated <a href="../../associations/#Associations.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> estimators.</p><p><strong>Concrete implementations</strong></p><ul><li><a href="#Associations.FPVP"><code>FPVP</code></a></li><li><a href="#Associations.GaussianCMI"><code>GaussianCMI</code></a></li><li><a href="#Associations.MesnerShalizi"><code>MesnerShalizi</code></a></li><li><a href="#Associations.Rahimzamani"><code>Rahimzamani</code></a></li><li><a href="#Associations.PoczosSchneiderCMI"><code>PoczosSchneiderCMI</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/core.jl#L67-L79">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.GaussianCMI" href="#Associations.GaussianCMI"><code>Associations.GaussianCMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianCMI &lt;: MutualInformationEstimator
GaussianCMI(definition = CMIShannon(); normalize::Bool = false)</code></pre><p><code>GaussianCMI</code> is a parametric <a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>  (<a href="../../references/#Vejmelka2008">Vejmelka and Paluš, 2008</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a> from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p><code>GaussianCMI</code> estimates Shannon CMI through a sum of two mutual information terms that each are estimated using <a href="#Associations.GaussianMI"><code>GaussianMI</code></a> (the <code>normalize</code> keyword is the same as for <a href="#Associations.GaussianMI"><code>GaussianMI</code></a>):</p><p class="math-container">\[\hat{I}_{Gaussian}(X; Y | Z) = \hat{I}_{Gaussian}(X; Y, Z) - \hat{I}_{Gaussian}(X; Z)\]</p><p><strong>Examples</strong></p><ul><li><a href="../../examples/examples_associations/#example_CMIShannon_GaussianCMI">Example 1</a>. Estimating <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a>. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/conditional_mutual_info_estimators/GaussianCMI.jl#L4-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.FPVP" href="#Associations.FPVP"><code>Associations.FPVP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FPVP &lt;: ConditionalMutualInformationEstimator
FPVP(definition = CMIShannon(); k = 1, w = 0)</code></pre><p>The Frenzel-Pompe-Vejmelka-Paluš (or <code>FPVP</code> for short) <a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a> is used to estimate the conditional mutual information using a <code>k</code>-th nearest neighbor approach that is analogous to that of the <a href="#Associations.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> mutual information estimator from <a href="../../references/#Frenzel2007">Frenzel and Pompe (2007)</a> and <a href="../../references/#Vejmelka2008">Vejmelka and Paluš (2008)</a>.</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute <a href="../../associations/#Associations.ConditionalMutualInformation"><code>ConditionalMutualInformation</code></a> measure   from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Examples</strong></p><ul><li><a href="../../examples/examples_associations/#example_CMIShannon_FPVP">Example 1</a>: Estimating <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/conditional_mutual_info_estimators/FPVP.jl#L8-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.MesnerShalizi" href="#Associations.MesnerShalizi"><code>Associations.MesnerShalizi</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MesnerShalizi &lt;: ConditionalMutualInformationEstimator
MesnerShalizi(definition = CMIShannon(); k = 1, w = 0)</code></pre><p>The <code>MesnerShalizi</code> <a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a> is designed for data that can be mixtures of discrete and continuous data (<a href="../../references/#Mesner2020">Mesner and Shalizi, 2020</a>).</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a> from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(MesnerShalizi(; k = 10), x, z, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/conditional_mutual_info_estimators/MesnerShalizi.jl#L3-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.Rahimzamani" href="#Associations.Rahimzamani"><code>Associations.Rahimzamani</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Rahimzamani &lt;: ConditionalMutualInformationEstimator
Rahimzamani(k = 1, w = 0)</code></pre><p>The <code>Rahimzamani</code> <a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a> is designed for data that can be mixtures of discrete and continuous data (<a href="../../references/#Rahimzamani2018">Rahimzamani <em>et al.</em>, 2018</a>).</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute a <a href="../../associations/#Associations.CMIShannon"><code>CMIShannon</code></a> from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>This estimator is very similar to the <a href="#Associations.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a> mutual information estimator, but has been expanded to the conditional mutual information case.</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(Rahimzamani(; k = 10), x, z, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/conditional_mutual_info_estimators/Rahimzamani.jl#L3-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.PoczosSchneiderCMI" href="#Associations.PoczosSchneiderCMI"><code>Associations.PoczosSchneiderCMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PoczosSchneiderCMI &lt;: ConditionalMutualInformationEstimator
PoczosSchneiderCMI(definition = CMIRenyiPoczos(); k = 1, w = 0)</code></pre><p>The <code>PoczosSchneiderCMI</code> <a href="#Associations.ConditionalMutualInformationEstimator"><code>ConditionalMutualInformationEstimator</code></a>  computes conditional mutual informations using a <code>k</code>-th nearest neighbor approach (<a href="../../references/#Poczos2012">Póczos and Schneider, 2012</a>).</p><p><code>k</code> is the number of nearest neighbors. <code>w</code> is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute <a href="../../associations/#Associations.CMIRenyiPoczos"><code>CMIRenyiPoczos</code></a> from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
association(PoczosSchneiderCMI(CMIRenyiPoczos(), k = 10), x, z, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/conditional_mutual_info_estimators/PoczosSchneiderCMI.jl#L5-L36">source</a></section></article><h4 id="Transfer-entropy-estimators"><a class="docs-heading-anchor" href="#Transfer-entropy-estimators">Transfer entropy estimators</a><a id="Transfer-entropy-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Transfer-entropy-estimators" title="Permalink"></a></h4><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.TransferEntropyEstimator" href="#Associations.TransferEntropyEstimator"><code>Associations.TransferEntropyEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The supertype of all dedicated transfer entropy estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/transfer_entropy_estimators/transfer_entropy_estimators.jl#L3-L5">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.Zhu1" href="#Associations.Zhu1"><code>Associations.Zhu1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Zhu1 &lt;: TransferEntropyEstimator
Zhu1(k = 1, w = 0, base = MathConstants.e)</code></pre><p>The <code>Zhu1</code> transfer entropy estimator (<a href="../../references/#Zhu2015">Zhu <em>et al.</em>, 2015</a>) for normalized input data  (as described in <a href="../../references/#Zhu2015">Zhu <em>et al.</em> (2015)</a>) for both for pairwise and conditional transfer entropy.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.TEShannon"><code>TEShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute <a href="../../associations/#Associations.TEShannon"><code>TEShannon</code></a> from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Description</strong></p><p>This estimator approximates probabilities within hyperrectangles surrounding each point <code>xᵢ ∈ x</code> using using <code>k</code> nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in <a href="../../references/#Singh2003">Singh <em>et al.</em> (2003)</a>.</p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p><strong>Description</strong></p><p>For a given points in the joint embedding space <code>jᵢ</code>, this estimator first computes the distance <code>dᵢ</code> from <code>jᵢ</code> to its <code>k</code>-th nearest neighbor. Then, for each point <code>mₖ[i]</code> in the <code>k</code>-th marginal space, it counts the number of points within radius <code>dᵢ</code>.</p><p>The Shannon transfer entropy is then computed as</p><p class="math-container">\[TE_S(X \to Y) =
\psi(k) + \dfrac{1}{N} \sum_{i}^n
\left[
    \sum_{k=1}^3 \left( \psi(m_k[i] + 1) \right)
\right],\]</p><p>where the index <code>k</code> references the three marginal subspaces <code>T</code>, <code>TTf</code> and <code>ST</code> for which neighbor searches are performed. Here this estimator has been modified to allow for  conditioning too (a simple modification to <a href="../../references/#Lindner2011">Lindner <em>et al.</em> (2011)</a>&#39;s equation 5 and 6). </p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
est = Zhu1(TEShannon(), k = 10)
association(est, x, z, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/transfer_entropy_estimators/Zhu1.jl#L10-L68">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.Lindner" href="#Associations.Lindner"><code>Associations.Lindner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lindner &lt;: TransferEntropyEstimator
Lindner(definition = Shannon(); k = 1, w = 0, base = 2)</code></pre><p>The <code>Lindner</code> transfer entropy estimator (<a href="../../references/#Lindner2011">Lindner <em>et al.</em>, 2011</a>), which is also used in the Trentool MATLAB toolbox, and is based on nearest neighbor searches.</p><p><strong>Compatible definitions</strong></p><ul><li><a href="../../associations/#Associations.TEShannon"><code>TEShannon</code></a></li></ul><p><strong>Usage</strong></p><ul><li>Use with <a href="../../associations/#Associations.association"><code>association</code></a> to compute <a href="../../associations/#Associations.TEShannon"><code>TEShannon</code></a> from input data.</li><li>Use with some <a href="../../independence/#Associations.IndependenceTest"><code>IndependenceTest</code></a> to test for independence between variables.</li></ul><p><strong>Keyword parameters</strong></p><p><code>w</code> is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to <code>0</code>, meaning that only the point itself is excluded when searching for neighbours).</p><p>The estimator can be used both for pairwise and conditional transfer entropy estimation.</p><p><strong>Description</strong></p><p>For a given points in the joint embedding space <code>jᵢ</code>, this estimator first computes the distance <code>dᵢ</code> from <code>jᵢ</code> to its <code>k</code>-th nearest neighbor. Then, for each point <code>mₖ[i]</code> in the <code>k</code>-th marginal space, it counts the number of points within radius <code>dᵢ</code>.</p><p>The Shannon transfer entropy is then computed as</p><p class="math-container">\[TE_S(X \to Y) =
\psi(k) + \dfrac{1}{N} \sum_{i}^n
\left[
    \sum_{k=1}^3 \left( \psi(m_k[i] + 1) \right)
\right],\]</p><p>where the index <code>k</code> references the three marginal subspaces <code>T</code>, <code>TTf</code> and <code>ST</code> for which neighbor searches are performed. Here this estimator has been modified to allow for  conditioning too (a simple modification to <a href="../../references/#Lindner2011">Lindner <em>et al.</em> (2011)</a>&#39;s equation 5 and 6). </p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x = rand(rng, 10000)
y = rand(rng, 10000) .+ x
z = rand(rng, 10000) .+ y
est = Lindner(TEShannon(), k = 10)
association(est, x, z, y) # should be near 0 (and can be negative)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/transfer_entropy_estimators/Lindner.jl#L7-L62">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.SymbolicTransferEntropy" href="#Associations.SymbolicTransferEntropy"><code>Associations.SymbolicTransferEntropy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SymbolicTransferEntropy &lt;: TransferEntropyEstimator
SymbolicTransferEntropy(definition = TEShannon(); m = 3, τ = 1, 
    lt = ComplexityMeasures.isless_rand</code></pre><p>A convenience estimator for symbolic transfer entropy (<a href="../../references/#Staniek2008">Staniek and Lehnertz, 2008</a>).</p><p><strong>Compatible measures</strong></p><ul><li><a href="../../associations/#Associations.TEShannon"><code>TEShannon</code></a></li></ul><p><strong>Description</strong></p><p><a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.100.158101">Symbolic transfer entropy</a> consists of two simple steps. First, the input time series are encoded using <a href="../discretization_counts_probs_api/#ComplexityMeasures.codify"><code>codify</code></a> with the <a href="../discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> discretization and the <a href="../discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a> outcome space. This  transforms the input time series into integer time series. Transfer entropy entropy is then  estimated from the encoded time series by applying  </p><p>Transfer entropy is then estimated as usual on the encoded timeseries with the embedding dictated by <code>definition</code> and the <a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator.</p><p><strong>Examples</strong></p><ul><li><a href="../../examples/examples_associations/#example_TEShannon_SymbolicTransferEntropy">Example 1</a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/transfer_entropy_estimators/SymbolicTransferEntropy.jl#L4-L29">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.Hilbert" href="#Associations.Hilbert"><code>Associations.Hilbert</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Hilbert(est;
    source::InstantaneousSignalProperty = Phase(),
    target::InstantaneousSignalProperty = Phase(),
    cond::InstantaneousSignalProperty = Phase())
) &lt;: TransferDifferentialEntropyEstimator</code></pre><p>Compute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are obtained by first applying the Hilbert transform to each signal, then extracting the phases/amplitudes of the resulting complex numbers (<a href="../../references/#Palus2014">Paluš, 2014</a>). Original time series are thus transformed to instantaneous phase/amplitude time series. Transfer entropy is then estimated using the provided <code>est</code> on those phases/amplitudes (use e.g. <a href="../discretization_counts_probs_api/#ComplexityMeasures.ValueBinning"><code>ValueBinning</code></a>, or <a href="../discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a>).</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Details on estimation of the transfer entropy (conditional mutual information) following the phase/amplitude extraction step is not given in Palus (2014). Here, after instantaneous phases/amplitudes have been obtained, these are treated as regular time series, from which transfer entropy is then computed as usual.</p></div></div><p>See also: <a href="#Associations.Phase"><code>Phase</code></a>, <a href="#Associations.Amplitude"><code>Amplitude</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/transfer_entropy_estimators/Hilbert.jl#L21-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.Phase" href="#Associations.Phase"><code>Associations.Phase</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Phase &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous phases of a signal should be used. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/transfer_entropy_estimators/Hilbert.jl#L14-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Associations.Amplitude" href="#Associations.Amplitude"><code>Associations.Amplitude</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Amplitude &lt;: InstantaneousSignalProperty</code></pre><p>Indicates that the instantaneous amplitudes of a signal should be used. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/Associations.jl/blob/27a2bb86b2f23abb0392aabf74f23743551b6cbe/src/methods/information/estimators/transfer_entropy_estimators/Hilbert.jl#L8-L11">source</a></section></article><h2 id="tutorial_infomeasures"><a class="docs-heading-anchor" href="#tutorial_infomeasures">A small tutorial</a><a id="tutorial_infomeasures-1"></a><a class="docs-heading-anchor-permalink" href="#tutorial_infomeasures" title="Permalink"></a></h2><p>Associations.jl extends the single-variate information API in <a href="https://github.com/JuliaDynamics/ComplexityMeasures.jl">ComplexityMeasures.jl</a> to information measures of multiple variables. </p><h3 id="Definitions"><a class="docs-heading-anchor" href="#Definitions">Definitions</a><a id="Definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Definitions" title="Permalink"></a></h3><p>We define <strong>&quot;information measure&quot;</strong> as some functional of probability  mass functions or probability densities. This definition may or may not agree with literature usage, depending on the context. We made this choice pragmatically based on user-friendlyness and coding-friendlyness, but still trying to maintain some level of meaningful terminology.</p><div class="admonition is-info"><header class="admonition-header">A note on naming: the same name for different things?</header><div class="admonition-body"><p>Upon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use <em>the same name for different concepts</em>. For novices, and experienced practitioners too, this can be confusing. Our API clearly distinguishes between methods that are conceptually the same but named differently in the literature due to differing <em>estimation</em> strategies, from methods that actually have different definitions.</p><ul><li>Multiple, equivalent definitions occur for example for the Shannon mutual   information (MI; <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a>), which has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral   over a joint probability mass function (pmf), as a sum of three entropy terms, and as   a Kullback-Leibler divergence between the joint pmf and the product of the marginal   distributions. Since these definitions are all equivalent, we only need once type   (<a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a>) to represent them.</li><li>But Shannon MI is not the  only type of mutual information! For example, &quot;Tsallis mutual information&quot;   has been proposed in different variants by various authors. Despite sharing the   same name, these are actually <em>nonequivalent definitions</em>. We&#39;ve thus assigned   them entirely different measure names (e.g. <a href="../../associations/#Associations.MITsallisFuruichi"><code>MITsallisFuruichi</code></a> and   <a href="../../associations/#Associations.MITsallisMartin"><code>MITsallisMartin</code></a>), with the author name at the end.</li></ul></div></div><h3 id="Basic-estimation-strategy"><a class="docs-heading-anchor" href="#Basic-estimation-strategy">Basic estimation strategy</a><a id="Basic-estimation-strategy-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-estimation-strategy" title="Permalink"></a></h3><p>To <em>estimate</em> a multivariate information measure in practice, you must first specify the <em>definition</em> of the measure, which is then used as input to an  <em>estimator</em> of that measure. This estimator is then given to <a href="../../associations/#Associations.association"><code>association</code></a>. Every information measure has at least one estimator: <a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a>. Many measures have several additional estimators, and an overview can be found in the docstring for <a href="#Associations.MultivariateInformationMeasureEstimator"><code>MultivariateInformationMeasureEstimator</code></a>.</p><h3 id="Distances/divergences"><a class="docs-heading-anchor" href="#Distances/divergences">Distances/divergences</a><a id="Distances/divergences-1"></a><a class="docs-heading-anchor-permalink" href="#Distances/divergences" title="Permalink"></a></h3><p>There are many information measures in the literature that aim to quantify the  distance/divergence between two probability mass functions (pmf) or densities. You can  find those that we implement <a href="../../associations/#divergences_and_distances">here</a>.</p><p>As an example, let&#39;s quantify the <a href="../../associations/#Associations.KLDivergence"><code>KLDivergence</code></a> between two probability  mass functions estimated by symbolizing two input vectors <code>x</code> and <code>y</code> using  <a href="../discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns"><code>OrdinalPatterns</code></a>. Since the discrete <a href="../../associations/#Associations.KLDivergence"><code>KLDivergence</code></a> can be  expressed as a function of a joint pmf, we can use the <a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator.</p><pre><code class="language-julia hljs">using Associations
using Random; rng = MersenneTwister(1234)
x, y = rand(rng, 1000), rand(rng, 1000)

# Specify a discretization. We discretize per column.
disc = CodifyVariables(OrdinalPatterns(m=2))

def = KLDivergence()
est = JointProbabilities(def, disc)
association(est, x, y) # should be close to 0</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.00034984097811277537</code></pre><p>Divergences are examples of <em>asymmetric</em> information measures, which we can see by  flipping the order of the input data.</p><pre><code class="language-julia hljs">association(est, y, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.0003498615463219182</code></pre><h3 id="Conditional-entropies"><a class="docs-heading-anchor" href="#Conditional-entropies">Conditional entropies</a><a id="Conditional-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Conditional-entropies" title="Permalink"></a></h3><p><a href="../../associations/#conditional_entropies">Conditional entropies</a> are another example of asymmetric information measures. They all have in common that  they are functions of a joint pmf, and can therefore also be estimated using the <a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator. This time, we&#39;ll use a rectangular binning with 3 bins along each dimension to discretize the data.</p><pre><code class="language-julia hljs">using Associations
using Random; rng = Xoshiro(1234)

x, y = randn(rng, 1000), randn(rng, 1000)
disc = CodifyVariables(ValueBinning(3))
def = ConditionalEntropyShannon(base = 2)
est = JointProbabilities(def, disc)
association(est, x, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.17479123360062</code></pre><h3 id="Joint-entropies"><a class="docs-heading-anchor" href="#Joint-entropies">Joint entropies</a><a id="Joint-entropies-1"></a><a class="docs-heading-anchor-permalink" href="#Joint-entropies" title="Permalink"></a></h3><p><a href="../../associations/#joint_entropies">Joint entropies</a>, on the other hand, are <em>symmetric</em>. Joint entropies are functionals of a joint pmf, so we can still use the <a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator. This time, we use a <a href="../discretization_counts_probs_api/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a> based discretization.</p><pre><code class="language-julia hljs">using Associations
using Random; rng = Xoshiro(1234)

x, y = randn(rng, 1000), randn(rng, 1000)
disc = CodifyVariables(Dispersion())
est = JointProbabilities(JointEntropyShannon(base = 2), disc)
association(est, x, y) ≈ association(est, y, x) # should be true</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><h3 id="Mutual-informations"><a class="docs-heading-anchor" href="#Mutual-informations">Mutual informations</a><a id="Mutual-informations-1"></a><a class="docs-heading-anchor-permalink" href="#Mutual-informations" title="Permalink"></a></h3><p>Mutual informations, in particular <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a> is an often-used symmetric  measure for quantifing the (possibly nonlinear) association between variables. It appears in both  discrete and differential form, and can be estimated in a multitude of ways. For  example, one can use dedicated <a href="#Associations.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s such as  <a href="#Associations.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a> or <a href="#Associations.GaussianMI"><code>GaussianMI</code></a>:</p><pre><code class="language-julia hljs">using Associations
using StateSpaceSets
# We&#39;ll construct two state space sets, to illustrate how we can discretize
# multidimensional inputs using `CodifyPoints`.
x, y = StateSpaceSet(rand(rng, 1000, 2)), StateSpaceSet(rand(rng, 1000, 2))
est = KSG1(MIShannon(base = 2), k = 10)
association(est, x, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-0.021186834467335727</code></pre><p>The result should be symmetric:</p><pre><code class="language-julia hljs">association(est, x, y) ≈ association(est, y, x) # should be true</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>One can also estimate mutual information using the <a href="#Associations.EntropyDecomposition"><code>EntropyDecomposition</code></a>  estimator, or (like above) using the <a href="#Associations.JointProbabilities"><code>JointProbabilities</code></a> estimator. Let&#39;s construct a differential entropy based estimator based on the <a href="../information_single_variable_api/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a> estimator.</p><pre><code class="language-julia hljs">est_diff = EntropyDecomposition(MIShannon(base = 2), Kraskov(Shannon(), k=10))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">EntropyDecomposition{MIShannon{Int64}, Kraskov{Shannon{Int64}, Int64}, Nothing, Nothing}(MIShannon{Int64}(2), Kraskov(definition = Shannon(base = 2), k = 10, w = 0, base = 2), nothing, nothing)</code></pre><pre><code class="language-julia hljs">association(est_diff, x, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-0.3826780635557282</code></pre><p>We can also construct a discrete entropy based estimator based on e.g. <a href="../information_single_variable_api/#ComplexityMeasures.PlugIn"><code>PlugIn</code></a> estimator of <a href="../information_single_variable_api/#ComplexityMeasures.Shannon"><code>Shannon</code></a> entropy.</p><pre><code class="language-julia hljs"># We know that `x` and `y` were generated from a uniform distribution above,
# so we set the minimum and maximum values of the encoding to 0 and 1,
# respectively.
encoding = RelativeMeanEncoding(0.0, 1.0; n = 4)
disc = CodifyPoints(encoding)
est_disc = JointProbabilities(MIShannon(base = 2), disc)
association(est_disc, x, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.00494036679990649</code></pre><h3 id="JointProbabilities:-fine-grained-discretization-control"><a class="docs-heading-anchor" href="#JointProbabilities:-fine-grained-discretization-control">JointProbabilities: fine grained discretization control</a><a id="JointProbabilities:-fine-grained-discretization-control-1"></a><a class="docs-heading-anchor-permalink" href="#JointProbabilities:-fine-grained-discretization-control" title="Permalink"></a></h3><p>For numerical data, we can estimate both counts and probabilities using <a href="../discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> with any count-based <a href="../discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace"><code>OutcomeSpace</code></a>. Here, we&#39;ll estimate <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a> using  one type of encoding for the first variable, and another type of encoding for the second variable.</p><pre><code class="language-julia hljs">using Associations
using Random; rng = Xoshiro(1234)
x, y = rand(rng, 100), rand(rng, 100)

# We must use outcome spaces with the same number of total outcomes.
# This works becuase these outcome spaces construct embedding points
# (&quot;windows&quot;) in the same way/order; be careful if that isn&#39;t the case!
ox = CosineSimilarityBinning(nbins = factorial(3))
oy = OrdinalPatterns(m = 3)

# Now estimate mutual information
discretization = CodifyVariables((ox, oy))
est = JointProbabilities(MIShannon(), discretization)
association(est, x, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.10752906020545049</code></pre><p>For more fine-grained control than <a href="../discretization_counts_probs_api/#Associations.CodifyVariables"><code>CodifyVariables</code></a> can offer, we can use <a href="../discretization_counts_probs_api/#Associations.CodifyPoints"><code>CodifyPoints</code></a> with one or several <a href="../discretization_counts_probs_api/#ComplexityMeasures.Encoding"><code>Encoding</code></a>s. Here&#39;s how we can estimate <a href="../../associations/#Associations.MIShannon"><code>MIShannon</code></a> one multivariate input  data by discretizing each input variable in arbitrary ways.</p><pre><code class="language-julia hljs">using Associations
using Random; rng = Xoshiro(1234)
x, y = StateSpaceSet(rand(rng, 1000, 2)), StateSpaceSet(rand(rng, 1000, 3))

 # min/max of the `rand` call is 0 and 1
precise = true # precise bin edges
r = range(0, 1; length = 3)
binning = FixedRectangularBinning(r, dimension(x), precise)
encoding_x = RectangularBinEncoding(binning, x)
encoding_y = CombinationEncoding(RelativeMeanEncoding(0.0, 1, n = 2), OrdinalPatternEncoding(3))
discretization = CodifyPoints(encoding_x, encoding_y)

# Now estimate mutual information
est = JointProbabilities(MIShannon(), discretization)
association(est, x, y)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.025991777755365167</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../information_single_variable_api/">« Single-variable information API</a><a class="docs-footer-nextpage" href="../cross_map_api/">Cross-map API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Sunday 4 August 2024 11:06">Sunday 4 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
