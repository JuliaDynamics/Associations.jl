{
    "docs": [
        {
            "location": "/", 
            "text": "CausalityTools.jl\n\n\nCausalityTools\n is a Julia package providing algorithms for detecting causal relations in complex systems based on time series data.\n\n\n\n\nGoals\n\n\n\n\nProvide a comprehensive, easy-to-use framework\n for the detection of directional causal influences in complex dynamical systems from time series.\n\n\nFunctional and efficient implementations\n of causality detection algorithm, with thorough documentation and references to primary literature.\n\n\nExtensive library of example dynamical systems\n for testing algorithm performance.\n\n\nWorked examples\n for all algorithms.\n\n\n\n\n\n\nPackage status\n\n\nThe package and documentation is under active development. Breaking changes may occur in CausalityTools and its dependencies until the 1.0 release.\n\n\n\n\nPackage structure\n\n\nCausalityTools.jl\n brings together the following packages into one environment:\n\n\n\n\n\n\n\n\npackage\n\n\nfunctionality\n\n\nversion\n\n\nbuild\n\n\n\n\n\n\n\n\n\n\nStateSpaceReconstruction.jl\n\n\nFully flexible state space reconstructions (embeddings), partitioning routines (variable-width rectangular, and triangulations), and partition refinement (equal-volume splitting of  simplices).\n\n\n0.3.1\n\n\n\n\n\n\n\n\nTimeseriesSurrogates.jl\n\n\nGenerate surrogate data from time series.\n\n\n0.2.1\n\n\n\n\n\n\n\n\nTransferEntropy.jl\n\n\nTransfer entropy estimators.\n\n\n0.3.1\n\n\n\n\n\n\n\n\nPerronFrobenius.jl\n\n\nTransfer (Perron-Frobenius) operator estimators.\n\n\n0.2.2\n\n\n\n\n\n\n\n\nSimplices.jl\n\n\nExact simplex intersections in N dimensions.\n\n\n0.2.2\n\n\n\n\n\n\n\n\nCrossMappings.jl\n\n\nExact simplex intersections in N dimensions.\n\n\n0.2.0\n\n\n\n\n\n\n\n\n\n\n\n\nContributors\n\n\n\n\nKristian Agas\u00f8ster Haaga (\n@kahaaga\n)\n\n\nDavid Diego (\n@susydiegolas\n)\n\n\nTor Einar M\u00f8ller (\n@tormolle\n)\n\n\n\n\n\n\nRelated software\n\n\n\n\nDynamicalSystems.jl\n provides a range of tools for exploring nonlinear dynamics and chaos, both for synthetic and observed systems. We provide seamless interaction with \nDataset\n outputs from DynamicalSystems.  Most of our example systems are also implemented as \nDiscreteDynamicalSystem\ns or \nContinuousDynamicalSystems\n from DynamicalSystems.", 
            "title": "Home"
        }, 
        {
            "location": "/#causalitytoolsjl", 
            "text": "CausalityTools  is a Julia package providing algorithms for detecting causal relations in complex systems based on time series data.", 
            "title": "CausalityTools.jl"
        }, 
        {
            "location": "/#goals", 
            "text": "Provide a comprehensive, easy-to-use framework  for the detection of directional causal influences in complex dynamical systems from time series.  Functional and efficient implementations  of causality detection algorithm, with thorough documentation and references to primary literature.  Extensive library of example dynamical systems  for testing algorithm performance.  Worked examples  for all algorithms.", 
            "title": "Goals"
        }, 
        {
            "location": "/#package-status", 
            "text": "The package and documentation is under active development. Breaking changes may occur in CausalityTools and its dependencies until the 1.0 release.", 
            "title": "Package status"
        }, 
        {
            "location": "/#package-structure", 
            "text": "CausalityTools.jl  brings together the following packages into one environment:     package  functionality  version  build      StateSpaceReconstruction.jl  Fully flexible state space reconstructions (embeddings), partitioning routines (variable-width rectangular, and triangulations), and partition refinement (equal-volume splitting of  simplices).  0.3.1     TimeseriesSurrogates.jl  Generate surrogate data from time series.  0.2.1     TransferEntropy.jl  Transfer entropy estimators.  0.3.1     PerronFrobenius.jl  Transfer (Perron-Frobenius) operator estimators.  0.2.2     Simplices.jl  Exact simplex intersections in N dimensions.  0.2.2     CrossMappings.jl  Exact simplex intersections in N dimensions.  0.2.0", 
            "title": "Package structure"
        }, 
        {
            "location": "/#contributors", 
            "text": "Kristian Agas\u00f8ster Haaga ( @kahaaga )  David Diego ( @susydiegolas )  Tor Einar M\u00f8ller ( @tormolle )", 
            "title": "Contributors"
        }, 
        {
            "location": "/#related-software", 
            "text": "DynamicalSystems.jl  provides a range of tools for exploring nonlinear dynamics and chaos, both for synthetic and observed systems. We provide seamless interaction with  Dataset  outputs from DynamicalSystems.  Most of our example systems are also implemented as  DiscreteDynamicalSystem s or  ContinuousDynamicalSystems  from DynamicalSystems.", 
            "title": "Related software"
        }, 
        {
            "location": "/discretization/", 
            "text": "Discretization schemes\n\n\nAcross the package, there are many functions that accept the \n\u03f5\n argument. This is an indication that the underlying algorithm in some way involves a discretization of a set of points or a delay embedding.  \n\n\n\n\nControlling the partitioning\n\n\nCurrently, there are four different ways of partitioning an embedding. The discretization scheme is controlled by \n\u03f5\n, and the following \n\u03f5\n will work:\n\n\n\n\n\u03f5::Int\n divides each axis into \n\u03f5\n intervals of the same size.\n\n\n\u03f5::Float\n divides each axis into intervals of size \n\u03f5\n.\n\n\n\u03f5::Vector{Int}\n divides the i-th axis into \n\u03f5\u1d62\n intervals of the same size.\n\n\n\u03f5::Vector{Float64}\n divides the i-th axis into intervals of size \n\u03f5\u1d62\n.\n\n\n\n\n\n\nWhere are partitions used?\n\n\nOne example of a binning based method is the \ntransferoperator_grid\n estimator for approximating the transfer (Perron-Frobenius) operator. The \ntransferentropy_transferoperator_grid\n and \ntransferentropy_visitfreq\n transfer entropy estimators also both derive probability distributions over partitioned delay embeddings.", 
            "title": "Discretization schemes"
        }, 
        {
            "location": "/discretization/#discretization-schemes", 
            "text": "Across the package, there are many functions that accept the  \u03f5  argument. This is an indication that the underlying algorithm in some way involves a discretization of a set of points or a delay embedding.", 
            "title": "Discretization schemes"
        }, 
        {
            "location": "/discretization/#controlling-the-partitioning", 
            "text": "Currently, there are four different ways of partitioning an embedding. The discretization scheme is controlled by  \u03f5 , and the following  \u03f5  will work:   \u03f5::Int  divides each axis into  \u03f5  intervals of the same size.  \u03f5::Float  divides each axis into intervals of size  \u03f5 .  \u03f5::Vector{Int}  divides the i-th axis into  \u03f5\u1d62  intervals of the same size.  \u03f5::Vector{Float64}  divides the i-th axis into intervals of size  \u03f5\u1d62 .", 
            "title": "Controlling the partitioning"
        }, 
        {
            "location": "/discretization/#where-are-partitions-used", 
            "text": "One example of a binning based method is the  transferoperator_grid  estimator for approximating the transfer (Perron-Frobenius) operator. The  transferentropy_transferoperator_grid  and  transferentropy_visitfreq  transfer entropy estimators also both derive probability distributions over partitioned delay embeddings.", 
            "title": "Where are partitions used?"
        }, 
        {
            "location": "/transferoperator/transferoperator_grid_docs/", 
            "text": "Transfer operator approximation\n\n\n\n\nGrid estimator\n\n\nFor longer time series, we use a rectangular grid to discretize the embedding. This approach is relatively fast, because no intersections between volumes have to be explicitly computed.\n\n\n\n\nDocumentation\n\n\n#\n\n\nPerronFrobenius.transferoperator_grid\n \n \nFunction\n.\n\n\ntransferoperator_grid(E::Embeddings.AbstractEmbedding,\n    \u03f5::Union{Int, Float64, Vector{Int}, Vector{Float64}};\n    allocate_frac::Float64 = 1.0,\n    boundary_condition = :exclude) -\n\n    RectangularBinningTransferOperator\n\n\n\n\nEstimates the transfer operator for an embedding.\n\n\nDiscretization scheme\n\n\nThe binning scheme is specified by \n\u03f5\n, and the following \n\u03f5\n are valid:\n\n\n\n\n\u03f5::Int\n divides each axis into \n\u03f5\n intervals of the same size.\n\n\n\u03f5::Float\n divides each axis into intervals of size \n\u03f5\n.\n\n\n\u03f5::Vector{Int}\n divides the i-th axis into \n\u03f5\u1d62\n intervals of the same size.\n\n\n\u03f5::Vector{Float64}\n divides the i-th axis into intervals of size \n\u03f5\u1d62\n.\n\n\n\n\nMemory allocation\n\n\nallocate_frac\n controls what fraction of the total number of possible transitions ($n_{states}^2$) we pre-allocate for. For short time series, you should leave this at the default value \n1.0\n. However, for longer time series, the transition matrix is sparse (usually, less than $10\\%$ of the entries are nonzero). In these case, you can safely lower \nallocate_frac\n.\n\n\nBoundary conditions (dealing with the last point)\n\n\nboundary_condition\n controls what to do with the forward map of the last point of the embedding. The default, \n:exclude\n, simply ignores the last point.\n\n\ntransferoperator_grid(pts::AbstractArray{T, 2},\n    \u03f5::Union{Int, Float64, Vector{Int}, Vector{Float64}};\n    allocate_frac::Float64 = 1.0,\n    boundary_condition = :none) where T -\n\n    RectangularBinningTransferOperator\n\n\n\n\nEstimates the transfer operator for a set of points, given as a     \ndim\n-by-\nnpoints\n array.\n\n\nDiscretization scheme\n\n\nThe binning scheme is specified by \n\u03f5\n, and the following \n\u03f5\n are valid:\n\n\n\n\n\u03f5::Int\n divides each axis into \n\u03f5\n intervals of the same size.\n\n\n\u03f5::Float\n divides each axis into intervals of size \n\u03f5\n.\n\n\n\u03f5::Vector{Int}\n divides the i-th axis into \n\u03f5\u1d62\n intervals of the same size.\n\n\n\u03f5::Vector{Float64}\n divides the i-th axis into intervals of size \n\u03f5\u1d62\n.\n\n\n\n\nMemory allocation\n\n\nallocate_frac\n controls what fraction of the total number of possible transitions ($n_{states}^2$) we pre-allocate for. For short time series, you should leave this at the default value \n1.0\n. However, for longer time series, the transition matrix is sparse (usually, less than $10\\%$ of the entries are nonzero). In these case, you can safely lower \nallocate_frac\n.\n\n\nBoundary conditions (dealing with the last point)\n\n\nboundary_condition\n controls what to do with the forward map of the last point of the embedding. The default, \n:exclude\n, simply ignores the last point.", 
            "title": "Grid estimator"
        }, 
        {
            "location": "/transferoperator/transferoperator_grid_docs/#transfer-operator-approximation", 
            "text": "", 
            "title": "Transfer operator approximation"
        }, 
        {
            "location": "/transferoperator/transferoperator_grid_docs/#grid-estimator", 
            "text": "For longer time series, we use a rectangular grid to discretize the embedding. This approach is relatively fast, because no intersections between volumes have to be explicitly computed.", 
            "title": "Grid estimator"
        }, 
        {
            "location": "/transferoperator/transferoperator_grid_docs/#documentation", 
            "text": "#  PerronFrobenius.transferoperator_grid     Function .  transferoperator_grid(E::Embeddings.AbstractEmbedding,\n    \u03f5::Union{Int, Float64, Vector{Int}, Vector{Float64}};\n    allocate_frac::Float64 = 1.0,\n    boundary_condition = :exclude) - \n    RectangularBinningTransferOperator  Estimates the transfer operator for an embedding.  Discretization scheme  The binning scheme is specified by  \u03f5 , and the following  \u03f5  are valid:   \u03f5::Int  divides each axis into  \u03f5  intervals of the same size.  \u03f5::Float  divides each axis into intervals of size  \u03f5 .  \u03f5::Vector{Int}  divides the i-th axis into  \u03f5\u1d62  intervals of the same size.  \u03f5::Vector{Float64}  divides the i-th axis into intervals of size  \u03f5\u1d62 .   Memory allocation  allocate_frac  controls what fraction of the total number of possible transitions ($n_{states}^2$) we pre-allocate for. For short time series, you should leave this at the default value  1.0 . However, for longer time series, the transition matrix is sparse (usually, less than $10\\%$ of the entries are nonzero). In these case, you can safely lower  allocate_frac .  Boundary conditions (dealing with the last point)  boundary_condition  controls what to do with the forward map of the last point of the embedding. The default,  :exclude , simply ignores the last point.  transferoperator_grid(pts::AbstractArray{T, 2},\n    \u03f5::Union{Int, Float64, Vector{Int}, Vector{Float64}};\n    allocate_frac::Float64 = 1.0,\n    boundary_condition = :none) where T - \n    RectangularBinningTransferOperator  Estimates the transfer operator for a set of points, given as a      dim -by- npoints  array.  Discretization scheme  The binning scheme is specified by  \u03f5 , and the following  \u03f5  are valid:   \u03f5::Int  divides each axis into  \u03f5  intervals of the same size.  \u03f5::Float  divides each axis into intervals of size  \u03f5 .  \u03f5::Vector{Int}  divides the i-th axis into  \u03f5\u1d62  intervals of the same size.  \u03f5::Vector{Float64}  divides the i-th axis into intervals of size  \u03f5\u1d62 .   Memory allocation  allocate_frac  controls what fraction of the total number of possible transitions ($n_{states}^2$) we pre-allocate for. For short time series, you should leave this at the default value  1.0 . However, for longer time series, the transition matrix is sparse (usually, less than $10\\%$ of the entries are nonzero). In these case, you can safely lower  allocate_frac .  Boundary conditions (dealing with the last point)  boundary_condition  controls what to do with the forward map of the last point of the embedding. The default,  :exclude , simply ignores the last point.", 
            "title": "Documentation"
        }, 
        {
            "location": "/invariantmeasure/invariantmeasure_docs/", 
            "text": "Invariant measures\n\n\nWhen the transfer operator has been computed for a state space discretization, we can derive an invariant measure (probability density) over the states of the system. \n\n\n\n\nDocumentation\n\n\n#\n\n\nPerronFrobenius.invariantmeasure\n \n \nFunction\n.\n\n\nleft_eigenvector(to::AbstractTransferOperator;\n        N::Int = 200,\n        tolerance::Float64 = 1e-8,\n        delta::Float64 = 1e-8)\n\n\n\n\nCompute the invariant probability distribution from a \nTransferOperator\n.\n\n\nComputing an invariant probability distribution\n\n\nThe distribution is taken as a left eigenvector of the transfer matrix, obtained by repeated application of the transfer operator on a randomly initialised distribution until the probability distribution converges.\n\n\nKeyword arguments\n\n\n\n\nN\n: the maximum number of iterations.\n\n\ntolerance\n and `delta: decides when convergence is achieved.", 
            "title": "Invariant measures"
        }, 
        {
            "location": "/invariantmeasure/invariantmeasure_docs/#invariant-measures", 
            "text": "When the transfer operator has been computed for a state space discretization, we can derive an invariant measure (probability density) over the states of the system.", 
            "title": "Invariant measures"
        }, 
        {
            "location": "/invariantmeasure/invariantmeasure_docs/#documentation", 
            "text": "#  PerronFrobenius.invariantmeasure     Function .  left_eigenvector(to::AbstractTransferOperator;\n        N::Int = 200,\n        tolerance::Float64 = 1e-8,\n        delta::Float64 = 1e-8)  Compute the invariant probability distribution from a  TransferOperator .  Computing an invariant probability distribution  The distribution is taken as a left eigenvector of the transfer matrix, obtained by repeated application of the transfer operator on a randomly initialised distribution until the probability distribution converges.  Keyword arguments   N : the maximum number of iterations.  tolerance  and `delta: decides when convergence is achieved.", 
            "title": "Documentation"
        }, 
        {
            "location": "/transferentropy/wrapper_TE/", 
            "text": "Transfer entropy (TE) estimators\n\n\n\n\nCommon interface\n\n\nWe provide a common \ntransferentropy(driver, response; kwargs...)\n function, which may be nice for initial exploration of your data.\n\n\nNote: the common interface uses default values for estimator parameters that may not be suited for your data. For real applications, it is highly recommended to use the underlying estimators directly.\n\n\n\n\nDocumentation\n\n\n#\n\n\nCausalityTools.te\n \n \nFunction\n.\n\n\nte(driver, response;\n    estimator = :tetogrid,\n    E = nothing, v = nothing,\n    \u03f5 = nothing, n_\u03f5 = 5, \u03bd = 1, \u03c4 = 1, dim = 3,\n    k1 = 2, k2 = 3, distance_metric = Chebyshev(),\n    which_is_surr = :none,\n    surr_func = aaft,\n    min_numbins = nothing,\n    max_numbins = nothing)\n\n\n\n\nCompute transfer entropy from a \ndriver\n time series to a \nresponse\n time series.\n\n\nMethod\n\n\nmethod\n sets the transfer entropy estimator to use. There are two types of estimators: grid-based approaches, and nearest-neighbor based approaches.\n\n\nEmbedding instructions\n\n\nSpecify embedding and instructions for how to compute marginal entropies are with \nE::AbstractEmbedding\n and \nv::TEVars\n. If either is missing, the data is embedded using the provided forward prediction lag \n\u03bd\n, with embedding dimension \ndim\n and embedding lag \n\u03b7\n.\n\n\nGrid-based estimators\n\n\n\n\n:transferentropy_transferoperator_grid\n, or \n:tetogrid\n. Grid-based transfer entropy   estimator based on the transfer operator.\n\n\n::transferentropy_visitfreq\n, or \n:tefreq\n. Simple visitation frequency based estimator.\n\n\n\n\nFor the grid based TE estimators, \n\u03f5\n sets the bin size. If \n\u03f5 == nothing\n, then the algorithm uses a bin size corresponding to a number of subdivisions \nN\n along each axis so that \nN \u2266 npoints^(1/(dim+1))\n. Transfer entropy is then computed as an average over \nn_\u03f5\n different bin sizes corresponding to \nN-1\n to \nN\n subdivisions along each axis. If \n\u03f5\n is specified and consists of multiple bin sizes, an average TE estimate over the bin sizes is returened. If \n\u03f5\n is a single value, then TE is estimated for that bin size only.\n\n\nNearest neighbor based estimator.\n\n\n\n\n:transferentropy_kraskov\n, \n:tekraskov\n, or \n:tekNN\n. A nearest-neighbor   based estimator that computes transfer entropy as the sum of two mutual   information terms. \nk1\n and \nk2\n sets the   number of nearest neighbors used to estimate the mutual information terms.   \ndistance_metric\n sets the distance metric (must be an instance   of a valid metric from \nDistances.jl\n).\n\n\n\n\nSurrogate analysis\n\n\nA surrogate analysis will be run if \nwhich_is_surr\n is set to     \n:both\n, \n:driver\n, \n:response\n. Default is \n:none\n.\n\n\n\n\nwhich_is_surr = :both\n will replace both time series with a surrogate.\n\n\nwhich_is_surr = :driver\n will replace the driver time series with a surrogate.\n\n\nwhich_is_surr = :response\n will replace the response time series with a surrogate.\n\n\n\n\nThe type of surrogate must be either \nrandomshuffle\n, \nrandomphases\n, \nrandomamplitudes\n, \naaft\n or \niaaft\n.\n\n\nsource", 
            "title": "Common interface"
        }, 
        {
            "location": "/transferentropy/wrapper_TE/#transfer-entropy-te-estimators", 
            "text": "", 
            "title": "Transfer entropy (TE) estimators"
        }, 
        {
            "location": "/transferentropy/wrapper_TE/#common-interface", 
            "text": "We provide a common  transferentropy(driver, response; kwargs...)  function, which may be nice for initial exploration of your data.  Note: the common interface uses default values for estimator parameters that may not be suited for your data. For real applications, it is highly recommended to use the underlying estimators directly.", 
            "title": "Common interface"
        }, 
        {
            "location": "/transferentropy/wrapper_TE/#documentation", 
            "text": "#  CausalityTools.te     Function .  te(driver, response;\n    estimator = :tetogrid,\n    E = nothing, v = nothing,\n    \u03f5 = nothing, n_\u03f5 = 5, \u03bd = 1, \u03c4 = 1, dim = 3,\n    k1 = 2, k2 = 3, distance_metric = Chebyshev(),\n    which_is_surr = :none,\n    surr_func = aaft,\n    min_numbins = nothing,\n    max_numbins = nothing)  Compute transfer entropy from a  driver  time series to a  response  time series.  Method  method  sets the transfer entropy estimator to use. There are two types of estimators: grid-based approaches, and nearest-neighbor based approaches.  Embedding instructions  Specify embedding and instructions for how to compute marginal entropies are with  E::AbstractEmbedding  and  v::TEVars . If either is missing, the data is embedded using the provided forward prediction lag  \u03bd , with embedding dimension  dim  and embedding lag  \u03b7 .  Grid-based estimators   :transferentropy_transferoperator_grid , or  :tetogrid . Grid-based transfer entropy   estimator based on the transfer operator.  ::transferentropy_visitfreq , or  :tefreq . Simple visitation frequency based estimator.   For the grid based TE estimators,  \u03f5  sets the bin size. If  \u03f5 == nothing , then the algorithm uses a bin size corresponding to a number of subdivisions  N  along each axis so that  N \u2266 npoints^(1/(dim+1)) . Transfer entropy is then computed as an average over  n_\u03f5  different bin sizes corresponding to  N-1  to  N  subdivisions along each axis. If  \u03f5  is specified and consists of multiple bin sizes, an average TE estimate over the bin sizes is returened. If  \u03f5  is a single value, then TE is estimated for that bin size only.  Nearest neighbor based estimator.   :transferentropy_kraskov ,  :tekraskov , or  :tekNN . A nearest-neighbor   based estimator that computes transfer entropy as the sum of two mutual   information terms.  k1  and  k2  sets the   number of nearest neighbors used to estimate the mutual information terms.    distance_metric  sets the distance metric (must be an instance   of a valid metric from  Distances.jl ).   Surrogate analysis  A surrogate analysis will be run if  which_is_surr  is set to      :both ,  :driver ,  :response . Default is  :none .   which_is_surr = :both  will replace both time series with a surrogate.  which_is_surr = :driver  will replace the driver time series with a surrogate.  which_is_surr = :response  will replace the response time series with a surrogate.   The type of surrogate must be either  randomshuffle ,  randomphases ,  randomamplitudes ,  aaft  or  iaaft .  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/transferentropy/estimator_TE_kNN/", 
            "text": "Transfer entropy (TE) estimators\n\n\n\n\nk Nearest neighbours (kNN) estimator\n\n\nThe kNN estimator computes transfer entropy as the sum of two mutual information terms, which are computed using the Kraskov estimator of mutual information (\nKraskov et al., 2004\n). Implemented for \nDiego et al. (2018)\n.\n\n\n\n\nDocumentation\n\n\n#\n\n\nTransferEntropy.tekNN\n \n \nFunction\n.\n\n\ntransferentropy_kraskov(points::AbstractArray{T, 2}, k1::Int, k2::Int,\n    v::TEVars; metric = Chebyshev())\n\n\n\n\nCompute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].\n\n\nArguments\n\n\n\n\npoints\n: The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size \ndim\n-by-\nn\n points.\n\n\nk1\n: The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nk2\n: The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   if $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nv\n: A \nTEVars\n instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.\n\n\n\n\nKeyword arguments\n\n\n\n\nmetric\n: The distance metric. Must be a valid metric from \nDistances.jl\n.\n\n\n\n\nReferences\n\n\n\n\nKraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.\n\n\n\n\ntransferentropy_kraskov(points::AbstractArray{T, 2}, k1::Int, k2::Int,\n    target_future::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    target_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    source_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    conditioned_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}};\n    metric = Chebyshev()) where T\n\n\n\n\nCompute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].\n\n\nArguments\n\n\n\n\npoints\n: The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size \ndim\n-by-\nn\n points.\n\n\nk1\n: The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nk2\n: The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   if $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nv\n: A \nTEVars\n instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.\n\n\ntarget_future\n: Which rows of \npoints\n correspond to future values of the   target variable?\n\n\ntarget_presentpast\n: Which rows of \npoints\n correspond to present and past   values of the target variable?\n\n\nsource_presentpast\n: Which rows of \npoints\n correspond to present and past   values of the source variable?\n\n\nconditioned_presentpast\n: Which rows of \npoints\n correspond to present and   past values of conditional variables?\n\n\n\n\nKeyword arguments\n\n\n\n\nmetric\n: The distance metric. Must be a valid metric from \nDistances.jl\n.\n\n\n\n\nReferences\n\n\n\n\nKraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.\n\n\n\n\ntransferentropy_kraskov(points::AbstractArray{T, 2}, k1::Int, k2::Int,\n    target_future::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    target_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    source_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}};\n    metric = Chebyshev()) where T\n\n\n\n\nCompute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].\n\n\nArguments\n\n\n\n\npoints\n: The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size \ndim\n-by-\nn\n points.\n\n\nk1\n: The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nk2\n: The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   if $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nv\n: A \nTEVars\n instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.\n\n\ntarget_future\n: Which rows of \npoints\n correspond to future values of the   target variable?\n\n\ntarget_presentpast\n: Which rows of \npoints\n correspond to present and past   values of the target variable?\n\n\nsource_presentpast\n: Which rows of \npoints\n correspond to present and past   values of the source variable?\n\n\n\n\nThis version of the function assumes there is no conditioning.\n\n\nKeyword arguments\n\n\n\n\nmetric\n: The distance metric. Must be a valid metric from \nDistances.jl\n.\n\n\n\n\nReferences\n\n\n\n\nKraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.\n\n\n\n\ntransferentropy_kraskov(E::StateSpaceReconstruction.AbstractEmbedding,\n        k1::Int, k2::Int, v::TEVars; metric = Chebyshev())\n\n\n\n\nCompute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].\n\n\nArguments:\n\n\n\n\nE\n: The embedding for which to compute transfer entropy.\n\n\nk1\n: The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nk2\n: The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   if $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nv\n: A \nTEVars\n instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.\n\n\n\n\nReferences\n\n\n\n\nKraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating\n\n\n\n\nmutual information.\" Physical review E 69.6 (2004): 066138.\n\n\ntransferentropy_kraskov(E::AbstractEmbedding, k1::Int, k2::Int,\n    target_future::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    target_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    source_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    conditioned_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}};\n    metric = Chebyshev()) where T\n\n\n\n\nCompute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].\n\n\nArguments\n\n\n\n\npoints\n: The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size \ndim\n-by-\nn\n points.\n\n\nk1\n: The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nk2\n: The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1 \n k_2$ if   if $min(k_1, k_2) \n 10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say \nk1 = k2 = 4\n, will suffice.\n\n\nv\n: A \nTEVars\n instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.\n\n\ntarget_future\n: Which rows of \npoints\n correspond to future values of the   target variable?\n\n\ntarget_presentpast\n: Which rows of \npoints\n correspond to present and past   values of the target variable?\n\n\nsource_presentpast\n: Which rows of \npoints\n correspond to present and past   values of the source variable?\n\n\nconditioned_presentpast\n: Which rows of \npoints\n correspond to present and   past values of conditional variables?\n\n\n\n\nKeyword arguments\n\n\n\n\nmetric\n: The distance metric. Must be a valid metric from \nDistances.jl\n.\n\n\n\n\nReferences\n\n\n\n\nKraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.\n\n\n\n\n\n\nReferences\n\n\nKraskov, A., St\u00f6gbauer, H., \n Grassberger, P. (2004). Estimating mutual information. Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics. \nhttps://doi.org/10.1103/PhysRevE.69.066138\n.\n\n\nDiego, D., Agas\u00f8ster Haaga, K., \n Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677. Retrieved from \nhttps://arxiv.org/abs/1811.01677\n.", 
            "title": "kNN"
        }, 
        {
            "location": "/transferentropy/estimator_TE_kNN/#transfer-entropy-te-estimators", 
            "text": "", 
            "title": "Transfer entropy (TE) estimators"
        }, 
        {
            "location": "/transferentropy/estimator_TE_kNN/#k-nearest-neighbours-knn-estimator", 
            "text": "The kNN estimator computes transfer entropy as the sum of two mutual information terms, which are computed using the Kraskov estimator of mutual information ( Kraskov et al., 2004 ). Implemented for  Diego et al. (2018) .", 
            "title": "k Nearest neighbours (kNN) estimator"
        }, 
        {
            "location": "/transferentropy/estimator_TE_kNN/#documentation", 
            "text": "#  TransferEntropy.tekNN     Function .  transferentropy_kraskov(points::AbstractArray{T, 2}, k1::Int, k2::Int,\n    v::TEVars; metric = Chebyshev())  Compute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].  Arguments   points : The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size  dim -by- n  points.  k1 : The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  k2 : The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   if $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  v : A  TEVars  instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.   Keyword arguments   metric : The distance metric. Must be a valid metric from  Distances.jl .   References   Kraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.   transferentropy_kraskov(points::AbstractArray{T, 2}, k1::Int, k2::Int,\n    target_future::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    target_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    source_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    conditioned_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}};\n    metric = Chebyshev()) where T  Compute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].  Arguments   points : The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size  dim -by- n  points.  k1 : The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  k2 : The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   if $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  v : A  TEVars  instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.  target_future : Which rows of  points  correspond to future values of the   target variable?  target_presentpast : Which rows of  points  correspond to present and past   values of the target variable?  source_presentpast : Which rows of  points  correspond to present and past   values of the source variable?  conditioned_presentpast : Which rows of  points  correspond to present and   past values of conditional variables?   Keyword arguments   metric : The distance metric. Must be a valid metric from  Distances.jl .   References   Kraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.   transferentropy_kraskov(points::AbstractArray{T, 2}, k1::Int, k2::Int,\n    target_future::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    target_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    source_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}};\n    metric = Chebyshev()) where T  Compute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].  Arguments   points : The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size  dim -by- n  points.  k1 : The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  k2 : The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   if $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  v : A  TEVars  instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.  target_future : Which rows of  points  correspond to future values of the   target variable?  target_presentpast : Which rows of  points  correspond to present and past   values of the target variable?  source_presentpast : Which rows of  points  correspond to present and past   values of the source variable?   This version of the function assumes there is no conditioning.  Keyword arguments   metric : The distance metric. Must be a valid metric from  Distances.jl .   References   Kraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.   transferentropy_kraskov(E::StateSpaceReconstruction.AbstractEmbedding,\n        k1::Int, k2::Int, v::TEVars; metric = Chebyshev())  Compute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].  Arguments:   E : The embedding for which to compute transfer entropy.  k1 : The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  k2 : The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   if $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  v : A  TEVars  instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.   References   Kraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating   mutual information.\" Physical review E 69.6 (2004): 066138.  transferentropy_kraskov(E::AbstractEmbedding, k1::Int, k2::Int,\n    target_future::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    target_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    source_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}},\n    conditioned_presentpast::Union{Int, UnitRange{Int}, Vector{Int}, Tuple{Int}};\n    metric = Chebyshev()) where T  Compute transfer entropy decomposed as the sum of mutual informations, using an adapted version of the Kraskov estimator for mutual information [1].  Arguments   points : The set of points representing the embedding for which to compute   transfer entropy. Must be provided as an array of size  dim -by- n  points.  k1 : The number of nearest neighbours for the highest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  k2 : The number of nearest neighbours for the lowest-dimensional mutual   information estimate. To minimize bias, choose $k_1   k_2$ if   if $min(k_1, k_2)   10$ (see fig. 16 in [1]). Beyond dimension 5, choosing   $k_1 = k_2$ results in fairly low bias, and a low number of nearest   neighbours, say  k1 = k2 = 4 , will suffice.  v : A  TEVars  instance, indicating which variables of the embedding should   be grouped as what when computing the marginal entropies that go into the   transfer entropy expression.  target_future : Which rows of  points  correspond to future values of the   target variable?  target_presentpast : Which rows of  points  correspond to present and past   values of the target variable?  source_presentpast : Which rows of  points  correspond to present and past   values of the source variable?  conditioned_presentpast : Which rows of  points  correspond to present and   past values of conditional variables?   Keyword arguments   metric : The distance metric. Must be a valid metric from  Distances.jl .   References   Kraskov, Alexander, Harald St\u00f6gbauer, and Peter Grassberger. \"Estimating  mutual information.\" Physical review E 69.6 (2004): 066138.", 
            "title": "Documentation"
        }, 
        {
            "location": "/transferentropy/estimator_TE_kNN/#references", 
            "text": "Kraskov, A., St\u00f6gbauer, H.,   Grassberger, P. (2004). Estimating mutual information. Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics.  https://doi.org/10.1103/PhysRevE.69.066138 .  Diego, D., Agas\u00f8ster Haaga, K.,   Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677. Retrieved from  https://arxiv.org/abs/1811.01677 .", 
            "title": "References"
        }, 
        {
            "location": "/transferentropy/estimator_TE_transferoperator_grid/", 
            "text": "Transfer entropy (TE) estimators\n\n\n\n\nTransfer operator grid estimator\n\n\nThe transfer operator grid estimator computes TE from the invariant distribution arising from an approximation to the transfer operator over a discretization of the state space reconstruction (delay embedding). This is a new estimator from \nDiego et al. (2018)\n.\n\n\n\n\nDocumentation\n\n\n#\n\n\nTransferEntropy.tetogrid\n \n \nFunction\n.\n\n\ntransferentropy_transferoperator_grid(\n        bins_visited_by_orbit::Array{Int, 2},\n        iv::PerronFrobenius.InvariantDistribution,\n        v::TEVars, normalise_to_tPP = false)\n\n\n\n\nUsing the invariant probability distribution obtained from the transfer operator to the visited bins of the partitioned state space. \nbins_visited_by_orbit\n are the bin labels assigned to each element of the partition that gets visited by the orbit.\n\n\nWe calculate transfer entropy from the embedding \nE\n, given a discretization scheme controlled by \n\u001bpsilon\n and information \nv::TEVars\n about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.\n\n\nIf \nnormalise_to_tPP = true\n, then the TE estimate is normalised to the entropy rate of the target variable, \nH(target_future |\u00a0target_presentpast)\n.\n\n\ntransferentropy\ntransferoperator\ngrid(                     E::Embeddings.AbstractEmbedding,                     \u03f5::Union{Int, Float64, Vector{Float64}, Vector{Int}},                     v::TransferEntropy.TEVars;                     normalise\nto\ntPP = false                     allocate_frac = 1) -\n Float64\n\n\nUsing the transfer operator to calculate probability distributions,  calculate transfer entropy from the embedding \nE\n, given a discretization scheme controlled by \n\u03f5\n and information \nv::TEVars\n about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.\n\n\nIf \nnormalise_to_tPP = true\n, then the TE estimate is normalised to the entropy rate of the target variable \nH(target_future |\u00a0target_presentpast)\n.\n\n\ntransferentropy_transferoperator_grid(E::Embeddings.AbstractEmbedding,\n    \u03f5::Vector{Union{Int, Float64, Vector{Float64}, Vector{Int}}},\n    v::TEVars, normalise_to_tPP = false)\n\n\n\n\nCompute transfer entropy over a range of bin sizes.\n\n\nUsing the transfer operator to calculate probability distributions,  calculate transfer entropy from the embedding \nE\n, given a discretization scheme controlled by the \n\u03f5\ns and information \nv::TEVars\n about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.\n\n\nIf \nnormalise_to_tPP = true\n, then the TE estimate is normalised to the entropy rate of the target variable, \nH(target_future |\u00a0target_presentpast)\n.\n\n\ntransferentropy_transferoperator_grid(pts::AbstractArray{T, 2},\n    \u03f5::Union{Int, Float64, Vector{Float64}, Vector{Int}},\n    v::TransferEntropy.TEVars)\n\n\n\n\nUsing the transfer operator to calculate probability distributions, calculate transfer entropy from the points \npts\n, given a discretization scheme controlled by \n\u03f5\n and information \nv::TEVars\n about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression. The points will be embedded behind the scenes.\n\n\n\n\nReferences\n\n\nDiego, D., Agas\u00f8ster Haaga, K., \n Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677. \nhttps://arxiv.org/abs/1811.01677\n.", 
            "title": "Transfer operator grid"
        }, 
        {
            "location": "/transferentropy/estimator_TE_transferoperator_grid/#transfer-entropy-te-estimators", 
            "text": "", 
            "title": "Transfer entropy (TE) estimators"
        }, 
        {
            "location": "/transferentropy/estimator_TE_transferoperator_grid/#transfer-operator-grid-estimator", 
            "text": "The transfer operator grid estimator computes TE from the invariant distribution arising from an approximation to the transfer operator over a discretization of the state space reconstruction (delay embedding). This is a new estimator from  Diego et al. (2018) .", 
            "title": "Transfer operator grid estimator"
        }, 
        {
            "location": "/transferentropy/estimator_TE_transferoperator_grid/#documentation", 
            "text": "#  TransferEntropy.tetogrid     Function .  transferentropy_transferoperator_grid(\n        bins_visited_by_orbit::Array{Int, 2},\n        iv::PerronFrobenius.InvariantDistribution,\n        v::TEVars, normalise_to_tPP = false)  Using the invariant probability distribution obtained from the transfer operator to the visited bins of the partitioned state space.  bins_visited_by_orbit  are the bin labels assigned to each element of the partition that gets visited by the orbit.  We calculate transfer entropy from the embedding  E , given a discretization scheme controlled by  \u001bpsilon  and information  v::TEVars  about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.  If  normalise_to_tPP = true , then the TE estimate is normalised to the entropy rate of the target variable,  H(target_future |\u00a0target_presentpast) .  transferentropy transferoperator grid(                     E::Embeddings.AbstractEmbedding,                     \u03f5::Union{Int, Float64, Vector{Float64}, Vector{Int}},                     v::TransferEntropy.TEVars;                     normalise to tPP = false                     allocate_frac = 1) -  Float64  Using the transfer operator to calculate probability distributions,  calculate transfer entropy from the embedding  E , given a discretization scheme controlled by  \u03f5  and information  v::TEVars  about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.  If  normalise_to_tPP = true , then the TE estimate is normalised to the entropy rate of the target variable  H(target_future |\u00a0target_presentpast) .  transferentropy_transferoperator_grid(E::Embeddings.AbstractEmbedding,\n    \u03f5::Vector{Union{Int, Float64, Vector{Float64}, Vector{Int}}},\n    v::TEVars, normalise_to_tPP = false)  Compute transfer entropy over a range of bin sizes.  Using the transfer operator to calculate probability distributions,  calculate transfer entropy from the embedding  E , given a discretization scheme controlled by the  \u03f5 s and information  v::TEVars  about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.  If  normalise_to_tPP = true , then the TE estimate is normalised to the entropy rate of the target variable,  H(target_future |\u00a0target_presentpast) .  transferentropy_transferoperator_grid(pts::AbstractArray{T, 2},\n    \u03f5::Union{Int, Float64, Vector{Float64}, Vector{Int}},\n    v::TransferEntropy.TEVars)  Using the transfer operator to calculate probability distributions, calculate transfer entropy from the points  pts , given a discretization scheme controlled by  \u03f5  and information  v::TEVars  about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression. The points will be embedded behind the scenes.", 
            "title": "Documentation"
        }, 
        {
            "location": "/transferentropy/estimator_TE_transferoperator_grid/#references", 
            "text": "Diego, D., Agas\u00f8ster Haaga, K.,   Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677.  https://arxiv.org/abs/1811.01677 .", 
            "title": "References"
        }, 
        {
            "location": "/transferentropy/estimator_TE_visitfreq/", 
            "text": "Transfer entropy (TE) estimators\n\n\n\n\nVisitation frequency estimator\n\n\nThe visitation frequency estimator computes TE, as the name implies, from a probability distribution over the discretized state space obtained by counting how often the orbit visits the different partition elements (boxes).\n\n\n\n\nDocumentation\n\n\n#\n\n\nTransferEntropy.tefreq\n \n \nFunction\n.\n\n\nCompute transfer entropy over a range of bin sizes.\n\n\nUsing the traditional method of estimation probability distribution by visitation frequencies [1], calculate transfer entropy from the embedding \nE\n, given a discretization scheme controlled by the \n\u001bpsilon\ns and information \nv::TEVars\nabout which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.\n\n\nIf \nnormalise_to_tPP = true\n, then the TE estimate is normalised to the entropy rate of the target variable, \nH(target_future |\u00a0target_presentpast)\n.\n\n\n\n\nReferences\n\n\nDiego, D., Agas\u00f8ster Haaga, K., \n Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677. \nhttps://arxiv.org/abs/1811.01677\n.", 
            "title": "Visitation frequency"
        }, 
        {
            "location": "/transferentropy/estimator_TE_visitfreq/#transfer-entropy-te-estimators", 
            "text": "", 
            "title": "Transfer entropy (TE) estimators"
        }, 
        {
            "location": "/transferentropy/estimator_TE_visitfreq/#visitation-frequency-estimator", 
            "text": "The visitation frequency estimator computes TE, as the name implies, from a probability distribution over the discretized state space obtained by counting how often the orbit visits the different partition elements (boxes).", 
            "title": "Visitation frequency estimator"
        }, 
        {
            "location": "/transferentropy/estimator_TE_visitfreq/#documentation", 
            "text": "#  TransferEntropy.tefreq     Function .  Compute transfer entropy over a range of bin sizes.  Using the traditional method of estimation probability distribution by visitation frequencies [1], calculate transfer entropy from the embedding  E , given a discretization scheme controlled by the  \u001bpsilon s and information  v::TEVars about which columns of the embedding to consider for each of the marginal distributions. From these marginal distributions, we calculate marginal entropies and insert these into the transfer entropy expression.  If  normalise_to_tPP = true , then the TE estimate is normalised to the entropy rate of the target variable,  H(target_future |\u00a0target_presentpast) .", 
            "title": "Documentation"
        }, 
        {
            "location": "/transferentropy/estimator_TE_visitfreq/#references", 
            "text": "Diego, D., Agas\u00f8ster Haaga, K.,   Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677.  https://arxiv.org/abs/1811.01677 .", 
            "title": "References"
        }, 
        {
            "location": "/surrogates/surrogates_overview/", 
            "text": "Surrogate method overview\n\n\nThe method of surrogate data is commonly used in the analysis of dynamical systems. For an overview of surrogate methods, see the recent review by \nLancaster et al., 2018\n.\n\n\n\n\nWhat is a surrogate realization?\n\n\nA surrogate realization of a dataset is a dataset that is formed either by shuffling the values of the original dataset in a particular way. There are two main ways of creating the data: constrained realizations and typical realizations (\nTheiler \n Prichard, 1996\n).\n\n\n\n\nConstrained realizations\n\n\nConstrained surrogate realizations are formed by shuffling the values of the input data series in a way that retains some property of the original data. For example, random shuffle surrogates (\nrandomshuffle\n) retain the histogram of the data. AAFT surrogates (\naaft\n) aim to preserve the periodogram of the original data series.  \n\n\n\n\nTypical realizations\n\n\nTypical surrogate realizations are generated by first fitting a model to the input data, then generating new data from that model. Random phase Fourier surrogates (\nrandomphases\n), for example, retain the amplitudes of the original data, but shuffles the phases.\n\n\n\n\nImplemented algorithms\n\n\nThe following surrogate methods are implemented. Function documentation and basic examples are available from the menu. For more details and demonstrations, visit the \nTimeseriesSurrogates.jl documentation\n.\n\n\n\n\n\n\n\n\nAlgorithm\n\n\nFunction\n\n\nType\n\n\nReference\n\n\n\n\n\n\n\n\n\n\nRandomly shuffling the values of the dataset\n\n\nrandomshuffle\n\n\nConstrained\n\n\nTheiler et al., 1992\n\n\n\n\n\n\nFourier transform phase surrogates\n\n\nrandomphases\n\n\nTypical\n\n\n\n\n\n\n\n\nFourier transform amplitude surrogates\n\n\nrandomamplitudes\n\n\nTypical\n\n\n\n\n\n\n\n\nAmplitude-adjusted Fourier transform surrogates (AAFT)\n\n\naaft\n\n\nConstrained\n\n\n\n\n\n\n\n\nIterated amplitude-adjusted Fourier transform surrogates (iAAFT)\n\n\niaaft\n\n\nConstrained\n\n\nSchreiber \n Schmitz, 1996\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\nLancaster, G., Iatsenko, D., Pidde, A., Ticcinelli, V., \n Stefanovska, A. (2018). Surrogate data for hypothesis testing of physical systems. Physics Reports. \nhttps://doi.org/10.1016/j.physrep.2018.06.001\n\n\nSchreiber, T., \n Schmitz, A. (1996). Improved surrogate data for nonlinearity tests. Physical Review Letters, 77(4), 635. \nhttps://journals.aps.org/prl/abstract/10.1103/PhysRevLett.77.635\n\n\nTheiler, J., Eubank, S., Longtin, A., Galdrikian, B., \n Doyne Farmer, J. (1992). Testing for nonlinearity in time series: the method of surrogate data. Physica D: Nonlinear Phenomena. \nhttps://doi.org/10.1016/0167-2789(92)90102-S\n\n\nTheiler, J., \n Prichard, D. (1996). Constrained-realization Monte-Carlo method for hypothesis testing. Physica D: Nonlinear Phenomena, 94(4), 221\u2013235. \nhttps://www.sciencedirect.com/science/article/pii/0167278996000504", 
            "title": "The method of surrogate data"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#surrogate-method-overview", 
            "text": "The method of surrogate data is commonly used in the analysis of dynamical systems. For an overview of surrogate methods, see the recent review by  Lancaster et al., 2018 .", 
            "title": "Surrogate method overview"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#what-is-a-surrogate-realization", 
            "text": "A surrogate realization of a dataset is a dataset that is formed either by shuffling the values of the original dataset in a particular way. There are two main ways of creating the data: constrained realizations and typical realizations ( Theiler   Prichard, 1996 ).", 
            "title": "What is a surrogate realization?"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#constrained-realizations", 
            "text": "Constrained surrogate realizations are formed by shuffling the values of the input data series in a way that retains some property of the original data. For example, random shuffle surrogates ( randomshuffle ) retain the histogram of the data. AAFT surrogates ( aaft ) aim to preserve the periodogram of the original data series.", 
            "title": "Constrained realizations"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#typical-realizations", 
            "text": "Typical surrogate realizations are generated by first fitting a model to the input data, then generating new data from that model. Random phase Fourier surrogates ( randomphases ), for example, retain the amplitudes of the original data, but shuffles the phases.", 
            "title": "Typical realizations"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#implemented-algorithms", 
            "text": "The following surrogate methods are implemented. Function documentation and basic examples are available from the menu. For more details and demonstrations, visit the  TimeseriesSurrogates.jl documentation .     Algorithm  Function  Type  Reference      Randomly shuffling the values of the dataset  randomshuffle  Constrained  Theiler et al., 1992    Fourier transform phase surrogates  randomphases  Typical     Fourier transform amplitude surrogates  randomamplitudes  Typical     Amplitude-adjusted Fourier transform surrogates (AAFT)  aaft  Constrained     Iterated amplitude-adjusted Fourier transform surrogates (iAAFT)  iaaft  Constrained  Schreiber   Schmitz, 1996", 
            "title": "Implemented algorithms"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#references", 
            "text": "Lancaster, G., Iatsenko, D., Pidde, A., Ticcinelli, V.,   Stefanovska, A. (2018). Surrogate data for hypothesis testing of physical systems. Physics Reports.  https://doi.org/10.1016/j.physrep.2018.06.001  Schreiber, T.,   Schmitz, A. (1996). Improved surrogate data for nonlinearity tests. Physical Review Letters, 77(4), 635.  https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.77.635  Theiler, J., Eubank, S., Longtin, A., Galdrikian, B.,   Doyne Farmer, J. (1992). Testing for nonlinearity in time series: the method of surrogate data. Physica D: Nonlinear Phenomena.  https://doi.org/10.1016/0167-2789(92)90102-S  Theiler, J.,   Prichard, D. (1996). Constrained-realization Monte-Carlo method for hypothesis testing. Physica D: Nonlinear Phenomena, 94(4), 221\u2013235.  https://www.sciencedirect.com/science/article/pii/0167278996000504", 
            "title": "References"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/", 
            "text": "Surrogate methods\n\n\n\n\nRandom shuffle surrogates\n\n\n\n\nValid inputs\n\n\nRandom shuffle surrogates may be generated from the following inputs:\n\n\n\n\nAbstractArray{T, 1}\n instances (scalar-valued data series)\n\n\nAbstractArray{Number, 2}\n instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.\n\n\nDataset\n instances from \nDynamicalSystems.jl\n, for which surrogates are generated column-wise.\n\n\nEmbedding\n instances, for which surrogates are generated variable-wise (row-wise on the points).\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nTimeseriesSurrogates.randomshuffle\n \n \nFunction\n.\n\n\nrandomshuffle(ts::AbstractArray{T, 1} where T)\n\n\n\n\nGenerate a random constrained surrogate for \nts\n. Destroys any linear correlation in the signal, but preserves its amplitude distribution.\n\n\nts\n Is the time series for which to generate an AAFT surrogate realization.\n\n\nrandomshuffle(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random shuffle surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomshuffle(E::Embeddings.AbstractEmbedding;\n                cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise random shuffle surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomshuffle(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random shuffle surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource", 
            "title": "Random shuffle"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/#random-shuffle-surrogates", 
            "text": "", 
            "title": "Random shuffle surrogates"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/#valid-inputs", 
            "text": "Random shuffle surrogates may be generated from the following inputs:   AbstractArray{T, 1}  instances (scalar-valued data series)  AbstractArray{Number, 2}  instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.  Dataset  instances from  DynamicalSystems.jl , for which surrogates are generated column-wise.  Embedding  instances, for which surrogates are generated variable-wise (row-wise on the points).", 
            "title": "Valid inputs"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/#documentation", 
            "text": "#  TimeseriesSurrogates.randomshuffle     Function .  randomshuffle(ts::AbstractArray{T, 1} where T)  Generate a random constrained surrogate for  ts . Destroys any linear correlation in the signal, but preserves its amplitude distribution.  ts  Is the time series for which to generate an AAFT surrogate realization.  randomshuffle(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))  Column-wise random shuffle surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  randomshuffle(E::Embeddings.AbstractEmbedding;\n                cols = 1:size(E.points, 1))  Column-wise random shuffle surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  randomshuffle(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise random shuffle surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/", 
            "text": "Surrogate methods\n\n\n\n\nRandom amplitude Fourier surrogates\n\n\n\n\nValid inputs\n\n\nRandom amplitude Fourier surrogates may be generated from the following inputs:\n\n\n\n\nAbstractArray{T, 1}\n instances (scalar-valued data series)\n\n\nAbstractArray{Number, 2}\n instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.\n\n\nDataset\n instances from \nDynamicalSystems.jl\n, for which surrogates are generated column-wise.\n\n\nEmbedding\n instances, for which surrogates are generated variable-wise (row-wise on the points).\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nTimeseriesSurrogates.randomamplitudes\n \n \nFunction\n.\n\n\nrandomamplitudes(ts::AbstractArray{T, 1} where T)\n\n\n\n\nCreate a random amplitude surrogate for \nts\n.\n\n\nA modification of the random phases surrogates [1] where amplitudes are adjusted instead of the phases after taking the Fourier transform.\n\n\nts\n Is the time series for which to generate an AAFT surrogate realization.\n\n\nLiterature references\n\n\n\n\nJ. Theiler et al., Physica D \n58\n (1992) 77-94 (1992))\n\n\n\n\nrandomamplitudes(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random amplitude Fourier surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomamplitudes(E::Embeddings.AbstractEmbedding;\n                    cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise random amplitude Fourier surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomamplitudes(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random amplitude Fourier surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource", 
            "title": "Random amplitudes"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/#random-amplitude-fourier-surrogates", 
            "text": "", 
            "title": "Random amplitude Fourier surrogates"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/#valid-inputs", 
            "text": "Random amplitude Fourier surrogates may be generated from the following inputs:   AbstractArray{T, 1}  instances (scalar-valued data series)  AbstractArray{Number, 2}  instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.  Dataset  instances from  DynamicalSystems.jl , for which surrogates are generated column-wise.  Embedding  instances, for which surrogates are generated variable-wise (row-wise on the points).", 
            "title": "Valid inputs"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/#documentation", 
            "text": "#  TimeseriesSurrogates.randomamplitudes     Function .  randomamplitudes(ts::AbstractArray{T, 1} where T)  Create a random amplitude surrogate for  ts .  A modification of the random phases surrogates [1] where amplitudes are adjusted instead of the phases after taking the Fourier transform.  ts  Is the time series for which to generate an AAFT surrogate realization.  Literature references   J. Theiler et al., Physica D  58  (1992) 77-94 (1992))   randomamplitudes(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))  Column-wise random amplitude Fourier surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  randomamplitudes(E::Embeddings.AbstractEmbedding;\n                    cols = 1:size(E.points, 1))  Column-wise random amplitude Fourier surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  randomamplitudes(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise random amplitude Fourier surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/surrogates/randomphases_docs/", 
            "text": "Surrogate methods\n\n\n\n\nRandom phases Fourier surrogates\n\n\n\n\nValid inputs\n\n\nRandom phase Fourier surrogates may be generated from the following inputs:\n\n\n\n\nAbstractArray{T, 1}\n instances (scalar-valued data series)\n\n\nAbstractArray{Number, 2}\n instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.\n\n\nDataset\n instances from \nDynamicalSystems.jl\n, for which surrogates are generated column-wise.\n\n\nEmbedding\n instances, for which surrogates are generated variable-wise (row-wise on the points).\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nTimeseriesSurrogates.randomphases\n \n \nFunction\n.\n\n\nrandomphases(ts::AbstractArray{T, 1} where T)\n\n\n\n\nCreate a random phases surrogate for \nts\n [1]. Surrogate realizations using the phase surrogates have the same linear correlation, or periodogram, as the original data.\n\n\nts\n Is the time series for which to generate an AAFT surrogate realization.\n\n\nLiterature references\n\n\n\n\nJ. Theiler et al., Physica D \n58\n (1992) 77-94 (1992).\n\n\n\n\nrandomphases(a::AbstractArray{Number, 2}, cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random phases Fourier surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomphases(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise random phases Fourier surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomphases(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random phases Fourier surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource", 
            "title": "Random phases"
        }, 
        {
            "location": "/surrogates/randomphases_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomphases_docs/#random-phases-fourier-surrogates", 
            "text": "", 
            "title": "Random phases Fourier surrogates"
        }, 
        {
            "location": "/surrogates/randomphases_docs/#valid-inputs", 
            "text": "Random phase Fourier surrogates may be generated from the following inputs:   AbstractArray{T, 1}  instances (scalar-valued data series)  AbstractArray{Number, 2}  instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.  Dataset  instances from  DynamicalSystems.jl , for which surrogates are generated column-wise.  Embedding  instances, for which surrogates are generated variable-wise (row-wise on the points).", 
            "title": "Valid inputs"
        }, 
        {
            "location": "/surrogates/randomphases_docs/#documentation", 
            "text": "#  TimeseriesSurrogates.randomphases     Function .  randomphases(ts::AbstractArray{T, 1} where T)  Create a random phases surrogate for  ts  [1]. Surrogate realizations using the phase surrogates have the same linear correlation, or periodogram, as the original data.  ts  Is the time series for which to generate an AAFT surrogate realization.  Literature references   J. Theiler et al., Physica D  58  (1992) 77-94 (1992).   randomphases(a::AbstractArray{Number, 2}, cols = 1:size(d, 2))  Column-wise random phases Fourier surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  randomphases(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))  Column-wise random phases Fourier surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  randomphases(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise random phases Fourier surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/surrogates/aaft_docs/", 
            "text": "Surrogate methods\n\n\n\n\nAmplitude-adjusted Fourier transform (AAFT) surrogates\n\n\n\n\nValid inputs\n\n\nAAFT surrogates may be generated from the following inputs:\n\n\n\n\nAbstractArray{T, 1}\n instances (scalar-valued data series)\n\n\nAbstractArray{Number, 2}\n instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.\n\n\nDataset\n instances from \nDynamicalSystems.jl\n, for which surrogates are generated column-wise.\n\n\nEmbedding\n instances, for which surrogates are generated variable-wise (row-wise on the points).\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nTimeseriesSurrogates.aaft\n \n \nFunction\n.\n\n\naaft(ts::AbstractArray{T, 1} where T)\n\n\n\n\nGenerate a realization of an amplitude adjusted Fourier transform (AAFT) surrogate [1].\n\n\nts\n Is the time series for which to generate an AAFT surrogate realization.\n\n\nLiterature references\n\n\n\n\nJ. Theiler et al., Physica D \n58\n (1992) 77-94 (1992).\n\n\n\n\naaft(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\naaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\naaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise amplitude-adjusted Fourier transform (AAFT) surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource", 
            "title": "AAFT"
        }, 
        {
            "location": "/surrogates/aaft_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/aaft_docs/#amplitude-adjusted-fourier-transform-aaft-surrogates", 
            "text": "", 
            "title": "Amplitude-adjusted Fourier transform (AAFT) surrogates"
        }, 
        {
            "location": "/surrogates/aaft_docs/#valid-inputs", 
            "text": "AAFT surrogates may be generated from the following inputs:   AbstractArray{T, 1}  instances (scalar-valued data series)  AbstractArray{Number, 2}  instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.  Dataset  instances from  DynamicalSystems.jl , for which surrogates are generated column-wise.  Embedding  instances, for which surrogates are generated variable-wise (row-wise on the points).", 
            "title": "Valid inputs"
        }, 
        {
            "location": "/surrogates/aaft_docs/#documentation", 
            "text": "#  TimeseriesSurrogates.aaft     Function .  aaft(ts::AbstractArray{T, 1} where T)  Generate a realization of an amplitude adjusted Fourier transform (AAFT) surrogate [1].  ts  Is the time series for which to generate an AAFT surrogate realization.  Literature references   J. Theiler et al., Physica D  58  (1992) 77-94 (1992).   aaft(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))  Column-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  aaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))  Column-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  aaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise amplitude-adjusted Fourier transform (AAFT) surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/surrogates/iaaft_docs/", 
            "text": "Surrogate methods\n\n\n\n\nIterated amplitude-adjusted Fourier transform (iAAFT) surrogates\n\n\n\n\nValid inputs\n\n\niAAFT surrogates may be generated from the following inputs:\n\n\n\n\nAbstractArray{T, 1}\n instances (scalar-valued data series)\n\n\nAbstractArray{Number, 2}\n instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.\n\n\nDataset\n instances from \nDynamicalSystems.jl\n, for which surrogates are generated column-wise.\n\n\nEmbedding\n instances, for which surrogates are generated variable-wise (row-wise on the points).\n\n\n\n\n\n\nDocumentation\n\n\n#\n\n\nTimeseriesSurrogates.iaaft\n \n \nFunction\n.\n\n\niaaft(ts::AbstractArray{T, 1} where T;\n        n_maxiter = 200, tol = 1e-6, n_windows = 50)\n\n\n\n\nGenerate an iteratively adjusted amplitude adjusted Fourier transform (IAAFT) [1] surrogate realization of \nts\n.\n\n\nArguments\n\n\n\n\nts\n is the time series for which to generate an AAFT surrogate realization.\n\n\nn_maxiter\n sets the maximum number of iterations to allow before ending   the algorithm (if convergence is slow).\n\n\ntol\n is the relative tolerance for deciding if convergence is achieved.\n\n\nn_window\n is the number is windows used when binning the periodogram (used   for determining convergence).\n\n\n\n\nLiterature references\n\n\n\n\nT. Schreiber; A. Schmitz (1996). \"Improved Surrogate Data for Nonlinearity\n\n\n\n\nTests\". Phys. Rev. Lett. 77 (4): 635\u2013638. doi:10.1103/PhysRevLett.77.635. PMID 10062864.\n\n\niaaft(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))\n\n\n\n\nIterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\niaaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\niaaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nIterated amplitude-adjusted Fourier transform (IAAFT) surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource", 
            "title": "iAAFT"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#iterated-amplitude-adjusted-fourier-transform-iaaft-surrogates", 
            "text": "", 
            "title": "Iterated amplitude-adjusted Fourier transform (iAAFT) surrogates"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#valid-inputs", 
            "text": "iAAFT surrogates may be generated from the following inputs:   AbstractArray{T, 1}  instances (scalar-valued data series)  AbstractArray{Number, 2}  instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.  Dataset  instances from  DynamicalSystems.jl , for which surrogates are generated column-wise.  Embedding  instances, for which surrogates are generated variable-wise (row-wise on the points).", 
            "title": "Valid inputs"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#documentation", 
            "text": "#  TimeseriesSurrogates.iaaft     Function .  iaaft(ts::AbstractArray{T, 1} where T;\n        n_maxiter = 200, tol = 1e-6, n_windows = 50)  Generate an iteratively adjusted amplitude adjusted Fourier transform (IAAFT) [1] surrogate realization of  ts .  Arguments   ts  is the time series for which to generate an AAFT surrogate realization.  n_maxiter  sets the maximum number of iterations to allow before ending   the algorithm (if convergence is slow).  tol  is the relative tolerance for deciding if convergence is achieved.  n_window  is the number is windows used when binning the periodogram (used   for determining convergence).   Literature references   T. Schreiber; A. Schmitz (1996). \"Improved Surrogate Data for Nonlinearity   Tests\". Phys. Rev. Lett. 77 (4): 635\u2013638. doi:10.1103/PhysRevLett.77.635. PMID 10062864.  iaaft(a::AbstractArray{Number, 2}; cols = 1:size(d, 2))  Iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  iaaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))  Column-wise iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  iaaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Documentation"
        }, 
        {
            "location": "/surrogates/randomshuffle_example/", 
            "text": "Surrogate methods\n\n\n\n\nRandom shuffle surrogates\n\n\n\n\nExample\n\n\nnpts = 100\nts = sin.(diff(rand(npts + 1)))*0.5 .+ cos.(LinRange(0, 8*pi, npts))\np1 = plot(ts, label = \nts\n, lc = :black)\np2 = plot(randomshuffle(ts), label = \nrandomshuffle(ts)\n, xlabel = \nTime step\n)\nplot(p1, p2, layout = (2, 1))\nylabel!(\nValue\n);", 
            "title": "Random shuffle"
        }, 
        {
            "location": "/surrogates/randomshuffle_example/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomshuffle_example/#random-shuffle-surrogates", 
            "text": "", 
            "title": "Random shuffle surrogates"
        }, 
        {
            "location": "/surrogates/randomshuffle_example/#example", 
            "text": "npts = 100\nts = sin.(diff(rand(npts + 1)))*0.5 .+ cos.(LinRange(0, 8*pi, npts))\np1 = plot(ts, label =  ts , lc = :black)\np2 = plot(randomshuffle(ts), label =  randomshuffle(ts) , xlabel =  Time step )\nplot(p1, p2, layout = (2, 1))\nylabel!( Value );", 
            "title": "Example"
        }, 
        {
            "location": "/surrogates/randomamplitudes_example/", 
            "text": "Surrogate methods\n\n\n\n\nRandom amplitude Fourier surrogates\n\n\n\n\nExample\n\n\n# Generate a dynamical system, create an orbit and extract a time series from\n# the first component.\ns = CausalityTools.Systems.logistic3()\norbit = trajectory(s, 150)\nx = orbit[:, 1]\n\n# Compare original time series and random amplitude surrogate\np1 = plot(x, label = \nx\n, lc = :black)\np2 = plot(randomamplitudes(x), label = \nrandomamplitudes(x)\n,\n            xlabel = \nTime step\n)\nplot(p1, p2, layout = (2, 1))\nylabel!(\nValue\n)", 
            "title": "Random amplitudes"
        }, 
        {
            "location": "/surrogates/randomamplitudes_example/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomamplitudes_example/#random-amplitude-fourier-surrogates", 
            "text": "", 
            "title": "Random amplitude Fourier surrogates"
        }, 
        {
            "location": "/surrogates/randomamplitudes_example/#example", 
            "text": "# Generate a dynamical system, create an orbit and extract a time series from\n# the first component.\ns = CausalityTools.Systems.logistic3()\norbit = trajectory(s, 150)\nx = orbit[:, 1]\n\n# Compare original time series and random amplitude surrogate\np1 = plot(x, label =  x , lc = :black)\np2 = plot(randomamplitudes(x), label =  randomamplitudes(x) ,\n            xlabel =  Time step )\nplot(p1, p2, layout = (2, 1))\nylabel!( Value )", 
            "title": "Example"
        }, 
        {
            "location": "/surrogates/randomphases_example/", 
            "text": "Surrogate methods\n\n\n\n\nRandom phases Fourier surrogates\n\n\n\n\nExample\n\n\nGenerating random phase surrogates is done as follows.\n\n\n# Generate a dynamical system, create an orbit and extract a time series from\n# the second component.\ns = CausalityTools.Systems.logistic3()\norbit = trajectory(s, 150)\nx = orbit[:, 2]\n\n\n# Plot the time series along with its random phase surrogate\ntimesteps = 1:size(orbit, 1)\np1 = plot(timesteps, x, label = \nx\n)\np2 = plot(timesteps, randomphases(x), label = \nrandomphases(x)\n,\n    xlabel = \nTime step\n)\nplot(p1, p2, layout = (2, 1))\nylabel!(\nValue\n)", 
            "title": "Random phases"
        }, 
        {
            "location": "/surrogates/randomphases_example/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomphases_example/#random-phases-fourier-surrogates", 
            "text": "", 
            "title": "Random phases Fourier surrogates"
        }, 
        {
            "location": "/surrogates/randomphases_example/#example", 
            "text": "Generating random phase surrogates is done as follows.  # Generate a dynamical system, create an orbit and extract a time series from\n# the second component.\ns = CausalityTools.Systems.logistic3()\norbit = trajectory(s, 150)\nx = orbit[:, 2]\n\n\n# Plot the time series along with its random phase surrogate\ntimesteps = 1:size(orbit, 1)\np1 = plot(timesteps, x, label =  x )\np2 = plot(timesteps, randomphases(x), label =  randomphases(x) ,\n    xlabel =  Time step )\nplot(p1, p2, layout = (2, 1))\nylabel!( Value )", 
            "title": "Example"
        }, 
        {
            "location": "/surrogates/aaft_example/", 
            "text": "Surrogate methods\n\n\n\n\nAmplitude-adjusted Fourier transform (AAFT) surrogates\n\n\n\n\nExample\n\n\nnpts = 200\nts = sin.(diff(rand(npts + 1)))*0.5 .+ cos.(LinRange(0, 14*pi, npts))\np1 = plot(ts, label = \nts\n, lc = :black)\np2 = plot(aaft(ts), label = \naaft(ts)\n, xlabel = \nTime step\n)\nplot(p1, p2, layout = (2, 1))\nylabel!(\nValue\n);", 
            "title": "AAFT"
        }, 
        {
            "location": "/surrogates/aaft_example/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/aaft_example/#amplitude-adjusted-fourier-transform-aaft-surrogates", 
            "text": "", 
            "title": "Amplitude-adjusted Fourier transform (AAFT) surrogates"
        }, 
        {
            "location": "/surrogates/aaft_example/#example", 
            "text": "npts = 200\nts = sin.(diff(rand(npts + 1)))*0.5 .+ cos.(LinRange(0, 14*pi, npts))\np1 = plot(ts, label =  ts , lc = :black)\np2 = plot(aaft(ts), label =  aaft(ts) , xlabel =  Time step )\nplot(p1, p2, layout = (2, 1))\nylabel!( Value );", 
            "title": "Example"
        }, 
        {
            "location": "/surrogates/iaaft_example/", 
            "text": "Surrogate methods\n\n\n\n\nIterated amplitude-adjusted Fourier transform (iAAFT) surrogates\n\n\n\n\nExample 1\n\n\nnpts = 200\nts = sin.(diff(rand(npts + 1)))*0.5 .+ cos.(LinRange(0, 14*pi, npts))\np1 = plot(ts, label = \nts\n, lc = :black)\np2 = plot(iaaft(ts), label = \niaaft(ts)\n, xlabel = \nTime step\n)\nplot(p1, p2, layout = (2, 1))\nylabel!(\nValue\n);\n\n\n\n\n\n\n\n\nExample 2\n\n\nThis gif shows iAAFT surrogate realizations for an cyclostationary AR2 (NSAR2) process (\nnsar2\n) from \nLucio et al. (2012)\n.\n\n\n\n\n\n\nExample 3\n\n\nThis gif shows iAAFT surrogate realizations for an AR1 process.\n\n\n\n\n\n\nReferences\n\n\nLucio et al., Phys. Rev. E \n85\n, 056202 (2012), after J. Timmer, Phys. Rev. E \n58\n, 5153 (1998). \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202", 
            "title": "iAAFT"
        }, 
        {
            "location": "/surrogates/iaaft_example/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/iaaft_example/#iterated-amplitude-adjusted-fourier-transform-iaaft-surrogates", 
            "text": "", 
            "title": "Iterated amplitude-adjusted Fourier transform (iAAFT) surrogates"
        }, 
        {
            "location": "/surrogates/iaaft_example/#example-1", 
            "text": "npts = 200\nts = sin.(diff(rand(npts + 1)))*0.5 .+ cos.(LinRange(0, 14*pi, npts))\np1 = plot(ts, label =  ts , lc = :black)\np2 = plot(iaaft(ts), label =  iaaft(ts) , xlabel =  Time step )\nplot(p1, p2, layout = (2, 1))\nylabel!( Value );", 
            "title": "Example 1"
        }, 
        {
            "location": "/surrogates/iaaft_example/#example-2", 
            "text": "This gif shows iAAFT surrogate realizations for an cyclostationary AR2 (NSAR2) process ( nsar2 ) from  Lucio et al. (2012) .", 
            "title": "Example 2"
        }, 
        {
            "location": "/surrogates/iaaft_example/#example-3", 
            "text": "This gif shows iAAFT surrogate realizations for an AR1 process.", 
            "title": "Example 3"
        }, 
        {
            "location": "/surrogates/iaaft_example/#references", 
            "text": "Lucio et al., Phys. Rev. E  85 , 056202 (2012), after J. Timmer, Phys. Rev. E  58 , 5153 (1998).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202", 
            "title": "References"
        }, 
        {
            "location": "/example_systems/ar1/", 
            "text": "Synthetic coupled dynamical systems\n\n\n\n\nBivariate AR1 system\n\n\nFor this example, we'll consider a bivariate, order one autoregressive model consisting of variable $x$ and $y$, where $x \\rightarrow y$, given by the difference equations:\n\n\n\n\n\n\\begin{aligned}\nx(t+1) &= a_1 x(t) + \\xi_1(t) \\\\\ny(t+1) &= b_1 y(t) + c_1 x(t)+ \\xi_2(t)\n\\end{aligned}\n\n\n\n\n\nwhere the parameter $c_1$ controls how strong the dynamical forcing from $x$ to $y$ is, and $\\xi_1$ and $\\xi_2$ are dynamical noise terms with zero mean and standard deviations $\\sigma$.\n\n\n\n\nWhere has the system been used?\n\n\nThis system was investigated by \nPalu\u0161 et al. (2018)\n.\n\n\n\n\nRepresent as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction eom_ar1(x, p, n)\n    a\u2081, b\u2081, c\u2081, \u03c3 = (p...,)\n    x, y = (x...,)\n    \u03be\u2081 = rand(Normal(0, \u03c3))\n    \u03be\u2082 = rand(Normal(0, \u03c3))\n\n    dx = a\u2081*x + \u03be\u2081\n    dy = b\u2081*y + c\u2081*x + \u03be\u2082\n    return SVector{2}(dx, dy)\nend\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any coupling strength \nc\n and initial condition \nu\u2080\n.\n\n\nfunction ar1(;u\u1d62 = rand(2), a\u2081 = 0.90693, b\u2081 = 0.40693, c\u2081 = 0.5, \u03c3 = 0.40662)\n    p = [a\u2081, b\u2081, c\u2081, \u03c3]\n    return DiscreteDynamicalSystem(eom_ar1, u\u1d62, p)\nend\n\n\n\n\nBy tuning the coupling strength \nc\u2081\n, we may control the strength of the influence $x$ has on $y$. An example realization of the system when the coupling strength is \nc\u2081 = 0.5\n is:\n\n\ns = ar1(c\u2081 = 0.5)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label = \nx\n, lc = :black)\nplot!(y, label = \ny\n, lc = :red)\nxlabel!(\nTime step\n); ylabel!(\nValue\n)\n\n\n\n\n\n\n\n\nPredefined system\n\n\nThis system is predefined in \nCausalityTools.Systems\n, and can be initialized using the \nar1\n function.\n\n\n\n\nReferences\n\n\nPalu\u0161, M., Krakovsk\u00e1, A., Jakub\u00edk, J., \n Chvostekov\u00e1, M. (2018). Causality, dynamical systems and the arrow of time. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(7), 075307. \nhttp://doi.org/10.1063/1.5019944", 
            "title": "ar1"
        }, 
        {
            "location": "/example_systems/ar1/#synthetic-coupled-dynamical-systems", 
            "text": "", 
            "title": "Synthetic coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/ar1/#bivariate-ar1-system", 
            "text": "For this example, we'll consider a bivariate, order one autoregressive model consisting of variable $x$ and $y$, where $x \\rightarrow y$, given by the difference equations:   \n\\begin{aligned}\nx(t+1) &= a_1 x(t) + \\xi_1(t) \\\\\ny(t+1) &= b_1 y(t) + c_1 x(t)+ \\xi_2(t)\n\\end{aligned}   where the parameter $c_1$ controls how strong the dynamical forcing from $x$ to $y$ is, and $\\xi_1$ and $\\xi_2$ are dynamical noise terms with zero mean and standard deviations $\\sigma$.", 
            "title": "Bivariate AR1 system"
        }, 
        {
            "location": "/example_systems/ar1/#where-has-the-system-been-used", 
            "text": "This system was investigated by  Palu\u0161 et al. (2018) .", 
            "title": "Where has the system been used?"
        }, 
        {
            "location": "/example_systems/ar1/#represent-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function eom_ar1(x, p, n)\n    a\u2081, b\u2081, c\u2081, \u03c3 = (p...,)\n    x, y = (x...,)\n    \u03be\u2081 = rand(Normal(0, \u03c3))\n    \u03be\u2082 = rand(Normal(0, \u03c3))\n\n    dx = a\u2081*x + \u03be\u2081\n    dy = b\u2081*y + c\u2081*x + \u03be\u2082\n    return SVector{2}(dx, dy)\nend  To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any coupling strength  c  and initial condition  u\u2080 .  function ar1(;u\u1d62 = rand(2), a\u2081 = 0.90693, b\u2081 = 0.40693, c\u2081 = 0.5, \u03c3 = 0.40662)\n    p = [a\u2081, b\u2081, c\u2081, \u03c3]\n    return DiscreteDynamicalSystem(eom_ar1, u\u1d62, p)\nend  By tuning the coupling strength  c\u2081 , we may control the strength of the influence $x$ has on $y$. An example realization of the system when the coupling strength is  c\u2081 = 0.5  is:  s = ar1(c\u2081 = 0.5)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label =  x , lc = :black)\nplot!(y, label =  y , lc = :red)\nxlabel!( Time step ); ylabel!( Value )", 
            "title": "Represent as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/example_systems/ar1/#predefined-system", 
            "text": "This system is predefined in  CausalityTools.Systems , and can be initialized using the  ar1  function.", 
            "title": "Predefined system"
        }, 
        {
            "location": "/example_systems/ar1/#references", 
            "text": "Palu\u0161, M., Krakovsk\u00e1, A., Jakub\u00edk, J.,   Chvostekov\u00e1, M. (2018). Causality, dynamical systems and the arrow of time. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(7), 075307.  http://doi.org/10.1063/1.5019944", 
            "title": "References"
        }, 
        {
            "location": "/example_systems/henon2/", 
            "text": "Synthetic coupled dynamical systems\n\n\n\n\nTwo coupled Henon maps\n\n\nFor this example, we'll consider set of unidirectionally coupled Henon maps, given by the difference equations\n\n\n\n\n\n\\begin{aligned}\nx_1(t+1) &= 1.4 - x_1^2(t) + 0.3x_2(t) \\\\\nx_2(t+1) &= x_1(t) \\\\\ny_1(t+1) &= 1.4 - [c x_1(t) y_1(t) + (1-c) y_1^2(t)] + 0.3 y_2(t) \\\\\ny_2(t+1) &= y_1(t),\n\\end{aligned}\n\n\n\n\n\nwhere the parameter \nc\n controls how strong the dynamical forcing is.\n\n\n\n\nWhere has the system been used?\n\n\nThis system was investigated by \nKrakovsk\u00e1 et al. (2018)\n to study the performance of different causality detection algorithms.\n\n\n\n\nRepresent as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction eom_henon2(x, p, n)\n    c = p[1]\n    x\u2081, x\u2082, y\u2081, y\u2082 = (x...,)\n    dx\u2081 = 1.4 - x\u2081^2 + 0.3*x\u2082\n    dx\u2082 = x\u2081\n    dy\u2081 = 1.4 - (c * x\u2081 * y\u2081  +  (1 - c)*y\u2081^2) + 0.3*y\u2082\n    dy\u2082 = y\u2081\n    return SVector{4}(dx\u2081, dx\u2082, dy\u2081, dy\u2082)\nend\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any coupling strength \nc\n and initial condition \nu\u2080\n.\n\n\nfunction henon2(;u\u2080 = rand(4), c = 0.1)\n    p = [c]\n    return DiscreteDynamicalSystem(eom_henon2, u\u2080, p)\nend\n\n\n\n\nBy tuning the coupling strength \nc\n, we may control the strength of the influence $x$ has on $y$. An example realization of the system when the coupling strength is \nc = 0.5\n is:\n\n\ns = henon2(c = 0.5)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label = \nx\n, lc = :black)\nplot!(y, label = \ny\n, lc = :red)\nxlabel!(\nTime step\n); ylabel!(\nValue\n)\n\n\n\n\n\n\n\n\nPredefined system\n\n\nThis system is predefined in \nCausalityTools.Systems\n, and can be initialized using the \nhenon2\n function.\n\n\n\n\nReferences\n\n\nKrakovsk\u00e1, A., Jakub\u00edk, J., Chvostekov\u00e1, M., Coufal, D., Jajcay, N., \n Palu\u0161, M. (2018). Comparison of six methods for the detection of causality in a bivariate time series. Physical Review E, 97(4), 042207. \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.97.042207", 
            "title": "henon2"
        }, 
        {
            "location": "/example_systems/henon2/#synthetic-coupled-dynamical-systems", 
            "text": "", 
            "title": "Synthetic coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/henon2/#two-coupled-henon-maps", 
            "text": "For this example, we'll consider set of unidirectionally coupled Henon maps, given by the difference equations   \n\\begin{aligned}\nx_1(t+1) &= 1.4 - x_1^2(t) + 0.3x_2(t) \\\\\nx_2(t+1) &= x_1(t) \\\\\ny_1(t+1) &= 1.4 - [c x_1(t) y_1(t) + (1-c) y_1^2(t)] + 0.3 y_2(t) \\\\\ny_2(t+1) &= y_1(t),\n\\end{aligned}   where the parameter  c  controls how strong the dynamical forcing is.", 
            "title": "Two coupled Henon maps"
        }, 
        {
            "location": "/example_systems/henon2/#where-has-the-system-been-used", 
            "text": "This system was investigated by  Krakovsk\u00e1 et al. (2018)  to study the performance of different causality detection algorithms.", 
            "title": "Where has the system been used?"
        }, 
        {
            "location": "/example_systems/henon2/#represent-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function eom_henon2(x, p, n)\n    c = p[1]\n    x\u2081, x\u2082, y\u2081, y\u2082 = (x...,)\n    dx\u2081 = 1.4 - x\u2081^2 + 0.3*x\u2082\n    dx\u2082 = x\u2081\n    dy\u2081 = 1.4 - (c * x\u2081 * y\u2081  +  (1 - c)*y\u2081^2) + 0.3*y\u2082\n    dy\u2082 = y\u2081\n    return SVector{4}(dx\u2081, dx\u2082, dy\u2081, dy\u2082)\nend  To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any coupling strength  c  and initial condition  u\u2080 .  function henon2(;u\u2080 = rand(4), c = 0.1)\n    p = [c]\n    return DiscreteDynamicalSystem(eom_henon2, u\u2080, p)\nend  By tuning the coupling strength  c , we may control the strength of the influence $x$ has on $y$. An example realization of the system when the coupling strength is  c = 0.5  is:  s = henon2(c = 0.5)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label =  x , lc = :black)\nplot!(y, label =  y , lc = :red)\nxlabel!( Time step ); ylabel!( Value )", 
            "title": "Represent as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/example_systems/henon2/#predefined-system", 
            "text": "This system is predefined in  CausalityTools.Systems , and can be initialized using the  henon2  function.", 
            "title": "Predefined system"
        }, 
        {
            "location": "/example_systems/henon2/#references", 
            "text": "Krakovsk\u00e1, A., Jakub\u00edk, J., Chvostekov\u00e1, M., Coufal, D., Jajcay, N.,   Palu\u0161, M. (2018). Comparison of six methods for the detection of causality in a bivariate time series. Physical Review E, 97(4), 042207.  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.97.042207", 
            "title": "References"
        }, 
        {
            "location": "/example_systems/logistic2/", 
            "text": "Synthetic coupled dynamical systems\n\n\n\n\nTwo unidirectionally coupled logistic maps\n\n\nFor this example, we'll consider a unidirectionally coupled system consisting of two logistic maps, given by the difference equations\n\n\n\n\n\n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}\n\n\n\n\n\nwith\n\n\n\n\n\n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x + \\sigma \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}\n\n\n\n\n\nThe parameter \nc\n controls how strong the dynamical forcing is. If \n\u03c3 \n 0\n, dynamical noise masking the influence of  $x$ on $y$, equivalent to $\\sigma \\cdot \\xi$, is added at each iteration. Here, $\\xi$ is a draw from a flat distribution on $[0, 1]$. Thus, setting \n\u03c3 = 0.05\n is equivalent to add dynamical noise corresponding to a maximum of $5 \\%$ of the possible range of values of the logistic map.\n\n\n\n\nWhere has the system been used?\n\n\nThis system was used in \nDiego et al. (2018)\n to study the performance of the transfer operator grid transfer entropy estimator.\n\n\n\n\nRepresent as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction eom_logistic2(dx, x, p, n)\n    c, r\u2081, r\u2082, \u03c3 = (p...,)\n    \u03be = rand() # random number from flat distribution on [0, 1]\n    x, y = x[1], x[2]\n    f_xy = (y +  (c*(x + \u03c3*\u03be)/2) ) / (1 + (c/2)*(1+\u03c3))\n\n    dx[1] = r\u2081 * x * (1 - x)\n    dx[2] = r\u2082 * (f_xy) * (1 - f_xy)\n    return\nend\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters \nr\u2081\n and \nr\u2082\n, coupling strength \nc\n, initial condition \nu\u2080\n and dynamical noise level \n\u03c3\n. Selecting parameter values on \n[3.6, 4.0]\n yield mostly chaotic realizations of the maps, so we set the default to some random values on this interval.\n\n\nfunction logistic2(;u\u2080 = rand(2), c = 0.0, r\u2081 = 3.66, r\u2082 = 3.77, \u03c3 = 0.05)\n    p = [c, r\u2081, r\u2082, \u03c3]\n    DiscreteDynamicalSystem(eom_logistic2, u\u2080, p)\nend\n\n\n\n\nBy tuning the coupling strength \nc\n, we may control the strength of the influence $x$ has on $y$. Depending on the particular values of \nr\u2081\n and \nr\u2082\n, the subsystems become synchronized at different values of \nc\n. Choosing \nc \u2208 [0, 2]\n usually still gives some independence between the subsystems.\n\n\nAn example realization of the system when there is no coupling is:\n\n\ns = logistic2(c = 0.0)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label = \nx\n, lc = :black)\nplot!(y, label = \ny\n, lc = :red)\nxlabel!(\nTime step\n); ylabel!(\nValue\n)\n\n\n\n\n\n\n\n\nPredefined system\n\n\nThis system is predefined in \nCausalityTools.Systems\n, and can be initialized using the \nlogistic2\n function.\n\n\n\n\nReferences\n\n\nDiego, D., Agas\u00f8ster Haaga, K., \n Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677. Retrieved from \nhttps://ui.adsabs.harvard.edu/#abs/2018arXiv181101677D", 
            "title": "logistic2"
        }, 
        {
            "location": "/example_systems/logistic2/#synthetic-coupled-dynamical-systems", 
            "text": "", 
            "title": "Synthetic coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/logistic2/#two-unidirectionally-coupled-logistic-maps", 
            "text": "For this example, we'll consider a unidirectionally coupled system consisting of two logistic maps, given by the difference equations   \n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}   with   \n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x + \\sigma \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}   The parameter  c  controls how strong the dynamical forcing is. If  \u03c3   0 , dynamical noise masking the influence of  $x$ on $y$, equivalent to $\\sigma \\cdot \\xi$, is added at each iteration. Here, $\\xi$ is a draw from a flat distribution on $[0, 1]$. Thus, setting  \u03c3 = 0.05  is equivalent to add dynamical noise corresponding to a maximum of $5 \\%$ of the possible range of values of the logistic map.", 
            "title": "Two unidirectionally coupled logistic maps"
        }, 
        {
            "location": "/example_systems/logistic2/#where-has-the-system-been-used", 
            "text": "This system was used in  Diego et al. (2018)  to study the performance of the transfer operator grid transfer entropy estimator.", 
            "title": "Where has the system been used?"
        }, 
        {
            "location": "/example_systems/logistic2/#represent-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function eom_logistic2(dx, x, p, n)\n    c, r\u2081, r\u2082, \u03c3 = (p...,)\n    \u03be = rand() # random number from flat distribution on [0, 1]\n    x, y = x[1], x[2]\n    f_xy = (y +  (c*(x + \u03c3*\u03be)/2) ) / (1 + (c/2)*(1+\u03c3))\n\n    dx[1] = r\u2081 * x * (1 - x)\n    dx[2] = r\u2082 * (f_xy) * (1 - f_xy)\n    return\nend  To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters  r\u2081  and  r\u2082 , coupling strength  c , initial condition  u\u2080  and dynamical noise level  \u03c3 . Selecting parameter values on  [3.6, 4.0]  yield mostly chaotic realizations of the maps, so we set the default to some random values on this interval.  function logistic2(;u\u2080 = rand(2), c = 0.0, r\u2081 = 3.66, r\u2082 = 3.77, \u03c3 = 0.05)\n    p = [c, r\u2081, r\u2082, \u03c3]\n    DiscreteDynamicalSystem(eom_logistic2, u\u2080, p)\nend  By tuning the coupling strength  c , we may control the strength of the influence $x$ has on $y$. Depending on the particular values of  r\u2081  and  r\u2082 , the subsystems become synchronized at different values of  c . Choosing  c \u2208 [0, 2]  usually still gives some independence between the subsystems.  An example realization of the system when there is no coupling is:  s = logistic2(c = 0.0)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label =  x , lc = :black)\nplot!(y, label =  y , lc = :red)\nxlabel!( Time step ); ylabel!( Value )", 
            "title": "Represent as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/example_systems/logistic2/#predefined-system", 
            "text": "This system is predefined in  CausalityTools.Systems , and can be initialized using the  logistic2  function.", 
            "title": "Predefined system"
        }, 
        {
            "location": "/example_systems/logistic2/#references", 
            "text": "Diego, D., Agas\u00f8ster Haaga, K.,   Hannisdal, B. (2018, November 1). Transfer entropy computation using the Perron-Frobenius operator. Eprint ArXiv:1811.01677. Retrieved from  https://ui.adsabs.harvard.edu/#abs/2018arXiv181101677D", 
            "title": "References"
        }, 
        {
            "location": "/example_systems/logistic3/", 
            "text": "Synthetic coupled dynamical systems\n\n\n\n\nThree coupled logistic maps\n\n\nFor this example, we'll consider a system of three coupled logistic maps ($x$, $y$ and $z$), where the dynamical influence goes in the directions $z \\rightarrow x$ and $z \\rightarrow y$.\n\n\nThe system is given by the following difference equations:\n\n\n\n\n\nx(t+1) = [x(t)(r_1 - r_1 x(t) - z(t) + \\sigma_{x(t)} \\eta_{x}(t))] \\mod 1 \\\\\ny(t+1) = [y(t)(r_2 - r_2 y(t) - z(t) + \\sigma_{y(t)} \\eta_{y}(t))] \\mod 1 \\\\\nz(t+1) = [z(t)(r_3 - r_3 z(t) + \\sigma_{z(t)} \\eta_{z}(t))] \\mod 1\n\n\n\n\n\nDynamical noise may be added to each of the dynamical variables by tuning the parameters \n\u03c3z\n, \n\u03c3x\n and \n\u03c3z\n. Default values for the parameters \nr\u2081\n, \nr\u2082\n and \nr\u2083\n are set such that the system exhibits chaotic behaviour, with \nr\u2081 = r\u2082 = r\u2083 = 4\n.\n\n\n\n\nWhere has the system been used?\n\n\nThis system was used in \nRunge (2018)\n, where he discusses the theoretical assumptions behind causal network reconstructions and practical estimators of such networks.\n\n\n\n\nRepresent as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction eom_logistic3(u, p, t)\n    r\u2081, r\u2082, r\u2083, \u03c3x, \u03c3y, \u03c3z = (p...,)\n    x, y, z = (u...,)\n\n    # Independent dynamical noise for each variable.\n    \u03b7x = rand()\n    \u03b7y = rand()\n    \u03b7z = rand()\n\n    dx = (x*(r\u2081 - r\u2081*x - z + \u03c3x*\u03b7x)) % 1\n    dy = (y*(r\u2082 - r\u2082*y - z + \u03c3y*\u03b7y)) % 1\n    dz = (z*(r\u2083 - r\u2083*z + \u03c3z*\u03b7z)) % 1\n    return SVector{3}(dx, dy, dz)\nend\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters \nr\u2081\n, \nr\u2082\n and \nr\u2083\n, initial condition \nu\u2080\n, and dynamical noise levels \n\u03c3x\n, \n\u03c3y\n and \n\u03c3z\n.\n\n\nfunction logistic3(;u\u2080 = rand(3), r\u2081 = 4, r\u2082 = 4, r\u2083 = 4,\n                    \u03c3x = 0.05, \u03c3y = 0.05, \u03c3z = 0.05)\n    p = [r\u2081, r\u2082, r\u2083, \u03c3x, \u03c3y, \u03c3z]\n    DiscreteDynamicalSystem(eom_logistic3, u\u2080, p)\nend\n\n\n\n\nAn example realization of the system is:\n\n\ns = logistic3()\norbit = trajectory(s, 100)\nx, y, z = orbit[:, 1], orbit[:, 2], orbit[:, 3]\nplot(x, label = \nx\n, lc = :black)\nplot!(y, label = \ny\n, lc = :red)\nplot!(z, label = \nz\n, lc = :blue)\nxlabel!(\nTime step\n); ylabel!(\nValue\n)\n\n\n\n\n\n\n\n\nPredefined system\n\n\nThis system is predefined in \nCausalityTools.Systems\n, and can be initialized using the \nlogistic3\n function.\n\n\n\n\nReferences\n\n\nRunge, Jakob. Causal network reconstruction from time series: From theoretical assumptions to practical estimation, Chaos 28, 075310 (2018); doi: 10.1063/1.5025050. \nhttps://aip.scitation.org/doi/abs/10.1063/1.5025050", 
            "title": "logistic3"
        }, 
        {
            "location": "/example_systems/logistic3/#synthetic-coupled-dynamical-systems", 
            "text": "", 
            "title": "Synthetic coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/logistic3/#three-coupled-logistic-maps", 
            "text": "For this example, we'll consider a system of three coupled logistic maps ($x$, $y$ and $z$), where the dynamical influence goes in the directions $z \\rightarrow x$ and $z \\rightarrow y$.  The system is given by the following difference equations:   \nx(t+1) = [x(t)(r_1 - r_1 x(t) - z(t) + \\sigma_{x(t)} \\eta_{x}(t))] \\mod 1 \\\\\ny(t+1) = [y(t)(r_2 - r_2 y(t) - z(t) + \\sigma_{y(t)} \\eta_{y}(t))] \\mod 1 \\\\\nz(t+1) = [z(t)(r_3 - r_3 z(t) + \\sigma_{z(t)} \\eta_{z}(t))] \\mod 1   Dynamical noise may be added to each of the dynamical variables by tuning the parameters  \u03c3z ,  \u03c3x  and  \u03c3z . Default values for the parameters  r\u2081 ,  r\u2082  and  r\u2083  are set such that the system exhibits chaotic behaviour, with  r\u2081 = r\u2082 = r\u2083 = 4 .", 
            "title": "Three coupled logistic maps"
        }, 
        {
            "location": "/example_systems/logistic3/#where-has-the-system-been-used", 
            "text": "This system was used in  Runge (2018) , where he discusses the theoretical assumptions behind causal network reconstructions and practical estimators of such networks.", 
            "title": "Where has the system been used?"
        }, 
        {
            "location": "/example_systems/logistic3/#represent-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function eom_logistic3(u, p, t)\n    r\u2081, r\u2082, r\u2083, \u03c3x, \u03c3y, \u03c3z = (p...,)\n    x, y, z = (u...,)\n\n    # Independent dynamical noise for each variable.\n    \u03b7x = rand()\n    \u03b7y = rand()\n    \u03b7z = rand()\n\n    dx = (x*(r\u2081 - r\u2081*x - z + \u03c3x*\u03b7x)) % 1\n    dy = (y*(r\u2082 - r\u2082*y - z + \u03c3y*\u03b7y)) % 1\n    dz = (z*(r\u2083 - r\u2083*z + \u03c3z*\u03b7z)) % 1\n    return SVector{3}(dx, dy, dz)\nend  To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters  r\u2081 ,  r\u2082  and  r\u2083 , initial condition  u\u2080 , and dynamical noise levels  \u03c3x ,  \u03c3y  and  \u03c3z .  function logistic3(;u\u2080 = rand(3), r\u2081 = 4, r\u2082 = 4, r\u2083 = 4,\n                    \u03c3x = 0.05, \u03c3y = 0.05, \u03c3z = 0.05)\n    p = [r\u2081, r\u2082, r\u2083, \u03c3x, \u03c3y, \u03c3z]\n    DiscreteDynamicalSystem(eom_logistic3, u\u2080, p)\nend  An example realization of the system is:  s = logistic3()\norbit = trajectory(s, 100)\nx, y, z = orbit[:, 1], orbit[:, 2], orbit[:, 3]\nplot(x, label =  x , lc = :black)\nplot!(y, label =  y , lc = :red)\nplot!(z, label =  z , lc = :blue)\nxlabel!( Time step ); ylabel!( Value )", 
            "title": "Represent as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/example_systems/logistic3/#predefined-system", 
            "text": "This system is predefined in  CausalityTools.Systems , and can be initialized using the  logistic3  function.", 
            "title": "Predefined system"
        }, 
        {
            "location": "/example_systems/logistic3/#references", 
            "text": "Runge, Jakob. Causal network reconstruction from time series: From theoretical assumptions to practical estimation, Chaos 28, 075310 (2018); doi: 10.1063/1.5025050.  https://aip.scitation.org/doi/abs/10.1063/1.5025050", 
            "title": "References"
        }, 
        {
            "location": "/example_systems/linearmap3d_nonlinearcoupling/", 
            "text": "Synthetic coupled dynamical systems\n\n\n\n\nThree linear maps with nonlinear coupling\n\n\nFor this example, we'll consider a 3d linear system ($x_1$, $x_2$ and $x_3$) with nonlinear coupling,  where the dynamical influence goes in the directions $x_1 \\rightarrow x_2$, $x_1 \\rightarrow x_3$ and $x_2 \\rightarrow x_3$.\n\n\nThe system is given by the following difference equations:\n\n\n\n\n\n\\small\n\\begin{aligned}\nx_1(t+1) &= a_1 x_1 (1-x_1(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{1}(t) \\\\\nx_2(t+1) &= a_1 x_2 (1-x_2(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{2}(t) + b x_1 x_2 \\\\\nx_3(t+1) &= a_3 x_3 (1-x_3(t))^2  e^{-x_3(t)^2} + 0.4 \\xi_{3}(t) + c x_{2}(t) + d x_{1}(t)^2.\n\\end{aligned}\n\\normalsize\n\n\n\n\n\nHere, $\\xi_{1,2,3}(t)$ are independent normally distributed noise processes, representing dynamical noise in the system, with zero mean and standard deviations $\\sigma_1$, $\\sigma_2$, $\\sigma_3$, respectively.\n\n\n\n\nWhere has the system been used?\n\n\nThis system was used in \nGour\u00e9vitch et al. (2006)\n,  where they review the linear Granger causality test, partial directed coherence and the nonlinear Granger causality test.\n\n\n\n\nRepresent as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction eom_linear3d_nonlinearcoupling(x, p, n)\n    x\u2081, x\u2082, x\u2083 = (x...,)\n    a\u2081, a\u2082, a\u2083, b, c, d, \u03c3\u2081, \u03c3\u2082, \u03c3\u2083 = (p...,)\n    \u03be\u2081 = rand(Normal(0, \u03c3\u2081))\n    \u03be\u2082 = rand(Normal(0, \u03c3\u2082))\n    \u03be\u2083 = rand(Normal(0, \u03c3\u2083))\n\n    dx\u2081 = a\u2081*x\u2081*(1-x\u2081)^2 * exp(-x\u2081^2) + 0.4*\u03be\u2081\n    dx\u2082 = a\u2082*x\u2082*(1-x\u2082)^2 * exp(-x\u2082^2) + 0.4*\u03be\u2082 + b*x\u2081*x\u2082\n    dx\u2083 = a\u2083*x\u2083*(1-x\u2083)^2 * exp(-x\u2083^2) + 0.4*\u03be\u2083 + c*x\u2082 + d*x\u2081^2\n\n    return SVector{3}(dx\u2081, dx\u2082, dx\u2083)\nend\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters \na\u2081\n, \na\u2082\n, \na\u2083\n, \nb\n, \nc\n and \nd\n, initial condition \nu\u2080\n, and dynamical noise levels \n\u03c3\u2081\n, \n\u03c3\u2082\n and \n\u03c3\u2083\n.\n\n\nfunction linear3d_nonlinearcoupling(;u\u1d62 = rand(3),\n                \u03c3\u2081 = 1.0, \u03c3\u2082 = 1.0, \u03c3\u2083 = 1.0,\n                a\u2081 = 3.4, a\u2082 = 3.4, a\u2083 = 3.4,\n                b = 0.5, c = 0.3, d = 0.5)\n    p = [a\u2081, a\u2082, a\u2083, b, c, d, \u03c3\u2081, \u03c3\u2082, \u03c3\u2083]\n    return DiscreteDynamicalSystem(eom_linear3d_nonlinearcoupling, u\u1d62, p)\nend\n\n\n\n\nAn example realization of the system is:\n\n\ns = linear3d_nonlinearcoupling()\norbit = trajectory(s, 100)\nx1, x2, x3 = orbit[:, 1], orbit[:, 2], orbit[:, 3]\nplot(x1, label = \nx\n, lc = :black)\nplot!(x2, label = \ny\n, lc = :red)\nplot!(x3, label = \nz\n, lc = :blue)\nxlabel!(\nTime step\n); ylabel!(\nValue\n)\n\n\n\n\n\n\n\n\nPredefined system\n\n\nThis system is predefined in \nCausalityTools.Systems\n, and can be initialized using the \nlinearmap3d_nonlinearcoupling\n function.\n\n\n\n\nReferences\n\n\nGour\u00e9vitch, B., Le Bouquin-Jeann\u00e8s, R., \n Faucon, G. (2006). Linear and nonlinear causality between signals: methods, examples and neurophysiological applications. Biological Cybernetics, 95(4), 349\u2013369. \nhttps://link.springer.com/article/10.1007/s00422-006-0098-0", 
            "title": "linearmap3d_nonlinearcoupling"
        }, 
        {
            "location": "/example_systems/linearmap3d_nonlinearcoupling/#synthetic-coupled-dynamical-systems", 
            "text": "", 
            "title": "Synthetic coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/linearmap3d_nonlinearcoupling/#three-linear-maps-with-nonlinear-coupling", 
            "text": "For this example, we'll consider a 3d linear system ($x_1$, $x_2$ and $x_3$) with nonlinear coupling,  where the dynamical influence goes in the directions $x_1 \\rightarrow x_2$, $x_1 \\rightarrow x_3$ and $x_2 \\rightarrow x_3$.  The system is given by the following difference equations:   \n\\small\n\\begin{aligned}\nx_1(t+1) &= a_1 x_1 (1-x_1(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{1}(t) \\\\\nx_2(t+1) &= a_1 x_2 (1-x_2(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{2}(t) + b x_1 x_2 \\\\\nx_3(t+1) &= a_3 x_3 (1-x_3(t))^2  e^{-x_3(t)^2} + 0.4 \\xi_{3}(t) + c x_{2}(t) + d x_{1}(t)^2.\n\\end{aligned}\n\\normalsize   Here, $\\xi_{1,2,3}(t)$ are independent normally distributed noise processes, representing dynamical noise in the system, with zero mean and standard deviations $\\sigma_1$, $\\sigma_2$, $\\sigma_3$, respectively.", 
            "title": "Three linear maps with nonlinear coupling"
        }, 
        {
            "location": "/example_systems/linearmap3d_nonlinearcoupling/#where-has-the-system-been-used", 
            "text": "This system was used in  Gour\u00e9vitch et al. (2006) ,  where they review the linear Granger causality test, partial directed coherence and the nonlinear Granger causality test.", 
            "title": "Where has the system been used?"
        }, 
        {
            "location": "/example_systems/linearmap3d_nonlinearcoupling/#represent-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function eom_linear3d_nonlinearcoupling(x, p, n)\n    x\u2081, x\u2082, x\u2083 = (x...,)\n    a\u2081, a\u2082, a\u2083, b, c, d, \u03c3\u2081, \u03c3\u2082, \u03c3\u2083 = (p...,)\n    \u03be\u2081 = rand(Normal(0, \u03c3\u2081))\n    \u03be\u2082 = rand(Normal(0, \u03c3\u2082))\n    \u03be\u2083 = rand(Normal(0, \u03c3\u2083))\n\n    dx\u2081 = a\u2081*x\u2081*(1-x\u2081)^2 * exp(-x\u2081^2) + 0.4*\u03be\u2081\n    dx\u2082 = a\u2082*x\u2082*(1-x\u2082)^2 * exp(-x\u2082^2) + 0.4*\u03be\u2082 + b*x\u2081*x\u2082\n    dx\u2083 = a\u2083*x\u2083*(1-x\u2083)^2 * exp(-x\u2083^2) + 0.4*\u03be\u2083 + c*x\u2082 + d*x\u2081^2\n\n    return SVector{3}(dx\u2081, dx\u2082, dx\u2083)\nend  To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters  a\u2081 ,  a\u2082 ,  a\u2083 ,  b ,  c  and  d , initial condition  u\u2080 , and dynamical noise levels  \u03c3\u2081 ,  \u03c3\u2082  and  \u03c3\u2083 .  function linear3d_nonlinearcoupling(;u\u1d62 = rand(3),\n                \u03c3\u2081 = 1.0, \u03c3\u2082 = 1.0, \u03c3\u2083 = 1.0,\n                a\u2081 = 3.4, a\u2082 = 3.4, a\u2083 = 3.4,\n                b = 0.5, c = 0.3, d = 0.5)\n    p = [a\u2081, a\u2082, a\u2083, b, c, d, \u03c3\u2081, \u03c3\u2082, \u03c3\u2083]\n    return DiscreteDynamicalSystem(eom_linear3d_nonlinearcoupling, u\u1d62, p)\nend  An example realization of the system is:  s = linear3d_nonlinearcoupling()\norbit = trajectory(s, 100)\nx1, x2, x3 = orbit[:, 1], orbit[:, 2], orbit[:, 3]\nplot(x1, label =  x , lc = :black)\nplot!(x2, label =  y , lc = :red)\nplot!(x3, label =  z , lc = :blue)\nxlabel!( Time step ); ylabel!( Value )", 
            "title": "Represent as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/example_systems/linearmap3d_nonlinearcoupling/#predefined-system", 
            "text": "This system is predefined in  CausalityTools.Systems , and can be initialized using the  linearmap3d_nonlinearcoupling  function.", 
            "title": "Predefined system"
        }, 
        {
            "location": "/example_systems/linearmap3d_nonlinearcoupling/#references", 
            "text": "Gour\u00e9vitch, B., Le Bouquin-Jeann\u00e8s, R.,   Faucon, G. (2006). Linear and nonlinear causality between signals: methods, examples and neurophysiological applications. Biological Cybernetics, 95(4), 349\u2013369.  https://link.springer.com/article/10.1007/s00422-006-0098-0", 
            "title": "References"
        }, 
        {
            "location": "/example_systems/verdes/", 
            "text": "Synthetic coupled dynamical systems\n\n\n\n\nNonlinear response of two periodic drivers\n\n\nFor this example, we'll consider a where the response $X$ is a highly nonlinear combination of two drivers $Y$ and $Z$.\n\n\nThe system is given by the following difference equations:\n\n\n\n\n\n\\begin{aligned}\nx(t+1) &= \\dfrac{y(t)(18y(t) - 27y(t)^2 + 10)}{2} + z(t)(1-z(t)) + \\eta_x \\\\\ny(t+1) &= \\dfrac{1 - \\dfrac{\\\\cos(2 \\pi)}{\\omega y}t}{2} + \\eta_y \\\\\nz(t+1) &= \\dfrac{1 - \\dfrac{\\\\sin(2 \\pi)}{\\omega z}t}{2} + \\eta_z,\n\\end{aligned}\n\n\n\n\n\nwhere $\u03b7_x$, $\u03b7_y$ and $\u03b7_z$ are gaussian noise terms with mean 0 and standard deviations $\u03c3_x$, $\u03c3_y$ and $\u03c3_z$.\n\n\n\n\nWhere has the system been used?\n\n\nThis system was used in \nVerdes (2005)\n where he studies a nonparametric test for causality in weakly coupled systems.\n\n\n\n\nRepresent as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction eom_verdes(u, p, t)\n    x, y, z = (u...,)\n    \u03c9y, \u03c9z, \u03c3x, \u03c3y, \u03c3z = (p...,)\n\n    \u03b7x = \u03c3x == 0 ? 0 : rand(Normal(0, \u03c3x))\n    \u03b7y = \u03c3y == 0 ? 0 : rand(Normal(0, \u03c3y))\n    \u03b7z = \u03c3z == 0 ? 0 : rand(Normal(0, \u03c3z))\n\n    dx = y*(18y - 27y^2 + 10)/2 + z*(1-z) + \u03b7x\n    dy = (1 - cos((2*pi/\u03c9y) * t))/2 + \u03b7y\n    dz = (1 - sin((2*pi/\u03c9z) * t))/2 + \u03b7z\n    return SVector{3}(dx, dy, dz)\nend\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters \nr\u2081\n, \nr\u2082\n and \nr\u2083\n, initial condition \nu\u2080\n, and dynamical noise levels \n\u03c3x\n, \n\u03c3y\n and \n\u03c3z\n.\n\n\nfunction verdes(;u\u2080 = rand(3), \u03c9y = 315, \u03c9z = 80,\n                \u03c3x = 0.01, \u03c3y = 0.01, \u03c3z = 0.01)\n    p = [\u03c9y, \u03c9z, \u03c3x, \u03c3y, \u03c3z]\n    DiscreteDynamicalSystem(eom_verdes, u\u2080, p)\nend\n\n\n\n\nAn example realization of the system is:\n\n\ns = verdes()\norbit = trajectory(s, 500)\nx, y, z = orbit[:, 1], orbit[:, 2], orbit[:, 3]\nplot(x, label = \nx\n, lc = :black)\nplot!(y, label = \ny\n, lc = :red)\nplot!(z, label = \nz\n, lc = :blue)\nxlabel!(\nTime step\n); ylabel!(\nValue\n)\n\n\n\n\n\n\n\n\nPredefined system\n\n\nThis system is predefined in \nCausalityTools.Systems\n, and can be initialized using the \nverdes\n function.\n\n\n\n\nReferences\n\n\nVerdes, P. F. \"Assessing causality from multivariate time series.\" Physical Review E 72.2 (2005): 026222. \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.72.026222", 
            "title": "verdes"
        }, 
        {
            "location": "/example_systems/verdes/#synthetic-coupled-dynamical-systems", 
            "text": "", 
            "title": "Synthetic coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/verdes/#nonlinear-response-of-two-periodic-drivers", 
            "text": "For this example, we'll consider a where the response $X$ is a highly nonlinear combination of two drivers $Y$ and $Z$.  The system is given by the following difference equations:   \n\\begin{aligned}\nx(t+1) &= \\dfrac{y(t)(18y(t) - 27y(t)^2 + 10)}{2} + z(t)(1-z(t)) + \\eta_x \\\\\ny(t+1) &= \\dfrac{1 - \\dfrac{\\\\cos(2 \\pi)}{\\omega y}t}{2} + \\eta_y \\\\\nz(t+1) &= \\dfrac{1 - \\dfrac{\\\\sin(2 \\pi)}{\\omega z}t}{2} + \\eta_z,\n\\end{aligned}   where $\u03b7_x$, $\u03b7_y$ and $\u03b7_z$ are gaussian noise terms with mean 0 and standard deviations $\u03c3_x$, $\u03c3_y$ and $\u03c3_z$.", 
            "title": "Nonlinear response of two periodic drivers"
        }, 
        {
            "location": "/example_systems/verdes/#where-has-the-system-been-used", 
            "text": "This system was used in  Verdes (2005)  where he studies a nonparametric test for causality in weakly coupled systems.", 
            "title": "Where has the system been used?"
        }, 
        {
            "location": "/example_systems/verdes/#represent-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function eom_verdes(u, p, t)\n    x, y, z = (u...,)\n    \u03c9y, \u03c9z, \u03c3x, \u03c3y, \u03c3z = (p...,)\n\n    \u03b7x = \u03c3x == 0 ? 0 : rand(Normal(0, \u03c3x))\n    \u03b7y = \u03c3y == 0 ? 0 : rand(Normal(0, \u03c3y))\n    \u03b7z = \u03c3z == 0 ? 0 : rand(Normal(0, \u03c3z))\n\n    dx = y*(18y - 27y^2 + 10)/2 + z*(1-z) + \u03b7x\n    dy = (1 - cos((2*pi/\u03c9y) * t))/2 + \u03b7y\n    dz = (1 - sin((2*pi/\u03c9z) * t))/2 + \u03b7z\n    return SVector{3}(dx, dy, dz)\nend  To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters  r\u2081 ,  r\u2082  and  r\u2083 , initial condition  u\u2080 , and dynamical noise levels  \u03c3x ,  \u03c3y  and  \u03c3z .  function verdes(;u\u2080 = rand(3), \u03c9y = 315, \u03c9z = 80,\n                \u03c3x = 0.01, \u03c3y = 0.01, \u03c3z = 0.01)\n    p = [\u03c9y, \u03c9z, \u03c3x, \u03c3y, \u03c3z]\n    DiscreteDynamicalSystem(eom_verdes, u\u2080, p)\nend  An example realization of the system is:  s = verdes()\norbit = trajectory(s, 500)\nx, y, z = orbit[:, 1], orbit[:, 2], orbit[:, 3]\nplot(x, label =  x , lc = :black)\nplot!(y, label =  y , lc = :red)\nplot!(z, label =  z , lc = :blue)\nxlabel!( Time step ); ylabel!( Value )", 
            "title": "Represent as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/example_systems/verdes/#predefined-system", 
            "text": "This system is predefined in  CausalityTools.Systems , and can be initialized using the  verdes  function.", 
            "title": "Predefined system"
        }, 
        {
            "location": "/example_systems/verdes/#references", 
            "text": "Verdes, P. F. \"Assessing causality from multivariate time series.\" Physical Review E 72.2 (2005): 026222.  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.72.026222", 
            "title": "References"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/", 
            "text": "Interop with DynamicalSystems.jl\n\n\n\n\nExample 1: Measuring information flow between two unidirectionally coupled logistic maps\n\n\nHere, we present an example of how one can measure the information flow between variables of a dynamical system using transfer entropy (TE).\n\n\n\n\nDefining a system\n\n\nFor this example, we'll consider a unidirectionally coupled system consisting of two logistic maps, given by the vector field\n\n\n\n\n\n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}\n\n\n\n\n\nwith\n\n\n\n\n\n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}\n\n\n\n\n\nThe parameter \nc\n controls how strong the dynamical forcing is. If \n\u03c3 \n 0\n, dynamical noise masking the influence of  $x$ on $y$, equivalent to $\\sigma \\cdot \\xi$, is added at each iteration. Here, $\\xi$ is a draw from a flat distribution on \n[0, 1]\n. Thus, setting \n\u03c3 = 0.05\n is equivalent to add dynamical noise corresponding to a maximum of $5 \\%$ of the possible range of values of the logistic map.\n\n\n\n\nRepresent as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction eom_logistic2(dx, x, p, n)\n    c, r\u2081, r\u2082, \u03c3 = (p...,)\n    \u03be = rand() # random number from flat distribution on [0, 1]\n    x, y = x[1], x[2]\n    f_xy = (y +  (c*(x + \u03c3*\u03be)/2) ) / (1 + (c/2)*(1+\u03c3))\n\n    dx[1] = r\u2081 * x * (1 - x)\n    dx[2] = r\u2082 * (f_xy) * (1 - f_xy)\n    return\nend\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters \nr\u2081\n and \nr\u2082\n, coupling strength \nc\n, initial condition \nu\u2080\n and dynamical noise level \n\u03c3\n. Selecting parameter values on \n[3.6, 4.0]\n yield mostly chaotic realizations of the maps, so we set the default to some random values on this interval.\n\n\nfunction logistic2(;u\u2080 = rand(2), c = 0.0, r\u2081 = 3.66, r\u2082 = 3.77, \u03c3 = 0.05)\n    p = [c, r\u2081, r\u2082, \u03c3]\n    DiscreteDynamicalSystem(eom_logistic2, u\u2080, p)\nend\n\n\n\n\nBy tuning the coupling strength \nc\n, we may control the strength of the influence $x$ has on $y$. Depending on the particular values of \nr\u2081\n and \nr\u2082\n, the subsystems become synchronized at different values of \nc\n. Choosing \nc \u2208 [0, 2]\n usually still gives some independence between the subsystems.\n\n\nAn example realization of the system when there is no coupling is:\n\n\ns = logistic2(c = 0.0)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label = \nx\n, lc = :black)\nplot!(y, label = \ny\n, lc = :red)\nxlabel!(\nTime step\n); ylabel!(\nValue\n)\n\n\n\n\n\n\n\n\nDelay embedding for TE\n\n\nThe minimum embedding dimension for this system is 4 (try to figure this out yourself using the machinery in DynamicalSystems.jl!).\n\n\nWe want to measure the information flow $x \\rightarrow y$. To do this, we express the transfer entropy as a conditional mutual information. For that, we need an embedding consisting of the following set of vectors\n\n\n\n\n\n\\begin{aligned}\nE = \\{ (y(t + \\nu), y(t), x(t), y(t - \\tau) ) \\},\n\\end{aligned}\n\n\n\n\n\nwhere $\\nu$ is the forward prediction lag and $\\tau$ is the embedding lag. If a higher dimension was needed, we would add more lagged instances of the target variable $y$.\n\n\n\n\nConstruct the embedding\n\n\nTo construct the embedding, we use the \nembed\n function as follows.\n\n\n\u03c4 = 1 # embedding lag\n\u03bd = 1 # forward prediction lag\nE = StateSpaceReconstruction.embed([x, y], [2, 2, 2, 1], [\u03bd, 0, -\u03c4, 0])\n\n\n\n\nEmbedding{4,Float64}([0.495132 0.942411 \u2026 0.204319 0.6129; 0.155522 0.495132 \u2026 0.942497 0.204319; 0.956889 0.155522 \u2026 0.499156 0.942497; 0.586572 0.88757 \u2026 0.745788 0.693892], EmbeddingData{4,Float64} consisting of 2 data series.\n)\n\n\n\n\nThis means that \ny\n appears in the 1st, 2nd and 3rd columns of the embedding, with lags 1, 0 and -1, respectively. The 4th column is occupied by \nx\n, which is not lagged.\n\n\n\n\nKeeping track of embedding information using TEVars\n\n\nKeeping track of how the embedding is organized is done using a \nTEVars\n instance, which has the following constructor:\n\n\nTEVars(target_future::Vector{Int},         target_presentpast::Vector{Int},         source_presentpast::Vector{Int})\n\n\nIt takes requires as inputs the column indices corresponding to 1) the future of the target variable, 2) the present and past of the target, and 3) the present and past of the source variable, in that order. Let's define this for our system:\n\n\nTf = [1]     # target, future\nTpp = [2, 3] # target, present and past\nSpp = [4]    # source, present (and past, if we wanted)\nv = TEVars(Tf, Tpp, Spp)\n\n\n\n\nTEVars([1], [2, 3], [4], Int64[])\n\n\n\n\nThe last field is an empty array because we are not doing any conditioning on other variables.\n\n\n\n\nTE estimator\n\n\nWe will use the transfer operator grid TE estimator, found in the \ntransferentropy_transferoperator_grid\n function, or its alias \ntetogrid\n. This estimator takes as input an embedding \nE\n, an \n\u03f5\n giving the binning scheme, and a \nTEVars instance\n. We will compute TE over a range of bin sizes, for a slightly longer time series than we plotted before, with \nc = 0.7\n.\n\n\n\n\nEmbedding\n\n\nLet's create a realization of the system, embed it and create a \nTEVars\n instance. We'll use these throughout the examples below.\n\n\n# Orbit of the system\ns = logistic2(c = 0.7)\norbit = trajectory(s, 500)\nx, y = orbit[:, 1], orbit[:, 2]\n\n# Embedding\n\u03c4 = 1 # embedding lag\n\u03bd = 1 # forward prediction lag\nE_xtoy = StateSpaceReconstruction.embed([x, y], [2, 2, 2, 1], [\u03bd, 0, -\u03c4, 0])\nE_ytox = StateSpaceReconstruction.embed([y, x], [2, 2, 2, 1], [\u03bd, 0, -\u03c4, 0])\n\n# Which variables go where?\nTf = [1]     # target, future\nTpp = [2, 3] # target, present and past\nSpp = [4]    # source, present (and past, if we wanted)\nv = TEVars(Tf, Tpp, Spp)\n\n\n\n\nTEVars([1], [2, 3], [4], Int64[])\n\n\n\n\n\n\nDifferent ways of partitioning\n\n\nThe \ntransferentropy_transferoperator_grid\n and \ntransferentropy_freq\n estimators both operate on partitions on the state space.\n\n\nThere are four different ways of partitioning the state space. The partition scheme is controlled by \n\u03f5\n, and the following \n\u03f5\n will work:\n\n\n\n\n\u03f5::Int\n divide each axis into \n\u03f5\n intervals of the same size.\n\n\n\u03f5::Float\n divide each axis into intervals of size \n\u03f5\n.\n\n\n\u03f5::Vector{Int}\n divide the i-th axis into \n\u03f5\u1d62\n intervals of the same size.\n\n\n\u03f5::Vector{Float64}\n divide the i-th axis into intervals of size \n\u03f5\u1d62\n.\n\n\n\n\nBelow, we demonstrate how TE may be computed using the four different ways of discretizing the state space.\n\n\n\n\nHyper-rectangles by subdivision of axes\n\n\nFirst, we use an integer number of subdivisions along each axis of the delay embedding when partitioning (\n\u03f5::Int\n).\n\n\n\u03f5s = 1:2:50 # integer number of subdivisions along each axis of the embedding\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nfor (i, \u03f5) in enumerate(\u03f5s)\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\nend\n\nplot(\u03f5s, te_estimates_xtoy, label = \nTE(x -\n y)\n, lc = :black)\nplot!(\u03f5s, te_estimates_ytox, label = \nTE(y -\n x)\n, lc = :red)\nxlabel!(\n# subdivisions along each axis\n)\nylabel!(\nTransfer entropy (bits)\n)\n\n\n\n\n\n\nAs expected, there is much higher information flow from \nx\n to \ny\n (where there is an underlying coupling) than from \ny\n to \nx\n, where there is no underlying coupling.\n\n\n\n\nHyper-cubes of fixed size\n\n\nWe do precisely the same, but use fixed-width hyper-cube bins (\n\u03f5::Float\n). The values of the logistic map take values on \n[0, 1]\n, so using bins width edge lengths \n0.1\n should give a covering corresponding to using \n10\n subdivisions along each axis of the delay embedding. We let \n\u03f5\n take values on \n[0.05, 0.5]\n\n\n\u03f5s = 0.05:0.05:0.5\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nfor (i, \u03f5) in enumerate(\u03f5s)\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\nend\n\nplot(\u03f5s, te_estimates_xtoy, label = \nTE(x -\n y)\n, lc = :black)\nplot!(\u03f5s, te_estimates_ytox, label = \nTE(y -\n x)\n, lc = :red)\nxlabel!(\nHypercube edge length\n)\nylabel!(\nTransfer entropy (bits)\n)\nxflip!()\n\n\n\n\n\n\n\n\nHyper-rectangles of fixed size\n\n\nIt is also possible to use hyper-rectangles  (\n\u03f5::Vector{Float}\n), by specifying the edge lengths along each coordinate axis of the delay embedding. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector of edge lengths.\n\n\n# Define slightly different edge lengths along each axis\n\u03f5s_x1 = LinRange(0.05, 0.5, 10)\n\u03f5s_x2 = LinRange(0.02, 0.4, 10)\n\u03f5s_x3 = LinRange(0.08, 0.6, 10)\n\u03f5s_x4 = LinRange(0.10, 0.3, 10)\n\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nmean_\u03f5s = zeros(10)\nfor i \u2208 1:10\n    \u03f5 = [\u03f5s_x1[i], \u03f5s_x2[i], \u03f5s_x3[i], \u03f5s_x4[i]]\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\n\n    # Store average edge length (for plotting)\n    mean_\u03f5s[i] = mean(\u03f5)\nend\n\nplot(mean_\u03f5s, te_estimates_xtoy, label = \nTE(x -\n y)\n, lc = :black)\nplot!(mean_\u03f5s, te_estimates_ytox, label = \nTE(y -\n x)\n, lc = :red)\nxlabel!(\nAverage hypercube edge length\n)\nylabel!(\nTransfer entropy (bits)\n)\nxflip!()\n\n\n\n\n\n\n\n\nHyper-rectangles by variable-width subdivision of axes\n\n\nAnother way to construct hyper-rectangles is to subdivide each coordinate axis into segments of equal length (\n\u03f5::Vector{Int}\n). In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector providing the number of subdivisions we want along each axis.\n\n\n# Define different number of subdivisions along each axis.\n\u03f5s = 3:50\nmean_\u03f5s = zeros(length(\u03f5s))\n\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nfor (i, \u03f5\u1d62) \u2208 enumerate(\u03f5s)\n    \u03f5 = [\u03f5\u1d62 - 1, \u03f5\u1d62, \u03f5\u1d62, \u03f5\u1d62 + 1]\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\n\n    # Store average number of subdivisions for plotting\n    mean_\u03f5s[i] = mean(\u03f5)\nend\n\nplot(mean_\u03f5s, te_estimates_xtoy, label = \nTE(x -\n y)\n, lc = :black)\nplot!(mean_\u03f5s, te_estimates_ytox, label = \nTE(y -\n x)\n, lc = :red)\nxlabel!(\nAverage number of subdivisions along the embedding axes\n)\nylabel!(\nTransfer entropy (bits)\n)\n\n\n\n\n\n\n\n\nConclusion\n\n\nThe value of the TE depends on the system under consideration, and on the way one chooses to discretize the state space reconstruction.\n\n\nFor this example, TE is consistently larger for the expected direction \nTE(x -\n y)\n than in the opposite direction \nTE(y -\n x)\n, where we expect no information flow.", 
            "title": "Example 1 - Information flow between coupled logistic maps"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#interop-with-dynamicalsystemsjl", 
            "text": "", 
            "title": "Interop with DynamicalSystems.jl"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#example-1-measuring-information-flow-between-two-unidirectionally-coupled-logistic-maps", 
            "text": "Here, we present an example of how one can measure the information flow between variables of a dynamical system using transfer entropy (TE).", 
            "title": "Example 1: Measuring information flow between two unidirectionally coupled logistic maps"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#defining-a-system", 
            "text": "For this example, we'll consider a unidirectionally coupled system consisting of two logistic maps, given by the vector field   \n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}   with   \n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}   The parameter  c  controls how strong the dynamical forcing is. If  \u03c3   0 , dynamical noise masking the influence of  $x$ on $y$, equivalent to $\\sigma \\cdot \\xi$, is added at each iteration. Here, $\\xi$ is a draw from a flat distribution on  [0, 1] . Thus, setting  \u03c3 = 0.05  is equivalent to add dynamical noise corresponding to a maximum of $5 \\%$ of the possible range of values of the logistic map.", 
            "title": "Defining a system"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#represent-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function eom_logistic2(dx, x, p, n)\n    c, r\u2081, r\u2082, \u03c3 = (p...,)\n    \u03be = rand() # random number from flat distribution on [0, 1]\n    x, y = x[1], x[2]\n    f_xy = (y +  (c*(x + \u03c3*\u03be)/2) ) / (1 + (c/2)*(1+\u03c3))\n\n    dx[1] = r\u2081 * x * (1 - x)\n    dx[2] = r\u2082 * (f_xy) * (1 - f_xy)\n    return\nend  To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters  r\u2081  and  r\u2082 , coupling strength  c , initial condition  u\u2080  and dynamical noise level  \u03c3 . Selecting parameter values on  [3.6, 4.0]  yield mostly chaotic realizations of the maps, so we set the default to some random values on this interval.  function logistic2(;u\u2080 = rand(2), c = 0.0, r\u2081 = 3.66, r\u2082 = 3.77, \u03c3 = 0.05)\n    p = [c, r\u2081, r\u2082, \u03c3]\n    DiscreteDynamicalSystem(eom_logistic2, u\u2080, p)\nend  By tuning the coupling strength  c , we may control the strength of the influence $x$ has on $y$. Depending on the particular values of  r\u2081  and  r\u2082 , the subsystems become synchronized at different values of  c . Choosing  c \u2208 [0, 2]  usually still gives some independence between the subsystems.  An example realization of the system when there is no coupling is:  s = logistic2(c = 0.0)\norbit = trajectory(s, 100)\nx, y = orbit[:, 1], orbit[:, 2]\nplot(x, label =  x , lc = :black)\nplot!(y, label =  y , lc = :red)\nxlabel!( Time step ); ylabel!( Value )", 
            "title": "Represent as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#delay-embedding-for-te", 
            "text": "The minimum embedding dimension for this system is 4 (try to figure this out yourself using the machinery in DynamicalSystems.jl!).  We want to measure the information flow $x \\rightarrow y$. To do this, we express the transfer entropy as a conditional mutual information. For that, we need an embedding consisting of the following set of vectors   \n\\begin{aligned}\nE = \\{ (y(t + \\nu), y(t), x(t), y(t - \\tau) ) \\},\n\\end{aligned}   where $\\nu$ is the forward prediction lag and $\\tau$ is the embedding lag. If a higher dimension was needed, we would add more lagged instances of the target variable $y$.", 
            "title": "Delay embedding for TE"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#construct-the-embedding", 
            "text": "To construct the embedding, we use the  embed  function as follows.  \u03c4 = 1 # embedding lag\n\u03bd = 1 # forward prediction lag\nE = StateSpaceReconstruction.embed([x, y], [2, 2, 2, 1], [\u03bd, 0, -\u03c4, 0])  Embedding{4,Float64}([0.495132 0.942411 \u2026 0.204319 0.6129; 0.155522 0.495132 \u2026 0.942497 0.204319; 0.956889 0.155522 \u2026 0.499156 0.942497; 0.586572 0.88757 \u2026 0.745788 0.693892], EmbeddingData{4,Float64} consisting of 2 data series.\n)  This means that  y  appears in the 1st, 2nd and 3rd columns of the embedding, with lags 1, 0 and -1, respectively. The 4th column is occupied by  x , which is not lagged.", 
            "title": "Construct the embedding"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#keeping-track-of-embedding-information-using-tevars", 
            "text": "Keeping track of how the embedding is organized is done using a  TEVars  instance, which has the following constructor:  TEVars(target_future::Vector{Int},         target_presentpast::Vector{Int},         source_presentpast::Vector{Int})  It takes requires as inputs the column indices corresponding to 1) the future of the target variable, 2) the present and past of the target, and 3) the present and past of the source variable, in that order. Let's define this for our system:  Tf = [1]     # target, future\nTpp = [2, 3] # target, present and past\nSpp = [4]    # source, present (and past, if we wanted)\nv = TEVars(Tf, Tpp, Spp)  TEVars([1], [2, 3], [4], Int64[])  The last field is an empty array because we are not doing any conditioning on other variables.", 
            "title": "Keeping track of embedding information using TEVars"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#te-estimator", 
            "text": "We will use the transfer operator grid TE estimator, found in the  transferentropy_transferoperator_grid  function, or its alias  tetogrid . This estimator takes as input an embedding  E , an  \u03f5  giving the binning scheme, and a  TEVars instance . We will compute TE over a range of bin sizes, for a slightly longer time series than we plotted before, with  c = 0.7 .", 
            "title": "TE estimator"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#embedding", 
            "text": "Let's create a realization of the system, embed it and create a  TEVars  instance. We'll use these throughout the examples below.  # Orbit of the system\ns = logistic2(c = 0.7)\norbit = trajectory(s, 500)\nx, y = orbit[:, 1], orbit[:, 2]\n\n# Embedding\n\u03c4 = 1 # embedding lag\n\u03bd = 1 # forward prediction lag\nE_xtoy = StateSpaceReconstruction.embed([x, y], [2, 2, 2, 1], [\u03bd, 0, -\u03c4, 0])\nE_ytox = StateSpaceReconstruction.embed([y, x], [2, 2, 2, 1], [\u03bd, 0, -\u03c4, 0])\n\n# Which variables go where?\nTf = [1]     # target, future\nTpp = [2, 3] # target, present and past\nSpp = [4]    # source, present (and past, if we wanted)\nv = TEVars(Tf, Tpp, Spp)  TEVars([1], [2, 3], [4], Int64[])", 
            "title": "Embedding"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#different-ways-of-partitioning", 
            "text": "The  transferentropy_transferoperator_grid  and  transferentropy_freq  estimators both operate on partitions on the state space.  There are four different ways of partitioning the state space. The partition scheme is controlled by  \u03f5 , and the following  \u03f5  will work:   \u03f5::Int  divide each axis into  \u03f5  intervals of the same size.  \u03f5::Float  divide each axis into intervals of size  \u03f5 .  \u03f5::Vector{Int}  divide the i-th axis into  \u03f5\u1d62  intervals of the same size.  \u03f5::Vector{Float64}  divide the i-th axis into intervals of size  \u03f5\u1d62 .   Below, we demonstrate how TE may be computed using the four different ways of discretizing the state space.", 
            "title": "Different ways of partitioning"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#hyper-rectangles-by-subdivision-of-axes", 
            "text": "First, we use an integer number of subdivisions along each axis of the delay embedding when partitioning ( \u03f5::Int ).  \u03f5s = 1:2:50 # integer number of subdivisions along each axis of the embedding\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nfor (i, \u03f5) in enumerate(\u03f5s)\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\nend\n\nplot(\u03f5s, te_estimates_xtoy, label =  TE(x -  y) , lc = :black)\nplot!(\u03f5s, te_estimates_ytox, label =  TE(y -  x) , lc = :red)\nxlabel!( # subdivisions along each axis )\nylabel!( Transfer entropy (bits) )   As expected, there is much higher information flow from  x  to  y  (where there is an underlying coupling) than from  y  to  x , where there is no underlying coupling.", 
            "title": "Hyper-rectangles by subdivision of axes"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#hyper-cubes-of-fixed-size", 
            "text": "We do precisely the same, but use fixed-width hyper-cube bins ( \u03f5::Float ). The values of the logistic map take values on  [0, 1] , so using bins width edge lengths  0.1  should give a covering corresponding to using  10  subdivisions along each axis of the delay embedding. We let  \u03f5  take values on  [0.05, 0.5]  \u03f5s = 0.05:0.05:0.5\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nfor (i, \u03f5) in enumerate(\u03f5s)\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\nend\n\nplot(\u03f5s, te_estimates_xtoy, label =  TE(x -  y) , lc = :black)\nplot!(\u03f5s, te_estimates_ytox, label =  TE(y -  x) , lc = :red)\nxlabel!( Hypercube edge length )\nylabel!( Transfer entropy (bits) )\nxflip!()", 
            "title": "Hyper-cubes of fixed size"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#hyper-rectangles-of-fixed-size", 
            "text": "It is also possible to use hyper-rectangles  ( \u03f5::Vector{Float} ), by specifying the edge lengths along each coordinate axis of the delay embedding. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector of edge lengths.  # Define slightly different edge lengths along each axis\n\u03f5s_x1 = LinRange(0.05, 0.5, 10)\n\u03f5s_x2 = LinRange(0.02, 0.4, 10)\n\u03f5s_x3 = LinRange(0.08, 0.6, 10)\n\u03f5s_x4 = LinRange(0.10, 0.3, 10)\n\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nmean_\u03f5s = zeros(10)\nfor i \u2208 1:10\n    \u03f5 = [\u03f5s_x1[i], \u03f5s_x2[i], \u03f5s_x3[i], \u03f5s_x4[i]]\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\n\n    # Store average edge length (for plotting)\n    mean_\u03f5s[i] = mean(\u03f5)\nend\n\nplot(mean_\u03f5s, te_estimates_xtoy, label =  TE(x -  y) , lc = :black)\nplot!(mean_\u03f5s, te_estimates_ytox, label =  TE(y -  x) , lc = :red)\nxlabel!( Average hypercube edge length )\nylabel!( Transfer entropy (bits) )\nxflip!()", 
            "title": "Hyper-rectangles of fixed size"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#hyper-rectangles-by-variable-width-subdivision-of-axes", 
            "text": "Another way to construct hyper-rectangles is to subdivide each coordinate axis into segments of equal length ( \u03f5::Vector{Int} ). In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector providing the number of subdivisions we want along each axis.  # Define different number of subdivisions along each axis.\n\u03f5s = 3:50\nmean_\u03f5s = zeros(length(\u03f5s))\n\nte_estimates_xtoy = zeros(length(\u03f5s))\nte_estimates_ytox = zeros(length(\u03f5s))\n\nfor (i, \u03f5\u1d62) \u2208 enumerate(\u03f5s)\n    \u03f5 = [\u03f5\u1d62 - 1, \u03f5\u1d62, \u03f5\u1d62, \u03f5\u1d62 + 1]\n    te_estimates_xtoy[i] = tetogrid(E_xtoy, \u03f5, v)\n    te_estimates_ytox[i] = tetogrid(E_ytox, \u03f5, v)\n\n    # Store average number of subdivisions for plotting\n    mean_\u03f5s[i] = mean(\u03f5)\nend\n\nplot(mean_\u03f5s, te_estimates_xtoy, label =  TE(x -  y) , lc = :black)\nplot!(mean_\u03f5s, te_estimates_ytox, label =  TE(y -  x) , lc = :red)\nxlabel!( Average number of subdivisions along the embedding axes )\nylabel!( Transfer entropy (bits) )", 
            "title": "Hyper-rectangles by variable-width subdivision of axes"
        }, 
        {
            "location": "/interop_dynamicalsystems_infoflow1/#conclusion", 
            "text": "The value of the TE depends on the system under consideration, and on the way one chooses to discretize the state space reconstruction.  For this example, TE is consistently larger for the expected direction  TE(x -  y)  than in the opposite direction  TE(y -  x) , where we expect no information flow.", 
            "title": "Conclusion"
        }
    ]
}