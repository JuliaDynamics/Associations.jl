
@article{Sun2015,
    author = {Sun, Jie and Taylor, Dane and Bollt, Erik M.},
    title = {Causal Network Inference by Optimal Causation Entropy},
    journal = {SIAM Journal on Applied Dynamical Systems},
    volume = {14},
    number = {1},
    pages = {73-106},
    year = {2015},
    doi = {10.1137/140956166},
    URL = {https://doi.org/10.1137/140956166},
    eprint = {https://doi.org/10.1137/140956166},
    abstract = {The broad abundance of time series data, which is in sharp contrast to limited knowledge of the underlying network dynamic processes that produce such observations, calls for a rigorous and efficient method of causal network inference. Here we develop mathematical theory of causation entropy, an information-theoretic statistic designed for model-free causality inference. For stationary Markov processes, we prove that for a given node in the network, its causal parents form the minimal set of nodes that maximizes causation entropy, a result we refer to as the optimal causation entropy principle. Furthermore, this principle guides us in developing computational and data efficient algorithms for causal network inference based on a two-step discovery and removal algorithm for time series data for a network-coupled dynamical system. Validation in terms of analytical and numerical results for Gaussian processes on large random networks highlights that inference by our algorithm outperforms previous leading methods, including conditional Granger causality and transfer entropy. Interestingly, our numerical results suggest that the number of samples required for accurate inference depends strongly on network characteristics such as the density of links and information diffusion rate and not necessarily on the number of nodes. }
}

@article{Kalisch2008,
    author = {Markus Kalisch and Peter Bühlmann},
    title = {Robustification of the PC-Algorithm for Directed Acyclic Graphs},
    journal = {Journal of Computational and Graphical Statistics},
    volume = {17},
    number = {4},
    pages = {773-789},
    year  = {2008},
    publisher = {Taylor & Francis},
    doi = {10.1198/106186008X381927},
    URL = {https://doi.org/10.1198/106186008X381927},
    eprint = {https://doi.org/10.1198/106186008X381927},
    abstract = {The PC-algorithm was shown to be a powerful method for estimating the equivalence class of a potentially very high-dimensional acyclic directed graph (DAG) with the corresponding Gaussian distribution. Here we propose a computationally eficient robustification of the PC-algorithm and prove its consistency. Furthermore, we compare the robustified and standard version of the PC-algorithm on simulated data using the new corresponding R package pcalg. }
}

@book{Spirtes2000,
    title={Causation, prediction, and search},
    author={Spirtes, Peter and Glymour, Clark N and Scheines, Richard},
    year={2000},
    publisher={MIT press},
}

@article{Colombo2014,
    title={Order-independent constraint-based causal structure learning},
    author={Diego Colombo and Marloes H. Maathuis},
    journal={J. Mach. Learn. Res.},
    year={2014},
    volume={15},
    pages={3741-3782},
    doi={10.5555/2627435.2750365},
    url={https://doi.org/10.5555/2627435.2750365},
}

@article{Sugihara2012,
    title={Detecting Causality in Complex Ecosystems},
    author={George Sugihara and Robert M. May and Hao Ye and Chih-hao Hsieh and Ethan R. Deyle and Michael Fogarty and Stephan B. Munch},
    journal={Science},
    year={2012},
    volume={338},
    pages={496 - 500},
    doi={10.1126/science.1227079},
    url={https://doi.org/10.1126/science.1227079},
}

@article{Luo2015,
    title={Questionable dynamical evidence for causality between galactic cosmic rays and interannual variation in global temperature},
    author={Ming Luo and Holger Kantz and Ngar-Cheung Lau and Wenwen Huang and Yu Zhou},
    journal={Proceedings of the National Academy of Sciences},
    year={2015},
    volume={112},
    pages={E4638 - E4639},
    doi={10.1073/pnas.1510571112},
    url={https://doi.org/10.1073/pnas.1510571112},
}

@article{McCracken2014,
    title={Convergent cross-mapping and pairwise asymmetric inference},
    author={McCracken, James M and Weigel, Robert S},
    journal={Physical Review E},
    volume={90},
    number={6},
    pages={062903},
    year={2014},
    publisher={APS},
    doi={10.1103/PhysRevE.90.062903},
    url={https://doi.org/10.1103/PhysRevE.90.062903},
}

@misc{Haaga2020,
    title={A simple test for causality in complex systems}, 
    author={Kristian Agasøster Haaga and David Diego and Jo Brendryen and Bjarte Hannisdal},
    year={2020},
    eprint={2005.01860},
    archivePrefix={arXiv},
    primaryClass={stat.AP}
}

@article{Murali1993,
  title={Chaotic dynamics of the driven Chua's circuit},
  author={Murali, K and Lakshmanan, M},
  journal={IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume={40},
  number={11},
  pages={836--840},
  year={1993},
  publisher={IEEE},
  doi={10.1109/81.251823},
  url={https://doi.org/10.1109/81.251823}
}

@article{Tang2001,
  title={Generation of n-scroll attractors via sine function},
  author={Tang, Wallace KS and Zhong, GQ and Chen, G and Man, KF},
  journal={IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume={48},
  number={11},
  pages={1369--1372},
  year={2001},
  publisher={IEEE},
  doi={10.1109/81.964432},
  url={https://doi.org/10.1109/81.964432}
}

@article{Elowitz2000,
  title={A synthetic oscillatory network of transcriptional regulators},
  author={Elowitz, Michael B and Leibler, Stanislas},
  journal={Nature},
  volume={403},
  number={6767},
  pages={335--338},
  year={2000},
  publisher={Nature Publishing Group UK London},
  doi={10.1038/35002125},
  url={https://doi.org/10.1038/35002125}
}

@article{Sun2014,
  title={Identifying the coupling structure in complex systems through the optimal causation entropy principle},
  author={Sun, Jie and Cafaro, Carlo and Bollt, Erik M},
  journal={Entropy},
  volume={16},
  number={6},
  pages={3416--3433},
  year={2014},
  publisher={MDPI},
  doi={10.3390/e16063416},
  url={https://doi.org/10.3390/e16063416}
}

@article{Martini2011,
  title={Inferring directional interactions from transient signals with symbolic transfer entropy},
  author={Martini, Marcel and Kranz, Thorsten A and Wagner, Tobias and Lehnertz, Klaus},
  journal={Physical review E},
  volume={83},
  number={1},
  pages={011919},
  year={2011},
  publisher={APS},
  doi={10.1103/PHYSREVE.83.011919},
  url={https://doi.org/10.1103/PHYSREVE.83.011919}
}

@article{Papana2013,
  title={Simulation study of direct causality measures in multivariate time series},
  author={Papana, Angeliki and Kyrtsou, Catherine and Kugiumtzis, Dimitris and Diks, Cees},
  journal={Entropy},
  volume={15},
  number={7},
  pages={2635--2661},
  year={2013},
  publisher={MDPI},
  doi={10.3390/e15072635},
  url={https://doi.org/10.3390/e15072635}
}

@article{Anishchenko1998,
  title={Irregular attractors},
  author={Anishchenko, Vadim S and Strelkova, Galina I and others},
  journal={Discrete dynamics in Nature and Society},
  volume={2},
  pages={53--72},
  year={1998},
  publisher={Hindawi},
  doi={10.1007/978-3-319-06871-8_8},
  url={https://doi.org/10.1007/978-3-319-06871-8_8}
}

@article{Chen2004,
  title={Analyzing multiple nonlinear time series with extended Granger causality},
  author={Chen, Yonghong and Rangarajan, Govindan and Feng, Jianfeng and Ding, Mingzhou},
  journal={Physics letters A},
  volume={324},
  number={1},
  pages={26--35},
  year={2004},
  publisher={Elsevier},
  doi={10.1016/j.physleta.2004.02.032},
  url={https://doi.org/10.1016/j.physleta.2004.02.032}
}

@article{Amigo2018,
  title={Detecting directional couplings from multivariate flows by the joint distance distribution.},
  author={José María Amigó and Yoshito Hirata},
  journal={Chaos},
  year={2018},
  volume={28},
  number={7},
  pages={075302},
  doi={10.1063/1.5010779},
  url={https://doi.org/10.1063/1.5010779}
}

@article{Schreiber2000,
  title={Measuring information transfer},
  author={Schreiber, Thomas},
  journal={Physical review letters},
  volume={85},
  number={2},
  pages={461},
  year={2000},
  publisher={APS},
  doi={10.1103/PhysRevLett.85.461},
  url={https://doi.org/10.1103/PhysRevLett.85.461}
}

@article{Verdes2005,
  title={Assessing causality from multivariate time series},
  author={Verdes, PF},
  journal={Physical Review E},
  volume={72},
  number={2},
  pages={026222},
  year={2005},
  publisher={APS},
  doi={10.1103/PHYSREVE.72.026222},
  url={https://doi.org/10.1103/PHYSREVE.72.026222}
}

@article{Cao1997,
  title={Modeling and predicting non-stationary time series},
  author={Cao, Liangyue and Mees, Alistair and Judd, Kevin},
  journal={International Journal of Bifurcation and Chaos},
  volume={7},
  number={08},
  pages={1823--1831},
  year={1997},
  publisher={World Scientific},
  doi={10.1142/S0218127497001394},
  url={https://doi.org/10.1142/S0218127497001394}
}

@article{Diego2019,
  title={Transfer entropy computation using the Perron-Frobenius operator},
  author={Diego, David and Haaga, Kristian Agasøster and Hannisdal, Bjarte},
  journal={Physical Review E},
  volume={99},
  number={4},
  pages={042212},
  year={2019},
  publisher={APS},
  doi={10.1103/PhysRevE.99.042212},
  url={https://doi.org/10.1103/PhysRevE.99.042212}
}

@article{Runge2018,
  title={Causal network reconstruction from time series: From theoretical assumptions to practical estimation},
  author={Runge, Jakob},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={28},
  number={7},
  year={2018},
  publisher={AIP Publishing}
}

@article{Ye2015,
  title={Distinguishing time-delayed causal interactions using convergent cross mapping},
  author={Ye, Hao and Deyle, Ethan R and Gilarranz, Luis J and Sugihara, George},
  journal={Scientific reports},
  volume={5},
  number={1},
  pages={14750},
  year={2015},
  publisher={Nature Publishing Group UK London},
  doi={10.1063/1.5025050},
  url={https://doi.org/10.1063/1.5025050}
}

@inproceedings{Runge2018LocalPerm,
  title = 	 {Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information},
  author = 	 {Runge, Jakob},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {938--947},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/runge18a/runge18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/runge18a.html},
  abstract = 	 {Conditional independence testing is a fundamental problem underlying causal discovery and a particularly challenging task in the presence of nonlinear dependencies. Here a fully non-parametric test for continuous data based on conditional mutual information combined with a local permutation scheme is presented. Numerical experiments covering sample sizes from $50$ to $2,000$ and dimensions up to $10$ demonstrate that the test reliably generates the null distribution. For smooth nonlinear dependencies, the test has higher power than kernel-based tests in lower dimensions and similar or slightly lower power in higher dimensions. For highly non-smooth densities the data-adaptive nearest neighbor approach is particularly well-suited while kernel methods yield much lower power. The experiments also show that kernel methods utilizing an analytical approximation of the null distribution are not well-calibrated for sample sizes below $1,000$. Combining the local permutation scheme with these kernel tests leads to better calibration but lower power.  For smaller sample sizes and lower dimensions, the proposed test is faster than random fourier feature-based kernel tests if (embarrassingly) parallelized, but the runtime increases more sharply with sample size and dimensionality. Thus, more theoretical research to analytically approximate the null distribution and speed up the estimation is desirable.  As illustrated on real data here, the test is ideally suited in combination with causal discovery algorithms.}
}

@article{Levy1978,
  title={Testing hypotheses concerning partial correlations: Some methods and discussion},
  author={Levy, Kenneth J and Narula, Subhash C},
  journal={International Statistical Review/Revue Internationale de Statistique},
  pages={215--218},
  year={1978},
  publisher={JSTOR},
  doi={10.2307/1402814},
  url={https://doi.org/10.2307/1402814}
}

@inproceedings{Schmidt2018,
  title={Order-independent constraint-based causal structure learning for gaussian distribution models using GPUs},
  author={Schmidt, Christopher and Huegle, Johannes and Uflacker, Matthias},
  booktitle={Proceedings of the 30th International Conference on Scientific and Statistical Database Management},
  pages={1--10},
  year={2018},
  doi={10.1145/3221269.3221292},
  url={https://doi.org/10.1145/3221269.3221292}
}

@article{Arnhold1999,
  title={A robust method for detecting interdependences: application to intracranially recorded EEG},
  author={Arnhold, Jochen and Grassberger, Peter and Lehnertz, Klaus and Elger, Christian Erich},
  journal={Physica D: Nonlinear Phenomena},
  volume={134},
  number={4},
  pages={419--430},
  year={1999},
  publisher={Elsevier},
  doi={10.1016/S0167-2789(99)00140-2},
  url={https://doi.org/10.1016/S0167-2789(99)00140-2}
}

@article{Chicharro2009,
  title={Reliable detection of directional couplings using rank statistics},
  author={Chicharro, Daniel and Andrzejak, Ralph G},
  journal={Physical Review E},
  volume={80},
  number={2},
  pages={026217},
  year={2009},
  publisher={APS},
  doi={10.1103/PHYSREVE.80.026217},
  url={https://doi.org/10.1103/PHYSREVE.80.026217}
}

@article{Andrzejak2003,
  title={Bivariate surrogate techniques: necessity, strengths, and caveats},
  author={Andrzejak, Ralph G and Kraskov, Alexander and Stögbauer, Harald and Mormann, Florian and Kreuz, Thomas},
  journal={Physical review E},
  volume={68},
  number={6},
  pages={066202},
  year={2003},
  publisher={APS},
  doi={10.1103/PHYSREVE.68.066202},
  url={https://doi.org/10.1103/PHYSREVE.68.066202},
}

@article{Quiroga2000,
  title={Learning driver-response relationships from synchronization patterns},
  author={Quiroga, R Quian and Arnhold, Jochen and Grassberger, Peter},
  journal={Physical Review E},
  volume={61},
  number={5},
  pages={5142},
  year={2000},
  publisher={APS},
  doi={10.1103/PhysRevE.61.5142},
  url={https://doi.org/10.1103/PhysRevE.61.5142},
}

@article{Furuichi2006,
  title={Information theoretical properties of Tsallis entropies},
  author={Furuichi, Shigeru},
  journal={Journal of Mathematical Physics},
  volume={47},
  number={2},
  year={2006},
  publisher={AIP Publishing},
  doi={10.1063/1.2165744},
  url={https://doi.org/10.1063/1.2165744}
}
@book{CoverThomas1999,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}
@article{Golshani2009,
  title={Some properties of Rényi entropy and Rényi entropy rate},
  author={Golshani, Leila and Pasha, Einollah and Yari, Gholamhossein},
  journal={Information Sciences},
  volume={179},
  number={14},
  pages={2426--2433},
  year={2009},
  publisher={Elsevier}
}

@article{Zhao2016,
  title={Part mutual information for quantifying direct associations in networks},
  author={Zhao, Juan and Zhou, Yiwei and Zhang, Xiujun and Chen, Luonan},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={18},
  pages={5130--5135},
  year={2016},
  publisher={National Acad Sciences},
  doi={10.1073/pnas.1522586113},
  url={https://www.pnas.org/doi/pdf/10.1073/pnas.1522586113},
}

@article{Jizba2012,
  title={Rényi's information transfer between financial time series},
  author={Jizba, Petr and Kleinert, Hagen and Shefaat, Mohammad},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={391},
  number={10},
  pages={2971--2989},
  year={2012},
  publisher={Elsevier},
  doi={10.3390/engproc2021005033},
  url={https://doi.org/10.3390/engproc2021005033},
}

@inproceedings{Sarbu2014,
  title={Rényi information transfer: Partial Rényi transfer entropy and partial Rényi mutual information},
  author={Sarbu, Septimia},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5666--5670},
  year={2014},
  organization={IEEE},
  doi={10.1109/ICASSP.2014.6854688},
  url={https://doi.org/10.1109/ICASSP.2014.6854688},
}

@article{Vejmelka2008,
  title={Inferring the directionality of coupling with conditional mutual information},
  author={Vejmelka, Martin and Paluš, Milan},
  journal={Physical Review E},
  volume={77},
  number={2},
  pages={026214},
  year={2008},
  publisher={APS},
  doi={10.1103/PHYSREVE.77.026214},
  url={https://doi.org/10.1103/PHYSREVE.77.026214},
}

@article{Mesner2020,
  title={Conditional mutual information estimation for mixed, discrete and continuous data},
  author={Mesner, Octavio César and Shalizi, Cosma Rohilla},
  journal={IEEE Transactions on Information Theory},
  volume={67},
  number={1},
  pages={464--484},
  year={2020},
  publisher={IEEE},
  doi={10.1109/TIT.2020.3024886},
  url={https://doi.org/10.1109/TIT.2020.3024886},
}

@article{Rahimzamani2018,
  title={Estimators for multivariate information measures in general probability spaces},
  author={Rahimzamani, Arman and Asnani, Himanshu and Viswanath, Pramod and Kannan, Sreeram},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018},
  url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf},
}

@article{Abe2001,
  title={Nonadditive conditional entropy and its significance for local realism},
  author={Abe, Sumiyoshi and Rajagopal, Attipat K},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={289},
  number={1-2},
  pages={157--164},
  year={2001},
  publisher={Elsevier},
  doi={10.1016/S0378-4371(00)00476-3},
  url={https://doi.org/10.1016/S0378-4371(00)00476-3},
}

@article{Martin2004,
  title={Fast and accurate image registration using Tsallis entropy and simultaneous perturbation stochastic approximation},
  author={S Martin and Gordon Morison and William Henry Nailon and Tariq S. Durrani},
  journal={Electronics Letters},
  year={2004},
  volume={40},
  pages={595-597},
  doi={10.1049/EL:20040375},
  url={https://doi.org/10.1049/EL:20040375}
}

@inproceedings{GaoKannanOhViswanath2017,
 author = {Gao, Weihao and Kannan, Sreeram and Oh, Sewoong and Viswanath, Pramod},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Estimating Mutual Information for Discrete-Continuous Mixtures},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/ef72d53990bc4805684c9b61fa64a102-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{Gao2018,
  author={Gao, Weihao and Oh, Sewoong and Viswanath, Pramod},
  journal={IEEE Transactions on Information Theory}, 
  title={Demystifying Fixed  $k$ -Nearest Neighbor Information Estimators}, 
  year={2018},
  volume={64},
  number={8},
  pages={5629-5661},
  doi={10.1109/TIT.2018.2807481},
  url={https://doi.org/10.1109/TIT.2018.2807481}
}

@article{Kraskov2004,
  title = {Estimating mutual information},
  author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
  journal = {Phys. Rev. E},
  volume = {69},
  issue = {6},
  pages = {066138},
  numpages = {16},
  year = {2004},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.69.066138},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138}
}

@inproceedings{Gao2017,
  author={Gao, Weihao and Oh, Sewoong and Viswanath, Pramod},
  booktitle={2017 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Density functional estimators with k-nearest neighbor bandwidths}, 
  year={2017},
  volume={},
  number={},
  pages={1351-1355},
  abstract={Estimating expected polynomials of density functions from samples is a basic problem with numerous applications in statistics and information theory. Although kernel density estimators are widely used in practice for such functional estimation problems, practitioners are left on their own to choose an appropriate bandwidth for each application in hand. Further, kernel density estimators suffer from boundary biases, which are prevalent in real world data with lower dimensional structures. We propose using the fixed-k nearest neighbor distances for the bandwidth, which adaptively adjusts to local geometry. Further, we propose a novel estimator based on local likelihood density estimators, that mitigates the boundary biases. Although such a choice of fixed-k nearest neighbor distances to bandwidths results in inconsistent estimators, we provide a simple debiasing scheme that precomputes the asymptotic bias and divides off this term. With this novel correction, we show consistency of this debiased estimator. We provide numerical experiments suggesting that it improves upon competing state-of-the-art methods.},
  doi={10.1109/ISIT.2017.8006749},
  url={https://doi.org/10.1109/ISIT.2017.8006749}
  ISSN={2157-8117},
  month={June},
}

@article{Ma2011,
title = {Mutual Information Is Copula Entropy},
journal = {Tsinghua Science & Technology},
volume = {16},
number = {1},
pages = {51-54},
year = {2011},
issn = {1007-0214},
doi = {https://doi.org/10.1016/S1007-0214(11)70008-6},
url = {https://www.sciencedirect.com/science/article/pii/S1007021411700086},
author = {Jian Ma and Zengqi Sun},
keywords = {copula entropy, mutual information, estimation, empirical copula},
abstract = {Mutual information (MI) is a basic concept in information theory. Therefore, estimates of the MI are fundamentally important in most information theory applications. This paper provides a new way of understanding and estimating the MI using the copula function. First, the entropy of the copula, named the copula entropy, is defined as a measure of the dependence uncertainty represented by the copula function and then the MI is shown to be equivalent to the negative copula entropy. With this equivalence, the MI can be estimated by first estimating the empirical copula and then estimating the entropy of the empirical copula. Thus, the MI estimate is an estimation of the entropy, which reduces the complexity and computational requirements. Tests show that the method is more effective than the traditional method.}
}

@inproceedings{Pál2010,
 author = {Pál, Dávid and Póczos, Barnabás and Szepesvári, Csaba},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Estimation of Rényi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/577ef1154f3240ad5b9b413aa7346a1e-Paper.pdf},
 volume = {23},
 year = {2010}
}


@article{Palus2014,
  AUTHOR = {Paluš, Milan},
  TITLE = {Cross-Scale Interactions and Information Transfer},
  JOURNAL = {Entropy},
  VOLUME = {16},
  YEAR = {2014},
  NUMBER = {10},
  PAGES = {5263--5289},
  URL = {https://www.mdpi.com/1099-4300/16/10/5263},
  ISSN = {1099-4300},
  ABSTRACT = {An information-theoretic approach for detecting interactions and informationtransfer between two systems is extended to interactions between dynamical phenomenaevolving on different time scales of a complex, multiscale process. The approach isdemonstrated in the detection of an information transfer from larger to smaller time scales ina model multifractal process and applied in a study of cross-scale interactions in atmosphericdynamics. Applying a form of the conditional mutual information and a statistical test basedon the Fourier transform and multifractal surrogate data to about a century long recordsof daily mean surface air temperature from various European locations, an informationtransfer from larger to smaller time scales has been observed as the influence of the phaseof slow oscillatory phenomena with the periods around 6–11 years on the amplitudes of thevariability characterized by the smaller temporal scales from a few months to 4–5 years.These directed cross-scale interactions have a non-negligible effect on interannual airtemperature variability in a large area of Europe.},
  DOI = {10.3390/e16105263}
}

@article{Staniek2008,
  title = {Symbolic Transfer Entropy},
  author = {Staniek, Matth\"aus and Lehnertz, Klaus},
  journal = {Phys. Rev. Lett.},
  volume = {100},
  issue = {15},
  pages = {158101},
  numpages = {4},
  year = {2008},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.100.158101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.158101}
}

@article{Lindner2011,
  title={TRENTOOL: A Matlab open source toolbox to analyse information flow in time series data with transfer entropy},
  author={Michael Lindner and Raul Vicente and Viola Priesemann and Michael Wibral},
  journal={BMC Neuroscience},
  year={2011},
  volume={12},
  pages={119 - 119},
  url={https://api.semanticscholar.org/CorpusID:6250448}
}

@article{Singh2003,
  title={Nearest Neighbor Estimates of Entropy},
  author={Harshinder Singh and Neeraj Misra and Vladimir Hnizdo and Adam Fedorowicz and Eugene Demchuk},
  journal={American Journal of Mathematical and Management Sciences},
  year={2003},
  volume={23},
  pages={301 - 321},
  url={https://api.semanticscholar.org/CorpusID:122506029}
}


@article{Zhu2015,
  AUTHOR = {Zhu, Jie and Bellanger, Jean-Jacques and Shu, Huazhong and Le Bouquin Jeannès, Régine},
  TITLE = {Contribution to Transfer Entropy Estimation via the k-Nearest-Neighbors Approach},
  JOURNAL = {Entropy},
  VOLUME = {17},
  YEAR = {2015},
  NUMBER = {6},
  PAGES = {4173--4201},
  URL = {https://www.mdpi.com/1099-4300/17/6/4173},
  ISSN = {1099-4300},
  ABSTRACT = {This paper deals with the estimation of transfer entropy based on the k-nearest neighbors (k-NN) method. To this end, we first investigate the estimation of Shannon entropy involving a rectangular neighboring region, as suggested in already existing literature, and develop two kinds of entropy estimators. Then, applying the widely-used error cancellation approach to these entropy estimators, we propose two novel transfer entropy estimators, implying no extra computational cost compared to existing similar k-NN algorithms. Experimental simulations allow the comparison of the new estimators with the transfer entropy estimator available in free toolboxes, corresponding to two different extensions to the transfer entropy estimation of the Kraskov–Stögbauer–Grassberger (KSG) mutual information estimator and prove the effectiveness of these new estimators.},
  DOI = {10.3390/e17064173}
}

@article{Montalto2014,
    doi = {10.1371/journal.pone.0109462},
    author = {Montalto, Alessandro AND Faes, Luca AND Marinazzo, Daniele},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {MuTE: A MATLAB Toolbox to Compare Established and Novel Estimators of the Multivariate Transfer Entropy},
    year = {2014},
    month = {10},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pone.0109462},
    pages = {1-13},
    abstract = {A challenge for physiologists and neuroscientists is to map information transfer between components of the systems that they study at different scales, in order to derive important knowledge on structure and function from the analysis of the recorded dynamics. The components of physiological networks often interact in a nonlinear way and through mechanisms which are in general not completely known. It is then safer that the method of choice for analyzing these interactions does not rely on any model or assumption on the nature of the data and their interactions. Transfer entropy has emerged as a powerful tool to quantify directed dynamical interactions. In this paper we compare different approaches to evaluate transfer entropy, some of them already proposed, some novel, and present their implementation in a freeware MATLAB toolbox. Applications to simulated and real data are presented.},
    number = {10},

}

@article{Loftsgaarden1965,
  title={A nonparametric estimate of a multivariate density function},
  author={Don O. Loftsgaarden and Charles P. Quesenberry},
  journal={Annals of Mathematical Statistics},
  year={1965},
  volume={36},
  pages={1049-1051},
  doi={10.1214/AOMS/1177700079},
  url={https://doi.org/10.1214/AOMS/1177700079}
}

@article{Romano2007,
  title = {Estimation of the direction of the coupling by conditional probabilities of recurrence},
  author = {Romano, M. Carmen and Thiel, Marco and Kurths, J\"urgen and Grebogi, Celso},
  journal = {Phys. Rev. E},
  volume = {76},
  issue = {3},
  pages = {036211},
  numpages = {9},
  year = {2007},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.76.036211},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.76.036211}
}

@article{Ramos2017,
  title = {Recurrence measure of conditional dependence and applications},
  author = {Ramos, Ant\^onio M. T. and Builes-Jaramillo, Alejandro and Poveda, Germ\'an and Goswami, Bedartha and Macau, Elbert E. N. and Kurths, J\"urgen and Marwan, Norbert},
  journal = {Phys. Rev. E},
  volume = {95},
  issue = {5},
  pages = {052206},
  numpages = {8},
  year = {2017},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.95.052206},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.95.052206}
}

@article{LeonenkoProzantoSavani2008,
  author = {Nikolai Leonenko and Luc Pronzato and Vippal Savani},
  title = {{A class of Rényi information estimators for multidimensional densities}},
  volume = {36},
  journal = {The Annals of Statistics},
  number = {5},
  publisher = {Institute of Mathematical Statistics},
  pages = {2153 -- 2182},
  abstract = {A class of estimators of the Rényi and Tsallis entropies of an unknown distribution f in ℝm is presented. These estimators are based on the kth nearest-neighbor distances computed from a sample of N i.i.d. vectors with distribution f. We show that entropies of any order q, including Shannon’s entropy, can be estimated consistently with minimal assumptions on f. Moreover, we show that it is straightforward to extend the nearest-neighbor method to estimate the statistical distance between two distributions using one i.i.d. sample from each.},
  keywords = {Entropy estimation, estimation of divergence, estimation of statistical distance, Havrda–Charvát entropy, nearest-neighbor distances, Rényi entropy, Tsallis entropy},
  year = {2008},
  doi = {10.1214/07-AOS539},
  URL = {https://doi.org/10.1214/07-AOS539}
}

@article{Frenzel2007,
  title = {Partial Mutual Information for Coupling Analysis of Multivariate Time Series},
  author = {Frenzel, Stefan and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {99},
  issue = {20},
  pages = {204101},
  numpages = {4},
  year = {2007},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.99.204101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.204101}
}

@ARTICLE{vanErven2014,
  author={van Erven, Tim and Harremos, Peter},
  journal={IEEE Transactions on Information Theory}, 
  title={Rényi Divergence and Kullback-Leibler Divergence}, 
  year={2014},
  volume={60},
  number={7},
  pages={3797-3820},
  doi={10.1109/TIT.2014.2320500}}

  @inproceedings{Poczos2012,
  title={Nonparametric estimation of conditional information and divergences},
  author={Póczos, Barnabás and Schneider, Jeff},
  booktitle={Artificial Intelligence and Statistics},
  pages={914--923},
  year={2012},
  organization={PMLR}
}

% For ComplexityMeasures.jl references

@article{Zahl1977,
  title={Jackknifing an index of diversity},
  author={Zahl, Samuel},
  journal={Ecology},
  volume={58},
  number={4},
  pages={907--913},
  year={1977},
  publisher={Wiley Online Library},
  doi={10.2307/1936227},
  url={https://doi.org/10.2307/1936227}
}


@article{Shannon1948,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs},
  url={https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
  doi={10.1002/j.1538-7305.1948.tb01338.x},
}

@inproceedings{Rényi1961,
  title={On measures of entropy and information},
  author={Rényi, Alfréd},
  booktitle={Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  volume={4},
  pages={547--562},
  year={1961},
  organization={University of California Press},
  url={https://projecteuclid.org/ebook/Download?urlid=bsmsp/1200512181&isFullBook=false}
}


@article{Tsallis1988,
  title={Possible generalization of Boltzmann-Gibbs statistics},
  author={Tsallis, Constantino},
  journal={Journal of statistical physics},
  volume={52},
  pages={479--487},
  year={1988},
  publisher={Springer},
  doi={10.1007/BF01016429},
  url={https://doi.org/10.1007/BF01016429},
}



@article{Arora2022,
  title={Estimating the Entropy of Linguistic Distributions}, 
  author={Aryaman Arora and Clara Meister and Ryan Cotterell},
  year={2022},
  eprint={2204.01469},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2204.01469},
  journal={arXiv}
}


@article{Miller1955,
  title={Note on the bias of information estimates},
  author={Miller, George},
  journal={Information theory in psychology: Problems and methods},
  year={1955},
  publisher={Free Press}
}


@article{Schurmann2004,
  title={Bias analysis in entropy estimation},
  author={Schuermann, Thomas},
  journal={Journal of Physics A: Mathematical and General},
  volume={37},
  number={27},
  pages={L295},
  year={2004},
  publisher={IOP Publishing},
  doi={10.1088/0305-4470/37/27/L02},
  url={https://doi.org/10.1088/0305-4470/37/27/L02},
}


@article{Grassberger2022,
  AUTHOR = {Grassberger, Peter},
  TITLE = {On Generalized Schuermann Entropy Estimators},
  JOURNAL = {Entropy},
  VOLUME = {24},
  YEAR = {2022},
  NUMBER = {5},
  ARTICLE-NUMBER = {680},
  URL = {https://www.mdpi.com/1099-4300/24/5/680},
  PubMedID = {35626564},
  ISSN = {1099-4300},
  ABSTRACT = {We present a new class of estimators of Shannon entropy for severely undersampled discrete distributions. It is based on a generalization of an estimator proposed by T. Schuermann, which itself is a generalization of an estimator proposed by myself. For a special set of parameters, they are completely free of bias and have a finite variance, something which is widely believed to be impossible. We present also detailed numerical tests, where we compare them with other recent estimators and with exact results, and point out a clash with Bayesian estimators for mutual information.},
  DOI = {10.3390/e24050680}
}


@article{Horvitz1952,
  author = { D. G.   Horvitz  and  D. J.   Thompson },
  title = {A Generalization of Sampling Without Replacement from a Finite Universe},
  journal = {Journal of the American Statistical Association},
  volume = {47},
  number = {260},
  pages = {663-685},
  year  = {1952},
  publisher = {Taylor & Francis},
  doi = {10.1080/01621459.1952.10483446},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446},
}

@article{Chao2003,
	abstract = {A biological community usually has a large number of species with relatively small abundances. When a random sample of individuals is selected and each individual is classified according to species identity, some rare species may not be discovered. This paper is concerned with the estimation of Shannon's index of diversity when the number of species and the species abundances are unknown. The traditional estimator that ignores the missing species underestimates when there is a non-negligible number of unseen species. We provide a different approach based on unequal probability sampling theory because species have different probabilities of being discovered in the sample. No parametric forms are assumed for the species abundances. The proposed estimation procedure combines the Horvitz--Thompson (1952) adjustment for missing species and the concept of sample coverage, which is used to properly estimate the relative abundances of species discovered in the sample. Simulation results show that the proposed estimator works well under various abundance models even when a relatively large fraction of the species is missing. Three real data sets, two from biology and the other one from numismatics, are given for illustration.},
	author = {Chao, Anne and Shen, Tsung-Jen},
	date = {2003/12/01},
	doi = {10.1023/A:1026096204727},
	id = {Chao2003},
	isbn = {1573-3009},
	journal = {Environmental and Ecological Statistics},
	number = {4},
	pages = {429--443},
	title = {Nonparametric estimation of Shannon's index of diversity when there are unseen species in sample},
	url = {https://doi.org/10.1023/A:1026096204727},
	volume = {10},
	year = {2003},
}


@article{BandtPompe2002,
  title = {Permutation Entropy: A Natural Complexity Measure for Time Series},
  author = {Bandt, Christoph and Pompe, Bernd},
  journal = {Phys. Rev. Lett.},
  volume = {88},
  issue = {17},
  pages = {174102},
  numpages = {4},
  year = {2002},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.88.174102},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.88.174102},
}



@article{He2016,
  title = {Multivariate permutation entropy and its application for complexity analysis of chaotic systems},
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {461},
  pages = {812-823},
  year = {2016},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2016.06.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0378437116302801},
  author = {Shaobo He and Kehui Sun and Huihai Wang},
  keywords = {Permutation entropy, Multivariate complexity, Simplified Lorenz system, Financial chaotic system},
  abstract = {To measure the complexity of multivariate systems, the multivariate permutation entropy (MvPE) algorithm is proposed. It is employed to measure complexity of multivariate system in the phase space. As an application, MvPE is applied to analyze the complexity of chaotic systems, including hyperchaotic Hénon map, fractional-order simplified Lorenz system and financial chaotic system. Results show that MvPE algorithm is effective for analyzing the complexity of the multivariate systems. It also shows that fractional-order system does not become more complex with derivative order varying. Compared with PE, MvPE has better robustness for noise and sampling interval, and the results are not affected by different normalization methods.}
}


@article{Zunino2017,
  title={Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions},
  author={Zunino, Luciano and Olivares, Felipe and Scholkmann, Felix and Rosso, Osvaldo A},
  journal={Physics Letters A},
  volume={381},
  number={22},
  pages={1883--1892},
  year={2017},
  publisher={Elsevier},
  doi={10.1016/j.physleta.2017.03.052},
  url={https://doi.org/10.1016/j.physleta.2017.03.052}
}


@article{Li2018,
  AUTHOR = {Li, Guohui and Guan, Qianru and Yang, Hong},
  TITLE = {Noise Reduction Method of Underwater Acoustic Signals Based on CEEMDAN, Effort-To-Compress Complexity, Refined Composite Multiscale Dispersion Entropy and Wavelet Threshold Denoising},
  JOURNAL = {Entropy},
  VOLUME = {21},
  YEAR = {2019},
  NUMBER = {1},
  ARTICLE-NUMBER = {11},
  URL = {https://www.mdpi.com/1099-4300/21/1/11},
  PubMedID = {33266727},
  ISSN = {1099-4300},
  ABSTRACT = {Owing to the problems that imperfect decomposition process of empirical mode decomposition (EMD) denoising algorithm and poor self-adaptability, it will be extremely difficult to reduce the noise of signal. In this paper, a noise reduction method of underwater acoustic signal denoising based on complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), effort-to-compress complexity (ETC), refined composite multiscale dispersion entropy (RCMDE) and wavelet threshold denoising is proposed. Firstly, the original signal is decomposed into several IMFs by CEEMDAN and noise IMFs can be identified according to the ETC of IMFs. Then, calculating the RCMDE of remaining IMFs, these IMFs are divided into three kinds of IMFs by RCMDE, namely noise-dominant IMFs, real signal-dominant IMFs, real IMFs. Finally, noise IMFs are removed, wavelet soft threshold denoising is applied to noise-dominant IMFs and real signal-dominant IMFs. The denoised signal can be obtained by combining the real IMFs with the denoised IMFs after wavelet soft threshold denoising. Chaotic signals with different signal-to-noise ratio (SNR) are used for denoising experiments by comparing with EMD_MSE_WSTD and EEMD_DE_WSTD, it shows that the proposed algorithm has higher SNR and smaller root mean square error (RMSE). In order to further verify the effectiveness of the proposed method, which is applied to noise reduction of real underwater acoustic signals. The results show that the denoised underwater acoustic signals not only eliminate noise interference also restore the topological structure of the chaotic attractors more clearly, which lays a foundation for the further processing of underwater acoustic signals.},
  DOI = {10.3390/e21010011},
}


@article{Rostaghi2016,
  title={Dispersion entropy: A measure for time-series analysis},
  author={Rostaghi, Mostafa and Azami, Hamed},
  journal={IEEE Signal Processing Letters},
  volume={23},
  number={5},
  pages={610--614},
  year={2016},
  publisher={IEEE},
  doi={10.1109/LSP.2016.2542881},
  url={https://doi.org/10.1109/LSP.2016.2542881},
}


@article{KozachenkoLeonenko1987,
  title={Sample estimate of the entropy of a random vector},
  author={Kozachenko, Lyudmyla F and Leonenko, Nikolai N},
  journal={Problemy Peredachi Informatsii},
  volume={23},
  number={2},
  pages={9--16},
  year={1987},
  publisher={Russian Academy of Sciences, Branch of Informatics, Computer Equipment and~…},
  url={https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=797&option_lang=eng}
}

@article{Charzyńska2015,
  AUTHOR = {Charzyńska, Agata and Gambin, Anna},
  TITLE = {Improvement of the k-nn Entropy Estimator with Applications in Systems Biology},
  JOURNAL = {Entropy},
  VOLUME = {18},
  YEAR = {2016},
  NUMBER = {1},
  ARTICLE-NUMBER = {13},
  URL = {https://www.mdpi.com/1099-4300/18/1/13},
  ISSN = {1099-4300},
  ABSTRACT = {In this paper, we investigate efficient estimation of differential entropy for multivariate random variables. We propose bias correction for the nearest neighbor estimator, which yields more accurate results in higher dimensions. In order to demonstrate the accuracy of the improvement, we calculated the corrected estimator for several families of random variables. For multivariate distributions, we considered the case of independent marginals and the dependence structure between the marginal distributions described by Gaussian copula. The presented solution may be particularly useful for high dimensional data, like those analyzed in the systems biology field. To illustrate such an application, we exploit differential entropy to define the robustness of biochemical kinetic models.},
  DOI = {10.3390/e18010013}
}


@inproceedings{Gao2015,
  title = {Efficient Estimation of Mutual Information for Strongly Dependent Variables},
  author = {Gao, Shuyang and Ver Steeg, Greg and Galstyan, Aram},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = {277--286},
  year = {2015},
  editor = {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = {38},
  series = {Proceedings of Machine Learning Research},
  address = {San Diego, California, USA},
  month = {09--12 May},
  publisher =  {PMLR},
  pdf = {http://proceedings.mlr.press/v38/gao15.pdf},
  url = {https://proceedings.mlr.press/v38/gao15.html},
  abstract = {We demonstrate that a popular class of non-parametric mutual information (MI) estimators based on k-nearest-neighbor graphs requires number of samples that scales exponentially with the true MI. Consequently, accurate estimation of MI between two strongly dependent variables is possible only for prohibitively large sample size. This important yet overlooked shortcoming of the existing estimators is due to their implicit reliance on  local uniformity of the underlying joint distribution. We introduce a new  estimator that is robust to local non-uniformity, works well with limited data, and is able to capture relationship strengths over many orders of magnitude. We demonstrate the superior performance of the proposed estimator on both synthetic and real-world data.}
}


@article{Goria2005,
  author = { M. N. Goria  and  N. N. Leonenko  and  V. V. Mergel and P. L. Novi Inverardi},
  title = {A new class of random vector entropy estimators and its applications in testing statistical hypotheses},
  journal = {Journal of Nonparametric Statistics},
  volume = {17},
  number = {3},
  pages = {277-297},
  year  = {2005},
  publisher = {Taylor & Francis},
  doi = {10.1080/104852504200026815},
  url = {https://doi.org/10.1080/104852504200026815},
  eprint = {https://doi.org/10.1080/104852504200026815},
}

@article{Lord2018,
  title={Geometric k-nearest neighbor estimation of entropy and mutual information},
  author={Lord, Warren M and Sun, Jie and Bollt, Erik M},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={28},
  number={3},
  year={2018},
  publisher={AIP Publishing},
  doi = {10.1063/1.5011683},
  url = {https://pubs.aip.org/aip/cha/article/28/3/033114/685022},
}


@article{LeonenkoProzantoSavani2008,
  author = {Nikolai Leonenko and Luc Pronzato and Vippal Savani},
  title = {A class of Rényi information estimators for multidimensional densities},
  volume = {36},
  journal = {The Annals of Statistics},
  number = {5},
  publisher = {Institute of Mathematical Statistics},
  pages = {2153 -- 2182},
  abstract = {A class of estimators of the Rényi and Tsallis entropies of an unknown distribution f in ℝm is presented. These estimators are based on the kth nearest-neighbor distances computed from a sample of N i.i.d. vectors with distribution f. We show that entropies of any order q, including Shannon’s entropy, can be estimated consistently with minimal assumptions on f. Moreover, we show that it is straightforward to extend the nearest-neighbor method to estimate the statistical distance between two distributions using one i.i.d. sample from each.},
  keywords = {Entropy estimation, estimation of divergence, estimation of statistical distance, Havrda–Charvát entropy, nearest-neighbor distances, Rényi entropy, Tsallis entropy},
  year = {2008},
  doi = {10.1214/07-AOS539},
  url = {https://doi.org/10.1214/07-AOS539}
}


@article{Vasicek1976,
  title={A test for normality based on sample entropy},
  author={Vasicek, Oldrich},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={38},
  number={1},
  pages={54--59},
  year={1976},
  publisher={Oxford University Press},
  doi={10.1111/j.2517-6161.1976.tb01566.x}
}


@article{Alizadeh2010,
  title={A new estimator of entropy},
  author={Alizadeh, Noughabi Hadi and Arghami, Naser Reza},
  year={2010},
  journal={Journal of the Iranian Statistical Society (JIRSS)},
  publisher={Journal of the Iranian Statistical Society (JIRSS)},
  url={http://jirss.irstat.ir/article-1-81-en.pdf},
}


@article{Ebrahimi1994,
  title = {Two measures of sample entropy},
  journal = {Statistics \& Probability Letters},
  volume = {20},
  number = {3},
  pages = {225-234},
  year = {1994},
  issn = {0167-7152},
  doi = {10.1016/0167-7152(94)90046-9},
  url = {https://www.sciencedirect.com/science/article/pii/0167715294900469},
  author = {Nader Ebrahimi and Kurt Pflughoeft and Ehsan S. Soofi},
  keywords = {Information theory, Entropy estimator, Exponential, Normal, Uniform},
  abstract = {In many statistical studies the entropy of a distribution function is of prime interest. This paper proposes two estimators of the entropy. Both estimators are obtained by modifying the estimator proposed by Vasicek (1976). Consistency of both estimators is proved, and comparisons have been made with Vasicek's estimator and its generalization proposed by Dudewicz and Van der Meulen (1987). The results indicate that the proposed estimators have less bias and have less mean squared error than Vasicek's estimator and its generalization}
}


@article{Correa1995,
  author = {Correa, Juan C.},
  title = {A new estimator of entropy},
  journal = {Communications in Statistics - Theory and Methods},
  volume = {24},
  number = {10},
  pages = {2439-2449},
  year  = {1995},
  publisher = {Taylor & Francis},
  doi = {10.1080/03610929508831626},
  URL = {https://doi.org/10.1080/03610929508831626},
  eprint = {https://doi.org/10.1080/03610929508831626}
}

@article{Berger2019,
  publisher={MDPI},
  author = {Berger, Sebastian and Kravtsiv, Andrii and Schneider, Gerhard and Jordan, Denis},
  title = {Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code},
  journal = {Entropy},
  volume = {21},
  year = {2019},
  number = {10},
  article-number = {1023},
  url = {https://www.mdpi.com/1099-4300/21/10/1023},
  issn = {1099-4300},
  abstract = {Ordinal patterns are the common basis of various techniques used in the study of dynamical systems and nonlinear time series analysis. The present article focusses on the computational problem of turning time series into sequences of ordinal patterns. In a first step, a numerical encoding scheme for ordinal patterns is proposed. Utilising the classical Lehmer code, it enumerates ordinal patterns by consecutive non-negative integers, starting from zero. This compact representation considerably simplifies working with ordinal patterns in the digital domain. Subsequently, three algorithms for the efficient extraction of ordinal patterns from time series are discussed, including previously published approaches that can be adapted to the Lehmer code. The respective strengths and weaknesses of those algorithms are discussed, and further substantiated by benchmark results. One of the algorithms stands out in terms of scalability: its run-time increases linearly with both the pattern order and the sequence length, while its memory footprint is practically negligible. These properties enable the study of high-dimensional pattern spaces at low computational cost. In summary, the tools described herein may improve the efficiency of virtually any ordinal pattern-based analysis method, among them quantitative measures like permutation entropy and symbolic transfer entropy, but also techniques like forbidden pattern identification. Moreover, the concepts presented may allow for putting ideas into practice that up to now had been hindered by computational burden. To enable smooth evaluation, a function library written in the C programming language, as well as language bindings and native implementations for various numerical computation environments are provided in the supplements.},
  doi = {10.3390/e21101023}
}


@article{Azami2016,
title = {Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {128},
pages = {40-51},
year = {2016},
issn = {0169-2607},
doi = {10.1016/j.cmpb.2016.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715301152},
author = {Hamed Azami and Javier Escudero},
keywords = {Signal irregularity, Amplitude-aware permutation entropy, Spike detection, Signal segmentation, Electroencephalogram, Extracellular neuronal data},
abstract = {Background and objective
Signal segmentation and spike detection are two important biomedical signal processing applications. Often, non-stationary signals must be segmented into piece-wise stationary epochs or spikes need to be found among a background of noise before being further analyzed. Permutation entropy (PE) has been proposed to evaluate the irregularity of a time series. PE is conceptually simple, structurally robust to artifacts, and computationally fast. It has been extensively used in many applications, but it has two key shortcomings. First, when a signal is symbolized using the Bandt–Pompe procedure, only the order of the amplitude values is considered and information regarding the amplitudes is discarded. Second, in the PE, the effect of equal amplitude values in each embedded vector is not addressed. To address these issues, we propose a new entropy measure based on PE: the amplitude-aware permutation entropy (AAPE).
Methods
AAPE is sensitive to the changes in the amplitude, in addition to the frequency, of the signals thanks to it being more flexible than the classical PE in the quantification of the signal motifs. To demonstrate how the AAPE method can enhance the quality of the signal segmentation and spike detection, a set of synthetic and realistic synthetic neuronal signals, electroencephalograms and neuronal data are processed. We compare the performance of AAPE in these problems against state-of-the-art approaches and evaluate the significance of the differences with a repeated ANOVA with post hoc Tukey's test.
Results
In signal segmentation, the accuracy of AAPE-based method is higher than conventional segmentation methods. AAPE also leads to more robust results in the presence of noise. The spike detection results show that AAPE can detect spikes well, even when presented with single-sample spikes, unlike PE. For multi-sample spikes, the changes in AAPE are larger than in PE.
Conclusion
We introduce a new entropy metric, AAPE, that enables us to consider amplitude information in the formulation of PE. The AAPE algorithm can be used in almost every irregularity-based application in various signal and image processing fields. We also made freely available the Matlab code of the AAPE.}
}

@article{Paninski2003,
  title={Estimation of entropy and mutual information},
  author={Paninski, Liam},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1191--1253},
  year={2003},
  publisher={MIT Press},
  doi={10.1162/089976603321780272},
  url={https://ieeexplore.ieee.org/abstract/document/6790247},
}


@article{PrichardTheiler1995,
  title={Generalized redundancies for time series analysis},
  author={Prichard, Dean and Theiler, James},
  journal={Physica D: Nonlinear Phenomena},
  volume={84},
  number={3-4},
  pages={476--493},
  year={1995},
  publisher={Elsevier},
  doi={10.1016/0167-2789(95)00041-2},
}


@article{Rosso2001,
  title = {Wavelet entropy: a new tool for analysis of short duration brain electrical signals},
  journal = {Journal of Neuroscience Methods},
  volume = {105},
  number = {1},
  pages = {65-75},
  year = {2001},
  issn = {0165-0270},
  doi = {10.1016/S0165-0270(00)00356-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0165027000003563},
  author = {Osvaldo A. Rosso and Susana Blanco and Juliana Yordanova and Vasil Kolev and Alejandra Figliola and Martin Schürmann and Erol Başar},
  keywords = {EEG, event-related potentials (ERP), Visual evoked potential, Time–frequency signal analysis, Wavelet analysis, Signal entropy},
  abstract = {Since traditional electrical brain signal analysis is mostly qualitative, the development of new quantitative methods is crucial for restricting the subjectivity in the study of brain signals. These methods are particularly fruitful when they are strongly correlated with intuitive physical concepts that allow a better understanding of brain dynamics. Here, new method based on orthogonal discrete wavelet transform (ODWT) is applied. It takes as a basic element the ODWT of the EEG signal, and defines the relative wavelet energy, the wavelet entropy (WE) and the relative wavelet entropy (RWE). The relative wavelet energy provides information about the relative energy associated with different frequency bands present in the EEG and their corresponding degree of importance. The WE carries information about the degree of order/disorder associated with a multi-frequency signal response, and the RWE measures the degree of similarity between different segments of the signal. In addition, the time evolution of the WE is calculated to give information about the dynamics in the EEG records. Within this framework, the major objective of the present work was to characterize in a quantitative way functional dynamics of order/disorder microstates in short duration EEG signals. For that aim, spontaneous EEG signals under different physiological conditions were analyzed. Further, specific quantifiers were derived to characterize how stimulus affects electrical events in terms of frequency synchronization (tuning) in the event related potentials.},
}

@article{Llanos2017,
  title={Power spectral entropy as an information-theoretic correlate of manner of articulation in American English},
  author={Llanos, Fernando and Alexander, Joshua M and Stilp, Christian E and Kluender, Keith R},
  journal={The Journal of the Acoustical Society of America},
  volume={141},
  number={2},
  pages={EL127--EL133},
  year={2017},
  publisher={AIP Publishing},
  doi={10.1121/1.4976109},
  url={https://pubmed.ncbi.nlm.nih.gov/28253693/}
}

@article{Tian2017,
  title={Spectral entropy can predict changes of working memory performance reduced by short-time training in the delayed-match-to-sample task},
  author={Tian, Yin and Zhang, Huiling and Xu, Wei and Zhang, Haiyong and Yang, Li and Zheng, Shuxing and Shi, Yupan},
  journal={Frontiers in human neuroscience},
  volume={11},
  pages={437},
  year={2017},
  publisher={Frontiers Media SA},
  doi={10.3389/fnhum.2017.00437},
}

@article{Datseris2024,
  title={ComplexityMeasures. jl: scalable software to unify and accelerate entropy and complexity timeseries analysis},
  author={Datseris, George and Haaga, Kristian Agas{\o}ster},
  journal={arXiv preprint arXiv:2406.05011},
  year={2024}
}
@article{Papapetrou2020,
  title={Tsallis conditional mutual information in investigating long range correlation in symbol sequences},
  author={Papapetrou, M and Kugiumtzis, D},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={540},
  pages={123016},
  year={2020},
  publisher={Elsevier}
}

@article{Szekely2007,
  author = {G{\'a}bor J. Sz{\'e}kely and Maria L. Rizzo and Nail K. Bakirov},
  title = {{Measuring and testing dependence by correlation of distances}},
  volume = {35},
  journal = {The Annals of Statistics},
  number = {6},
  publisher = {Institute of Mathematical Statistics},
  pages = {2769 -- 2794},
  keywords = {Distance correlation, distance covariance, multivariate independence},
  year = {2007},
  doi = {10.1214/009053607000000505},
  URL = {https://doi.org/10.1214/009053607000000505}
}


@article{Szekely2014,
author = {G{\'a}bor J. Sz{\'e}kely and Maria L. Rizzo},
title = {{Partial distance correlation with methods for dissimilarities}},
volume = {42},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2382 -- 2412},
keywords = {dissimilarity, energy statistics, independence, multivariate, partial distance correlation},
year = {2014},
doi = {10.1214/14-AOS1255},
URL = {https://doi.org/10.1214/14-AOS1255}
}

@article{Manis2017,
  title={Bubble entropy: An entropy almost free of parameters},
  author={Manis, George and Aktaruzzaman, MD and Sassi, Roberto},
  journal={IEEE Transactions on Biomedical Engineering},
  volume={64},
  number={11},
  pages={2711--2718},
  year={2017},
  publisher={IEEE}
}

@article{Wang2020,
  title={Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery},
  author={Wang, Xianzhi and Si, Shubin and Li, Yongbo},
  journal={IEEE Transactions on Industrial Informatics},
  volume={17},
  number={8},
  pages={5419--5429},
  year={2020},
  publisher={IEEE},
  doi={10.1109/TII.2020.3022369},
}

@book{Tsallis2009,
  title={Introduction to nonextensive statistical mechanics: approaching a complex world},
  author={Tsallis, Constantino},
  volume={1},
  number={1},
  year={2009},
  publisher={Springer},
  url={https://link.springer.com/book/10.1007/978-0-387-85359-8}
}

@article{Arnhold1999,
  title={A robust method for detecting interdependences: application to intracranially recorded EEG},
  author={Arnhold, Jochen and Grassberger, Peter and Lehnertz, Klaus and Elger, Christian Erich},
  journal={Physica D: Nonlinear Phenomena},
  volume={134},
  number={4},
  pages={419--430},
  year={1999},
  publisher={Elsevier}
}

@article{Chatterjee2021,
  title={A new coefficient of correlation},
  author={Chatterjee, Sourav},
  journal={Journal of the American Statistical Association},
  volume={116},
  number={536},
  pages={2009--2022},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{Shi2022,
  title={On the power of Chatterjee’s rank correlation},
  author={Shi, Hongjian and Drton, Mathias and Han, Fang},
  journal={Biometrika},
  volume={109},
  number={2},
  pages={317--333},
  year={2022},
  publisher={Oxford University Press}
}

@article{Dette2013,
  title={A Copula-Based Non-parametric Measure of Regression Dependence},
  author={Dette, Holger and Siburg, Karl F and Stoimenov, Pavel A},
  journal={Scandinavian Journal of Statistics},
  volume={40},
  number={1},
  pages={21--41},
  year={2013},
  publisher={Wiley Online Library}
}

@article{Azadkia2021,
  title={A simple measure of conditional dependence},
  author={Azadkia, Mona and Chatterjee, Sourav},
  journal={The Annals of Statistics},
  volume={49},
  number={6},
  pages={3070--3102},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@article{Kubkowski2021,
  title={How to gain on power: novel conditional independence tests based on short expansion of conditional mutual information},
  author={Kubkowski, Mariusz and Mielniczuk, Jan and Teisseyre, Pawe{\l}},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={62},
  pages={1--57},
  year={2021}
}