var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Abe, S. and Rajagopal, A. K. (2001). Nonadditive conditional entropy and its significance for local realism. Physica A: Statistical Mechanics and its Applications 289, 157–164.\n\n\n\nAlizadeh, N. H. and Arghami, N. R. (2010). A new estimator of entropy. Journal of the Iranian Statistical Society (JIRSS).\n\n\n\nAmigó, J. M. and Hirata, Y. (2018). Detecting directional couplings from multivariate flows by the joint distance distribution. Chaos 28, 075302.\n\n\n\nAndrzejak, R. G.; Kraskov, A.; Stögbauer, H.; Mormann, F. and Kreuz, T. (2003). Bivariate surrogate techniques: necessity, strengths, and caveats. Physical review E 68, 066202.\n\n\n\nArnhold, J.; Grassberger, P.; Lehnertz, K. and Elger, C. E. (1999). A robust method for detecting interdependences: application to intracranially recorded EEG. Physica D: Nonlinear Phenomena 134, 419–430.\n\n\n\nArora, A.; Meister, C. and Cotterell, R. (2022). Estimating the Entropy of Linguistic Distributions, arXiv, arXiv:2204.01469 [cs.CL].\n\n\n\nAzami, H. and Escudero, J. (2016). Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation. Computer Methods and Programs in Biomedicine 128, 40–51.\n\n\n\nBandt, C. and Pompe, B. (2002). Permutation Entropy: A Natural Complexity Measure for Time Series. Phys. Rev. Lett. 88, 174102.\n\n\n\nBerger, S.; Kravtsiv, A.; Schneider, G. and Jordan, D. (2019). Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code. Entropy 21.\n\n\n\nChao, A. and Shen, T.-J. (2003). Nonparametric estimation of Shannon's index of diversity when there are unseen species in sample. Environmental and Ecological Statistics 10, 429–443.\n\n\n\nCharzyńska, A. and Gambin, A. (2016). Improvement of the k-nn Entropy Estimator with Applications in Systems Biology. Entropy 18.\n\n\n\nChicharro, D. and Andrzejak, R. G. (2009). Reliable detection of directional couplings using rank statistics. Physical Review E 80, 026217.\n\n\n\nCorrea, J. C. (1995). A new estimator of entropy. Communications in Statistics - Theory and Methods 24, 2439–2449, arXiv:https://doi.org/10.1080/03610929508831626.\n\n\n\nCover, T. M. (1999). Elements of information theory (John Wiley & Sons).\n\n\n\nDatseris, G. and Haaga, K. A. (2024). ComplexityMeasures. jl: scalable software to unify and accelerate entropy and complexity timeseries analysis, arXiv preprint arXiv:2406.05011.\n\n\n\nEbrahimi, N.; Pflughoeft, K. and Soofi, E. S. (1994). Two measures of sample entropy. Statistics & Probability Letters 20, 225–234.\n\n\n\nvan Erven, T. and Harremos, P. (2014). Rényi Divergence and Kullback-Leibler Divergence. IEEE Transactions on Information Theory 60, 3797–3820.\n\n\n\nFrenzel, S. and Pompe, B. (2007). Partial Mutual Information for Coupling Analysis of Multivariate Time Series. Phys. Rev. Lett. 99, 204101.\n\n\n\nFuruichi, S. (2006). Information theoretical properties of Tsallis entropies. Journal of Mathematical Physics 47.\n\n\n\nGao, S.; Ver Steeg, G. and Galstyan, A. (09–12 May 2015). Efficient Estimation of Mutual Information for Strongly Dependent Variables. In: Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, Vol. 38 of Proceedings of Machine Learning Research, edited by Lebanon, G. and Vishwanathan, S. V. (PMLR, San Diego, California, USA); pp. 277–286.\n\n\n\nGao, W.; Kannan, S.; Oh, S. and Viswanath, P. (2017). Estimating Mutual Information for Discrete-Continuous Mixtures. In: Advances in Neural Information Processing Systems, Vol. 30, edited by Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S. and Garnett, R. (Curran Associates, Inc.).\n\n\n\nGao, W.; Oh, S. and Viswanath, P. (2018). Demystifying Fixed  k -Nearest Neighbor Information Estimators. IEEE Transactions on Information Theory 64, 5629–5661.\n\n\n\nGolshani, L.; Pasha, E. and Yari, G. (2009). Some properties of Rényi entropy and Rényi entropy rate. Information Sciences 179, 2426–2433.\n\n\n\nGoria, M. N.; Leonenko, N. N.; Mergel, V. V. and Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics 17, 277–297, arXiv:https://doi.org/10.1080/104852504200026815.\n\n\n\nGrassberger, P. (2022). On Generalized Schuermann Entropy Estimators. Entropy 24.\n\n\n\nHe, S.; Sun, K. and Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications 461, 812–823.\n\n\n\nHorvitz, D. G. and Thompson, D. J. (1952). A Generalization of Sampling Without Replacement from a Finite Universe. Journal of the American Statistical Association 47, 663–685, arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446.\n\n\n\nJizba, P.; Kleinert, H. and Shefaat, M. (2012). Rényi's information transfer between financial time series. Physica A: Statistical Mechanics and its Applications 391, 2971–2989.\n\n\n\nKalisch, M. and Bühlmann, P. (2008). Robustification of the PC-Algorithm for Directed Acyclic Graphs. Journal of Computational and Graphical Statistics 17, 773–789, arXiv:https://doi.org/10.1198/106186008X381927.\n\n\n\nKozachenko, L. F. and Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii 23, 9–16.\n\n\n\nKraskov, A.; Stögbauer, H. and Grassberger, P. (2004). Estimating mutual information. Phys. Rev. E 69, 066138.\n\n\n\nLeonenko, N.; Pronzato, L. and Savani, V. (2008). A class of Rényi information estimators for multidimensional densities. The Annals of Statistics 36, 2153–2182.\n\n\n\nLevy, K. J. and Narula, S. C. (1978). Testing hypotheses concerning partial correlations: Some methods and discussion. International Statistical Review/Revue Internationale de Statistique, 215–218.\n\n\n\nLi, G.; Guan, Q. and Yang, H. (2019). Noise Reduction Method of Underwater Acoustic Signals Based on CEEMDAN, Effort-To-Compress Complexity, Refined Composite Multiscale Dispersion Entropy and Wavelet Threshold Denoising. Entropy 21.\n\n\n\nLindner, M.; Vicente, R.; Priesemann, V. and Wibral, M. (2011). TRENTOOL: A Matlab open source toolbox to analyse information flow in time series data with transfer entropy. BMC Neuroscience 12, 119–119.\n\n\n\nLord, W. M.; Sun, J. and Bollt, E. M. (2018). Geometric k-nearest neighbor estimation of entropy and mutual information. Chaos: An Interdisciplinary Journal of Nonlinear Science 28.\n\n\n\nLuo, M.; Kantz, H.; Lau, N.-C.; Huang, W. and Zhou, Y. (2015). Questionable dynamical evidence for causality between galactic cosmic rays and interannual variation in global temperature. Proceedings of the National Academy of Sciences 112, E4638 - E4639.\n\n\n\nManis, G.; Aktaruzzaman, M. and Sassi, R. (2017). Bubble entropy: An entropy almost free of parameters. IEEE Transactions on Biomedical Engineering 64, 2711–2718.\n\n\n\nMartin, S.; Morison, G.; Nailon, W. H. and Durrani, T. S. (2004). Fast and accurate image registration using Tsallis entropy and simultaneous perturbation stochastic approximation. Electronics Letters 40, 595–597.\n\n\n\nMcCracken, J. M. and Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E 90, 062903.\n\n\n\nMesner, O. C. and Shalizi, C. R. (2020). Conditional mutual information estimation for mixed, discrete and continuous data. IEEE Transactions on Information Theory 67, 464–484.\n\n\n\nMiller, G. (1955). Note on the bias of information estimates. Information theory in psychology: Problems and methods.\n\n\n\nPaluš, M. (2014). Cross-Scale Interactions and Information Transfer. Entropy 16, 5263–5289.\n\n\n\nPaninski, L. (2003). Estimation of entropy and mutual information. Neural computation 15, 1191–1253.\n\n\n\nPapapetrou, M. and Kugiumtzis, D. (2020). Tsallis conditional mutual information in investigating long range correlation in symbol sequences. Physica A: Statistical Mechanics and its Applications 540, 123016.\n\n\n\nPóczos, B. and Schneider, J. (2012). Nonparametric estimation of conditional information and divergences. In: Artificial Intelligence and Statistics (PMLR); pp. 914–923.\n\n\n\nQuiroga, R. Q.; Arnhold, J. and Grassberger, P. (2000). Learning driver-response relationships from synchronization patterns. Physical Review E 61, 5142.\n\n\n\nRahimzamani, A.; Asnani, H.; Viswanath, P. and Kannan, S. (2018). Estimators for multivariate information measures in general probability spaces. Advances in Neural Information Processing Systems 31.\n\n\n\nRamos, A. M.; Builes-Jaramillo, A.; Poveda, G.; Goswami, B.; Macau, E. E.; Kurths, J. and Marwan, N. (2017). Recurrence measure of conditional dependence and applications. Phys. Rev. E 95, 052206.\n\n\n\nRomano, M. C.; Thiel, M.; Kurths, J. and Grebogi, C. (2007). Estimation of the direction of the coupling by conditional probabilities of recurrence. Phys. Rev. E 76, 036211.\n\n\n\nRostaghi, M. and Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters 23, 610–614.\n\n\n\nRunge, J. (09–11 Apr 2018). Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information. In: Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, Vol. 84 of Proceedings of Machine Learning Research, edited by Storkey, A. and Perez-Cruz, F. (PMLR); pp. 938–947.\n\n\n\nRényi, A. (1961). On measures of entropy and information. In: Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, Vol. 4 (University of California Press); pp. 547–562.\n\n\n\nSarbu, S. (2014). Rényi information transfer: Partial Rényi transfer entropy and partial Rényi mutual information. In: 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (IEEE); pp. 5666–5670.\n\n\n\nSchmidt, C.; Huegle, J. and Uflacker, M. (2018). Order-independent constraint-based causal structure learning for gaussian distribution models using GPUs. In: Proceedings of the 30th International Conference on Scientific and Statistical Database Management; pp. 1–10.\n\n\n\nSchreiber, T. (2000). Measuring information transfer. Physical review letters 85, 461.\n\n\n\nSchuermann, T. (2004). Bias analysis in entropy estimation. Journal of Physics A: Mathematical and General 37, L295.\n\n\n\nShannon, C. E. (1948). A mathematical theory of communication. The Bell system technical journal 27, 379–423.\n\n\n\nSingh, H.; Misra, N.; Hnizdo, V.; Fedorowicz, A. and Demchuk, E. (2003). Nearest Neighbor Estimates of Entropy. American Journal of Mathematical and Management Sciences 23, 301–321.\n\n\n\nSpirtes, P.; Glymour, C. N. and Scheines, R. (2000). Causation, prediction, and search (MIT press).\n\n\n\nStaniek, M. and Lehnertz, K. (2008). Symbolic Transfer Entropy. Phys. Rev. Lett. 100, 158101.\n\n\n\nSugihara, G.; May, R. M.; Ye, H.; Hsieh, C.-h.; Deyle, E. R.; Fogarty, M. and Munch, S. B. (2012). Detecting Causality in Complex Ecosystems. Science 338, 496–500.\n\n\n\nSun, J.; Taylor, D. and Bollt, E. M. (2015). Causal Network Inference by Optimal Causation Entropy. SIAM Journal on Applied Dynamical Systems 14, 73–106, arXiv:https://doi.org/10.1137/140956166.\n\n\n\nSzékely, G. J. and Rizzo, M. L. (2014). Partial distance correlation with methods for dissimilarities. The Annals of Statistics 42, 2382–2412.\n\n\n\nSzékely, G. J.; Rizzo, M. L. and Bakirov, N. K. (2007). Measuring and testing dependence by correlation of distances. The Annals of Statistics 35, 2769–2794.\n\n\n\nTsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics 52, 479–487.\n\n\n\nTsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Vol. 1 no. 1 (Springer).\n\n\n\nVasicek, O. (1976). A test for normality based on sample entropy. Journal of the Royal Statistical Society Series B: Statistical Methodology 38, 54–59.\n\n\n\nVejmelka, M. and Paluš, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E 77, 026214.\n\n\n\nWang, X.; Si, S. and Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics 17, 5419–5429.\n\n\n\nZahl, S. (1977). Jackknifing an index of diversity. Ecology 58, 907–913.\n\n\n\nZhao, J.; Zhou, Y.; Zhang, X. and Chen, L. (2016). Part mutual information for quantifying direct associations in networks. Proceedings of the National Academy of Sciences 113, 5130–5135.\n\n\n\nZhu, J.; Bellanger, J.-J.; Shu, H. and Le Bouquin Jeannès, R. (2015). Contribution to Transfer Entropy Estimation via the k-Nearest-Neighbors Approach. Entropy 17, 4173–4201.\n\n\n\nZunino, L.; Olivares, F.; Scholkmann, F. and Rosso, O. A. (2017). Permutation entropy based time series analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A 381, 1883–1892.\n\n\n\n","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/discretization_counts_probs_api/#Discretization-API","page":"Discretization API","title":"Discretization API","text":"","category":"section"},{"location":"api/discretization_counts_probs_api/#Encoding-multiple-input-datasets","page":"Discretization API","title":"Encoding multiple input datasets","text":"","category":"section"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"A fundamental operation when computing multivariate information measures from data is discretization.  When discretizing, what happens is that we \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\". This is useful in several ways:","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"Once a dataset has been encoded into integers, we can estimate Counts or Probabilities (tutorial).\nOnce probabilities have been estimated, one can use these to estimate MultivariateInformationMeasure (tutorial).","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"The following functions and types are used by Associations.jl to perform discretization of input data.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"Discretization\nCodifyVariables\nCodifyPoints\ncodify","category":"page"},{"location":"api/discretization_counts_probs_api/#Associations.Discretization","page":"Discretization API","title":"Associations.Discretization","text":"Discretization\n\nThe supertype of all discretization schemes.\n\nConcrete implementations\n\nCodifyVariables\nCodifyPoints\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#Associations.CodifyVariables","page":"Discretization API","title":"Associations.CodifyVariables","text":"CodifyVariables <: Discretization\nCodifyVariables(outcome_space::OutcomeSpace)\n\nThe CodifyVariables discretization scheme quantises input data in a column-wise manner using the given outcome_space.\n\nCompatible outcome spaces\n\nUniqueElements (for when data are pre-discretized)\nBubbleSortSwaps\nCosineSimilarityBinning\nOrdinalPatterns\nDispersion\n\nDescription\n\nThe main difference between CodifyVariables and [CodifyPoints] is that the former uses OutcomeSpaces for discretization. This usually means that some transformation is applied to the data before discretizing. For example, some outcome constructs a delay embedding from the input (and thus encodes sequential information) before encoding the data.\n\nSpecifically, given x::AbstractStateSpaceSet..., where the i-th dataset x[i]  is assumed to represent a single series of measurements, CodifyVariables encodes  x[i] by codify-ing into a series of integers  using an appropriate  OutcomeSpace. This is typically done by first  sequentially transforming the data and then running sliding window (the width of  the window is controlled by outcome_space) across the data, and then encoding the  values within each window to an integer.\n\nExamples\n\nusing Associations\nx, y = rand(100), rand(100)\nd = CodifyVariables(OrdinalPatterns(m=2))\ncx, cy = codify(d, x, y)\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#Associations.CodifyPoints","page":"Discretization API","title":"Associations.CodifyPoints","text":"CodifyPoints{N}\nCodifyPoints(encodings::NTuple{N, Encoding})\n\nCodifyPoints points is a Discretization scheme that encodes input data points without applying any sequential transformation to the input (as opposed to  CodifyVariables, which may apply some transformation before encoding).\n\nUsage\n\nUse with codify to encode/discretize input variable on a point-by-point basis.\n\nCompatible encodings\n\nGaussianCDFEncoding\nOrdinalPatternEncoding\nRelativeMeanEncoding\nRelativeFirstDifferenceEncoding\nUniqueElementsEncoding\nRectangularBinEncoding\nCombinationEncoding\n\nDescription\n\nGiven x::AbstractStateSpaceSet..., where the i-th dataset is assumed to represent a single series of measurements, CodifyPoints encodes each point pₖ ∈ x[i]  using some Encoding(s), without applying any (sequential) transformation to the x[i] first. This behaviour is different to CodifyVariables, which does apply a transformation to x[i] before encoding.\n\nIf length(x) == N (i.e. there are N input dataset), then encodings must be a tuple of N Encoding. Alternatively, if encodings is a single Encoding, then that same encoding is applied to every x[i].\n\nExamples\n\nusing Associations\n\n# The same encoding on two input datasets\nx = StateSpaceSet(rand(100, 3))\ny = StateSpaceSet(rand(100, 3))\nencoding_ord = OrdinalPatternEncoding(3)\ncx, cy = codify(CodifyPoints(encoding_ord), x, y)\n\n# Different encodings on multiple datasets\nz = StateSpaceSet(rand(100, 2))\nencoding_bin = RectangularBinEncoding(RectangularBinning(3), z)\nd = CodifyPoints(encoding_ord, encoding_ord, encoding_bin)\ncx, cy, cz = codify(d, x, y, z)\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.codify","page":"Discretization API","title":"ComplexityMeasures.codify","text":"codify(o::OutcomeSpace, x::Vector) → s::Vector{Int}\ncodify(o::OutcomeSpace, x::AbstractStateSpaceSet{D}) → s::NTuple{D, Vector{Int}\n\nCodify x according to the outcome space o. If x is a Vector, then a Vector{<:Integer} is returned. If x is a StateSpaceSet{D}, then symbolization is done column-wise and an NTuple{D, Vector{<:Integer}} is returned, where D = dimension(x).\n\nDescription\n\nThe reason this function exists is that we don't always want to encode the entire input x at once. Sometimes, it is desirable to first apply some transformation to x first, then apply Encodings in a point-wise manner in the transformed space. (the OutcomeSpace dictates this transformation). This is useful for encoding timeseries data.\n\nThe length of the returned s depends on the OutcomeSpace. Some outcome spaces preserve the input data length (e.g. UniqueElements), while some outcome spaces (e.g. OrdinalPatterns) do e.g. delay embeddings before encoding, so that length(s) < length(x).\n\n\n\n\n\n","category":"function"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"In summary, the two main ways of discretizing data in Associations are as follows.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"The CodifyPoints discretization scheme encodes input data on a point-by-point    basis by applying some Encoding to each point.\nThe CodifyVariables discretization scheme encodes input data on a column-by-column   basis by applying a sliding window to each column, and encoding the data within the sliding window according to some OutcomeSpace (Internally, this uses codify).","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"note: Note\nEncoding, OutcomeSpace and codify are all from ComplexityMeasures.jl. In this package, they are used to discretize multiple input variables instead of just one input variable.","category":"page"},{"location":"api/discretization_counts_probs_api/#Encoding-per-point/row","page":"Discretization API","title":"Encoding per point/row","text":"","category":"section"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"In some cases, it may be desireable to encode data on a row-wise basis. This  typically happens when working with pre-embedded time series or StateSpaceSets  (respecting the fact that time ordering is already taken care of by the  embedding procedure).  If we want to apply something like OrdinalPatternEncoding to such data, then  we must encode each point individually, such that vectors like [1.2, 2.4, 4.5] or  [\"howdy\", \"partner\"] gets mapped to an integer. The CodifyPoints discretization  intstruction ensures input data are encoded on a point-by-point basis.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"A point-by-point discretization using CodifyPoints is formally done by applying some Encoding to each input data point. You can pick between the following encodings, or combine  them in arbitrary ways using CombinationEncoding.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"Encoding\nGaussianCDFEncoding\nOrdinalPatternEncoding\nRelativeMeanEncoding\nRelativeFirstDifferenceEncoding\nUniqueElementsEncoding\nRectangularBinEncoding\nCombinationEncoding","category":"page"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.Encoding","page":"Discretization API","title":"ComplexityMeasures.Encoding","text":"Encoding\n\nThe supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions encode and decode. Some probability estimators utilize encodings internally.\n\nCurrent available encodings are:\n\nOrdinalPatternEncoding.\nGaussianCDFEncoding.\nRectangularBinEncoding.\nRelativeMeanEncoding.\nRelativeFirstDifferenceEncoding.\nUniqueElementsEncoding.\nBubbleSortSwapsEncoding.\nPairDistanceEncoding.\nCombinationEncoding, which can combine any of the above encodings.\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.GaussianCDFEncoding","page":"Discretization API","title":"ComplexityMeasures.GaussianCDFEncoding","text":"GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding{m}(; μ, σ, c::Int = 3)\n\nAn encoding scheme that encodes a scalar or vector χ into one of the integers sᵢ ∈ [1, 2, …, c] based on the normal cumulative distribution function (NCDF), and decodes the sᵢ into subintervals of [0, 1] (with some loss of information).\n\nInitializing a GaussianCDFEncoding\n\nThe size of the input to be encoded must be known beforehand. One must therefore set m = length(χ), where χ is the input (m = 1 for scalars, m ≥ 2 for vectors). To do so, one must explicitly give m as a type parameter: e.g. encoding = GaussianCDFEncoding{3}(; μ = 0.0, σ = 0.1) to encode 3-element vectors, or encoding = GaussianCDFEncoding{1}(; μ = 0.0, σ = 0.1) to encode scalars.\n\nDescription\n\nEncoding/decoding scalars\n\nGaussianCDFEncoding first maps an input scalar χ to a new real number y_ in 0 1 by using the normal cumulative distribution function (CDF) with the given mean μ and standard deviation σ, according to the map\n\nx to y  y = dfrac1 sigma\n    sqrt2 pi int_-infty^x e^(-(x - mu)^2)(2 sigma^2) dx\n\nNext, the interval [0, 1] is equidistantly binned and enumerated 1 2 ldots c,  and y is linearly mapped to one of these integers using the linear map  y to z  z = textfloor(y(c-1)) + 1.\n\nBecause of the floor operation, some information is lost, so when used with decode, each decoded sᵢ is mapped to a subinterval of [0, 1]. This subinterval is returned as a length-1 Vector{SVector}.\n\nNotice that the decoding step does not yield an element of any outcome space of the estimators that use GaussianCDFEncoding internally, such as Dispersion. That is because these estimators additionally delay embed the encoded data.\n\nEncoding/decoding vectors\n\nIf GaussianCDFEncoding is used with a vector χ, then each element of χ is encoded separately, resulting in a length(χ) sequence of integers which may be treated as a CartesianIndex. The encoded symbol s ∈ [1, 2, …, c] is then just the linear index corresponding to this cartesian index (similar to how CombinationEncoding works).\n\nWhen decoded, the integer symbol s is converted back into its CartesianIndex representation,  which is just a sequence of integers that refer to subdivisions of the [0, 1] interval. The relevant subintervals are then returned as a length-χ Vector{SVector}.\n\nExamples\n\njulia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> μ, σ = mean(x), std(x); encoding = GaussianCDFEncoding(; μ, σ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatternEncoding","page":"Discretization API","title":"ComplexityMeasures.OrdinalPatternEncoding","text":"OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding{m}(lt = ComplexityMeasures.isless_rand)\n\nAn encoding scheme that encodes length-m vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by OrdinalPatterns and similar estimators, see that for a description of the outcome space.\n\nThe ordinal/permutation pattern of a vector χ is simply sortperm(χ), which gives the indices that would sort χ in ascending order.\n\nDescription\n\nThe Lehmer code, as implemented here, is a bijection between the set of factorial(m) possible permutations for a length-m sequence, and the integers 1, 2, …, factorial(m). The encoding step uses algorithm 1 in Berger et al. (2019), which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!).\n\nExample\n\njulia> using ComplexityMeasures\n\njulia> χ = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, χ)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3\n\nIf you want to encode something that is already a permutation pattern, then you can use the non-exported permutation_to_integer function.\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.RelativeMeanEncoding","page":"Discretization API","title":"ComplexityMeasures.RelativeMeanEncoding","text":"RelativeMeanEncoding <: Encoding\nRelativeMeanEncoding(minval::Real, maxval::Real; n = 2)\n\nRelativeMeanEncoding encodes a vector based on the relative position the mean of the vector has with respect to a predefined minimum and maximum value (minval and maxval, respectively).\n\nDescription\n\nThis encoding is inspired by Azami and Escudero (2016)'s algorithm for amplitude-aware permutation entropy. They use a linear combination of amplitude information and first differences information of state vectors to correct probabilities. Here, however, we explicitly encode the amplitude-part of the correction as an a integer symbol Λ ∈ [1, 2, …, n]. The first-difference part of the encoding is available as the RelativeFirstDifferenceEncoding encoding.\n\nEncoding/decoding\n\nWhen used with encode, an m-element state vector bfx = (x_1 x_2 ldots x_m) is encoded as Λ = dfrac1Nsum_i=1^m abs(x_i). The value of Λ is then normalized to lie on the interval [0, 1], assuming that the minimum/maximum value any single element x_i can take is minval/maxval, respectively. Finally, the interval [0, 1] is discretized into n discrete bins, enumerated by positive integers 1, 2, …, n, and the number of the bin that the normalized Λ falls into is returned.\n\nWhen used with decode, the left-edge of the bin that the normalized Λ fell into is returned.\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.RelativeFirstDifferenceEncoding","page":"Discretization API","title":"ComplexityMeasures.RelativeFirstDifferenceEncoding","text":"RelativeFirstDifferenceEncoding <: Encoding\nRelativeFirstDifferenceEncoding(minval::Real, maxval::Real; n = 2)\n\nRelativeFirstDifferenceEncoding encodes a vector based on the relative position the average of the first differences of the vectors has with respect to a predefined minimum and maximum value (minval and maxval, respectively).\n\nDescription\n\nThis encoding is inspired by Azami and Escudero (2016)'s algorithm for amplitude-aware permutation entropy. They use a linear combination of amplitude information and first differences information of state vectors to correct probabilities. Here, however, we explicitly encode the first differences part of the correction as an a integer symbol Λ ∈ [1, 2, …, n]. The amplitude part of the encoding is available as the RelativeMeanEncoding encoding.\n\nEncoding/decoding\n\nWhen used with encode, an m-element state vector bfx = (x_1 x_2 ldots x_m) is encoded as Λ = dfrac1m - 1sum_k=2^m x_k - x_k-1. The value of Λ is then normalized to lie on the interval [0, 1], assuming that the minimum/maximum value any single abs(x_k - x_k-1) can take is minval/maxval, respectively. Finally, the interval [0, 1] is discretized into n discrete bins, enumerated by positive integers 1, 2, …, n, and the number of the bin that the normalized Λ falls into is returned. The smaller the mean first difference of the state vector is, the smaller the bin number is. The higher the mean first difference of the state vectors is, the higher the bin number is.\n\nWhen used with decode, the left-edge of the bin that the normalized Λ fell into is returned.\n\nPerformance tips\n\nIf you are encoding multiple input vectors, it is more efficient to construct a RelativeFirstDifferenceEncoding instance and re-use it:\n\nminval, maxval = 0, 1\nencoding = RelativeFirstDifferenceEncoding(minval, maxval; n = 4)\npts = [rand(3) for i = 1:1000]\n[encode(encoding, x) for x in pts]\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElementsEncoding","page":"Discretization API","title":"ComplexityMeasures.UniqueElementsEncoding","text":"UniqueElementsEncoding <: Encoding\nUniqueElementsEncoding(x)\n\nUniqueElementsEncoding is a generic encoding that encodes each xᵢ ∈ unique(x) to one of the positive integers. The xᵢ are encoded according to the order of their first appearance in the input data.\n\nThe constructor requires the input data x, since the number of possible symbols is length(unique(x)).\n\nExample\n\nusing ComplexityMeasures\nx = ['a', 2, 5, 2, 5, 'a']\ne = UniqueElementsEncoding(x)\nencode.(Ref(e), x) == [1, 2, 3, 2, 3, 1] # true\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.RectangularBinEncoding","page":"Discretization API","title":"ComplexityMeasures.RectangularBinEncoding","text":"RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning)\n\nAn encoding scheme that encodes points χ ∈ x into their histogram bins.\n\nThe first call signature simply initializes a FixedRectangularBinning and then calls the second call signature.\n\nSee FixedRectangularBinning for info on mapping points to bins.\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.CombinationEncoding","page":"Discretization API","title":"ComplexityMeasures.CombinationEncoding","text":"CombinationEncoding <: Encoding\nCombinationEncoding(encodings)\n\nA CombinationEncoding takes multiple Encodings and creates a combined encoding that can be used to encode inputs that are compatible with the given encodings.\n\nEncoding/decoding\n\nWhen used with encode, each Encoding in encodings returns integers in the set 1, 2, …, n_e, where n_e is the total number of outcomes for a particular encoding. For k different encodings, we can thus construct the cartesian coordinate (c₁, c₂, …, cₖ) (cᵢ ∈ 1, 2, …, n_i), which can uniquely be identified by an integer. We can thus identify each unique combined encoding with a single integer.\n\nWhen used with decode, the integer symbol is converted to its corresponding cartesian coordinate, which is used to retrieve the decoded symbols for each of the encodings, and a tuple of the decoded symbols are returned.\n\nThe total number of outcomes is prod(total_outcomes(e) for e in encodings).\n\nExamples\n\nusing ComplexityMeasures\n\n# We want to encode the vector `x`.\nx = [0.9, 0.2, 0.3]\n\n# To do so, we will use a combination of first-difference encoding, amplitude encoding,\n# and ordinal pattern encoding.\n\nencodings = (\n    RelativeFirstDifferenceEncoding(0, 1; n = 2),\n    RelativeMeanEncoding(0, 1; n = 5),\n    OrdinalPatternEncoding(3) # x is a three-element vector\n    )\nc = CombinationEncoding(encodings)\n\n# Encode `x` as integer\nω = encode(c, x)\n\n# Decode symbol (into a vector of decodings, one for each encodings `e ∈ encodings`).\n# In this particular case, the first two element will be left-bin edges, and\n# the last element will be the decoded ordinal pattern (indices that would sort `x`).\nd = decode(c, ω)\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#tutorial_codify_points","page":"Discretization API","title":"Examples: encoding rows (one point at a time)","text":"","category":"section"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"We'll here use the OrdinalPatternEncoding with differing parameter m to encode  multiple StateSpaceSet of differing dimensions.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"using Associations\nusing StateSpaceSets\nusing Random; rng = Xoshiro(1234)\n\n# The first variable is 2-dimensional and has 50 points\nx = StateSpaceSet(rand(rng, 50, 2))\n# The second variable is 3-dimensional and has 50 points\ny = StateSpaceSet(rand(rng, 50, 3))\n# The third variable is 4-dimensional and has 50 points\nz = StateSpaceSet(rand(rng, 50, 4))\n\n# One encoding scheme per input variable\n# encode `x` using `ox` on a point-by-point basis (Vector{SVector{4}} → Vector{Int})\n# encode `y` using `oy` on a point-by-point basis (Vector{SVector{3}} → Vector{Int})\n# encode `z` using `oz` on a point-by-point basis (Vector{SVector{2}} → Vector{Int})\nox = OrdinalPatternEncoding(2)\noy = OrdinalPatternEncoding(3)\noz = OrdinalPatternEncoding(4)\n\n# This given three column vectors of integers.\ncx, cy, cz = codify(CodifyPoints(ox, oy, oz), x, y, z)\n\n[cx cy cz]","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"Notice that the 2-dimensional x has been encoded into integer values 1 or 2, because there are 2! possible ordinal patterns for dimension m = 2. The 3-dimensional y has  been encoded into integers in the range 1 to 3! = 6, while the 4-dimensional z is  encoded into an even larger range of integers, because the number of possible ordinal patterns is 4! = 24 for 4-dimensional embedding vectors.","category":"page"},{"location":"api/discretization_counts_probs_api/#Encoding-per-variable/column","page":"Discretization API","title":"Encoding per variable/column","text":"","category":"section"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"Sometimes, it may be desireable to encode input data one variable/column at a time. This typically happens when the input are either a single or multiple timeseries.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"To encode columns, we move a sliding window across each input variable/column and  encode points within that window. Formally, such a sliding-window discretization  is done by using the CodifyVariables discretization scheme, which takes as input some OutcomeSpace that dictates how each window is encoded, and  also dictates the width of the encoding windows. ","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"For column/variable-wise encoding, you can pick between the following outcome spaces.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"OutcomeSpace\nUniqueElements\nCosineSimilarityBinning\nDispersion\nOrdinalPatterns\nBubbleSortSwaps\nValueBinning\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.OutcomeSpace","page":"Discretization API","title":"ComplexityMeasures.OutcomeSpace","text":"OutcomeSpace\n\nThe supertype for all outcome space implementation.\n\nDescription\n\nIn ComplexityMeasures.jl, an outcome space defines a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L  (some form of discretization). In the literature, the outcome space is often also called an \"alphabet\", while each outcome is called a \"symbol\" or an \"event\".\n\nAn outcome space also defines a set of rules for mapping input data to to each outcome omega_i, a processes called encoding or symbolizing or discretizing in the literature (see encodings). Some OutcomeSpaces first apply a transformation, e.g. a delay embedding, to the data before discretizing/encoding, while other OutcomeSpaces discretize/encode the data directly.\n\nImplementations\n\nOutcome space Principle Input data Counting-compatible\nUniqueElements Count of unique elements Any ✔\nValueBinning Binning (histogram) Vector, StateSpaceSet ✔\nOrdinalPatterns Ordinal patterns Vector, StateSpaceSet ✔\nSpatialOrdinalPatterns Ordinal patterns in space Array ✔\nDispersion Dispersion patterns Vector ✔\nSpatialDispersion Dispersion patterns in space Array ✔\nCosineSimilarityBinning Cosine similarity Vector ✔\nBubbleSortSwaps Swap counts when sorting Vector ✔\nSequentialPairDistances Sequential state vector distances Vector, StateSpaceSet ✔\nTransferOperator Binning (transfer operator) Vector, StateSpaceSet ✖\nNaiveKernel Kernel density estimation StateSpaceSet ✖\nWeightedOrdinalPatterns Ordinal patterns Vector, StateSpaceSet ✖\nAmplitudeAwareOrdinalPatterns Ordinal patterns Vector, StateSpaceSet ✖\nWaveletOverlap Wavelet transform Vector ✖\nPowerSpectrum Fourier transform Vector ✖\n\nIn the column \"input data\" it is assumed that the eltype of the input is <: Real.\n\nUsage\n\nOutcome spaces are used as input to\n\nprobabilities/allprobabilities_and_outcomes for computing   probability mass functions.\noutcome_space, which returns the elements of the outcome space.\ntotal_outcomes, which returns the cardinality of the outcome space.\ncounts/counts_and_outcomes/allcounts_and_outcomes, for    obtaining raw counts instead of probabilities (only for counting-compatible outcome   spaces).\n\nCounting-compatible vs. non-counting compatible outcome spaces\n\nThere are two main types of outcome spaces.\n\nCounting-compatible outcome spaces have a well-defined   way of counting how often each point in the (encoded) input data is mapped to a   particular outcome omega_i. These outcome spaces use   encode to discretize the input data. Examples are   OrdinalPatterns (which encodes input data into ordinal patterns) or   ValueBinning (which discretizes points onto a regular grid).   The table below lists which outcome spaces are counting compatible.\nNon-counting compatible outcome spaces have no well-defined way of counting explicitly   how often each point in the input data is mapped to a particular outcome omega_i.   Instead, these outcome spaces returns a vector of pre-normalized \"relative counts\", one   for each outcome omega_i. Examples are WaveletOverlap or   PowerSpectrum.\n\nCounting-compatible outcome spaces can be used with any ProbabilitiesEstimator to convert counts into probability mass functions. Non-counting-compatible outcome spaces can only be used with the maximum likelihood (RelativeAmount) probabilities estimator, which estimates probabilities precisely by the relative frequency of each outcome (formally speaking, the RelativeAmount estimator also requires counts, but for the sake of code consistency, we allow it to be used with relative frequencies as well).\n\nThe function is_counting_based can be used to check whether an outcome space is based on counting.\n\nDeducing the outcome space (from data)\n\nSome outcome space models can deduce Omega without knowledge of the input, such as OrdinalPatterns. Other outcome spaces require knowledge of the input data for concretely specifying Omega, such as ValueBinning with RectangularBinning. If o is some outcome space model and x some input data, then outcome_space(o, x) returns the possible outcomes Omega. To get the cardinality of Omega, use total_outcomes.\n\nImplementation details\n\nThe element type of Omega varies between outcome space models, but it is guaranteed to be hashable and sortable. This allows for conveniently tracking the counts of a specific event across experimental realizations, by using the outcome as a dictionary key and the counts as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.UniqueElements","page":"Discretization API","title":"ComplexityMeasures.UniqueElements","text":"UniqueElements()\n\nAn OutcomeSpace based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input. Hence, input x is needed for a well-defined outcome_space.\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.CosineSimilarityBinning","page":"Discretization API","title":"ComplexityMeasures.CosineSimilarityBinning","text":"CosineSimilarityBinning(; m::Int, τ::Int, nbins::Int)\n\nA OutcomeSpace based on the cosine similarity (Wang et al., 2020).\n\nIt can be used with information to compute the \"diversity entropy\" of an input timeseries (Wang et al., 2020).\n\nThe implementation here allows for τ != 1, which was not considered in the original paper.\n\nDescription\n\nCosineSimilarityBinning probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag τ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mτ.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mτ-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals (including the value +1).\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for CosineSimilarityBinning is the bins of the [-1, 1] interval, and the return configuration is the same as in ValueBinning (left bin edge).\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.Dispersion","page":"Discretization API","title":"ComplexityMeasures.Dispersion","text":"Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)\n\nAn OutcomeSpace based on dispersion patterns, originally used by Rostaghi and Azami (2016) to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values (Li et al., 2019) are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is encoded into a symbol timeseries S using the Gaussian encoding GaussianCDFEncoding with empirical mean μ and empirical standard deviation σ (both determined from X), and c as given to Dispersion.\n\nThen, S is embedded into an m-dimensional time series, using an embedding lag of tau, which yields a total of N - (m - 1)tau delay vectors z_i, or \"dispersion patterns\". Since each element of z_i can take on c different values, and each delay vector has m entries, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nThe returned probabilities are simply the frequencies of the unique dispersion patterns present in S (i.e., the UniqueElements of S).\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of S.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2019) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\nFor a version of this estimator that can be used on high-dimensional arrays, see SpatialDispersion.\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.OrdinalPatterns","page":"Discretization API","title":"ComplexityMeasures.OrdinalPatterns","text":"OrdinalPatterns <: OutcomeSpace\nOrdinalPatterns{m}(τ = 1, lt::Function = ComplexityMeasures.isless_rand)\n\nAn OutcomeSpace based on lengh-m ordinal permutation patterns, originally introduced in Bandt and Pompe (2002)'s paper on permutation entropy. Note that m is given as a type parameter, so that when it is a literal integer there are performance accelerations.\n\nWhen passed to probabilities the output depends on the input data type:\n\nUnivariate data. If applied to a univariate timeseries (AbstractVector), then the timeseries   is first embedded using embedding delay τ and dimension m, resulting in embedding   vectors  bfx_i _i=1^N-(m-1)tau. Then, for each bfx_i,   we find its permutation pattern pi_i. Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using UniqueElements. When giving the resulting probabilities to   information, the original permutation entropy is computed (Bandt and Pompe, 2002).\nMultivariate data. If applied to a an D-dimensional StateSpaceSet,   then no embedding is constructed, m must be equal to D and τ is ignored.   Each vector bfx_i of the dataset is mapped   directly to its permutation pattern pi_i by comparing the   relative magnitudes of the elements of bfx_i.   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy (He et al., 2016), although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of He et al. (2016)).\n\nInternally, OrdinalPatterns uses the OrdinalPatternEncoding to represent ordinal patterns as integers for efficient computations.\n\nSee WeightedOrdinalPatterns and AmplitudeAwareOrdinalPatterns for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see SpatialOrdinalPatterns.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt and Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution (Zunino et al., 2017). Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = ComplexityMeasures.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless.\n\nOutcome space\n\nThe outcome space Ω for OrdinalPatterns is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nFor example, the outcome [2, 3, 1] corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also OrdinalPatternEncoding.\n\nIn-place symbolization\n\nOrdinalPatterns also implements the in-place probabilities! for StateSpaceSet input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example\n\nusing ComplexityMeasures\nm, N = 2, 100\nest = OrdinalPatterns{m}(τ)\nx = StateSpaceSet(rand(N, m)) # some input dataset\nπs_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(πs_ts, est, x)\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.BubbleSortSwaps","page":"Discretization API","title":"ComplexityMeasures.BubbleSortSwaps","text":"BubbleSortSwaps <: CountBasedOutcomeSpace\nBubbleSortSwaps(; m = 3, τ = 1)\n\nThe BubbleSortSwaps outcome space is based on Manis et al. (2017)'s  paper on \"bubble entropy\". \n\nDescription\n\nBubbleSortSwaps does the following:\n\nEmbeds the input data using embedding dimension m and  embedding lag τ\nFor each state vector in the embedding, counting how many swaps are necessary for   the bubble sort algorithm to sort state vectors.\n\nFor counts_and_outcomes, we then define a distribution over the number of  necessary swaps. This distribution can then be used to estimate probabilities using  probabilities_and_outcomes, which again can be used to estimate any  InformationMeasure. An example of how to compute the \"Shannon bubble entropy\" is given below.\n\nOutcome space\n\nThe outcome_space for BubbleSortSwaps are the integers 0:N, where N = (m * (m - 1)) / 2 + 1 (the worst-case number of swaps). Hence, the number of total_outcomes is N + 1.\n\nImplements\n\ncodify. Returns the number of swaps required for each embedded state vector.\n\nExamples\n\nWith the BubbleSortSwaps outcome space, we can easily compute a \"bubble entropy\" inspired by (Manis et al., 2017). Note: this is not actually a new entropy - it is just  a new way of discretizing the input data. To reproduce the bubble entropy complexity measure from (Manis et al., 2017), see BubbleEntropy.\n\nExamples\n\nusing ComplexityMeasures\nx = rand(100000)\no = BubbleSortSwaps(; m = 5) # 5-dimensional embedding vectors\ninformation(Shannon(; base = 2), o, x)\n\n# We can also compute any other \"bubble quantity\", for example the \n# \"Tsallis bubble extropy\", with arbitrary probabilities estimators:\ninformation(TsallisExtropy(), BayesianRegularization(), o, x)\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.ValueBinning","page":"Discretization API","title":"ComplexityMeasures.ValueBinning","text":"ValueBinning(b::AbstractBinning) <: OutcomeSpace\n\nAn OutcomeSpace based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are subtypes of AbstractBinning.\n\nThe ValueBinning estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueBinning(ϵ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueBinning is the unique bins constructed from b. Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals [a, b). The bins are in data units, not integer (cartesian indices units), and are returned as SVectors, i.e., same type as input data.\n\nFor convenience, outcome_space returns the outcomes in the same array format as the underlying binning (e.g., Matrix for 2D input).\n\nFor FixedRectangularBinning the outcome_space is well-defined from the binning, but for RectangularBinning input x is needed as well.\n\nImplements\n\ncodify. Used for encoding inputs where ordering matters (e.g. time series).\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.RectangularBinning","page":"Discretization API","title":"ComplexityMeasures.RectangularBinning","text":"RectangularBinning(ϵ, precise = false) <: AbstractBinning\n\nRectangular box partition of state space using the scheme ϵ, deducing the histogram extent and bin width from the input data.\n\nRectangularBinning is a convenience struct. It is re-cast into FixedRectangularBinning once the data are provided, so see that docstring for info on the bin calculation and the meaning of precise.\n\nBinning instructions are deduced from the type of ϵ as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals  that cover all data.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals that cover all data.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  ϵ[i], starting from the axis minima until the data is completely covered by boxes.\n\nRectangularBinning ensures all input data are covered by extending the created ranges if need be.\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#ComplexityMeasures.FixedRectangularBinning","page":"Discretization API","title":"ComplexityMeasures.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false)\n\nRectangular box partition of state space where the partition along each dimension is explicitly given by each range ranges, which is a tuple of AbstractRange subtypes. Typically, each range is the output of the range Base function, e.g., ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)). All ranges must be sorted.\n\nThe optional second argument precise dictates whether Julia Base's TwicePrecision is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is false.\n\nPoints falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open: [a, b). This means that the last value of each of the ranges dictates the last right-closing value. This value does not belong to the histogram! E.g., if given a range r = range(0, 1; length = 11), with r[end] = 1, the value 1 is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here [0.9, 1))!\n\nEquivalently, the size of the histogram is histsize = map(r -> length(r)-1, ranges)!\n\nFixedRectangularBinning leads to a well-defined outcome space without knowledge of input data, see ValueBinning.\n\n\n\n\n\n","category":"type"},{"location":"api/discretization_counts_probs_api/#Example:-encoding-*columns*-(one-variable-at-a-time)","page":"Discretization API","title":"Example: encoding columns (one variable at a time)","text":"","category":"section"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"Some OutcomeSpaces dictate a sliding window which has the width of one element when used with CodifyVariables. ValueBinning is such an outcome space.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n\nx = rand(rng, 100)\no = ValueBinning(3)\ncx = codify(CodifyVariables(o), x)","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"We can verify that ValueBinning preserves the cardinality of the input dataset.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"length(x) == length(cx)","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"Other outcome spaces such as Dispersion or OrdinalPatterns do not  preserve the cardinality of the input dataset when used with CodifyVariables. This is  because when they are applied in a sliding window, they compress sliding windows consisting of  potentially multiple points into single integers. This means that some points at the  end of each input variable are lost. For example, with OrdinalPatterns, the number  of encoded points decrease with the embedding parameter m.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n\nx = rand(rng, 100)\no = OrdinalPatterns(m = 3)\ncx = codify(CodifyVariables(o), x)","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"We can simultaneously encode multiple variable/columns of a StateSpaceSet using  the same outcome space, as long as the operation will result in the same number of encoded  data points for each column.","category":"page"},{"location":"api/discretization_counts_probs_api/","page":"Discretization API","title":"Discretization API","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n\nx = rand(rng, 100)\ny = rand(rng, 100)\no = OrdinalPatterns(m = 3)\n# Alternatively provide a tuple of input time series: codify(CodifyVariables(o), (x, y))\ncx, cy = codify(CodifyVariables(o), StateSpaceSet(x, y)) \n\n[cx cy]","category":"page"},{"location":"examples/examples_infer_graphs/#examples_network_inference","page":"Causal graph inference","title":"Causal graphs","text":"","category":"section"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"Before introducing the causal graph examples, let's create a function that can plot directed graphs that we'll use below.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"using Graphs, CairoMakie, GraphMakie\n\nfunction plotgraph(g; nlabels = repr.(1:nv(g)))\n    f, ax, p = graphplot(g,\n        ilabels = nlabels,\n        ilabels_color = [:white for i in 1:nv(g)],\n        node_color = :blue,\n        node_size = 80,\n        arrow_size = 15,\n        figure_padding = 10\n    )\n    offsets = 0.02 * (p[:node_pos][] .- p[:node_pos][][1])\n    offsets[1] = Point2f(0, 0.2)\n    p.nlabels_offset[] = offsets\n    autolimits!(ax)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    ax.aspect = DataAspect()\n    return f\nend","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"We'll also implement a set of chained logistic maps with unidirectional coupling.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"using DynamicalSystemsBase\nBase.@kwdef struct Logistic4Chain{V, RX, RY, RZ, RW, C1, C2, C3, Σ1, Σ2, Σ3, RNG}\n    xi::V = [0.1, 0.2, 0.3, 0.4]\n    rx::RX = 3.9\n    ry::RY = 3.6\n    rz::RZ = 3.6\n    rw::RW = 3.8\n    c_xy::C1 = 0.4\n    c_yz::C2 = 0.4\n    c_zw::C3 = 0.35\n    σ_xy::Σ1 = 0.05\n    σ_yz::Σ2 = 0.05\n    σ_zw::Σ3 = 0.05\n    rng::RNG = Random.default_rng()\nend\n\nfunction eom_logistic4_chain(u, p::Logistic4Chain, t)\n    (; xi, rx, ry, rz, rw, c_xy, c_yz, c_zw, σ_xy, σ_yz, σ_zw, rng) = p\n    x, y, z, w = u\n    f_xy = (y +  c_xy*(x + σ_xy * rand(rng)) ) / (1 + c_xy*(1+σ_xy))\n    f_yz = (z +  c_yz*(y + σ_yz * rand(rng)) ) / (1 + c_yz*(1+σ_yz))\n    f_zw = (w +  c_zw*(z + σ_zw * rand(rng)) ) / (1 + c_zw*(1+σ_zw))\n    dx = rx * x * (1 - x)\n    dy = ry * (f_xy) * (1 - f_xy)\n    dz = rz * (f_yz) * (1 - f_yz)\n    dw = rw * (f_zw) * (1 - f_zw)\n    return SVector{4}(dx, dy, dz, dw)\nend\n\n\nfunction system(definition::Logistic4Chain)\n    return DiscreteDynamicalSystem(eom_logistic4_chain, definition.xi, definition)\nend","category":"page"},{"location":"examples/examples_infer_graphs/#oce_example","page":"Causal graph inference","title":"Optimal causation entropy","text":"","category":"section"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"Here, we use the OCE algorithm to infer a time series graph. We use a SurrogateAssociationTest for the initial step, and a LocalPermutationTest for the conditional steps.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"using Associations\nusing StableRNGs\nrng = StableRNG(123)\n\n# An example system where `X → Y → Z → W`.\nsys = system(Logistic4Chain(; rng))\nx, y, z, w = columns(first(trajectory(sys, 300, Ttr = 10000)))\n\n# Independence tests for unconditional and conditional stages.\nuest = KSG2(MIShannon(); k = 3, w = 1)\nutest = SurrogateAssociationTest(uest; rng, nshuffles = 19)\ncest =  MesnerShalizi(CMIShannon(); k = 3, w = 1)\nctest = LocalPermutationTest(cest; rng, nshuffles = 19)\n\n# Infer graph\nalg = OCE(; utest, ctest, α = 0.05, τmax = 1)\nparents = infer_graph(alg, [x, y, z, w])\n\n# Convert to graph and inspect edges\ng = SimpleDiGraph(parents)\ncollect(edges(g))","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"The algorithm nicely recovers the true causal directions. We can also plot the graph using the function we made above.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"plotgraph(g; nlabels = [\"x\", \"y\", \"z\", \"w\"])","category":"page"},{"location":"examples/examples_infer_graphs/#pc_examples","page":"Causal graph inference","title":"PC-algorithm","text":"","category":"section"},{"location":"examples/examples_infer_graphs/#pc_examples_corr","page":"Causal graph inference","title":"Correlation-based tests","text":"","category":"section"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"Here, we demonstrate the use of the PC-algorithm with the correlation-based CorrTest both for the pairwise (i.e. using PearsonCorrelation) and conditional (i.e. using PartialCorrelation) case.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"We'll reproduce the first example from CausalInference.jl, where they also use a parametric correlation test to infer the skeleton graph for some  normally distributed data.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"using Associations\nusing StableRNGs\nrng = StableRNG(123)\nn = 300\nv = randn(rng, n)\nx = v + randn(rng, n)*0.25\nw = x + randn(rng, n)*0.25\nz = v + w + randn(rng, n)*0.25\ns = z + randn(rng, n)*0.25\nX = [x, v, w, z, s]\n\n# Infer a completed partially directed acyclic graph (CPDAG)\nalg = PC(CorrTest(), CorrTest(); α = 0.05)\nest_cpdag_parametric = infer_graph(alg, X; verbose = false)\n\n# Plot the graph\nplotgraph(est_cpdag_parametric; nlabels = [\"x\", \"v\", \"w\", \"z\", \"s\"])","category":"page"},{"location":"examples/examples_infer_graphs/#pc_examples_nonparametric","page":"Causal graph inference","title":"Nonparametric tests","text":"","category":"section"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"The main difference between the PC algorithm implementation here and in CausalInference.jl is that our implementation automatically works with any compatible and IndependenceTest, and thus any combination of (nondirectional) AssociationMeasure and estimator.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"Here, we replicate the example above, but using a nonparametric SurrogateAssociationTest with the Shannon mutual information MIShannon measure and the GaoOhViswanath estimator for the pairwise independence tests, and a LocalPermutationTest with conditional mutual information CMIShannon and the MesnerShalizi.","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"using Associations\nusing StableRNGs\nrng = StableRNG(123)\n\n# Use fewer observations, because MI/CMI takes longer to estimate\nn = 300\nv = randn(rng, n)\nx = v + randn(rng, n)*0.25\nw = x + randn(rng, n)*0.25\nz = v + w + randn(rng, n)*0.25\ns = z + randn(rng, n)*0.25\nX = [x, v, w, z, s]\n\nest_pairwise = JointProbabilities(MIShannon(), CodifyVariables(ValueBinning(3)))\nest_cond = MesnerShalizi(CMIShannon(); k = 5)\npairwise_test = SurrogateAssociationTest(est_pairwise; rng, nshuffles = 50)\ncond_test = LocalPermutationTest(est_cond; rng, nshuffles = 50)\nalg = PC(pairwise_test, cond_test; α = 0.05)\nest_cpdag_nonparametric = infer_graph(alg, X; verbose = false)\nplotgraph(est_cpdag_nonparametric)","category":"page"},{"location":"examples/examples_infer_graphs/","page":"Causal graph inference","title":"Causal graph inference","text":"We get the same basic structure of the graph, but which directional associations  are correctly ruled out varies. In general, using different types of  association measures with different independence tests, applied to general  non-gaussian data, will not give the same results as the correlation-based tests.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/information_multivariate_api/#information_api","page":"Multivariate information API","title":"Multivariate information API","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"information(::MultivariateInformationMeasureEstimator)\nMultivariateInformationMeasureEstimator","category":"page"},{"location":"api/information_multivariate_api/#ComplexityMeasures.information-Tuple{MultivariateInformationMeasureEstimator}","page":"Multivariate information API","title":"ComplexityMeasures.information","text":"information(est::MultivariateInformationMeasureEstimator, x...)\n\nEstimate some MultivariateInformationMeasure on input data x..., using the given MultivariateInformationMeasureEstimator.\n\nThis is just a convenience wrapper around association(est, x...).\n\n\n\n\n\n","category":"method"},{"location":"api/information_multivariate_api/#Associations.MultivariateInformationMeasureEstimator","page":"Multivariate information API","title":"Associations.MultivariateInformationMeasureEstimator","text":"MultivariateInformationMeasureEstimator\n\nThe supertype for all estimators of multivariate information measures.\n\nGeneric implementations\n\nJointProbabilities\nEntropyDecomposition\nMIDecomposition\nCMIDecomposition\n\nDedicated implementations\n\nMutualInformationEstimators:\n\nKraskovStögbauerGrassberger1\nKraskovStögbauerGrassberger2\nGaoOhViswanath\nGaoKannanOhViswanath\nGaussianMI\n\nConditionalMutualInformationEstimators:\n\nFPVP\nMesnerShalizi\nRahimzamani\nPoczosSchneiderCMI\nGaussianCMI\n\nTransferEntropyEstimators:\n\nZhu1\nLindner\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Estimators","page":"Multivariate information API","title":"Estimators","text":"","category":"section"},{"location":"api/information_multivariate_api/#Generic-estimators","page":"Multivariate information API","title":"Generic estimators","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"We provide a set of generic estimators that can be used to calculate  potentially several types of information measures.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"JointProbabilities\nEntropyDecomposition\nMIDecomposition\nCMIDecomposition","category":"page"},{"location":"api/information_multivariate_api/#Associations.JointProbabilities","page":"Multivariate information API","title":"Associations.JointProbabilities","text":"JointProbabilities <: InformationMeasureEstimator\nJointProbabilities(\n    definition::MultivariateInformationMeasure,\n    discretization::Discretization\n)\n\nJointProbabilities is a generic estimator for multivariate discrete information measures.\n\nUsage\n\nUse with association to compute an information measure from input data.\n\nDescription\n\nIt first encodes the input data according to the given discretization, then constructs  probs, a multidimensional Probabilities instance. Finally, probs are  forwarded to a PlugIn estimator, which computes the measure according to  definition.\n\nCompatible encoding schemes\n\nCodifyVariables (encode each variable/column of the input data independently by    applying an encoding in a sliding window over each input variable).  \nCodifyPoints (encode each point/column of the input data)\n\nWorks for any OutcomeSpace that implements codify.\n\nnote: Joint probabilities vs decomposition methods\nUsing JointProbabilities to compute an information measure,  e.g. conditional mutual estimation, is typically slower than other dedicated estimation procedures like EntropyDecomposition. The reason is that measures such as CMIShannon can be formulated as a sum of four entropies, which can be estimated individually and summed afterwards.  This decomposition is fast because because we avoid explicitly estimating the entire joint pmf,  which demands many extra calculation steps, However, the decomposition is biased,  because it fails to fully take into consideration the joint relationships between the variables. Pick your estimator according to your needs.\n\nSee also: Counts, Probabilities, ProbabilitiesEstimator, OutcomeSpace, DiscreteInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.EntropyDecomposition","page":"Multivariate information API","title":"Associations.EntropyDecomposition","text":"EntropyDecomposition(definition::MultivariateInformationMeasure, \n    est::DifferentialInfoEstimator)\nEntropyDecomposition(definition::MultivariateInformationMeasure,\n    est::DiscreteInfoEstimator,\n    discretization::CodifyVariables{<:OutcomeSpace},\n    pest::ProbabilitiesEstimator = RelativeAmount())\n\nEstimate the multivariate information measure specified by definition by rewriting its formula into some combination of entropy terms. \n\nIf calling the second method (discrete variant), then discretization is always done  per variable/column and each column is encoded into integers using codify.\n\nUsage\n\nUse with association to compute a MultivariateInformationMeasure   from input data: association(est::EntropyDecomposition, x...).\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nThe entropy terms are estimated using est, and then combined to form the final  estimate of definition. No bias correction is applied. If est is a DifferentialInfoEstimator, then discretization and pest  are ignored. If est is a DiscreteInfoEstimator, then discretization and a probabilities estimator pest must also be provided (default to RelativeAmount,  which uses naive plug-in probabilities).\n\nCompatible differential information estimators\n\nIf using the first signature, any compatible DifferentialInfoEstimator can be  used.\n\nCompatible outcome spaces for discrete estimation\n\nIf using the second signature, the outcome spaces can be used for discretisation.  Note that not all outcome spaces will work with all measures.\n\nEstimator Principle\nUniqueElements Count of unique elements\nValueBinning Binning (histogram)\nOrdinalPatterns Ordinal patterns\nDispersion Dispersion patterns\nBubbleSortSwaps Sorting complexity\nCosineSimilarityBinning Cosine similarities histogram\n\nBias\n\nEstimating the definition by decomposition into a combination of entropy terms, which are estimated independently, will in general be more biased than when using a dedicated estimator. One reason is that this decomposition may miss out on crucial information in the joint space. To remedy this, dedicated information measure  estimators typically derive the marginal estimates by first considering the joint space, and then does some clever trick to eliminate the bias that is introduced through a naive decomposition. Unless specified below, no bias correction is  applied for EntropyDecomposition.\n\nHandling of overlapping parameters\n\nIf there are overlapping parameters between the measure to be estimated, and the lower-level decomposed measures, then the top-level measure parameter takes precedence. For example, if we want to estimate CMIShannon(base = 2) through a decomposition  of entropies using the Kraskov(Shannon(base = ℯ)) Shannon entropy estimator, then base = 2 is used.\n\ninfo: Info\nNot all measures have the property that they can be decomposed into more fundamental information theoretic quantities. For example, MITsallisMartin can be  decomposed into a combination of marginal entropies, while MIRenyiSarbu cannot. An error will be thrown if decomposition is not possible.\n\nDiscrete entropy decomposition\n\nThe second signature is for discrete estimation using DiscreteInfoEstimators, for example PlugIn. The given discretization scheme (typically an  OutcomeSpace) controls how the joint/marginals are discretized, and the probabilities estimator pest controls how probabilities are estimated from counts.\n\nnote: Bias\nLike for DifferentialInfoEstimator, using a dedicated estimator  for the measure in question will be more reliable than using a decomposition estimate. Here's how different discretizations are applied:ValueBinning. Bin visitation frequencies are counted in the joint space   XY, then marginal visitations are obtained from the joint bin visits.   This behaviour is the same for both FixedRectangularBinning and   RectangularBinning (which adapts the grid to the data).   When using FixedRectangularBinning, the range along the first dimension   is used as a template for all other dimensions. This is a bit slower than naively    binning each marginal, but lessens bias.\nOrdinalPatterns. Each timeseries is separately codify-ed   according to its ordinal pattern (no bias correction).\nDispersion. Each timeseries is separately codify-ed according   to its dispersion pattern  (no bias correction).\n\nExamples\n\nExample 1:   MIShannon estimation using decomposition into discrete Shannon   entropy estimated using CodifyVariables with ValueBinning.\nExample 2:   MIShannon estimation using decomposition into discrete Shannon   entropy estimated using CodifyVariables with BubbleSortSwaps.\nExample 3:   MIShannon estimation using decomposition into differental Shannon   entropy estimated using the Kraskov estimator.\n\nSee also: MutualInformationEstimator, MultivariateInformationMeasure.\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.MIDecomposition","page":"Multivariate information API","title":"Associations.MIDecomposition","text":"MIDecomposition(definition::MultivariateInformationMeasure, \n    est::MutualInformationEstimator)\n\nEstimate the MultivariateInformationMeasure specified by definition by by decomposing, the measure, if possible, into a combination of mutual information terms. These terms are individually estimated using the given MutualInformationEstimator est, and finally combined to form the final  value of the measure. \n\nUsage\n\nUse with association to compute a MultivariateInformationMeasure   from input data: association(est::MIDecomposition, x...).\nUse with some IndependenceTest to test for independence between variables.\n\nExamples\n\nExample 1: Estimating CMIShannon   using a decomposition into MIShannon terms using    the KraskovStögbauerGrassberger1 mutual information estimator.\n\nSee also: MultivariateInformationMeasureEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.CMIDecomposition","page":"Multivariate information API","title":"Associations.CMIDecomposition","text":"CMIDecomposition(definition::MultivariateInformationMeasure, \n    est::ConditionalMutualInformationEstimator)\n\nEstimate some multivariate information measure specified by definition, by decomposing it into a combination of conditional mutual information terms. \n\nUsage\n\nUse with association to compute a MultivariateInformationMeasure   from input data: association(est::CMIDecomposition, x...).\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nEach of the conditional mutual information terms are estimated using est, which  can be any ConditionalMutualInformationEstimator. Finally, these estimates  are combined according to the relevant decomposition formula.\n\nThis estimator is similar to EntropyDecomposition, but definition is expressed as  conditional mutual information terms instead of entropy terms.\n\nExamples\n\nExample 1: Estimating TEShannon   by decomposing it into CMIShannon which is estimated using the   FPVP estimator.\n\nSee also: ConditionalMutualInformationEstimator,  MultivariateInformationMeasureEstimator, MultivariateInformationMeasure.\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Mutual-information-estimators","page":"Multivariate information API","title":"Mutual information estimators","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"MutualInformationEstimator\nKraskovStögbauerGrassberger1\nKraskovStögbauerGrassberger2\nGaoKannanOhViswanath\nGaoOhViswanath\nGaussianMI","category":"page"},{"location":"api/information_multivariate_api/#Associations.MutualInformationEstimator","page":"Multivariate information API","title":"Associations.MutualInformationEstimator","text":"MutualInformationEstimator\n\nThe supertype for dedicated MutualInformation estimators.\n\nConcrete implementations\n\nKraskovStögbauerGrassberger1\nKraskovStögbauerGrassberger2\nGaoOhViswanath\nGaoKannanOhViswanath\nGaussianMI\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger1","page":"Multivariate information API","title":"Associations.KraskovStögbauerGrassberger1","text":"KSG1 <: MutualInformationEstimator\nKraskovStögbauerGrassberger1 <: MutualInformationEstimator\nKraskovStögbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovStögbauerGrassberger1 mutual information estimator (you can use KSG1 for short) is the I^(1) k-th nearest neighbor estimator from Kraskov et al. (2004).\n\nCompatible definitions\n\nMIShannon\n\nUsage\n\nUse with association to compute Shannon mutual information from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nExample\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000); y = rand(rng, 10000)\nassociation(KSG1(; k = 10), x, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.KraskovStögbauerGrassberger2","page":"Multivariate information API","title":"Associations.KraskovStögbauerGrassberger2","text":"KSG2 <: MutualInformationEstimator\nKraskovStögbauerGrassberger2 <: MutualInformationEstimator\nKraskovStögbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovStögbauerGrassberger2 mutual information estimator (you can use KSG2 for short) is the I^(2) k-th nearest neighbor estimator from (Kraskov et al., 2004).\n\nCompatible definitions\n\nMIShannon\n\nUsage\n\nUse with association to compute Shannon mutual information from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nDescription\n\nLet the joint StateSpaceSet X = bfX_1 bfX_2 ldots bfX_m  be defined by the concatenation of the marginal StateSpaceSets  bfX_k _k=1^m, where each bfX_k is potentially multivariate. Let bfx_1 bfx_2 ldots bfx_N be the points in the joint space X.\n\nThe KraskovStögbauerGrassberger2 estimator first locates, for each bfx_i in X, the point bfn_i in X, the k-th nearest neighbor to bfx_i, according to the maximum norm (Chebyshev metric). Let epsilon_i be the distance d(bfx_i bfn_i).\n\nConsider x_i^m in bfX_m, the i-th point in the marginal space bfX_m. For each bfx_i^m, we determine theta_i^m := the number of points bfx_k^m in bfX_m that are a distance less than epsilon_i away from bfx_i^m. That is, we use the distance from a query point bfx_i in X (in the joint space) to count neighbors of x_i^m in bfX_m (in the marginal space).\n\nShannon mutual information between the variables bfX_1 bfX_2 ldots bfX_m is then estimated as\n\nhatI_KSG2(bfX) =\n    psi(k) -\n    dfracm - 1k +\n    (m - 1)psi(N) -\n    dfrac1N sum_i = 1^N sum_j = 1^m psi(theta_i^j + 1)\n\nExample\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000); y = rand(rng, 10000)\nassociation(KSG2(; k = 10), x, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.GaoKannanOhViswanath","page":"Multivariate information API","title":"Associations.GaoKannanOhViswanath","text":"GaoKannanOhViswanath <: MutualInformationEstimator\nGaoKannanOhViswanath(; k = 1, w = 0)\n\nThe GaoKannanOhViswanath (Shannon) estimator is designed for estimating Shannon mutual information between variables that may be either discrete, continuous or a mixture of both (Gao et al., 2017).\n\nCompatible definitions\n\nMIShannon\n\nUsage\n\nUse with association to compute Shannon mutual information from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nThe estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using k-nearest neighbor distances from empirical samples.\n\nThe estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator \"strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.\"\n\nwarn: Implementation note\nIn Gao et al. (2017), they claim (roughly speaking) that the estimator reduces to the KraskovStögbauerGrassberger1 estimator for continuous-valued data. However, KraskovStögbauerGrassberger1 uses the digamma function, while GaoKannanOhViswanath uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the KraskovStögbauerGrassberger1 approach. The KraskovStögbauerGrassberger1 estimator uses k-th neighbor distances in the joint space, while the GaoKannanOhViswanath algorithm selects the maximum k-th nearest distances among the two marginal spaces, which are in general not the same as the k-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in GaoKannanOhViswanath. We have modified it in a way that mimics KraskovStögbauerGrassberger1 for continous data. Note that because of using the log function instead of digamma, there will be slight differences between the methods. See the source code for more details.\n\nnote: Explicitly convert your discrete data to floats\nEven though the GaoKannanOhViswanath estimator is designed to handle discrete data, our implementation demands that all input data are StateSpaceSets whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to association.\n\nExamples\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000); y = rand(rng, 10000)\nassociation(GaoKannanOhViswanath(; k = 10), x, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.GaoOhViswanath","page":"Multivariate information API","title":"Associations.GaoOhViswanath","text":"GaoOhViswanath <: MutualInformationEstimator\n\nThe GaoOhViswanath is a mutual information estimator based on nearest neighbors, and is also called the bias-improved-KSG estimator, or BI-KSG, by (Gao et al., 2018).\n\nCompatible definitions\n\nMIShannon\n\nUsage\n\nUse with association to compute Shannon mutual information from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nThe estimator is given by\n\nbeginalign*\nhatH_GAO(X Y)\n= hatH_KSG(X) + hatH_KSG(Y) - hatH_KZL(X Y) \n= psi(k) +\n    log(N) +\n    log\n        left(\n            dfracc_d_x 2 c_d_y 2c_d_x + d_y 2\n        right)\n     - \n     dfrac1N sum_i=1^N left( log(n_x i 2) + log(n_y i 2) right)\nendalign*\n\nwhere c_d 2 = dfracpi^fracd2Gamma(dfracd2 + 1) is the volume of a d-dimensional unit mathcall_2-ball.\n\nExample\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000); y = rand(rng, 10000)\nassociation(GaoOhViswanath(; k = 10), x, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.GaussianMI","page":"Multivariate information API","title":"Associations.GaussianMI","text":"GaussianMI <: MutualInformationEstimator\nGaussianMI(; normalize::Bool = false)\n\nGaussianMI is a parametric estimator for Shannon mutual information.\n\nCompatible definitions\n\nMIShannon\n\nUsage\n\nUse with association to compute Shannon mutual information from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nGiven d_x-dimensional and d_y-dimensional input data X and Y, GaussianMI first constructs the d_x + d_y-dimensional joint StateSpaceSet XY. If normalize == true, then we follow the approach in Vejmelka & Palus (2008)(Vejmelka and Paluš, 2008) and transform each column in XY to have zero mean and unit standard deviation. If normalize == false, then the algorithm proceeds without normalization.\n\nNext, the C_{XY}, the correlation matrix for the (normalized) joint data XY is computed. The mutual information estimate GaussianMI assumes the input variables are distributed according to normal distributions with zero means and unit standard deviations. Therefore, given d_x-dimensional and d_y-dimensional input data X and Y, GaussianMI first constructs the joint StateSpaceSet XY, then transforms each column in XY to have zero mean and unit standard deviation, and finally computes the \\Sigma, the correlation matrix for XY.\n\nThe mutual information estimated (for normalize == false) is then estimated as\n\nhatI^S_Gaussian(X Y) = dfrac12\ndfrac det(Sigma_X) det(Sigma_Y)) det(Sigma))\n\nwhere we Sigma_X and Sigma_Y appear in Sigma as\n\nSigma = beginbmatrix\nSigma_X  Sigma^\nSigma^  Sigma_Y\nendbmatrix\n\nIf normalize == true, then the mutual information is estimated as\n\nhatI^S_Gaussian(X Y) = -dfrac12 sum_i = 1^d_x + d_y sigma_i\n\nwhere sigma_i are the eigenvalues for Sigma.\n\nExample\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000); y = rand(rng, 10000)\nassociation(GaussianMI(), x, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Conditional-mutual-information-estimators","page":"Multivariate information API","title":"Conditional mutual information estimators","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"ConditionalMutualInformationEstimator\nGaussianCMI\nFPVP\nMesnerShalizi\nRahimzamani\nPoczosSchneiderCMI","category":"page"},{"location":"api/information_multivariate_api/#Associations.ConditionalMutualInformationEstimator","page":"Multivariate information API","title":"Associations.ConditionalMutualInformationEstimator","text":"ConditionalMutualInformationEstimator\n\nThe supertype for dedicated ConditionalMutualInformation estimators.\n\nConcrete implementations\n\nFPVP\nGaussianCMI\nMesnerShalizi\nRahimzamani\nPoczosSchneiderCMI\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.GaussianCMI","page":"Multivariate information API","title":"Associations.GaussianCMI","text":"GaussianCMI <: MutualInformationEstimator\nGaussianCMI(definition = CMIShannon(); normalize::Bool = false)\n\nGaussianCMI is a parametric ConditionalMutualInformationEstimator  (Vejmelka and Paluš, 2008).\n\nCompatible definitions\n\nCMIShannon\n\nUsage\n\nUse with association to compute CMIShannon from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nGaussianCMI estimates Shannon CMI through a sum of two mutual information terms that each are estimated using GaussianMI (the normalize keyword is the same as for GaussianMI):\n\nhatI_Gaussian(X Y  Z) = hatI_Gaussian(X Y Z) - hatI_Gaussian(X Z)\n\nExamples\n\nExample 1. Estimating CMIShannon. \n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.FPVP","page":"Multivariate information API","title":"Associations.FPVP","text":"FPVP <: ConditionalMutualInformationEstimator\nFPVP(definition = CMIShannon(); k = 1, w = 0)\n\nThe Frenzel-Pompe-Vejmelka-Paluš (or FPVP for short) ConditionalMutualInformationEstimator is used to estimate the conditional mutual information using a k-th nearest neighbor approach that is analogous to that of the KraskovStögbauerGrassberger1 mutual information estimator from Frenzel and Pompe (2007) and Vejmelka and Paluš (2008).\n\nk is the number of nearest neighbors. w is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.\n\nCompatible definitions\n\nCMIShannon\n\nUsage\n\nUse with association to compute ConditionalMutualInformation measure   from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nExamples\n\nExample 1: Estimating CMIShannon\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.MesnerShalizi","page":"Multivariate information API","title":"Associations.MesnerShalizi","text":"MesnerShalizi <: ConditionalMutualInformationEstimator\nMesnerShalizi(definition = CMIShannon(); k = 1, w = 0)\n\nThe MesnerShalizi ConditionalMutualInformationEstimator is designed for data that can be mixtures of discrete and continuous data (Mesner and Shalizi, 2020).\n\nk is the number of nearest neighbors. w is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.\n\nCompatible definitions\n\nCMIShannon\n\nUsage\n\nUse with association to compute CMIShannon from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nExamples\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000)\ny = rand(rng, 10000) .+ x\nz = rand(rng, 10000) .+ y\nassociation(MesnerShalizi(; k = 10), x, z, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.Rahimzamani","page":"Multivariate information API","title":"Associations.Rahimzamani","text":"Rahimzamani <: ConditionalMutualInformationEstimator\nRahimzamani(k = 1, w = 0)\n\nThe Rahimzamani ConditionalMutualInformationEstimator is designed for data that can be mixtures of discrete and continuous data (Rahimzamani et al., 2018).\n\nCompatible definitions\n\nCMIShannon\n\nUsage\n\nUse with association to compute a CMIShannon from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nThis estimator is very similar to the GaoKannanOhViswanath mutual information estimator, but has been expanded to the conditional mutual information case.\n\nk is the number of nearest neighbors. w is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.\n\nExamples\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000)\ny = rand(rng, 10000) .+ x\nz = rand(rng, 10000) .+ y\nassociation(Rahimzamani(; k = 10), x, z, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.PoczosSchneiderCMI","page":"Multivariate information API","title":"Associations.PoczosSchneiderCMI","text":"PoczosSchneiderCMI <: ConditionalMutualInformationEstimator\nPoczosSchneiderCMI(definition = CMIRenyiPoczos(); k = 1, w = 0)\n\nThe PoczosSchneiderCMI ConditionalMutualInformationEstimator  computes conditional mutual informations using a k-th nearest neighbor approach (Póczos and Schneider, 2012).\n\nk is the number of nearest neighbors. w is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.\n\nCompatible definitions\n\nCMIRenyiPoczos\n\nUsage\n\nUse with association to compute CMIRenyiPoczos from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nExamples\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000)\ny = rand(rng, 10000) .+ x\nz = rand(rng, 10000) .+ y\nassociation(PoczosSchneiderCMI(CMIRenyiPoczos(), k = 10), x, z, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Transfer-entropy-estimators","page":"Multivariate information API","title":"Transfer entropy estimators","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"TransferEntropyEstimator\nZhu1\nLindner\nSymbolicTransferEntropy\nHilbert\nPhase\nAmplitude","category":"page"},{"location":"api/information_multivariate_api/#Associations.TransferEntropyEstimator","page":"Multivariate information API","title":"Associations.TransferEntropyEstimator","text":"The supertype of all dedicated transfer entropy estimators.\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.Zhu1","page":"Multivariate information API","title":"Associations.Zhu1","text":"Zhu1 <: TransferEntropyEstimator\nZhu1(k = 1, w = 0, base = MathConstants.e)\n\nThe Zhu1 transfer entropy estimator (Zhu et al., 2015) for normalized input data  (as described in Zhu et al. (2015)) for both for pairwise and conditional transfer entropy.\n\nCompatible definitions\n\nTEShannon\n\nUsage\n\nUse with association to compute TEShannon from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nDescription\n\nThis estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nFor a given points in the joint embedding space jᵢ, this estimator first computes the distance dᵢ from jᵢ to its k-th nearest neighbor. Then, for each point mₖ[i] in the k-th marginal space, it counts the number of points within radius dᵢ.\n\nThe Shannon transfer entropy is then computed as\n\nTE_S(X to Y) =\npsi(k) + dfrac1N sum_i^n\nleft\n    sum_k=1^3 left( psi(m_ki + 1) right)\nright\n\nwhere the index k references the three marginal subspaces T, TTf and ST for which neighbor searches are performed. Here this estimator has been modified to allow for  conditioning too (a simple modification to Lindner et al. (2011)'s equation 5 and 6). \n\nExample\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000)\ny = rand(rng, 10000) .+ x\nz = rand(rng, 10000) .+ y\nest = Zhu1(TEShannon(), k = 10)\nassociation(est, x, z, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.Lindner","page":"Multivariate information API","title":"Associations.Lindner","text":"Lindner <: TransferEntropyEstimator\nLindner(definition = Shannon(); k = 1, w = 0, base = 2)\n\nThe Lindner transfer entropy estimator (Lindner et al., 2011), which is also used in the Trentool MATLAB toolbox, and is based on nearest neighbor searches.\n\nCompatible definitions\n\nTEShannon\n\nUsage\n\nUse with association to compute TEShannon from input data.\nUse with some IndependenceTest to test for independence between variables.\n\nKeyword parameters\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nThe estimator can be used both for pairwise and conditional transfer entropy estimation.\n\nDescription\n\nFor a given points in the joint embedding space jᵢ, this estimator first computes the distance dᵢ from jᵢ to its k-th nearest neighbor. Then, for each point mₖ[i] in the k-th marginal space, it counts the number of points within radius dᵢ.\n\nThe Shannon transfer entropy is then computed as\n\nTE_S(X to Y) =\npsi(k) + dfrac1N sum_i^n\nleft\n    sum_k=1^3 left( psi(m_ki + 1) right)\nright\n\nwhere the index k references the three marginal subspaces T, TTf and ST for which neighbor searches are performed. Here this estimator has been modified to allow for  conditioning too (a simple modification to Lindner et al. (2011)'s equation 5 and 6). \n\nExample\n\nusing Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 10000)\ny = rand(rng, 10000) .+ x\nz = rand(rng, 10000) .+ y\nest = Lindner(TEShannon(), k = 10)\nassociation(est, x, z, y) # should be near 0 (and can be negative)\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.SymbolicTransferEntropy","page":"Multivariate information API","title":"Associations.SymbolicTransferEntropy","text":"SymbolicTransferEntropy <: TransferEntropyEstimator\nSymbolicTransferEntropy(definition = TEShannon(); m = 3, τ = 1, \n    lt = ComplexityMeasures.isless_rand\n\nA convenience estimator for symbolic transfer entropy (Staniek and Lehnertz, 2008).\n\nCompatible measures\n\nTEShannon\n\nDescription\n\nSymbolic transfer entropy consists of two simple steps. First, the input time series are encoded using codify with the CodifyVariables discretization and the OrdinalPatterns outcome space. This  transforms the input time series into integer time series. Transfer entropy entropy is then  estimated from the encoded time series by applying  \n\nTransfer entropy is then estimated as usual on the encoded timeseries with the embedding dictated by definition and the JointProbabilities estimator.\n\nExamples\n\nExample 1\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.Hilbert","page":"Multivariate information API","title":"Associations.Hilbert","text":"Hilbert(est;\n    source::InstantaneousSignalProperty = Phase(),\n    target::InstantaneousSignalProperty = Phase(),\n    cond::InstantaneousSignalProperty = Phase())\n) <: TransferDifferentialEntropyEstimator\n\nCompute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are obtained by first applying the Hilbert transform to each signal, then extracting the phases/amplitudes of the resulting complex numbers (Paluš, 2014). Original time series are thus transformed to instantaneous phase/amplitude time series. Transfer entropy is then estimated using the provided est on those phases/amplitudes (use e.g. ValueBinning, or OrdinalPatterns).\n\ninfo: Info\nDetails on estimation of the transfer entropy (conditional mutual information) following the phase/amplitude extraction step is not given in Palus (2014). Here, after instantaneous phases/amplitudes have been obtained, these are treated as regular time series, from which transfer entropy is then computed as usual.\n\nSee also: Phase, Amplitude.\n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.Phase","page":"Multivariate information API","title":"Associations.Phase","text":"Phase <: InstantaneousSignalProperty\n\nIndicates that the instantaneous phases of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#Associations.Amplitude","page":"Multivariate information API","title":"Associations.Amplitude","text":"Amplitude <: InstantaneousSignalProperty\n\nIndicates that the instantaneous amplitudes of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"api/information_multivariate_api/#tutorial_infomeasures","page":"Multivariate information API","title":"A small tutorial","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"Associations.jl extends the single-variate information API in ComplexityMeasures.jl to information measures of multiple variables. ","category":"page"},{"location":"api/information_multivariate_api/#Definitions","page":"Multivariate information API","title":"Definitions","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"We define \"information measure\" as some functional of probability  mass functions or probability densities. This definition may or may not agree with literature usage, depending on the context. We made this choice pragmatically based on user-friendlyness and coding-friendlyness, but still trying to maintain some level of meaningful terminology.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"note: A note on naming: the same name for different things?\nUpon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use the same name for different concepts. For novices, and experienced practitioners too, this can be confusing. Our API clearly distinguishes between methods that are conceptually the same but named differently in the literature due to differing estimation strategies, from methods that actually have different definitions.Multiple, equivalent definitions occur for example for the Shannon mutual   information (MI; MIShannon), which has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral   over a joint probability mass function (pmf), as a sum of three entropy terms, and as   a Kullback-Leibler divergence between the joint pmf and the product of the marginal   distributions. Since these definitions are all equivalent, we only need once type   (MIShannon) to represent them.\nBut Shannon MI is not the  only type of mutual information! For example, \"Tsallis mutual information\"   has been proposed in different variants by various authors. Despite sharing the   same name, these are actually nonequivalent definitions. We've thus assigned   them entirely different measure names (e.g. MITsallisFuruichi and   MITsallisMartin), with the author name at the end.","category":"page"},{"location":"api/information_multivariate_api/#Basic-estimation-strategy","page":"Multivariate information API","title":"Basic estimation strategy","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"To estimate a multivariate information measure in practice, you must first specify the definition of the measure, which is then used as input to an  estimator of that measure. This estimator is then given to association. Every information measure has at least one estimator: JointProbabilities. Many measures have several additional estimators, and an overview can be found in the docstring for MultivariateInformationMeasureEstimator.","category":"page"},{"location":"api/information_multivariate_api/#Distances/divergences","page":"Multivariate information API","title":"Distances/divergences","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"There are many information measures in the literature that aim to quantify the  distance/divergence between two probability mass functions (pmf) or densities. You can  find those that we implement here.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"As an example, let's quantify the KLDivergence between two probability  mass functions estimated by symbolizing two input vectors x and y using  OrdinalPatterns. Since the discrete KLDivergence can be  expressed as a function of a joint pmf, we can use the JointProbabilities estimator.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx, y = rand(rng, 1000), rand(rng, 1000)\n\n# Specify a discretization. We discretize per column.\ndisc = CodifyVariables(OrdinalPatterns(m=2))\n\ndef = KLDivergence()\nest = JointProbabilities(def, disc)\nassociation(est, x, y) # should be close to 0","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"Divergences are examples of asymmetric information measures, which we can see by  flipping the order of the input data.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"association(est, y, x)","category":"page"},{"location":"api/information_multivariate_api/#Conditional-entropies","page":"Multivariate information API","title":"Conditional entropies","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"Conditional entropies are another example of asymmetric information measures. They all have in common that  they are functions of a joint pmf, and can therefore also be estimated using the JointProbabilities estimator. This time, we'll use a rectangular binning with 3 bins along each dimension to discretize the data.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n\nx, y = randn(rng, 1000), randn(rng, 1000)\ndisc = CodifyVariables(ValueBinning(3))\ndef = ConditionalEntropyShannon(base = 2)\nest = JointProbabilities(def, disc)\nassociation(est, x, y)","category":"page"},{"location":"api/information_multivariate_api/#Joint-entropies","page":"Multivariate information API","title":"Joint entropies","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"Joint entropies, on the other hand, are symmetric. Joint entropies are functionals of a joint pmf, so we can still use the JointProbabilities estimator. This time, we use a Dispersion based discretization.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n\nx, y = randn(rng, 1000), randn(rng, 1000)\ndisc = CodifyVariables(Dispersion())\nest = JointProbabilities(JointEntropyShannon(base = 2), disc)\nassociation(est, x, y) ≈ association(est, y, x) # should be true","category":"page"},{"location":"api/information_multivariate_api/#Mutual-informations","page":"Multivariate information API","title":"Mutual informations","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"Mutual informations, in particular MIShannon is an often-used symmetric  measure for quantifing the (possibly nonlinear) association between variables. It appears in both  discrete and differential form, and can be estimated in a multitude of ways. For  example, one can use dedicated MutualInformationEstimators such as  KraskovStögbauerGrassberger2 or GaussianMI:","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"using Associations\nusing StateSpaceSets\n# We'll construct two state space sets, to illustrate how we can discretize \n# multidimensional inputs using `CodifyPoints`.\nx, y = StateSpaceSet(rand(rng, 1000, 2)), StateSpaceSet(rand(rng, 1000, 2))\nest = KSG1(MIShannon(base = 2), k = 10)\nassociation(est, x, y)","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"The result should be symmetric:","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"association(est, x, y) ≈ association(est, y, x) # should be true","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"One can also estimate mutual information using the EntropyDecomposition  estimator, or (like above) using the JointProbabilities estimator. Let's construct a differential entropy based estimator based on the Kraskov estimator.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"est_diff = EntropyDecomposition(MIShannon(base = 2), Kraskov(Shannon(), k=10))","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"association(est_diff, x, y)","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"We can also construct a discrete entropy based estimator based on e.g. PlugIn estimator of Shannon entropy.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"# We know that `x` and `y` were generated from a uniform distribution above,\n# so we set the minimum and maximum values of the encoding to 0 and 1,\n# respectively.\nencoding = RelativeMeanEncoding(0.0, 1.0; n = 4)\ndisc = CodifyPoints(encoding)\nest_disc = JointProbabilities(MIShannon(base = 2), disc)\nassociation(est_disc, x, y)","category":"page"},{"location":"api/information_multivariate_api/#JointProbabilities:-fine-grained-discretization-control","page":"Multivariate information API","title":"JointProbabilities: fine grained discretization control","text":"","category":"section"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"For numerical data, we can estimate both counts and probabilities using CodifyVariables with any count-based OutcomeSpace. Here, we'll estimate MIShannon using  one type of encoding for the first variable, and another type of encoding for the second variable.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y = rand(rng, 100), rand(rng, 100)\n\n# We must use outcome spaces with the same number of total outcomes.\n# This works becuase these outcome spaces construct embedding points\n# (\"windows\") in the same way/order; be careful if that isn't the case!\nox = CosineSimilarityBinning(nbins = factorial(3))\noy = OrdinalPatterns(m = 3)\n\n# Now estimate mutual information\ndiscretization = CodifyVariables((ox, oy))\nest = JointProbabilities(MIShannon(), discretization)\nassociation(est, x, y)","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"For more fine-grained control than CodifyVariables can offer, we can use CodifyPoints with one or several Encodings. Here's how we can estimate MIShannon one multivariate input  data by discretizing each input variable in arbitrary ways.","category":"page"},{"location":"api/information_multivariate_api/","page":"Multivariate information API","title":"Multivariate information API","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y = StateSpaceSet(rand(rng, 1000, 2)), StateSpaceSet(rand(rng, 1000, 3))\n\n # min/max of the `rand` call is 0 and 1\nprecise = true # precise bin edges\nr = range(0, 1; length = 3)\nbinning = FixedRectangularBinning(r, dimension(x), precise)\nencoding_x = RectangularBinEncoding(binning, x)\nencoding_y = CombinationEncoding(RelativeMeanEncoding(0.0, 1, n = 2), OrdinalPatternEncoding(3))\ndiscretization = CodifyPoints(encoding_x, encoding_y)\n\n# Now estimate mutual information\nest = JointProbabilities(MIShannon(), discretization)\nassociation(est, x, y)","category":"page"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"CollapsedDocStrings = true","category":"page"},{"location":"associations/#association_measures","page":"Association measures","title":"Associations","text":"","category":"section"},{"location":"associations/#Association-API","page":"Association measures","title":"Association API","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"The most basic components of Associations.jl are a collection of statistics that in some manner quantify the \"association\" between input datasets. Precisely what is meant by \"association\" depends on the measure, and precisely what is meant by \"quantify\" depends on the estimator of that measure. We formalize this notion below with the association function, which dispatches on AssociationMeasureEstimator and AssociationMeasure.","category":"page"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"association\nAssociationMeasure\nAssociationMeasureEstimator","category":"page"},{"location":"associations/#Associations.association","page":"Association measures","title":"Associations.association","text":"association(estimator::AssociationMeasureEstimator, x, y, [z, ...]) → r\nassociation(definition::AssociationMeasure, x, y, [z, ...]) → r\n\nEstimate the (conditional) association between input variables x, y, z, … using  the given estimator (an AssociationMeasureEstimator) or definition (an AssociationMeasure).  \n\ninfo: Info\nThe type of the return value r depends on the measure/estimator. The interpretation of the returned value also depends on the specific measure and estimator used.\n\nExamples\n\nThe examples section of the online documentation has numerous  using association.\n\n\n\n\n\n","category":"function"},{"location":"associations/#Associations.AssociationMeasure","page":"Association measures","title":"Associations.AssociationMeasure","text":"AssociationMeasure\n\nThe supertype of all association measures. \n\nAbstract implementations\n\nCurrently, the association measures are classified by abstract classes listed below. These abstract classes offer common functionality among association measures that are  conceptually similar. This makes maintenance and framework extension easier than  if each measure was implemented \"in isolation\".\n\nMultivariateInformationMeasure\nCrossmapMeasure\nClosenessMeasure\nCorrelationMeasure\n\nConcrete implementations\n\nConcrete subtypes are given as input to association. Many of these types require an AssociationMeasureEstimator to compute.\n\nType AssociationMeasure Pairwise Conditional\nCorrelation PearsonCorrelation ✓ ✖\nCorrelation DistanceCorrelation ✓ ✓\nCloseness SMeasure ✓ ✖\nCloseness HMeasure ✓ ✖\nCloseness MMeasure ✓ ✖\nCloseness (ranks) LMeasure ✓ ✖\nCloseness JointDistanceDistribution ✓ ✖\nCross-mapping PairwiseAsymmetricInference ✓ ✖\nCross-mapping ConvergentCrossMapping ✓ ✖\nConditional recurrence MCR ✓ ✖\nConditional recurrence RMCD ✓ ✓\nShared information MIShannon ✓ ✖\nShared information MIRenyiJizba ✓ ✖\nShared information MIRenyiSarbu ✓ ✖\nShared information MITsallisFuruichi ✓ ✖\nShared information PartialCorrelation ✖ ✓\nShared information CMIShannon ✖ ✓\nShared information CMIRenyiSarbu ✖ ✓\nShared information CMIRenyiJizba ✖ ✓\nShared information CMIRenyiPoczos ✖ ✓\nShared information CMITsallisPapapetrou ✖ ✓\nInformation transfer TEShannon ✓ ✓\nInformation transfer TERenyiJizba ✓ ✓\nPartial mutual information PartialMutualInformation ✖ ✓\nInformation measure JointEntropyShannon ✓ ✖\nInformation measure JointEntropyRenyi ✓ ✖\nInformation measure JointEntropyTsallis ✓ ✖\nInformation measure ConditionalEntropyShannon ✓ ✖\nInformation measure ConditionalEntropyTsallisAbe ✓ ✖\nInformation measure ConditionalEntropyTsallisFuruichi ✓ ✖\nDivergence HellingerDistance ✓ ✖\nDivergence KLDivergence ✓ ✖\nDivergence RenyiDivergence ✓ ✖\nDivergence VariationDistance ✓ ✖\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.AssociationMeasureEstimator","page":"Association measures","title":"Associations.AssociationMeasureEstimator","text":"AssociationMeasureEstimator\n\nThe supertype of all association measure estimators.\n\nConcrete subtypes are given as input to association.\n\nAbstract subtypes\n\nMultivariateInformationMeasureEstimator\nCrossmapEstimator\n\nConcrete implementations\n\nAssociationMeasure Estimators\nPearsonCorrelation Not required\nDistanceCorrelation Not required\nPartialCorrelation Not required\nSMeasure Not required\nHMeasure Not required\nMMeasure Not required\nLMeasure Not required\nJointDistanceDistribution Not required\nPairwiseAsymmetricInference RandomVectors, RandomSegment\nConvergentCrossMapping RandomVectors, RandomSegment\nMCR Not required\nRMCD Not required\nMIShannon JointProbabilities, EntropyDecomposition, KraskovStögbauerGrassberger1, KraskovStögbauerGrassberger2, GaoOhViswanath, GaoKannanOhViswanath, GaussianMI\nMIRenyiJizba JointProbabilities, EntropyDecomposition\nMIRenyiSarbu JointProbabilities\nMITsallisFuruichi JointProbabilities, EntropyDecomposition\nMITsallisMartin JointProbabilities, EntropyDecomposition\nCMIShannon JointProbabilities, EntropyDecomposition, MIDecomposition, GaussianCMI, FPVP, MesnerShalizi, Rahimzamani\nCMIRenyiSarbu JointProbabilities\nCMIRenyiJizba JointProbabilities, EntropyDecomposition\nCMIRenyiPoczos PoczosSchneiderCMI\nCMITsallisPapapetrou JointProbabilities\nTEShannon JointProbabilities, EntropyDecomposition, Zhu1, Lindner\nTERenyiJizba JointProbabilities\nPartialMutualInformation JointProbabilities\nJointEntropyShannon JointProbabilities\nJointEntropyRenyi JointProbabilities\nJointEntropyTsallis JointProbabilities\nConditionalEntropyShannon JointProbabilities\nConditionalEntropyTsallisAbe JointProbabilities\nConditionalEntropyTsallisFuruichi JointProbabilities\nHellingerDistance JointProbabilities\nKLDivergence JointProbabilities\nRenyiDivergence JointProbabilities\nVariationDistance JointProbabilities\n\n\n\n\n\n","category":"type"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"Here are some examples of how to use association.","category":"page"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"using Associations\nx, y, z = rand(1000), rand(1000), rand(1000);\nassociation(LMeasure(), x, y)\nassociation(DistanceCorrelation(), x, y)\nassociation(JointProbabilities(JointEntropyShannon(), CodifyVariables(Dispersion(c = 3, m = 2))), x, y)\nassociation(EntropyDecomposition(MIShannon(), PlugIn(Shannon()), CodifyVariables(OrdinalPatterns(m=3))), x, y)\nassociation(KSG2(MIShannon(base = 2)), x, y)\nassociation(JointProbabilities(PartialMutualInformation(), CodifyVariables(OrdinalPatterns(m=3))), x, y, z)\nassociation(FPVP(CMIShannon(base = 2)), x, y, z)","category":"page"},{"location":"associations/#information_api","page":"Association measures","title":"Information measures","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"MultivariateInformationMeasure","category":"page"},{"location":"associations/#Associations.MultivariateInformationMeasure","page":"Association measures","title":"Associations.MultivariateInformationMeasure","text":"MultivariateInformationMeasure <: AssociationMeasure\n\nThe supertype for all multivariate information-based measure definitions.\n\nDefinition\n\nFollowing Datseris and Haaga (2024), we define a multivariate information measure as any functional  of a multidimensional probability mass functions (PMFs) or multidimensional probability density.\n\nImplementations\n\nJointEntropy definitions:\n\nJointEntropyShannon\nJointEntropyRenyi\nJointEntropyTsallis\n\nConditionalEntropy definitions:\n\nConditionalEntropyShannon\nConditionalEntropyTsallisAbe\nConditionalEntropyTsallisFuruichi\n\nDivergenceOrDistance definitions:\n\nHellingerDistance\nKLDivergence\nRenyiDivergence\nVariationDistance\n\nMutualInformation definitions:\n\nMIShannon\nMIRenyiJizba\nMIRenyiSarbu\nMITsallisMartin\nMITsallisFuruichi\n\nConditionalMutualInformation definitions:\n\nCMIShannon\nCMITsallisPapapetrou\nCMIRenyiJizba\nCMIRenyiPoczos\nCMIRenyiSarbu\n\nTransferEntropy definitions:\n\nTEShannon\nTERenyiJizba\n\nOther definitions:\n\nPartialMutualInformation\n\n\n\n\n\n","category":"type"},{"location":"associations/#conditional_entropies","page":"Association measures","title":"Conditional entropies","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"ConditionalEntropy\nConditionalEntropyShannon\nConditionalEntropyTsallisFuruichi\nConditionalEntropyTsallisAbe","category":"page"},{"location":"associations/#Associations.ConditionalEntropy","page":"Association measures","title":"Associations.ConditionalEntropy","text":"ConditionalEntropy <: MultivariateInformationMeasure\n\nThe supertype for all conditional entropy measures.\n\nConcrete subtypes\n\nConditionalEntropyShannon\nConditionalEntropyTsallisAbe\nConditionalEntropyTsallisFuruichi\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.ConditionalEntropyShannon","page":"Association measures","title":"Associations.ConditionalEntropyShannon","text":"ConditionalEntropyShannon <: ConditionalEntropy\nConditionalEntropyShannon(; base = 2)\n\nThe Shannon conditional entropy measure.\n\nUsage\n\nUse with association to compute the Shannon conditional entropy between    two variables.\n\nCompatible estimators\n\nJointProbabilities\n\nDiscrete definition\n\nSum formulation\n\nThe conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH^S(X  Y) = -sum_x in mathcalX y in mathcalY p(x y) log(p(x  y))\n\nThis is the definition used when calling association with a JointProbabilities estimator.\n\nTwo-entropies formulation\n\nEquivalently, the following differenConditionalEntropy of entropies hold\n\nH^S(X  Y) = H^S(X Y) - H^S(Y)\n\nwhere H^S(cdot) and H^S(cdot  cdot) are the Shannon entropy and Shannon joint entropy, respectively. This is the definition used when calling association with a ProbabilitiesEstimator.\n\nDifferential definition\n\nThe differential conditional Shannon entropy is analogously defined as\n\nH^S(X  Y) = h^S(X Y) - h^S(Y)\n\nwhere h^S(cdot) and h^S(cdot  cdot) are the Shannon differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling association with a DifferentialInfoEstimator.\n\nEstimation\n\nExample 1: Analytical example from Cover & Thomas's book.\nExample 2:    JointProbabilities estimator withCodifyVariables discretization and    UniqueElements outcome space on categorical data.\nExample 3:    JointProbabilities estimator with CodifyPoints discretization and UniqueElementsEncoding   encoding of points on numerical data.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.ConditionalEntropyTsallisFuruichi","page":"Association measures","title":"Associations.ConditionalEntropyTsallisFuruichi","text":"ConditionalEntropyTsallisFuruichi <: ConditionalEntropy\nConditionalEntropyTsallisFuruichi(; base = 2, q = 1.5)\n\nFuruichi (2006)'s discrete Tsallis conditional entropy definition.\n\nUsage\n\nUse with association to compute the Tsallis-Furuichi conditional entropy between    two variables.\n\nCompatible estimators\n\nJointProbabilities\n\nDefinition\n\nFuruichi's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T(X  Y) = -sum_x in mathcalX y in mathcalY\np(x y)^q log_q(p(x  y))\n\nln_q(x) = fracx^1-q - 11 - q and q neq 1. For q = 1, H_q^T(X  Y) reduces to the Shannon conditional entropy:\n\nH_q=1^T(X  Y) = -sum_x in mathcalX y in mathcalY =\np(x y) log(p(x  y))\n\nIf any of the entries of the marginal distribution for Y are zero, or the q-logarithm  is undefined for a particular value, then the measure is undefined and NaN is returned.\n\nEstimation\n\nExample 1:    JointProbabilities estimator withCodifyVariables discretization and    UniqueElements outcome space on categorical data.\nExample 2:    JointProbabilities estimator with CodifyPoints discretization and UniqueElementsEncoding   encoding of points on numerical data.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.ConditionalEntropyTsallisAbe","page":"Association measures","title":"Associations.ConditionalEntropyTsallisAbe","text":"ConditionalEntropyTsallisAbe <: ConditionalEntropy\nConditionalEntropyTsallisAbe(; base = 2, q = 1.5)\n\nAbe and Rajagopal (2001)'s discrete Tsallis conditional entropy measure.\n\nUsage\n\nUse with association to compute the Tsallis-Abe conditional entropy between    two variables.\n\nCompatible estimators\n\nJointProbabilities\n\nDefinition\n\nAbe & Rajagopal's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T_A(X  Y) = dfracH_q^T(X Y) - H_q^T(Y)1 + (1-q)H_q^T(Y)\n\nwhere H_q^T(cdot) and H_q^T(cdot cdot) is the Tsallis entropy and the joint Tsallis entropy.\n\nEstimation\n\nExample 1:    JointProbabilities estimator withCodifyVariables discretization and    UniqueElements outcome space on categorical data.\nExample 2:    JointProbabilities estimator with CodifyPoints discretization and UniqueElementsEncoding   encoding of points on numerical data.\n\n\n\n\n\n","category":"type"},{"location":"associations/#divergences_and_distances","page":"Association measures","title":"Divergences and distances","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"DivergenceOrDistance\nHellingerDistance\nKLDivergence\nRenyiDivergence\nVariationDistance","category":"page"},{"location":"associations/#Associations.DivergenceOrDistance","page":"Association measures","title":"Associations.DivergenceOrDistance","text":"DivergenceOrDistance <: BivariateInformationMeasure\n\nThe supertype for bivariate information measures aiming to quantify some sort of divergence, distance or closeness between two probability distributions.\n\nSome of these measures are proper metrics, while others are not, but they have in common that they aim to quantify how \"far from each other\" two probabilities distributions are.\n\nConcrete implementations\n\nHellingerDistance\nKLDivergence\nRenyiDivergence\nVariationDistance\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.HellingerDistance","page":"Association measures","title":"Associations.HellingerDistance","text":"HellingerDistance <: DivergenceOrDistance\n\nThe Hellinger distance.\n\nUsage\n\nUse with association to compute the compute the Hellinger distance between two pre-computed   probability distributions, or from raw data using one of the estimators listed below.\n\nCompatible estimators\n\nJointProbabilities\n\nDescription\n\nThe Hellinger distance between two probability distributions P_X = (p_x(omega_1) ldots p_x(omega_n)) and P_Y = (p_y(omega_1) ldots p_y(omega_m)), both defined over the same OutcomeSpace Omega = omega_1 ldots omega_n , is defined as\n\nD_H(P_Y(Omega)  P_Y(Omega)) =\ndfrac1sqrt2 sum_omega in Omega (sqrtp_x(omega) - sqrtp_y(omega))^2\n\nEstimation\n\nExample 1: From precomputed probabilities\nExample 2:    JointProbabilities with OrdinalPatterns outcome space\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.KLDivergence","page":"Association measures","title":"Associations.KLDivergence","text":"KLDivergence <: DivergenceOrDistance\n\nThe Kullback-Leibler (KL) divergence.\n\nUsage\n\nUse with association to compute the compute the KL-divergence between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.\n\nCompatible estimators\n\nJointDistanceDistribution\n\nEstimators\n\nJointProbabilities.\n\nDescription\n\nThe KL-divergence between two probability distributions P_X = (p_x(omega_1) ldots p_x(omega_n)) and P_Y = (p_y(omega_1) ldots p_y(omega_m)), both defined over the same OutcomeSpace Omega = omega_1 ldots omega_n , is defined as\n\nD_KL(P_Y(Omega)  P_Y(Omega)) =\nsum_omega in Omega p_x(omega) logdfracp_x(omega)p_y(omega)\n\nImplements\n\nassociation. Used to compute the KL-divergence between two pre-computed   probability distributions. If used with RelativeAmount, the KL divergence may   be undefined to due some outcomes having zero counts. Use some other   ProbabilitiesEstimator like BayesianRegularization to ensure   all estimated probabilities are nonzero.\n\nnote: Note\nDistances.jl also defines KLDivergence. Quality it if you're loading both  packages, i.e. do association(Associations.KLDivergence(), x, y).\n\nEstimation\n\nExample 1: From precomputed probabilities\nExample 2:    JointProbabilities with OrdinalPatterns outcome space\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.RenyiDivergence","page":"Association measures","title":"Associations.RenyiDivergence","text":"RenyiDivergence <: DivergenceOrDistance\nRenyiDivergence(q; base = 2)\n\nThe Rényi divergence of positive order q.\n\nUsage\n\nUse with association to compute the compute the Rényi divergence between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.\n\nCompatible estimators\n\nJointDistanceDistribution\n\nDescription\n\nThe Rényi divergence between two probability distributions P_X = (p_x(omega_1) ldots p_x(omega_n)) and P_Y = (p_y(omega_1) ldots p_y(omega_m)), both defined over the same OutcomeSpace Omega = omega_1 ldots omega_n , is defined as van Erven and Harremos (2014).\n\nD_q(P_Y(Omega)  P_Y(Omega)) =\ndfrac1q - 1 log sum_omega in Omegap_x(omega)^qp_y(omega)^1-alpha\n\nImplements\n\ninformation. Used to compute the Rényi divergence between two pre-computed   probability distributions. If used with RelativeAmount, the KL divergence may   be undefined to due some outcomes having zero counts. Use some other   ProbabilitiesEstimator like BayesianRegularization to ensure   all estimated probabilities are nonzero.\n\nnote: Note\nDistances.jl also defines RenyiDivergence. Quality it if you're loading both  packages, i.e. do association(Associations.RenyiDivergence(), x, y).\n\nEstimation\n\nExample 1: From precomputed probabilities\nExample 2:    JointProbabilities with OrdinalPatterns outcome space\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.VariationDistance","page":"Association measures","title":"Associations.VariationDistance","text":"VariationDistance <: DivergenceOrDistance\n\nThe variation distance.\n\nUsage\n\nUse with association to compute the compute the variation distance between two    pre-computed probability distributions, or from raw data using one of the estimators   listed below.\n\nCompatible estimators\n\nJointDistanceDistribution\n\nDescription\n\nThe variation distance between two probability distributions P_X = (p_x(omega_1) ldots p_x(omega_n)) and P_Y = (p_y(omega_1) ldots p_y(omega_m)), both defined over the same OutcomeSpace Omega = omega_1 ldots omega_n , is defined as\n\nD_V(P_Y(Omega)  P_Y(Omega)) =\ndfrac12 sum_omega in Omega  p_x(omega) - p_y(omega) \n\nExamples\n\nExample 1: From precomputed probabilities\nExample 2:    JointProbabilities with OrdinalPatterns outcome space\n\n\n\n\n\n","category":"type"},{"location":"associations/#joint_entropies","page":"Association measures","title":"Joint entropies","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"JointEntropy\nJointEntropyShannon\nJointEntropyTsallis\nJointEntropyRenyi","category":"page"},{"location":"associations/#Associations.JointEntropy","page":"Association measures","title":"Associations.JointEntropy","text":"JointEntropy <: BivariateInformationMeasure\n\nThe supertype for all joint entropy measures.\n\nConcrete implementations\n\nJointEntropyShannon\nJointEntropyRenyi\nJointEntropyTsallis\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.JointEntropyShannon","page":"Association measures","title":"Associations.JointEntropyShannon","text":"JointEntropyShannon <: JointEntropy\nJointEntropyShannon(; base = 2)\n\nThe Shannon joint entropy measure (Cover, 1999).\n\nUsage\n\nUse with association to compute the Shannon joint entropy between    two variables.\n\nCompatible estimators\n\nJointProbabilities\n\nDefinition\n\nGiven two two discrete random variables X and Y with ranges mathcalX and mathcalX, Cover (1999) defines the Shannon joint entropy as\n\nH^S(X Y) = -sum_xin mathcalX y in mathcalY p(x y) log p(x y)\n\nwhere we define log(p(x y)) = 0 if p(x y) = 0.\n\nEstimation\n\nExample 1:    JointProbabilities with Dispersion outcome space\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.JointEntropyTsallis","page":"Association measures","title":"Associations.JointEntropyTsallis","text":"JointEntropyTsallis <: JointEntropy\nJointEntropyTsallis(; base = 2, q = 1.5)\n\nThe Tsallis joint entropy definition from Furuichi (2006). \n\nUsage\n\nUse with association to compute the Furuichi-Tsallis joint entropy between    two variables.\n\nCompatible estimators\n\nJointProbabilities\n\nDefinition\n\nGiven two two discrete random variables X and Y with ranges mathcalX and mathcalX, Furuichi (2006) defines the Tsallis joint entropy as\n\nH_q^T(X Y) = -sum_xin mathcalX y in mathcalY p(x y)^q log_q p(x y)\n\nwhere log_q(x q) = dfracx^1-q - 11-q is the q-logarithm, and  we define log_q(x q) = 0 if q = 0.\n\nEstimation\n\nExample 1:    JointProbabilities with OrdinalPatterns outcome space\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.JointEntropyRenyi","page":"Association measures","title":"Associations.JointEntropyRenyi","text":"JointEntropyRenyi <: JointEntropy\nJointEntropyRenyi(; base = 2, q = 1.5)\n\nThe Rényi joint entropy measure (Golshani et al., 2009).\n\nUsage\n\nUse with association to compute the Golshani-Rényi joint entropy between    two variables.\n\nCompatible estimators\n\nJointProbabilities\n\nDefinition\n\nGiven two two discrete random variables X and Y with ranges mathcalX and mathcalX, Golshani et al. (2009) defines the Rényi joint entropy as\n\nH_q^R(X Y) = dfrac11-alpha log sum_i = 1^N p_i^q\n\nwhere q  0 and q = 1.\n\nEstimation\n\nExample 1:    JointProbabilities with ValueBinning outcome space\n\n\n\n\n\n","category":"type"},{"location":"associations/#Mutual-informations","page":"Association measures","title":"Mutual informations","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"MutualInformation\nMIShannon\nMITsallisFuruichi\nMITsallisMartin\nMIRenyiJizba\nMIRenyiSarbu","category":"page"},{"location":"associations/#Associations.MutualInformation","page":"Association measures","title":"Associations.MutualInformation","text":"MutualInformation\n\nAbstract type for all mutual information measures.\n\nConcrete implementations\n\nMIShannon\nMITsallisMartin\nMITsallisFuruichi\nMIRenyiJizba\nMIRenyiSarbu\n\nSee also: MutualInformationEstimator\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.MIShannon","page":"Association measures","title":"Associations.MIShannon","text":"MIShannon <: BivariateInformationMeasure\nMIShannon(; base = 2)\n\nThe Shannon mutual information I_S(X Y).\n\nUsage\n\nUse with association to compute the raw Shannon mutual information from input data   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise dependence using   the Shannon mutual information.\n\nCompatible estimators\n\nJointProbabilities (generic)\nEntropyDecomposition (generic)\nKraskovStögbauerGrassberger1\nKraskovStögbauerGrassberger2\nGaoOhViswanath\nGaoKannanOhViswanath\nGaussianMI\n\nDiscrete definition\n\nThere are many equivalent formulations of discrete Shannon mutual information, meaning that  it can be estimated in several ways, either using JointProbabilities  (double-sum formulation), EntropyDecomposition (three-entropies decomposition), or some dedicated estimator.\n\nDouble sum formulation\n\nAssume we observe samples barbfX_1N_y = barbfX_1 ldots barbfX_n  and barbfY_1N_x = barbfY_1 ldots barbfY_n  from two discrete random variables X and Y with finite supports mathcalX =  x_1 x_2 ldots x_M_x  and mathcalY = y_1 y_2 ldots x_M_y. The double-sum estimate is obtained by replacing the double sum\n\nhatI_DS(X Y) =\n sum_x_i in mathcalX y_i in mathcalY p(x_i y_j) log left( dfracp(x_i y_i)p(x_i)p(y_j) right)\n\nwhere  hatp(x_i) = fracn(x_i)N_x, hatp(y_i) = fracn(y_j)N_y, and hatp(x_i x_j) = fracn(x_i)N, and N = N_x N_y. This definition is used by association when called with a JointProbabilities estimator.\n\nThree-entropies formulation\n\nAn equivalent formulation of discrete Shannon mutual information is\n\nI^S(X Y) = H^S(X) + H_q^S(Y) - H^S(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint discrete Shannon entropies. This definition is used by association when called with a EntropyDecomposition estimator and a discretization.\n\nDifferential mutual information\n\nOne possible formulation of differential Shannon mutual information is\n\nI^S(X Y) = h^S(X) + h_q^S(Y) - h^S(X Y)\n\nwhere h^S(cdot) and h^S(cdot cdot) are the marginal and joint differential Shannon entropies. This definition is used by association when called with EntropyDecomposition estimator and a DifferentialInfoEstimator.\n\nEstimation\n\nExample 1: JointProbabilities with ValueBinning outcome space.\nExample 2: JointProbabilities with UniqueElements outcome space on string data.\nExample 3: Dedicated GaussianMI estimator.\nExample 4: Dedicated KraskovStögbauerGrassberger1 estimator.\nExample 5: Dedicated KraskovStögbauerGrassberger2 estimator.\nExample 6: Dedicated GaoKannanOhViswanath estimator.\nExample 7: EntropyDecomposition with Kraskov estimator.\nExample 8: EntropyDecomposition with BubbleSortSwaps.\nExample 9: EntropyDecomposition with Jackknife estimator and ValueBinning outcome space.\nExample 10: Reproducing Kraskov et al. (2004).\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.MITsallisFuruichi","page":"Association measures","title":"Associations.MITsallisFuruichi","text":"MITsallisFuruichi <: BivariateInformationMeasure\nMITsallisFuruichi(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Furuichi (2006)(Furuichi, 2006), which in that paper is called the mutual entropy.\n\nUsage\n\nUse with association to compute the raw Tsallis-Furuichi mutual information from input data   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise dependence using   the Tsallis-Furuichi mutual information.\n\nCompatible estimators\n\nJointProbabilities\nEntropyDecomposition\n\nDescription\n\nFuruichi's Tsallis mutual entropy between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_q^T(X Y) = H_q^T(X) - H_q^T(X  Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X Y)\n\nwhere H^T(cdot) and H^T(cdot cdot) are the marginal and joint Tsallis entropies, and q is the Tsallis-parameter.\n\nEstimation\n\nExample 1: JointProbabilities with UniqueElements outcome space.\nExample 2: EntropyDecomposition with LeonenkoProzantoSavani estimator.\nExample 3: EntropyDecomposition with Dispersion\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.MITsallisMartin","page":"Association measures","title":"Associations.MITsallisMartin","text":"MITsallisMartin <: BivariateInformationMeasure\nMITsallisMartin(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Martin et al. (2004).\n\nUsage\n\nUse with association to compute the raw Tsallis-Martin mutual information from input data   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise dependence using   the Tsallis-Martin mutual information.\n\nCompatible estimators\n\nJointProbabilities\nEntropyDecomposition\n\nDescription\n\nMartin et al.'s Tsallis mutual information between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_textMartin^T(X Y q) = H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint Shannon entropies, and q is the Tsallis-parameter.\n\nEstimation\n\nExample 1: JointProbabilities with UniqueElements outcome space.\nExample 2: EntropyDecomposition with LeonenkoProzantoSavani estimator.\nExample 3: EntropyDecomposition with OrdinalPatterns outcome space.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.MIRenyiJizba","page":"Association measures","title":"Associations.MIRenyiJizba","text":"MIRenyiJizba <: <: BivariateInformationMeasure\nMIRenyiJizba(; q = 1.5, base = 2)\n\nThe Rényi mutual information I_q^R_J(X Y) defined in (Jizba et al., 2012).\n\nUsage\n\nUse with association to compute the raw Rényi-Jizba mutual information from input data   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise dependence using   the Rényi-Jizba mutual information.\n\nCompatible estimators\n\nJointProbabilities.\nEntropyDecomposition.\n\nDefinition\n\nI_q^R_J(X Y) = H_q^R(X) + H_q^R(Y) - H_q^R(X Y)\n\nwhere H_q^R(cdot) is the Rényi entropy.\n\nEstimation\n\nExample 1: JointProbabilities with UniqueElements outcome space.\nExample 2: EntropyDecomposition with LeonenkoProzantoSavani.\nExample 3: EntropyDecomposition with ValueBinning.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.MIRenyiSarbu","page":"Association measures","title":"Associations.MIRenyiSarbu","text":"MIRenyiSarbu <: BivariateInformationMeasure\nMIRenyiSarbu(; base = 2, q = 1.5)\n\nThe discrete Rényi mutual information from Sarbu (2014).\n\nUsage\n\nUse with association to compute the raw Rényi-Sarbu mutual information from input data   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise dependence using   the Rényi-Sarbu mutual information.\n\nCompatible estimators\n\nJointProbabilities.\n\nDescription\n\nSarbu (2014) defines discrete Rényi mutual information as the Rényi alpha-divergence between the conditional joint probability mass function p(x y) and the product of the conditional marginals, p(x) cdot p(y):\n\nI(X Y)^R_q =\ndfrac1q-1\nlog left(\n    sum_x in X y in Y\n    dfracp(x y)^qleft( p(x)cdot p(y) right)^q-1\nright)\n\nEstimation\n\nExample 1: JointProbabilities with UniqueElements for categorical data.\nExample 2: JointProbabilities with CosineSimilarityBinning for numerical data.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Conditional-mutual-informations","page":"Association measures","title":"Conditional mutual informations","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"ConditionalMutualInformation\nCMIShannon\nCMIRenyiSarbu\nCMIRenyiJizba\nCMIRenyiPoczos\nCMITsallisPapapetrou","category":"page"},{"location":"associations/#Associations.ConditionalMutualInformation","page":"Association measures","title":"Associations.ConditionalMutualInformation","text":"CondiitionalMutualInformation\n\nAbstract type for all mutual information measures.\n\nConcrete implementations\n\nCMIShannon\nCMITsallisPapapetrou\nCMIRenyiJizba\nCMIRenyiSarbu\nCMIRenyiPoczos\n\nSee also: ConditionalMutualInformationEstimator\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.CMIShannon","page":"Association measures","title":"Associations.CMIShannon","text":"CMIShannon <: ConditionalMutualInformation\nCMIShannon(; base = 2)\n\nThe Shannon conditional mutual information (CMI) I^S(X Y  Z).\n\nUsage\n\nUse with association to compute the raw Shannon conditional mutual information   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise conditional    independence using the Shannon conditional mutual information.\n\nCompatible estimators\n\nJointProbabilities\nEntropyDecomposition\nMIDecomposition\nFPVP\nMesnerShalizi\nRahimzamani\nPoczosSchneiderCMI\nGaussianCMI\n\nSupported definitions\n\nConsider random variables X in mathbbR^d_X and Y in mathbbR^d_Y, given Z in mathbbR^d_Z. The Shannon conditional mutual information is defined as\n\nbeginalign*\nI(X Y  Z)\n= H^S(X Z) + H^S(Y z) - H^S(X Y Z) - H^S(Z) \n= I^S(X Y Z) + I^S(X Y)\nendalign*\n\nwhere I^S(cdot cdot) is the Shannon mutual information MIShannon, and H^S(cdot) is the Shannon entropy.\n\nDifferential Shannon CMI is obtained by replacing the entropies by differential entropies.\n\nEstimation\n\nExample 1:    EntropyDecomposition with Kraskov estimator.\nExample 2:   EntropyDecomposition with ValueBinning estimator.\nExample 3:    MIDecomposition with KraskovStögbauerGrassberger1 estimator.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.CMIRenyiSarbu","page":"Association measures","title":"Associations.CMIRenyiSarbu","text":"CMIRenyiSarbu <: ConditionalMutualInformation\nCMIRenyiSarbu(; base = 2, q = 1.5)\n\nThe Rényi conditional mutual information from Sarbu (2014).\n\nUsage\n\nUse with association to compute the raw  Rényi-Sarbu conditional mutual information   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Sarbu conditional mutual information.\n\nCompatible estimators\n\nJointProbabilities\n\nDiscrete description\n\nAssume we observe three discrete random variables X, Y and Z. Sarbu (2014) defines discrete conditional Rényi mutual information as the conditional Rényi alpha-divergence between the conditional joint probability mass function p(x y  z) and the product of the conditional marginals, p(x z) cdot p(yz):\n\nI(X Y Z)^R_q =\ndfrac1q-1 sum_z in Z p(Z = z)\nlog left(\n    sum_x in Xsum_y in Y\n    dfracp(x yz)^qleft( p(xz)cdot p(yz) right)^q-1\nright)\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.CMIRenyiJizba","page":"Association measures","title":"Associations.CMIRenyiJizba","text":"CMIRenyiJizba <: ConditionalMutualInformation\nCMIRenyiJizba(; base = 2, q = 1.5)\n\nThe Rényi conditional mutual information I_q^R_J(X Y  Z) defined in Jizba et al. (2012).\n\nUsage\n\nUse with association to compute the raw  Rényi-Jizba conditional mutual information   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Jizba conditional mutual information.\n\nCompatible estimators\n\nJointProbabilities\nEntropyDecomposition\n\nDefinition\n\nI_q^R_J(X Y  Z) = I_q^R_J(X Y Z) - I_q^R_J(X Z)\n\nwhere I_q^R_J(X Z) is the MIRenyiJizba mutual information.\n\nEstimation\n\nExample 1:    JointProbabilities with BubbleSortSwaps outcome space.\nExample 2:    EntropyDecomposition with OrdinalPatterns outcome space.\nExample 3:    EntropyDecomposition with differential entropy estimator LeonenkoProzantoSavani.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.CMIRenyiPoczos","page":"Association measures","title":"Associations.CMIRenyiPoczos","text":"CMIRenyiPoczos <: ConditionalMutualInformation\nCMIRenyiPoczos(; base = 2, q = 1.5)\n\nThe differential Rényi conditional mutual information I_q^R_P(X Y  Z) defined in Póczos and Schneider (2012).\n\nUsage\n\nUse with association to compute the raw Rényi-Poczos conditional mutual information   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise conditional    independence using the Rényi-Poczos conditional mutual information.\n\nCompatible estimators\n\nPoczosSchneiderCMI\n\nDefinition\n\nbeginalign*\nI_q^R_P(X Y  Z) = dfrac1q-1\nint int int dfracp_Z(z) p_X Y  Z^q( p_XZ(xz) p_YZ(yz) )^q-1 \n= mathbbE_X Y Z sim p_X Y Z\nleft dfracp_X Z^1-q(X Z) p_Y Z^1-q(Y Z) p_X Y Z^1-q(X Y Z) p_Z^1-q(Z) right\nendalign*\n\nEstimation\n\nExample 1: Dedicated PoczosSchneiderCMI estimator.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.CMITsallisPapapetrou","page":"Association measures","title":"Associations.CMITsallisPapapetrou","text":"CMITsallisPapapetrou <: ConditionalMutualInformation\nCMITsallisPapapetrou(; base = 2, q = 1.5)\n\nThe Tsallis-Papapetrou conditional mutual information (Papapetrou and Kugiumtzis, 2020).\n\nUsage\n\nUse with association to compute the raw Tsallis-Papapetrou conditional mutual information   using of of the estimators listed below.\nUse with independence to perform a formal hypothesis test for pairwise conditional    independence using the Tsallis-Papapetrou conditional mutual information.\n\nCompatible estimators\n\nJointProbabilities\n\nDefinition\n\nTsallis-Papapetrou conditional mutual information is defined as \n\nI_T^q(X Y mid Z) = frac11 - q left( 1 - sum_XYZ fracp(x y z)^qp(x mid z)^q-1 p(y mid z)^q-1 p(z)^q-1 right)\n\n\n\n\n\n","category":"type"},{"location":"associations/#Transfer-entropy","page":"Association measures","title":"Transfer entropy","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"TransferEntropy\nTEShannon\nTERenyiJizba","category":"page"},{"location":"associations/#Associations.TransferEntropy","page":"Association measures","title":"Associations.TransferEntropy","text":"TransferEntropy <: AssociationMeasure\n\nThe supertype of all transfer entropy measures. Concrete subtypes are\n\nTEShannon\nTERenyiJizba\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.TEShannon","page":"Association measures","title":"Associations.TEShannon","text":"TEShannon <: TransferEntropy\nTEShannon(; base = 2; embedding = EmbeddingTE()) <: TransferEntropy\n\nThe Shannon-type transfer entropy measure.\n\nUsage\n\nUse with association to compute the raw transfer entropy.\nUse with an IndependenceTest to perform a formal hypothesis test for pairwise   and conditional dependence.\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I^S(T^+ S^-  T^-) \nTE(S to T  C) = I^S(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I(T^+ S^-  T^-) is the Shannon conditional mutual information (CMIShannon). The - and + subscripts on the marginal variables T^+, T^-, S^- and C^- indicate that the embedding vectors for that marginal are constructed using present/past values and future values, respectively.\n\nEstimation\n\nExample 1:    EntropyDecomposition with TransferOperator outcome space.\nExample 2: Estimation using the   SymbolicTransferEntropy estimator.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.TERenyiJizba","page":"Association measures","title":"Associations.TERenyiJizba","text":"TERenyiJizba() <: TransferEntropy\n\nThe Rényi transfer entropy from Jizba et al. (2012).\n\nUsage\n\nUse with association to compute the raw transfer entropy.\nUse with an IndependenceTest to perform a formal hypothesis test for pairwise   and conditional dependence.\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I_q^R_J(T^+ S^-  T^-) \nTE(S to T  C) = I_q^R_J(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I_q^R_J(T^+ S^-  T^-) is Jizba et al. (2012)'s definition of conditional mutual information (CMIRenyiJizba). The - and + subscripts on the marginal variables T^+, T^-, S^- and C^- indicate that the embedding vectors for that marginal are constructed using present/past values and future values, respectively.\n\nEstimation\n\nEstimating Jizba's Rényi transfer entropy is a bit complicated, since it doesn't have  a dedicated estimator. Instead, we re-write the Rényi transfer entropy as a  Rényi conditional mutual information, and estimate it using an  EntropyDecomposition with a suitable discrete/differential Rényi entropy estimator from the list below as its input.\n\nEstimator Sub-estimator Principle\nEntropyDecomposition LeonenkoProzantoSavani Four-entropies decomposition\nEntropyDecomposition ValueBinning Four-entropies decomposition\nEntropyDecomposition Dispersion Four-entropies decomposition\nEntropyDecomposition OrdinalPatterns Four-entropies decomposition\nEntropyDecomposition UniqueElements Four-entropies decomposition\nEntropyDecomposition TransferOperator Four-entropies decomposition\n\nAny of these estimators must be given as input to a `CMIDecomposition estimator.\n\nEstimation\n\nExample 1: EntropyDecomposition with TransferOperator outcome space.\n\n\n\n\n\n","category":"type"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"The following utility functions and types are also useful for transfer entropy estimation.","category":"page"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"optimize_marginals_te\nEmbeddingTE","category":"page"},{"location":"associations/#Associations.optimize_marginals_te","page":"Association measures","title":"Associations.optimize_marginals_te","text":"optimize_marginals_te([scheme = OptimiseTraditional()], s, t, [c]) → EmbeddingTE\n\nOptimize marginal embeddings for transfer entropy computation from source time series s to target time series t, conditioned on c if c is given, using the provided optimization scheme.\n\n\n\n\n\n","category":"function"},{"location":"associations/#Associations.EmbeddingTE","page":"Association measures","title":"Associations.EmbeddingTE","text":"EmbeddingTE(; dS = 1, dT = 1, dTf = 1, dC = 1, τS = -1, τT = -1, ηTf = 1, τC = -1)\nEmbeddingTE(opt::OptimiseTraditional, s, t, [c])\n\nEmbeddingTE provide embedding parameters for transfer entropy analysis using either TEShannon, TERenyiJizba, or in general any subtype of TransferEntropy.\n\nThe second method finds parameters using the \"traditional\" optimised embedding techniques from DynamicalSystems.jl\n\nConvention for generalized delay reconstruction\n\nWe use the following convention. Let s(i) be time series for the source variable, t(i) be the time series for the target variable and c(i) the time series for the conditional variable. To compute transfer entropy, we need the following marginals:\n\nbeginaligned\nT^+ = t(i+eta^1) t(i+eta^2) ldots (t(i+eta^d_T^+)  \nT^- =  (t(i+tau^0_T) t(i+tau^1_T) t(i+tau^2_T) ldots t(t + tau^d_T - 1_T))  \nS^- =  (s(i+tau^0_S) s(i+tau^1_S) s(i+tau^2_S) ldots s(t + tau^d_S - 1_S))  \nC^- =  (c(i+tau^0_C) c(i+tau^1_C) c(i+tau^2_C) ldots c(t + tau^d_C - 1_C)) \nendaligned\n\nDepending on the application, the delay reconstruction lags tau^k_T leq 0, tau^k_S leq 0, and tau^k_C leq 0 may be equally spaced, or non-equally spaced. The same applied to the prediction lag(s), but typically only a only a single predictions lag eta^k is used (so that d_T^+ = 1).\n\nFor transfer entropy, traditionally at least one tau^k_T, one tau^k_S and one tau^k_C equals zero. This way, the T^-, S^- and C^- marginals always contains present/past states, while the mathcal T marginal contain future states relative to the other marginals. However, this is not a strict requirement, and modern approaches that searches for optimal embeddings can return embeddings without the intantaneous lag.\n\nCombined, we get the generalized delay reconstruction mathbbE = (T^+_(d_T^+) T^-_(d_T) S^-_(d_S) C^-_(d_C)). Transfer entropy is then computed as\n\nbeginaligned\nTE_S rightarrow T  C = int_mathbbE P(T^+ T^- S^- C^-)\nlog_bleft(fracP(T^+  T^- S^- C^-)P(T^+  T^- C^-)right)\nendaligned\n\nor, if conditionals are not relevant,\n\nbeginaligned\nTE_S rightarrow T = int_mathbbE P(T^+ T^- S^-)\nlog_bleft(fracP(T^+  T^- S^-)P(T^+  T^-)right)\nendaligned\n\nHere,\n\nT^+ denotes the d_T^+-dimensional set of vectors furnishing the future   states of T (almost always equal to 1 in practical applications),\nT^- denotes the d_T-dimensional set of vectors furnishing the past and   present states of T,\nS^- denotes the d_S-dimensional set of vectors furnishing the past and   present of S, and\nC^- denotes the d_C-dimensional set of vectors furnishing the past and   present of C.\n\nKeyword arguments\n\ndS, dT, dC, dTf (f for future) are the dimensions of the S^-,   T^-, C^- and T^+ marginals. The parameters dS, dT, dC and dTf   must each be a positive integer number.\nτS, τT, τC are the embedding lags for S^-, T^-, C^-.   Each parameter are integers ∈ 𝒩⁰⁻, or a vector of integers ∈ 𝒩⁰⁻, so   that S^-, T^-, C^- always represents present/past values.   If e.g. τT is an integer, then for the T^- marginal is constructed using   lags tau_T = 0 tau 2tau ldots (d_T- 1)tau_T .   If is a vector, e.g. τΤ = [-1, -5, -7], then the dimension dT must match the lags,   and precisely those lags are used: tau_T = -1 -5 -7 .\nThe prediction lag(s) ηTf is a positive integer. Combined with the requirement   that the other delay parameters are zero or negative, this ensures that we're   always predicting from past/present to future. In typical applications,   ηTf = 1 is used for transfer entropy.\n\nExamples\n\nSay we wanted to compute the Shannon transfer entropy TE^S(S to T) = I^S(T^+ S^-  T^-). Using some modern procedure for determining optimal embedding parameters using methods from DynamicalSystems.jl, we find that the optimal embedding of T^- is three-dimensional and is given by the lags [0, -5, -8]. Using the same procedure, we find that the optimal embedding of S^- is two-dimensional with lags -1 -8. We want to predicting a univariate version of the target variable one time step into the future (ηTf = 1). The total embedding is then the set of embedding vectors\n\nE_TE =  (T(i+1) S(i-1) S(i-8) T(i) T(i-5) T(i-8)) . Translating this to code, we get:\n\nusing Associations\njulia> EmbeddingTE(dT=3, τT=[0, -5, -8], dS=2, τS=[-1, -4], ηTf=1)\n\n# output\nEmbeddingTE(dS=2, dT=3, dC=1, dTf=1, τS=[-1, -4], τT=[0, -5, -8], τC=-1, ηTf=1)\n\n\n\n\n\n","category":"type"},{"location":"associations/#Partial-mutual-information","page":"Association measures","title":"Partial mutual information","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"PartialMutualInformation","category":"page"},{"location":"associations/#Associations.PartialMutualInformation","page":"Association measures","title":"Associations.PartialMutualInformation","text":"PartialMutualInformation <: MultivariateInformationMeasure\nPartialMutualInformation(; base = 2)\n\nThe partial mutual information (PMI) measure of conditional association (Zhao et al., 2016).\n\nDefinition\n\nPMI is defined as for variables X, Y and Z as\n\nPMI(X Y  Z) = D(p(x y z)  p^*(xz) p^*(yz) p(z))\n\nwhere p(x y z) is the joint distribution for X, Y and Z, and D(cdot cdot) is the extended Kullback-Leibler divergence from p(x y z) to p^*(xz) p^*(yz) p(z). See Zhao et al. (2016) for details.\n\nEstimation\n\nThe PMI is estimated by first estimating a 3D probability mass function using  probabilities, then computing PMI(X Y  Z) from those probaiblities.\n\nProperties\n\nFor the discrete case, the following identities hold in theory (when estimating PMI, they may not).\n\nPMI(X, Y, Z) >= CMI(X, Y, Z) (where CMI is the Shannon CMI). Holds in theory, but   when estimating PMI, the identity may not hold.\nPMI(X, Y, Z) >= 0. Holds both in theory and when estimating using discrete estimators.\nX ⫫ Y | Z => PMI(X, Y, Z) = CMI(X, Y, Z) = 0 (in theory, but not necessarily for   estimation).\n\n\n\n\n\n","category":"type"},{"location":"associations/#correlation_api","page":"Association measures","title":"Correlation measures","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"CorrelationMeasure\nPearsonCorrelation\nPartialCorrelation\nDistanceCorrelation","category":"page"},{"location":"associations/#Associations.CorrelationMeasure","page":"Association measures","title":"Associations.CorrelationMeasure","text":"CorrelationMeasure <: AssociationMeasure end\n\nThe supertype for correlation measures.\n\nConcrete implementations\n\nPearsonCorrelation\nPartialCorrelation\nDistanceCorrelation\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.PearsonCorrelation","page":"Association measures","title":"Associations.PearsonCorrelation","text":"PearsonCorrelation\n\nThe Pearson correlation of two variables.\n\nUsage\n\nUse with association to compute the raw Pearson correlation coefficient.\nUse with independence to perform a formal hypothesis test for pairwise dependence   using the Pearson correlation coefficient.\n\nDescription\n\nThe sample Pearson correlation coefficient for real-valued random variables X and Y with associated samples x_i_i=1^N and y_i_i=1^N is defined as\n\nrho_xy = dfracsum_i=1^n (x_i - barx)(y_i - bary) sqrtsum_i=1^N (x_i - barx)^2sqrtsum_i=1^N (y_i - bary)^2\n\nwhere barx and bary are the means of the observations x_k and y_k, respectively.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.PartialCorrelation","page":"Association measures","title":"Associations.PartialCorrelation","text":"PartialCorrelation <: AssociationMeasure\n\nThe correlation of two variables, with the effect of a set of conditioning variables removed.\n\nUsage\n\nUse with association to compute the raw partial correlation coefficient.\nUse with independence to perform a formal hypothesis test for   correlated-based conditional independence.\n\nDescription\n\nThere are several ways of estimating the partial correlation. We follow the matrix inversion method, because for StateSpaceSets, we can very efficiently compute the required joint covariance matrix Sigma for the random variables.\n\nFormally, let X_1 X_2 ldots X_n be a set of n real-valued random variables. Consider the joint precision matrix,P = (p_ij) = Sigma^-1. The partial correlation of any pair of variables (X_i X_j), given the remaining variables bfZ = X_k_i=1 i neq i j^n, is defined as\n\nrho_X_i X_j  bfZ = -dfracp_ijsqrt p_ii p_jj \n\nIn practice, we compute the estimate\n\nhatrho_X_i X_j  bfZ =\n-dfrachatp_ijsqrt hatp_ii hatp_jj \n\nwhere hatP = hatSigma^-1 is the sample precision matrix.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.DistanceCorrelation","page":"Association measures","title":"Associations.DistanceCorrelation","text":"DistanceCorrelation\n\nThe distance correlation (Székely et al., 2007) measure quantifies potentially nonlinear associations between pairs of variables. If applied to three variables, the partial distance correlation (Székely and Rizzo, 2014) is computed.\n\nUsage\n\nUse with association to compute the raw (partial) distance correlation   coefficient.\nUse with independence to perform a formal hypothesis test for   pairwise dependence.\n\nDescription\n\nThe distance correlation can be used to compute the association between two variables, or the conditional association between three variables, like so:\n\nassociation(DistanceCorrelation(), x, y) → dcor ∈ [0, 1]\nassociation(DistanceCorrelation(), x, y, z) → pdcor\n\nWith two variable, we comptue dcor, which is called the empirical/sample distance  correlation (Székely et al., 2007). With three variables, the  partial distance correlation pdcor is computed (Székely and Rizzo, 2014).\n\nwarn: Warn\nA partial distance correlation distance_correlation(X, Y, Z) = 0 doesn't always guarantee conditional independence X ⫫ Y | Z. Székely and Rizzo (2014) for an in-depth discussion.\n\n\n\n\n\n","category":"type"},{"location":"associations/#cross_map_api","page":"Association measures","title":"Cross-map measures","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"The cross-map measures define different ways of quantifying association based on the  concept of \"cross mapping\", which has appeared in many contexts in the literature, and gained huge popularity with  Sugihara et al. (2012)'s on convergent cross mapping.","category":"page"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"Since their paper, several cross mapping methods and frameworks have emerged in the literature. In Associations.jl, we provide a unified interface for using these cross mapping methods.","category":"page"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"CrossmapMeasure\nConvergentCrossMapping\nPairwiseAsymmetricInference","category":"page"},{"location":"associations/#Associations.CrossmapMeasure","page":"Association measures","title":"Associations.CrossmapMeasure","text":"CrossmapMeasure <: AssociationMeasure\n\nThe supertype for all cross-map measures. Concrete subtypes are\n\nConvergentCrossMapping, or CCM for short.\nPairwiseAsymmetricInference, or PAI for short.\n\nSee also: CrossmapEstimator.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.ConvergentCrossMapping","page":"Association measures","title":"Associations.ConvergentCrossMapping","text":"ConvergentCrossMapping <: CrossmapMeasure\nConvergentCrossMapping(; d::Int = 2, τ::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe convergent cross mapping measure (Sugihara et al., 2012).\n\nUsage\n\nUse with association together with a CrossmapEstimator to compute the    cross-map correlation between input variables.\n\nCompatible estimators\n\nRandomSegment\nRandomVectors\nExpandingSegment\n\nDescription\n\nThe Theiler window w controls how many temporal neighbors are excluded during neighbor  searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nLet S(i) be the source time series variable and T(i) be the target time series variable. This version produces regular embeddings with fixed dimension d and embedding lag τ as follows:\n\n( S(i) S(i+tau) S(i+2tau) ldots S(i+(d-1)tau T(i))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D-1 variables, while the last (D-th) variable is to be predicted.\n\nWith this convention, τ < 0 implies \"past/present values of source used to predict target\", and τ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if τ > 0 (embed_warn = false turns off warnings).\n\nEstimation\n\nExample 1.    Estimation with RandomVectors estimator.\nExample 2.    Estimation with RandomSegment estimator.\nExample 3: Reproducing    figures from Sugihara et al. (2012).\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.PairwiseAsymmetricInference","page":"Association measures","title":"Associations.PairwiseAsymmetricInference","text":"PairwiseAsymmetricInference <: CrossmapMeasure\nPairwiseAsymmetricInference(; d::Int = 2, τ::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe pairwise asymmetric inference (PAI) measure (McCracken and Weigel, 2014) is a version of ConvergentCrossMapping that searches for neighbors in mixed embeddings (i.e. both source and target variables included); otherwise, the algorithms are identical.\n\nUsage\n\nUse with association to compute the pairwise asymmetric inference measure    between variables.\n\nCompatible estimators\n\nRandomSegment\nRandomVectors\nExpandingSegment\n\nDescription\n\nThe Theiler window w controls how many temporal neighbors are excluded during neighbor  searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nThere are many possible ways of defining the embedding for PAI. Currently, we only implement the \"add one non-lagged source timeseries to an embedding of the target\" approach, which is used as an example in McCracken & Weigel's paper. Specifically: Let S(i) be the source time series variable and T(i) be the target time series variable. PairwiseAsymmetricInference produces regular embeddings with fixed dimension d and embedding lag τ as follows:\n\n(S(i) T(i+(d-1)tau ldots T(i+2tau) T(i+tau) T(i)))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D variables, while the last variable is to be predicted.\n\nWith this convention, τ < 0 implies \"past/present values of source used to predict target\", and τ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if τ > 0 (embed_warn = false turns off warnings).\n\nEstimation\n\nExample 1.    Estimation with RandomVectors estimator.\nExample 2.    Estimation with RandomSegment estimator.\nExample 3. Reproducing    McCracken & Weigel's results from the original paper.\n\n\n\n\n\n","category":"type"},{"location":"associations/#closeness_api","page":"Association measures","title":"Closeness measures","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"ClosenessMeasure\nJointDistanceDistribution\nSMeasure\nHMeasure\nMMeasure\nLMeasure","category":"page"},{"location":"associations/#Associations.ClosenessMeasure","page":"Association measures","title":"Associations.ClosenessMeasure","text":"ClosenessMeasure <: AssociationMeasure\n\nThe supertype for all multivariate information-based measure definitions.\n\nImplementations\n\nJointDistanceDistribution\nSMeasure\nHMeasure\nMMeasure\nLMeasure\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.JointDistanceDistribution","page":"Association measures","title":"Associations.JointDistanceDistribution","text":"JointDistanceDistribution <: AssociationMeasure end\nJointDistanceDistribution(; metric = Euclidean(), B = 10, D = 2, τ = -1, μ = 0.0)\n\nThe joint distance distribution (JDD) measure (Amigó and Hirata, 2018).\n\nUsage\n\nUse with association to compute the joint distance distribution measure Δ from   Amigó and Hirata (2018).\nUse with independence to perform a formal hypothesis test for directional   dependence.\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.   Defaults to Euclidean().\nB::Int: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances.\nD::Int: Embedding dimension.\nτ::Int: Embedding delay. By convention, τ is negative.\nμ: The hypothetical mean value of the joint distance distribution if there   is no coupling between x and y (default is μ = 0.0).\n\nDescription\n\nFrom input time series x(t) and y(t), we first construct the delay embeddings (note the positive sign in the embedding lags; therefore the input parameter τ is by convention negative).\n\nbeginalign*\nbfx_i  = (x_i x_i+tau ldots x_i+(d_x - 1)tau)  \nbfy_i  = (y_i y_i+tau ldots y_i+(d_y - 1)tau)  \nendalign*\n\nThe algorithm then proceeds to analyze the distribution of distances between points of these embeddings, as described in Amigó and Hirata (2018).\n\nExamples\n\nIndependence testing using JDD\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.SMeasure","page":"Association measures","title":"Associations.SMeasure","text":"SMeasure < ClosenessMeasure\nSMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nSMeasure is a bivariate association measure from Arnhold et al. (1999) and Quiroga et al. (2000) that measure directional dependence between two input (potentially multivariate) time series.\n\nNote that τx and τy are negative; see explanation below.\n\nUsage\n\nUse with association to compute the raw s-measure statistic.\nUse with independence to perform a formal hypothesis test for directional dependence.\n\nDescription\n\nThe steps of the algorithm are:\n\nFrom input time series x(t) and y(t), construct the delay embeddings (note  the positive sign in the embedding lags; therefore inputs parameters  τx and τy are by convention negative).\n\nbeginalign*\nbfx_i  = (x_i x_i+tau_x ldots x_i+(d_x - 1)tau_x)  \nbfy_i  = (y_i y_i+tau_y ldots y_i+(d_y - 1)tau_y)  \nendalign*\n\nLet r_ij and s_ij be the indices of the K-th nearest neighbors  of bfx_i and bfy_i, respectively. Neighbors closed than w time indices  are excluded during searches (i.e. w is the Theiler window).\nCompute the the mean squared Euclidean distance to the K nearest neighbors  for each x_i, using the indices r_i j.\n\nR_i^(k)(x) = dfrac1k sum_i=1^k(bfx_i bfx_r_ij)^2\n\nCompute the y-conditioned mean squared Euclidean distance to the K nearest   neighbors for each x_i, now using the indices s_ij.\n\nR_i^(k)(xy) = dfrac1k sum_i=1^k(bfx_i bfx_s_ij)^2\n\nDefine the following measure of independence, where 0 leq S leq 1, and   low values indicate independence and values close to one occur for   synchronized signals.\n\nS^(k)(xy) = dfrac1N sum_i=1^N dfracR_i^(k)(x)R_i^(k)(xy)\n\nInput data\n\nThe algorithm is slightly modified from (Arnhold et al., 1999) to allow univariate timeseries as input.\n\nIf x and y are StateSpaceSets then use x and y as is and ignore the parameters   dx/τx and dy/τy.\nIf x and y are scalar time series, then create dx and dy dimensional embeddings,   respectively, of both x and y, resulting in N different m-dimensional embedding points   X = x_1 x_2 ldots x_N  and Y = y_1 y_2 ldots y_N .   τx and τy control the embedding lags for x and y.\nIf x is a scalar-valued vector and y is a StateSpaceSet, or vice versa,   then create an embedding of the scalar timeseries using parameters dx/τx or dy/τy.\n\nIn all three cases, input StateSpaceSets are length-matched by eliminating points at the end of the longest StateSpaceSet (after the embedding step, if relevant) before analysis.\n\nSee also: ClosenessMeasure.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.HMeasure","page":"Association measures","title":"Associations.HMeasure","text":"HMeasure <: AssociationMeasure\nHMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nThe HMeasure (Arnhold et al., 1999) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.\n\nNote that τx and τy are negative by convention. See docstring for SMeasure for an explanation.\n\nUsage\n\nUse with association to compute the raw h-measure statistic.\nUse with independence to perform a formal hypothesis test for directional dependence.\n\nDescription\n\nThe HMeasure (Arnhold et al., 1999) is similar to the SMeasure, but the numerator of the formula is replaced by R_i(x), the mean squared Euclidean distance to all other points, and there is a log-term inside the sum:\n\nH^(k)(xy) = dfrac1N sum_i=1^N\nlog left( dfracR_i(x)R_i^(k)(xy) right)\n\nParameters are the same and R_i^(k)(xy) is computed as for SMeasure.\n\nSee also: ClosenessMeasure.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.MMeasure","page":"Association measures","title":"Associations.MMeasure","text":"MMeasure <: ClosenessMeasure\nMMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nThe MMeasure (Andrzejak et al., 2003) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.\n\nNote that τx and τy are negative by convention. See docstring for SMeasure for an explanation.\n\nUsage\n\nUse with association to compute the raw m-measure statistic.\nUse with independence to perform a formal hypothesis test for directional dependence.\n\nDescription\n\nThe MMeasure is based on SMeasure and HMeasure. It is given by\n\nM^(k)(xy) = dfrac1N sum_i=1^N\nlog left( dfracR_i(x) - R_i^(k)(xy)R_i(x) - R_i^k(x) right)\n\nwhere R_i(x) is computed as for HMeasure, while R_i^k(x) and R_i^(k)(xy) is computed as for SMeasure. Parameters also have the same meaning as for SMeasure/HMeasure.\n\nSee also: ClosenessMeasure.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.LMeasure","page":"Association measures","title":"Associations.LMeasure","text":"LMeasure <: ClosenessMeasure\nLMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nThe LMeasure (Chicharro and Andrzejak, 2009) is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.\n\nNote that τx and τy are negative by convention. See docstring for SMeasure for an explanation.\n\nUsage\n\nUse with association to compute the raw L-measure statistic.\nUse with independence to perform a formal hypothesis test for directional dependence.\n\nDescription\n\nLMeasure is similar to MMeasure, but uses distance ranks instead of the raw distances.\n\nLet bfx_i be an embedding vector, and let g_ij denote the rank that the distance between bfx_i and some other vector bfx_j in a sorted ascending list of distances between bfx_i and bfx_i neq j In other words, g_ij this is just the N-1 nearest neighbor distances sorted )\n\nLMeasure is then defined as\n\nL^(k)(xy) = dfrac1N sum_i=1^N\nlog left( dfracG_i(x) - G_i^(k)(xy)G_i(x) - G_i^k(x) right)\n\nwhere G_i(x) = fracN2 and G_i^K(x) = frack+12 are the mean and minimal rank, respectively.\n\nThe y-conditioned mean rank is defined as\n\nG_i^(k)(xy) = dfrac1Ksum_j=1^K g_iw_i j\n\nwhere w_ij is the index of the j-th nearest neighbor of bfy_i.\n\nSee also: ClosenessMeasure.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Recurrence-measures","page":"Association measures","title":"Recurrence measures","text":"","category":"section"},{"location":"associations/","page":"Association measures","title":"Association measures","text":"MCR\nRMCD","category":"page"},{"location":"associations/#Associations.MCR","page":"Association measures","title":"Associations.MCR","text":"MCR <: AssociationMeasure\nMCR(; r, metric = Euclidean())\n\nAn association measure based on mean conditional probabilities of recurrence (MCR) introduced by Romano et al. (2007).\n\nUsage\n\nUse with association to compute the raw MCR for pairwise or conditional association.\nUse with IndependenceTest to perform a formal hypothesis test for pairwise   or conditional association.\n\nDescription\n\nr is  mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of AbstractRecurrenceType from RecurrenceAnalysis.jl. To use any r that is not a real number, you have to do using RecurrenceAnalysis first. The metric is any valid metric from Distances.jl.\n\nFor input variables X and Y, the conditional probability of recurrence is defined as\n\nM(X  Y) = dfrac1N sum_i=1^N p(bfy_i  bfx_i) =\ndfrac1N sum_i=1^N dfracsum_i=1^N J_R_i j^X Ysum_i=1^N R_i j^X\n\nwhere R_i j^X is the recurrence matrix and J_R_i j^X Y is the joint recurrence matrix, constructed using the given metric. The measure M(Y  X) is defined analogously.\n\nRomano et al. (2007)'s interpretation of this quantity is that if X drives Y, then M(X|Y) > M(Y|X), if Y drives X, then M(Y|X) > M(X|Y), and if coupling is symmetric,  then M(Y|X) = M(X|Y).\n\nInput data\n\nX and Y can be either both univariate timeseries, or both multivariate StateSpaceSets.\n\nEstimation\n\nExample 1. Pairwise versus conditional MCR.\n\n\n\n\n\n","category":"type"},{"location":"associations/#Associations.RMCD","page":"Association measures","title":"Associations.RMCD","text":"RMCD <: AssociationMeasure\nRMCD(; r, metric = Euclidean(), base = 2)\n\nThe recurrence measure of conditional dependence, or RMCD (Ramos et al., 2017), is a recurrence-based measure that mimics the conditional mutual information, but uses recurrence probabilities.\n\nUsage\n\nUse with association to compute the raw RMCD for pairwise    or conditional association.\nUse with IndependenceTest to perform a formal hypothesis test for pairwise   or conditional association.\n\nDescription\n\nr is a mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of AbstractRecurrenceType from RecurrenceAnalysis.jl. To use any r that is not a real number, you have to do using RecurrenceAnalysis first. The metric is any valid metric from Distances.jl.\n\nBoth the pairwise and conditional RMCD is non-negative, but due to round-off error, negative values may occur. If that happens, an RMCD value of 0.0 is returned.\n\nDescription\n\nThe RMCD measure is defined by\n\nI_RMCD(X Y  Z) = dfrac1N\nsum_i left\ndfrac1N sum_j R_ij^X Y Z\nlog left(\n    dfracsum_j R_ij^X Y Z sum_j R_ij^Z sum_j sum_j R_ij^X Z sum_j sum_j R_ij^Y Z\n    right)\nright\n\nwhere  base controls the base of the logarithm. I_RMCD(X Y  Z) is zero when Z = X, Z = Y or when X, Y and Z are mutually independent.\n\nOur implementation allows dropping the third/last argument, in which case the following mutual information-like quantitity is computed (not discussed in Ramos et al. (2017).\n\nI_RMCD(X Y) = dfrac1N\nsum_i left\ndfrac1N sum_j R_ij^X Y\nlog left(\n    dfracsum_j R_ij^X  R_ij^Y sum_j R_ij^X Y\n    right)\nright\n\nEstimation\n\nExample 1. Pairwise versus conditional RMCD.\n\n\n\n\n\n","category":"type"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/counts_and_probabilities_api/#counts_and_probabilities_api","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"","category":"section"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"For counting and probabilities, Associations.jl extends the single-variable machinery in ComplexityMeasures.jl to multiple variables.","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"Associations.Counts\nAssociations.counts(::OutcomeSpace)","category":"page"},{"location":"api/counts_and_probabilities_api/#ComplexityMeasures.Counts","page":"Multivariate counts and probabilities API","title":"ComplexityMeasures.Counts","text":"Counts <: Array{<:Integer, N}\nCounts(counts [, outcomes [, dimlabels]]) → c\n\nCounts stores an N-dimensional array of integer counts corresponding to a set of outcomes. This is typically called a \"frequency table\" or \"contingency table\".\n\nIf c isa Counts, then c.outcomes[i] is an abstract vector containing the outcomes along the i-th dimension, where c[i][j] is the count corresponding to the outcome c.outcomes[i][j], and c.dimlabels[i] is the label of the i-th dimension. Both labels and outcomes are assigned automatically if not given. c itself can be manipulated and iterated over like its stored array.\n\n\n\n\n\n","category":"type"},{"location":"api/counts_and_probabilities_api/#ComplexityMeasures.counts-Tuple{OutcomeSpace}","page":"Multivariate counts and probabilities API","title":"ComplexityMeasures.counts","text":"counts(o::UniqueElements, x₁, x₂, ..., xₙ) → Counts{N}\ncounts(encoding::CodifyPoints, x₁, x₂, ..., xₙ) → Counts{N}\ncounts(encoding::CodifyVariables, x₁, x₂, ..., xₙ) → Counts{N}\n\nConstruct an N-dimensional contingency table from the input iterables x₁, x₂, ..., xₙ which are such that  length(x₁) == length(x₂) == ⋯ == length(xₙ).\n\nIf x₁, x₂, ..., xₙ are already discrete, then use UniqueElements as  the first argument to directly construct the joint contingency table.\n\nIf x₁, x₂, ..., xₙ need to be discretized, provide as the first argument\n\nCodifyPoints (encodes every point in each of the input variables xᵢs individually)\nCodifyVariables (encodes every xᵢ individually using a sliding window encoding). NB: If    using different OutcomeSpaces for the different xᵢ, then total_outcomes must    be the same for every outcome space.\n\nExamples\n\n# Discretizing some non-discrete data using a sliding-window encoding for each variable\nx, y = rand(100), rand(100)\nc = CodifyVariables(OrdinalPatterns(m = 4))\ncounts(c, x, y)\n\n# Discretizing the data by binning each individual data point\nbinning = RectangularBinning(3)\nencoding = RectangularBinEncoding(binning, [x; y]) # give input values to ensure binning covers all data\nc = CodifyPoints(encoding)\ncounts(c, x, y)\n\n# Counts table for already discrete data\nn = 50 # all variables must have the same number of elements\nx = rand([\"dog\", \"cat\", \"mouse\"], n)\ny = rand(1:3, n)\nz = rand([(1, 2), (2, 1)], n)\n\ncounts(UniqueElements(), x, y, z)\n\nSee also: CodifyPoints, CodifyVariables, UniqueElements, OutcomeSpace, probabilities.\n\n\n\n\n\n","category":"method"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"Associations.Probabilities\nAssociations.probabilities(::OutcomeSpace)","category":"page"},{"location":"api/counts_and_probabilities_api/#ComplexityMeasures.Probabilities","page":"Multivariate counts and probabilities API","title":"ComplexityMeasures.Probabilities","text":"Probabilities <: Array{<:AbstractFloat, N}\nProbabilities(probs::Array [, outcomes [, dimlabels]]) → p\nProbabilities(counts::Counts [, outcomes [, dimlabels]]) → p\n\nProbabilities stores an N-dimensional array of probabilities, while ensuring that the array sums to 1 (normalized probability mass). In most cases the array is a standard vector. p itself can be manipulated and iterated over, just like its stored array.\n\nThe probabilities correspond to outcomes that describe the axes of the array. If p isa Probabilities, then p.outcomes[i] is an an abstract vector containing the outcomes along the i-th dimension. The outcomes have the same ordering as the probabilities, so that p[i][j] is the probability for outcome p.outcomes[i][j]. The dimensions of the array are named, and can be accessed by p.dimlabels, where p.dimlabels[i] is the label of the i-th dimension. Both outcomes and dimlabels are assigned automatically if not given. If the input is a set of Counts, and outcomes and dimlabels are not given, then the labels and outcomes are inherited from the counts.\n\nExamples\n\njulia> probs = [0.2, 0.2, 0.2, 0.2]; Probabilities(probs) # will be normalized to sum to 1\n Probabilities{Float64,1} over 4 outcomes\n Outcome(1)  0.25\n Outcome(2)  0.25\n Outcome(3)  0.25\n Outcome(4)  0.25\n\njulia> c = Counts([12, 16, 12], [\"out1\", \"out2\", \"out3\"]); Probabilities(c)\n Probabilities{Float64,1} over 3 outcomes\n \"out1\"  0.3\n \"out2\"  0.4\n \"out3\"  0.3\n\n\n\n\n\n","category":"type"},{"location":"api/counts_and_probabilities_api/#ComplexityMeasures.probabilities-Tuple{OutcomeSpace}","page":"Multivariate counts and probabilities API","title":"ComplexityMeasures.probabilities","text":"probabilities(o::UniqueElements, x₁, x₂, ..., xₙ) → Counts{N}\nprobabilities(encoding::CodifyPoints, x₁, x₂, ..., xₙ) → Counts{N}\nprobabilities(encoding::CodifyVariables, x₁, x₂, ..., xₙ) → Counts{N}\n\nConstruct an N-dimensional Probabilities array from the input iterables x₁, x₂, ..., xₙ which are such that  length(x₁) == length(x₂) == ⋯ == length(xₙ).\n\nDescription\n\nProbabilities are computed by first constructing a joint contingency matrix in the form  of a Counts instance. \n\nIf x₁, x₂, ..., xₙ are already discrete, then use UniqueElements as  the first argument to directly construct the joint contingency table.\n\nIf x₁, x₂, ..., xₙ need to be discretized, provide as the first argument\n\nCodifyPoints (encodes every point in each of the input variables xᵢs individually)\nCodifyVariables (encodes every xᵢ individually using a sliding window encoding).\n\nExamples\n\n# Discretizing some non-discrete data using a sliding-window encoding for each variable\nx, y = rand(100), rand(100)\nc = CodifyVariables(OrdinalPatterns(m = 4))\nprobabilities(c, x, y)\n\n# Discretizing the data by binning each individual data point\nbinning = RectangularBinning(3)\nencoding = RectangularBinEncoding(binning, [x; y]) # give input values to ensure binning covers all data\nc = CodifyPoints(encoding)\nprobabilities(c, x, y)\n\n# Joint probabilities for already discretized data\nn = 50 # all variables must have the same number of elements\nx = rand([\"dog\", \"cat\", \"mouse\"], n)\ny = rand(1:3, n)\nz = rand([(1, 2), (2, 1)], n)\n\nprobabilities(UniqueElements(), x, y, z)\n\nSee also: CodifyPoints, CodifyVariables, UniqueElements, OutcomeSpace.\n\n\n\n\n\n","category":"method"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"The utility function marginal is also useful.","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"marginal","category":"page"},{"location":"api/counts_and_probabilities_api/#Associations.marginal","page":"Multivariate counts and probabilities API","title":"Associations.marginal","text":"marginal(p::Probabilities; dims = 1:ndims(p))\nmarginal(c::Counts; dims = 1:ndims(p))\n\nGiven a set of counts c (a contingency table), or a multivariate probability mass function p, return the marginal counts/probabilities along the given dims.\n\n\n\n\n\n","category":"function"},{"location":"api/counts_and_probabilities_api/#tutorial_probabilities","page":"Multivariate counts and probabilities API","title":"Example: estimating Counts and Probabilities","text":"","category":"section"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"Estimating multivariate counts (contingency matrices) and PMFs is simple. If the data are pre-discretized, then we can use UniqueElements to simply count the number of occurrences.","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"using Associations\nn = 50 # the number of samples must be the same for each input variable\nx = rand([\"dog\", \"cat\", \"snake\"], n)\ny = rand(1:4, n)\nz = rand([(2, 1), (0, 0), (1, 1)], n)\ndiscretization = CodifyVariables(UniqueElements())\ncounts(discretization, x, y, z)","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"Probabilities are computed analogously, except counts are normalized to sum to 1.","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"discretization = CodifyVariables(UniqueElements())\nprobabilities(discretization, x, y, z)","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"For numerical data, we can estimate both counts and probabilities using CodifyVariables with any count-based OutcomeSpace.","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"using Associations\nx, y = rand(100), rand(100)\ndiscretization = CodifyVariables(BubbleSortSwaps(m = 4))\nprobabilities(discretization, x, y)","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"For more fine-grained control, we can use CodifyPoints with one or several Encodings.","category":"page"},{"location":"api/counts_and_probabilities_api/","page":"Multivariate counts and probabilities API","title":"Multivariate counts and probabilities API","text":"using Associations\nx, y = StateSpaceSet(rand(1000, 2)), StateSpaceSet(rand(1000, 3))\n\n # min/max of the `rand` call is 0 and 1\nprecise = true # precise bin edges\nr = range(0, 1; length = 3)\nbinning = FixedRectangularBinning(r, dimension(x), precise)\nencoding_x = RectangularBinEncoding(binning, x)\nencoding_y = CombinationEncoding(RelativeMeanEncoding(0.0, 1, n = 2), OrdinalPatternEncoding(3))\ndiscretization = CodifyPoints(encoding_x, encoding_y)\n\n# now estimate probabilities\nprobabilities(discretization, x, y)","category":"page"},{"location":"examples/examples_associations/#examples_associations","page":"Associations","title":"Examples of association measure estimation","text":"","category":"section"},{"location":"examples/examples_associations/#[HellingerDistance](@ref)","page":"Associations","title":"HellingerDistance","text":"","category":"section"},{"location":"examples/examples_associations/#example_HellingerDistance_precomputed_probabilities","page":"Associations","title":"From precomputed probabilities","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\n# From pre-computed PMFs\np1 = Probabilities([0.1, 0.5, 0.2, 0.2])\np2 = Probabilities([0.3, 0.3, 0.2, 0.2])\nassociation(HellingerDistance(), p1, p2)","category":"page"},{"location":"examples/examples_associations/#example_HellingerDistance_JointProbabilities_OrdinalPatterns","page":"Associations","title":"JointProbabilities + OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We expect the Hellinger distance between two uncorrelated variables to be close to zero.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 100000\nx, y = rand(rng, n), rand(rng, n)\nest = JointProbabilities(HellingerDistance(), CodifyVariables(OrdinalPatterns(m=3)))\ndiv_hd = association(est, x, y) # pretty close to zero","category":"page"},{"location":"examples/examples_associations/#[KLDivergence](@ref)","page":"Associations","title":"KLDivergence","text":"","category":"section"},{"location":"examples/examples_associations/#example_KLDivergence_precomputed_probabilities","page":"Associations","title":"From precomputed probabilities","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\n# From pre-computed PMFs\np1 = Probabilities([0.1, 0.5, 0.2, 0.2])\np2 = Probabilities([0.3, 0.3, 0.2, 0.2])\nassociation(KLDivergence(), p1, p2)","category":"page"},{"location":"examples/examples_associations/#example_KLDivergence_JointProbabilities_OrdinalPatterns","page":"Associations","title":"JointProbabilities + OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We expect the KLDivergence between two uncorrelated variables to be close to zero.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 100000\nx, y = rand(rng, n), rand(rng, n)\nest = JointProbabilities(KLDivergence(), CodifyVariables(OrdinalPatterns(m=3)))\ndiv_hd = association(est, x, y) # pretty close to zero","category":"page"},{"location":"examples/examples_associations/#[RenyiDivergence](@ref)","page":"Associations","title":"RenyiDivergence","text":"","category":"section"},{"location":"examples/examples_associations/#example_RenyiDivergence_precomputed_probabilities","page":"Associations","title":"From precomputed probabilities","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\n# From pre-computed PMFs\np1 = Probabilities([0.1, 0.5, 0.2, 0.2])\np2 = Probabilities([0.3, 0.3, 0.2, 0.2])\nassociation(RenyiDivergence(), p1, p2)","category":"page"},{"location":"examples/examples_associations/#example_RenyiDivergence_JointProbabilities_OrdinalPatterns","page":"Associations","title":"JointProbabilities + OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We expect the RenyiDivergence between two uncorrelated variables to be close to zero.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 100000\nx, y = rand(rng, n), rand(rng, n)\nest = JointProbabilities(RenyiDivergence(), CodifyVariables(OrdinalPatterns(m=3)))\ndiv_hd = association(est, x, y) # pretty close to zero","category":"page"},{"location":"examples/examples_associations/#[VariationDistance](@ref)","page":"Associations","title":"VariationDistance","text":"","category":"section"},{"location":"examples/examples_associations/#example_VariationDistance_precomputed_probabilities","page":"Associations","title":"From precomputed probabilities","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\n# From pre-computed PMFs\np1 = Probabilities([0.1, 0.5, 0.2, 0.2])\np2 = Probabilities([0.3, 0.3, 0.2, 0.2])\nassociation(VariationDistance(), p1, p2)","category":"page"},{"location":"examples/examples_associations/#example_VariationDistance_JointProbabilities_OrdinalPatterns","page":"Associations","title":"JointProbabilities + OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We expect the VariationDistance between two uncorrelated variables to be close to zero.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 100000\nx, y = rand(rng, n), rand(rng, n)\nest = JointProbabilities(VariationDistance(), CodifyVariables(OrdinalPatterns(m=3)))\ndiv_hd = association(est, x, y) # pretty close to zero","category":"page"},{"location":"examples/examples_associations/#[JointEntropyShannon](@ref)","page":"Associations","title":"JointEntropyShannon","text":"","category":"section"},{"location":"examples/examples_associations/#example_JointEntropyShannon_Dispersion","page":"Associations","title":"JointProbabilities with Dispersion","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y = rand(rng, 100), rand(rng, 100)\nmeasure = JointEntropyShannon()\ndiscretization = CodifyVariables(Dispersion(m = 2, c = 3))\nest = JointProbabilities(measure, discretization)\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#[JointEntropyTsallis](@ref)","page":"Associations","title":"JointEntropyTsallis","text":"","category":"section"},{"location":"examples/examples_associations/#example_JointEntropyTsallis_OrdinalPatterns","page":"Associations","title":"JointProbabilities with OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y = rand(rng, 100), rand(rng, 100)\nmeasure = JointEntropyTsallis()\ndiscretization = CodifyVariables(OrdinalPatterns(m = 3))\nest = JointProbabilities(measure, discretization)\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#[JointEntropyRenyi](@ref)","page":"Associations","title":"JointEntropyRenyi","text":"","category":"section"},{"location":"examples/examples_associations/#example_JointEntropyRenyi_ValueBinning","page":"Associations","title":"JointProbabilities with OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y = rand(rng, 100), rand(rng, 100)\nmeasure = JointEntropyRenyi(q = 0.5)\ndiscretization = CodifyVariables(ValueBinning(2))\nest = JointProbabilities(measure, discretization)\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#[ConditionalEntropyShannon](@ref)","page":"Associations","title":"ConditionalEntropyShannon","text":"","category":"section"},{"location":"examples/examples_associations/#example_ConditionalEntropyShannon_analytical","page":"Associations","title":"Analytical examples","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"This is essentially example 2.2.1 in Cover & Thomas (2006), where they use the following relative frequency table as an example. Notethat Julia is column-major, so we need to transpose their example. Then their X is in the first dimension of our table (along columns) and their Y is our second dimension (rows).","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nfreqs_yx = [1//8 1//16 1//32 1//32; \n    1//16 1//8  1//32 1//32;\n    1//16 1//16 1//16 1//16; \n    1//4  0//1  0//1  0//1];\n# `freqs_yx` is already normalized, se we can feed it directly to `Probabilities`\npxy = Probabilities(freqs_yx)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The marginal distribution for x (first dimension) is","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"marginal(pxy, dims = 2)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The marginal distribution for y (second dimension) is","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"marginal(pxy, dims = 1)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"And the Shannon conditional entropy H^S(X  Y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"ce_x_given_y = association(ConditionalEntropyShannon(), pxy) |> Rational","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"This is the same as in their example. Hooray! To compute H^S(Y  X), we just need to flip the contingency matrix.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"pyx = Probabilities(transpose(freqs_yx))\nce_y_given_x = association(ConditionalEntropyShannon(), pyx) |> Rational","category":"page"},{"location":"examples/examples_associations/#example_ConditionalEntropyShannon_JointProbabilities_CodifyVariables_UniqueElements","page":"Associations","title":"JointProbabilities + CodifyVariables + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We can of course also estimate conditional entropy from data. To do so, we'll use the  JointProbabilities estimator, which constructs a multivariate PMF for us. Thus, we don't explicitly need a set of counts, like in the example above, because they are estimated under the hood for us. ","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Let's first demonstrate on some categorical data. For that, we must use UniqueElements as the discretization (i.e. just count unique elements).","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 1000\nrating = rand(rng, 1:6, n)\nmovie = rand(rng, [\"The Witcher: the movie\", \"Lord of the Rings\"], n)\n\ndisc = CodifyVariables(UniqueElements())\nest = JointProbabilities(ConditionalEntropyShannon(), disc)\nassociation(est, rating, movie)","category":"page"},{"location":"examples/examples_associations/#example_ConditionalEntropyShannon_JointProbabilities_CodifyPoints_UniqueElementsEncoding","page":"Associations","title":"JointProbabilities + CodifyPoints + UniqueElementsEncoding","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y, z = rand(rng, 1:5, 100), rand(rng, 1:5, 100), rand(rng, 1:3, 100)\nX = StateSpaceSet(x, z)\nY = StateSpaceSet(y, z)\ndisc = CodifyPoints(UniqueElementsEncoding(X), UniqueElementsEncoding(Y));\nest = JointProbabilities(ConditionalEntropyShannon(), disc);\nassociation(est, X, Y)","category":"page"},{"location":"examples/examples_associations/#[ConditionalEntropyTsallisAbe](@ref)","page":"Associations","title":"ConditionalEntropyTsallisAbe","text":"","category":"section"},{"location":"examples/examples_associations/#example_ConditionalEntropyTsallisAbe_JointProbabilities_CodifyVariables_UniqueElements","page":"Associations","title":"JointProbabilities + CodifyVariables + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We'll here repeat the analysis we did for ConditionalEntropyShannon above.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 1000\nrating = rand(rng, 1:6, n)\nmovie = rand(rng, [\"The Witcher: the movie\", \"Lord of the Rings\"], n)\n\ndisc = CodifyVariables(UniqueElements())\nest = JointProbabilities(ConditionalEntropyTsallisAbe(q =1.5), disc)\nassociation(est, rating, movie)","category":"page"},{"location":"examples/examples_associations/#example_ConditionalEntropyTsallisAbe_JointProbabilities_CodifyPoints_UniqueElementsEncoding","page":"Associations","title":"JointProbabilities + CodifyPoints + UniqueElementsEncoding","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y, z = rand(rng, 1:5, 100), rand(rng, 1:5, 100), rand(rng, 1:3, 100)\nX = StateSpaceSet(x, z)\nY = StateSpaceSet(y, z)\ndisc = CodifyPoints(UniqueElementsEncoding(X), UniqueElementsEncoding(Y));\nest = JointProbabilities(ConditionalEntropyTsallisAbe(q = 1.5), disc);\nassociation(est, X, Y)","category":"page"},{"location":"examples/examples_associations/#[ConditionalEntropyTsallisFuruichi](@ref)","page":"Associations","title":"ConditionalEntropyTsallisFuruichi","text":"","category":"section"},{"location":"examples/examples_associations/#example_ConditionalEntropyTsallisFuruichi_JointProbabilities_CodifyVariables_UniqueElements","page":"Associations","title":"JointProbabilities + CodifyVariables + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We'll here repeat the analysis we did for ConditionalEntropyShannon and ConditionalEntropyTsallisAbe above.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 1000\nrating = rand(rng, 1:6, n)\nmovie = rand(rng, [\"The Witcher: the movie\", \"Lord of the Rings\"], n)\n\ndisc = CodifyVariables(UniqueElements())\nest = JointProbabilities(ConditionalEntropyTsallisFuruichi(q =0.5), disc)\nassociation(est, rating, movie)","category":"page"},{"location":"examples/examples_associations/#example_ConditionalEntropyTsallisFuruichi_JointProbabilities_CodifyPoints_UniqueElementsEncoding","page":"Associations","title":"JointProbabilities + CodifyPoints + UniqueElementsEncoding","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y, z = rand(rng, 1:5, 100), rand(rng, 1:5, 100), rand(rng, 1:3, 100)\nX = StateSpaceSet(x, z)\nY = StateSpaceSet(y, z)\ndisc = CodifyPoints(UniqueElementsEncoding(X), UniqueElementsEncoding(Y));\nest = JointProbabilities(ConditionalEntropyTsallisFuruichi(q = 0.5), disc);\nassociation(est, X, Y)","category":"page"},{"location":"examples/examples_associations/#[MIShannon](@ref)","page":"Associations","title":"MIShannon","text":"","category":"section"},{"location":"examples/examples_associations/#example_MIShannon_JointProbabilities_ValueBinning","page":"Associations","title":"JointProbabilities + ValueBinning","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 1000)\ny = rand(rng, 1000)\ndiscretization = CodifyVariables(ValueBinning(FixedRectangularBinning(0, 1, 5)))\nest = JointProbabilities(MIShannon(), discretization)\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_JointProbabilities_UniqueElements","page":"Associations","title":"JointProbabilities + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The JointProbabilities estimator can also be used with categorical data. For example, let's compare the Shannon mutual information between the preferences of a population sample with regards to different foods.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nn = 1000\npreferences = rand([\"neutral\", \"like it\", \"hate it\"], n);\nrandom_foods = rand([\"water\", \"flour\", \"bananas\", \"booze\", \"potatoes\", \"beans\", \"soup\"], n)\nbiased_foods = map(preferences) do preference\n    if cmp(preference, \"neutral\") == 1\n        return rand([\"water\", \"flour\"])\n    elseif cmp(preference, \"like it\") == 1\n        return rand([\"bananas\", \"booze\"])\n    else\n        return rand([\"potatoes\", \"beans\", \"soup\"])\n    end\nend\n\nest = JointProbabilities(MIShannon(), UniqueElements())\nassociation(est, preferences, biased_foods), association(est, preferences, random_foods)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_GaussianMI","page":"Associations","title":"Dedicated GaussianMI estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Statistics\n\nn = 1000\nusing Associations\nx = randn(1000)\ny = rand(1000) .+ x\nassociation(GaussianMI(MIShannon()), x, y) # defaults to `MIShannon()`","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_KSG1","page":"Associations","title":"Dedicated KraskovStögbauerGrassberger1 estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nx, y = rand(1000), rand(1000)\nassociation(KSG1(MIShannon(); k = 5), x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_KSG2","page":"Associations","title":"Dedicated KraskovStögbauerGrassberger2 estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nx, y = rand(1000), rand(1000)\nassociation(KSG2(MIShannon(); k = 5), x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_GaoKannanOhViswanath","page":"Associations","title":"Dedicated GaoKannanOhViswanath estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nx, y = rand(1000), rand(1000)\nassociation(GaoKannanOhViswanath(MIShannon(); k = 10), x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_EntropyDecomposition_Kraskov","page":"Associations","title":"EntropyDecomposition + Kraskov","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We can compute MIShannon by naively applying a DifferentialInfoEstimator. Note that this doesn't apply any bias correction.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nx, y = rand(1000), rand(1000)\nassociation(EntropyDecomposition(MIShannon(), Kraskov(k = 3)), x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_EntropyDecomposition_BubbleSortSwaps","page":"Associations","title":"EntropyDecomposition + BubbleSortSwaps","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We can also compute MIShannon by naively applying a DiscreteInfoEstimator. Note that this doesn't apply any bias correction.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nx, y = rand(1000), rand(1000)\ndisc = CodifyVariables(BubbleSortSwaps(m=5))\nhest = PlugIn(Shannon())\nassociation(EntropyDecomposition(MIShannon(), hest, disc), x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_EntropyDecomposition_Jackknife_ValueBinning","page":"Associations","title":"EntropyDecomposition + Jackknife + ValueBinning","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Shannon mutual information can be written as a sum of marginal entropy terms. Here, we use CodifyVariables with ValueBinning bin the data  and compute discrete Shannon mutual information.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 50)\ny = rand(rng, 50)\n\n# Use the H3-estimation method with a discrete visitation frequency based \n# probabilities estimator over a fixed grid covering the range of the data,\n# which is on [0, 1].\ndiscretization = CodifyVariables(ValueBinning(FixedRectangularBinning(0, 1, 5)))\nhest = Jackknife(Shannon())\nest = EntropyDecomposition(MIShannon(), hest, discretization)\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIShannon_reproducing_Kraskov","page":"Associations","title":"Reproducing Kraskov et al. (2004)","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Here, we'll reproduce Figure 4 from Kraskov et al. (2004)'s seminal paper on the nearest-neighbor based mutual information estimator. We'll estimate the mutual information between marginals of a bivariate Gaussian for a fixed time series length of 1000, varying the number of neighbors. Note: in the original paper, they show multiple curves corresponding to different time series length. We only show two single curves: one for the KraskovStögbauerGrassberger1 estimator and one for the KraskovStögbauerGrassberger2 estimator.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing LinearAlgebra: det\nusing Distributions: MvNormal\nusing StateSpaceSets: StateSpaceSet\nusing CairoMakie\nusing Statistics\n\nN = 800\nc = 0.9\nΣ = [1 c; c 1]\nN2 = MvNormal([0, 0], Σ)\nmitrue = -0.5*log(det(Σ)) # in nats\nks = [2; 5; 7; 10:10:70] .* 2\n\nnreps = 10 # plot average over 10 independent realizations\nmis_ksg1 = zeros(nreps, length(ks))\nmis_ksg2 = zeros(nreps, length(ks))\nfor i = 1:nreps\n    D2 = StateSpaceSet([rand(N2) for i = 1:N])\n    X = D2[:, 1] |> StateSpaceSet\n    Y = D2[:, 2] |> StateSpaceSet\n    for (j, k) in enumerate(ks)\n        est1 = KSG1(MIShannon(; base = ℯ); k)\n        est2 = KSG2(MIShannon(; base = ℯ); k)\n        mis_ksg1[i, j] = association(est1, X, Y)\n        mis_ksg2[i, j] = association(est2, X, Y)\n    end\nend\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"k / N\", ylabel = \"Mutual infomation (nats)\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg1, dims = 1) |> vec, label = \"KSG1\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg2, dims = 1) |> vec, label = \"KSG2\")\nhlines!(ax, [mitrue], color = :black, linewidth = 3, label = \"I (true)\")\naxislegend()\nfig","category":"page"},{"location":"examples/examples_associations/#Estimator-comparison-for-[MIShannon](@ref)","page":"Associations","title":"Estimator comparison for MIShannon","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Most estimators suffer from significant bias when applied to discrete, finite data. One possible resolution is to add a small amount of noise to discrete variables, so that the data becomes continuous in practice.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"But instead of adding noise to your data, you can also consider using an estimator that is specifically designed to deal with continuous-discrete mixture data.  One example is the GaoKannanOhViswanath estimator. Below, we compare its performance to KraskovStögbauerGrassberger1 on uniformly distributed discrete multivariate data. The true mutual information is zero. While the \"naive\" KraskovStögbauerGrassberger1 estimator  diverges from the true value for these data, the GaoKannanOhViswanath converges to the true value.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Statistics\nusing StateSpaceSets: StateSpaceSet\nusing Statistics: mean\nusing CairoMakie\n\nfunction compare_ksg_gkov(;\n        k = 5,\n        base = 2,\n        nreps = 10,\n        Ls = [500:100:1000; 1500; 2500; 5000; 7000])\n\n\n    mis_ksg1_mix = zeros(nreps, length(Ls))\n    mis_ksg1_discrete = zeros(nreps, length(Ls))\n    mis_ksg1_cont = zeros(nreps, length(Ls))\n    mis_gkov_mix = zeros(nreps, length(Ls))\n    mis_gkov_discrete = zeros(nreps, length(Ls))\n    mis_gkov_cont = zeros(nreps, length(Ls))\n\n    for (j, L) in enumerate(Ls)\n        for i = 1:nreps\n            X = StateSpaceSet(float.(rand(1:8, L, 2)))\n            Y = StateSpaceSet(float.(rand(1:8, L, 2)))\n            Z = StateSpaceSet(rand(L, 2))\n            W = StateSpaceSet(rand(L, 2))\n            est_gkov = GaoKannanOhViswanath(MIShannon(; base = ℯ); k)\n            est_ksg1 = KSG1(MIShannon(; base = ℯ); k)\n            mis_ksg1_discrete[i, j] = association(est_ksg1, X, Y)\n            mis_gkov_discrete[i, j] = association(est_gkov, X, Y)\n            mis_ksg1_mix[i, j] = association(est_ksg1, X, Z)\n            mis_gkov_mix[i, j] = association(est_gkov, X, Z)\n            mis_ksg1_cont[i, j] = association(est_ksg1, Z, W)\n            mis_gkov_cont[i, j] = association(est_gkov, Z, W)\n        end\n    end\n    return mis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n        mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont\nend\n\nfig = Figure()\nax = Axis(fig[1, 1], \n    xlabel = \"Sample size\", \n    ylabel = \"Mutual information (bits)\")\nLs = [100; 200; 500; 1000; 2500; 5000; 7000]\nnreps = 5\nk = 3\nmis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n    mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont = \n    compare_ksg_gkov(; nreps, k, Ls)\n\nscatterlines!(ax, Ls, mean(mis_ksg1_mix, dims = 1) |> vec, \n    label = \"KSG1 (mixed)\", color = :black, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_ksg1_discrete, dims = 1) |> vec, \n    label = \"KSG1 (discrete)\", color = :black, \n    linestyle = :dash, marker = '▲')\nscatterlines!(ax, Ls, mean(mis_ksg1_cont, dims = 1) |> vec, \n    label = \"KSG1 (continuous)\", color = :black, \n    linestyle = :dot, marker = '●')\nscatterlines!(ax, Ls, mean(mis_gkov_mix, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (mixed)\", color = :red, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_gkov_discrete, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (discrete)\", color = :red, \n    linestyle = :dash, marker = '▲')\nscatterlines!(ax, Ls, mean(mis_gkov_cont, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (continuous)\", color = :red, \n    linestyle = :dot, marker = '●')\naxislegend(position = :rb)\nfig","category":"page"},{"location":"examples/examples_associations/#Estimation-using-[DifferentialInfoEstimator](@ref)s:-a-comparison","page":"Associations","title":"Estimation using DifferentialInfoEstimators: a comparison","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Let's compare the performance of a subset of the implemented mutual information estimators. We'll use example data from Lord et al., where the analytical mutual information is known.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing LinearAlgebra: det\nusing StateSpaceSets: StateSpaceSet\nusing Distributions: MvNormal\nusing LaTeXStrings\nusing CairoMakie\n\n# adapted from https://juliadatascience.io/makie_colors\nfunction new_cycle_theme()\n    # https://nanx.me/ggsci/reference/pal_locuszoom.html\n    my_colors = [\"#D43F3AFF\", \"#EEA236FF\", \"#5CB85CFF\", \"#46B8DAFF\",\n        \"#357EBDFF\", \"#9632B8FF\", \"#B8B8B8FF\"]\n    cycle = Cycle([:color, :linestyle, :marker], covary=true) # alltogether\n    my_markers = [:circle, :rect, :utriangle, :dtriangle, :diamond,\n        :pentagon, :cross, :xcross]\n    my_linestyle = [nothing, :dash, :dot, :dashdot, :dashdotdot]\n    return Theme(\n        fontsize = 22, font=\"CMU Serif\",\n        colormap = :linear_bmy_10_95_c78_n256,\n        palette = (\n            color = my_colors, \n            marker = my_markers, \n            linestyle = my_linestyle,\n        ),\n        Axis = (\n            backgroundcolor= (:white, 0.2), \n            xgridstyle = :dash, \n            ygridstyle = :dash\n        ),\n        Lines = (\n            cycle= cycle,\n        ), \n        ScatterLines = (\n            cycle = cycle,\n        ),\n        Scatter = (\n            cycle = cycle,\n        ),\n        Legend = (\n            bgcolor = (:grey, 0.05), \n            framecolor = (:white, 0.2),\n            labelsize = 13,\n        )\n    )\nend\n\nrun(est; f::Function, # function that generates data\n        base::Real = ℯ, \n        nreps::Int = 10, \n        αs = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], \n        n::Int = 1000) =\n    map(α -> association(est, f(α, n)...), αs)\n\nfunction compute_results(f::Function; estimators, k = 5, k_lord = 20,\n        n = 1000, base = ℯ, nreps = 10,\n        as = 7:-1:0,\n        αs = [1/10^(a) for a in as])\n    \n    is = [zeros(length(αs)) for est in estimators]\n    for (k, est) in enumerate(estimators)\n        tmp = zeros(length(αs))\n        for i = 1:nreps\n            tmp .+= run(est; f = f, αs, base, n)\n        end\n        is[k] .= tmp ./ nreps\n    end\n\n    return is\nend\n\nfunction plot_results(f::Function, ftrue::Function; \n        base, estimators, k_lord, k, \n        as = 7:-1:0, αs = [1/10^(a) for a in as], kwargs...\n    )\n    is = compute_results(f; \n        base, estimators, k_lord, k, as, αs, kwargs...)\n    itrue = [ftrue(α; base) for α in αs]\n\n    xmin, xmax = minimum(αs), maximum(αs)\n    \n    ymin = floor(Int, min(minimum(itrue), minimum(Iterators.flatten(is))))\n    ymax = ceil(Int, max(maximum(itrue), maximum(Iterators.flatten(is))))\n    f = Figure()\n    ax = Axis(f[1, 1],\n        xlabel = \"α\", ylabel = \"I (nats)\",\n        xscale = log10, aspect = 1,\n        xticks = (αs, [latexstring(\"10^{$(-a)}\") for a in as]),\n        yticks = (ymin:ymax)\n        )\n    xlims!(ax, (1/10^first(as), 1/10^last(as)))\n    ylims!(ax, (ymin, ymax))\n    lines!(ax, αs, itrue, \n        label = \"I (true)\", linewidth = 4, color = :black)\n    for (i, est) in enumerate(estimators)\n        if est isa EntropyDecomposition\n            es = typeof(est.est).name.name |> String\n        else\n            es = typeof(est).name.name |> String\n        end\n        @show es\n        lbl = occursin(\"Lord\", es) ? \"$es (k = $k_lord)\" : \"$es (k = $k)\"\n        scatter!(ax, αs, is[i], label = lbl)\n        lines!(ax, αs, is[i])\n\n    end\n    axislegend()\n    return f\nend\n\nset_theme!(new_cycle_theme())\nk_lord = 20\nk = 5\nbase = ℯ\n\ndef = MIShannon(base = ℯ)\nestimators = [\n    EntropyDecomposition(def, Kraskov(; k)),\n    EntropyDecomposition(def, KozachenkoLeonenko()),\n    EntropyDecomposition(def, Zhu(; k)),\n    EntropyDecomposition(def, ZhuSingh(; k)),\n    EntropyDecomposition(def, Gao(; k)),\n    EntropyDecomposition(def, Lord(; k = k_lord)),\n    EntropyDecomposition(def, LeonenkoProzantoSavani(Shannon(); k)),\n    KSG1(def; k),\n    KSG2(def; k),\n    GaoOhViswanath(def; k),\n    GaoKannanOhViswanath(def; k),\n    GaussianMI(def),\n];","category":"page"},{"location":"examples/examples_associations/#Example-system:-family-1","page":"Associations","title":"Example system: family 1","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"In this system, samples are concentrated around the diagonal X = Y, and the strip of samples gets thinner as alpha to 0.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"function family1(α, n::Int)\n    x = rand(n)\n    v = rand(n)\n    y = x + α * v\n    return StateSpaceSet(x), StateSpaceSet(y)\nend\n\n# True mutual information values for these data\nfunction ifamily1(α; base = ℯ)\n    mi = -log(α) - α - log(2)\n    return mi / log(base, ℯ)\nend\n\nfig = plot_results(family1, ifamily1; \n    k_lord = k_lord, k = k, nreps = 10, n = 800,\n    estimators = estimators,\n    base = base)","category":"page"},{"location":"examples/examples_associations/#Example-system:-family-2","page":"Associations","title":"Example system: family 2","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"function family2(α, n::Int)\n    Σ = [1 α; α 1]\n    N2 = MvNormal(zeros(2), Σ)\n    D2 = StateSpaceSet([rand(N2) for i = 1:n])\n    X = StateSpaceSet(D2[:, 1])\n    Y = StateSpaceSet(D2[:, 2])\n    return X, Y\nend\n\nfunction ifamily2(α; base = ℯ)\n    return (-0.5 * log(1 - α^2)) / log(ℯ, base)\nend\n\nαs = 0.05:0.05:0.95\nestimators = estimators\nwith_theme(new_cycle_theme()) do\n    f = Figure();\n    ax = Axis(f[1, 1], xlabel = \"α\", ylabel = \"I (nats)\")\n    is_true = map(α -> ifamily2(α), αs)\n    is_est = map(est -> run(est; f = family2, αs, nreps = 20), estimators)\n    lines!(ax, αs, is_true, \n        label = \"I (true)\", color = :black, linewidth = 3)\n    for (i, est) in enumerate(estimators)\n        if est isa EntropyDecomposition\n            estname = typeof(est.est).name.name |> String\n        else\n            estname = typeof(est).name.name |> String\n        end\n        scatterlines!(ax, αs, is_est[i], label = estname)\n    end\n    axislegend(position = :lt)\n    return f\nend","category":"page"},{"location":"examples/examples_associations/#Example-system:-family-3","page":"Associations","title":"Example system: family 3","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"In this system, we draw samples from a 4D Gaussian distribution distributed as specified in the ifamily3 function below. We let X be the two first variables, and Y be the two last variables.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"function ifamily3(α; base = ℯ)\n    Σ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+α]\n    Σx = Σ[1:2, 1:2]; Σy = Σ[3:4, 3:4]\n    mi = 0.5*log(det(Σx) * det(Σy) / det(Σ))\n    return mi / log(ℯ, base)\nend\n\nfunction family3(α, n::Int)\n    Σ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+α]\n    N4 = MvNormal(zeros(4), Σ)\n    D4 = StateSpaceSet([rand(N4) for i = 1:n])\n    X = D4[:, 1:2]\n    Y = D4[:, 3:4]\n    return X, Y\nend\n\nfig = plot_results(family3, ifamily3; \n    k_lord = k_lord, k = k, nreps = 5, n = 800,\n    estimators = estimators, base = base)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We see that the Lord estimator, which estimates local volume elements using a singular-value decomposition (SVD) of local neighborhoods, outperforms the other estimators by a large margin.","category":"page"},{"location":"examples/examples_associations/#[MIRenyiJizba](@ref)","page":"Associations","title":"MIRenyiJizba","text":"","category":"section"},{"location":"examples/examples_associations/#example_MIRenyiJizba_JointProbabilities_UniqueElements","page":"Associations","title":"JointProbabilities + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"MIRenyiJizba can be estimated for categorical data using JointProbabilities estimator with the UniqueElements outcome space.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, [\"a\", \"b\", \"c\"], 200);\ny = rand(rng, [\"hello\", \"yoyo\", \"heyhey\"], 200);\nest = JointProbabilities(MIRenyiJizba(), UniqueElements())\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIRenyiJizba_JointProbabilities_LeonenkoProzantoSavani","page":"Associations","title":"EntropyDecomposition + LeonenkoProzantoSavani","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"MIRenyiJizba can also estimated for numerical data using EntropyDecomposition in combination with any DifferentialInfoEstimator capable of estimating differential  Renyi entropy.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = randn(rng, 50); y = randn(rng, 50);\ndef = MIRenyiJizba()\nest_diff = EntropyDecomposition(def, LeonenkoProzantoSavani(Renyi(), k=3))\nassociation(est_diff, x, y) ","category":"page"},{"location":"examples/examples_associations/#example_MIRenyiJizba_EntropyDecomposition_ValueBinning","page":"Associations","title":"EntropyDecomposition + LeonenkoProzantoSavani","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"MIRenyiJizba can also estimated for numerical data using EntropyDecomposition in combination with any DiscreteInfoEstimator capable of estimating differential  Renyi entropy over some OutcomeSpace, e.g. ValueBinning.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = randn(rng, 50); y = randn(rng, 50);\ndef = MIRenyiJizba()\n\ndisc = CodifyVariables(ValueBinning(2))\nest_disc = EntropyDecomposition(def, PlugIn(Renyi()), disc);\nassociation(est_disc, x, y)","category":"page"},{"location":"examples/examples_associations/#[MIRenyiSarbu](@ref)","page":"Associations","title":"MIRenyiSarbu","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"MIRenyiSarbu can be estimated using the JointProbabilities estimator  in combination with any CodifyVariables or CodifyPoints discretization scheme.","category":"page"},{"location":"examples/examples_associations/#example_MIRenyiSarbu_JointProbabilities_UniqueElements","page":"Associations","title":"JointProbabilities + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, [\"a\", \"b\", \"c\"], 200)\ny = rand(rng, [\"hello\", \"yoyo\", \"heyhey\"], 200)\n\nest = JointProbabilities(MIRenyiSarbu(), CodifyVariables(UniqueElements()))\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#example_MIRenyiSarbu_JointProbabilities_CosineSimilarityBinning","page":"Associations","title":"JointProbabilities + CosineSimilarityBinning","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 200)\ny = rand(rng, 200)\n\nest = JointProbabilities(MIRenyiSarbu(), CodifyVariables(CosineSimilarityBinning()))\nassociation(est, x, y)","category":"page"},{"location":"examples/examples_associations/#[MITsallisFuruichi](@ref)","page":"Associations","title":"MITsallisFuruichi","text":"","category":"section"},{"location":"examples/examples_associations/#example_MITsallisFuruichi_JointProbabilities_UniqueElements","page":"Associations","title":"JointProbabilities + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"MITsallisFuruichi can be estimated using the JointProbabilities estimator  in combination with any CodifyVariables or CodifyPoints discretization scheme.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 200)\ny = rand(rng, 200)\n\nest = JointProbabilities(MITsallisFuruichi(q = 0.3), UniqueElements())\nassociation(est, x, y) ","category":"page"},{"location":"examples/examples_associations/#example_MITsallisFuruichi_EntropyDecomposition_LeonenkoProzantoSavani","page":"Associations","title":"EntropyDecomposition + LeonenkoProzantoSavani","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 200)\ny = rand(rng, 200)\n\nest_diff = EntropyDecomposition(MITsallisFuruichi(), LeonenkoProzantoSavani(Tsallis(q= 2)))\nassociation(est_diff, x, y)","category":"page"},{"location":"examples/examples_associations/#example_MITsallisFuruichi_EntropyDecomposition_Dispersion","page":"Associations","title":"EntropyDecomposition + Dispersion","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 200)\ny = rand(rng, 200)\ndisc = CodifyVariables(Dispersion())\nest_disc = EntropyDecomposition(MITsallisFuruichi(), PlugIn(Tsallis()), disc)\n\nassociation(est_disc, x, y)","category":"page"},{"location":"examples/examples_associations/#[MITsallisMartin](@ref)","page":"Associations","title":"MITsallisMartin","text":"","category":"section"},{"location":"examples/examples_associations/#example_MITsallisMartin_JointProbabilities_UniqueElements","page":"Associations","title":"JointProbabilities + UniqueElements","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 200)\ny = rand(rng, 200)\n\nest = JointProbabilities(MITsallisMartin(q = 1.5), UniqueElements())\nassociation(est, x, y) ","category":"page"},{"location":"examples/examples_associations/#example_MITsallisMartin_EntropyDecomposition_LeonenkoProzantoSavani","page":"Associations","title":"EntropyDecomposition + LeonenkoProzantoSavani","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"MITsallisMartin can be estimated using a decomposition into entropy  terms using EntropyDecomposition with any compatible estimator  that can estimate differential Tsallis entropy. ","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 500)\ny = rand(rng, 500)\n\nest_diff = EntropyDecomposition(MITsallisMartin(), LeonenkoProzantoSavani(Tsallis(q= 1.5)))\nassociation(est_diff, x, y)","category":"page"},{"location":"examples/examples_associations/#example_MITsallisMartin_EntropyDecomposition_OrdinalPatterns","page":"Associations","title":"EntropyDecomposition + OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 200)\ny = rand(rng, 200)\ndisc = CodifyVariables(OrdinalPatterns())\nest_disc = EntropyDecomposition(MITsallisMartin(), PlugIn(Tsallis()), disc)\n\nassociation(est_disc, x, y)","category":"page"},{"location":"examples/examples_associations/#[CMIShannon](@ref)","page":"Associations","title":"CMIShannon","text":"","category":"section"},{"location":"examples/examples_associations/#example_CMIShannon_GaussianCMI","page":"Associations","title":"CMIShannon with GaussianCMI","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = randn(1000)\ny = randn(1000) .+ x\nz = randn(1000) .+ y\nassociation(GaussianCMI(), x, z, y) # defaults to `CMIShannon()`","category":"page"},{"location":"examples/examples_associations/#example_CMIShannon_FPVP","page":"Associations","title":"CMIShannon with FPVP","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = rand(Normal(-1, 0.5), n)\ny = rand(BetaPrime(0.5, 1.5), n) .+ x\nz = rand(Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\nassociation(FPVP(k = 5), x, z, y) # defaults to `CMIShannon()`","category":"page"},{"location":"examples/examples_associations/#[CMIShannon](@ref)-with-[MesnerShalizi](@ref)","page":"Associations","title":"CMIShannon with MesnerShalizi","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Statistics\nusing Random; rng = Xoshiro(1234)\n\nn = 1000\n# A chain X → Y → Z\nx = rand(rng, Normal(-1, 0.5), n)\ny = rand(rng, BetaPrime(0.5, 1.5), n) .+ x\nz = rand(rng, Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\nassociation(MesnerShalizi(; k = 10), x, z, y) # defaults to `CMIShannon()`","category":"page"},{"location":"examples/examples_associations/#[CMIShannon](@ref)-with-[Rahimzamani](@ref)","page":"Associations","title":"CMIShannon with Rahimzamani","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Statistics\nusing Random; rng = Xoshiro(1234)\n\nn = 1000\n# A chain X → Y → Z\nx = rand(rng, Normal(-1, 0.5), n)\ny = rand(rng, BetaPrime(0.5, 1.5), n) .+ x\nz = rand(rng, Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\nassociation(Rahimzamani(CMIShannon(base = 10); k = 10), x, z, y)","category":"page"},{"location":"examples/examples_associations/#example_CMIShannon_MIDecomposition","page":"Associations","title":"MIDecomposition","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Shannon-type conditional mutual information can be decomposed as a sum of  mutual information terms, which we can each estimate with any dedicated MutualInformationEstimator estimator.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 300)\ny = rand(rng, 300) .+ x\nz = rand(rng, 300) .+ y\n\nest = MIDecomposition(CMIShannon(), KSG1(MIShannon(base = 2), k = 3))\nassociation(est, x, z, y) # should be near 0 (and can be negative)","category":"page"},{"location":"examples/examples_associations/#[CMIRenyiPoczos](@ref)","page":"Associations","title":"CMIRenyiPoczos","text":"","category":"section"},{"location":"examples/examples_associations/#CMIRenyiPoczos_PoczosSchneiderCMI","page":"Associations","title":"PoczosSchneiderCMI","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Statistics\nusing Random; rng = Xoshiro(1234)\n\nn = 1000\n# A chain X → Y → Z\nx = rand(rng, Normal(-1, 0.5), n)\ny = rand(rng, BetaPrime(0.5, 1.5), n) .+ x\nz = rand(rng, Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\nest = PoczosSchneiderCMI(CMIRenyiPoczos(base = 2, q = 1.2); k = 5)\nassociation(est, x, z, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"In addition to the dedicated ConditionalMutualInformationEstimators, any MutualInformationEstimator can also be used to compute conditional mutual information using the chain rule of mutual information. However, the naive application of these estimators don't perform any bias correction when taking the difference of mutual information terms.","category":"page"},{"location":"examples/examples_associations/#[CMIShannon](@ref)-2","page":"Associations","title":"CMIShannon","text":"","category":"section"},{"location":"examples/examples_associations/#example_CMIShannon_MIDecomposition_KSG1","page":"Associations","title":"MIDecomposition + KraskovStögbauerGrassberger1","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = rand(Normal(-1, 0.5), n)\ny = rand(BetaPrime(0.5, 1.5), n) .+ x\nz = rand(Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\nest = MIDecomposition(CMIShannon(base = 2), KSG1(k = 10))\nassociation(est, x, z, y)","category":"page"},{"location":"examples/examples_associations/#example_CMIShannon_EntropyDecomposition_Kraskov","page":"Associations","title":"EntropyDecomposition + Kraskov","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Any DifferentialInfoEstimator can also be used to compute conditional mutual information using a sum of entropies. For that, we  usethe EntropyDecomposition estimator. No bias correction is applied for  EntropyDecomposition either.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Random; rng = Xoshiro(1234)\nn = 500\n# A chain X → Y → Z\nx = rand(rng, Epanechnikov(0.5, 1.0), n)\ny = rand(rng, Normal(0, 0.2), n) .+ x\nz = rand(rng, FDist(3, 2), n)\nest = EntropyDecomposition(CMIShannon(), Kraskov(k = 5))\nassociation(est, x, z, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Any DiscreteInfoEstimator that computes entropy can also be used to compute conditional mutual information using a sum of entropies. For that, we also use EntropyDecomposition. In the discrete case, we also have to specify a discretization (an OutcomeSpace).","category":"page"},{"location":"examples/examples_associations/#example_CMIShannon_EntropyDecomposition_ValueBinning","page":"Associations","title":"EntropyDecomposition + ValueBinning","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Distributions\nusing Random; rng = Xoshiro(1234)\nn = 500\n# A chain X → Y → Z\nx = rand(rng, Epanechnikov(0.5, 1.0), n)\ny = rand(rng, Normal(0, 0.2), n) .+ x\nz = rand(rng, FDist(3, 2), n)\ndiscretization = CodifyVariables(ValueBinning(RectangularBinning(5)))\nhest = PlugIn(Shannon())\nest = EntropyDecomposition(CMIShannon(), hest, discretization)\nassociation(est, x, y, z)","category":"page"},{"location":"examples/examples_associations/#[CMIRenyiJizba](@ref)","page":"Associations","title":"CMIRenyiJizba","text":"","category":"section"},{"location":"examples/examples_associations/#example_CMIRenyiJizba_JointProbabilities_BubbleSortSwaps","page":"Associations","title":"JointProbabilities + BubbleSortSwaps","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 100)\ny = x .+ rand(rng, 100)\nz = y .+ rand(rng, 100)\ndisc = CodifyVariables(BubbleSortSwaps(m = 4))\nest = JointProbabilities(CMIRenyiJizba(), disc)\nassociation(est, x, z, y)","category":"page"},{"location":"examples/examples_associations/#example_CMIRenyiJizba_EntropyDecomposition_LeonenkoProzantoSavani","page":"Associations","title":"EntropyDecomposition + LeonenkoProzantoSavani","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y, z = rand(rng, 1000), rand(rng, 1000), rand(rng, 1000)\ndef = CMIRenyiJizba(q = 1.5)\n\n# Using a differential Rényi entropy estimator\nest = EntropyDecomposition(def, LeonenkoProzantoSavani(Renyi(), k = 10))\nassociation(est, x, y, z)","category":"page"},{"location":"examples/examples_associations/#example_CMIRenyiJizba_EntropyDecomposition_OrdinalPatterns","page":"Associations","title":"EntropyDecomposition + OrdinalPatterns","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx, y, z = rand(rng, 1000), rand(rng, 1000), rand(rng, 1000)\ndef = CMIRenyiJizba(q = 1.5)\n\n# Using a plug-in Rényi entropy estimator, discretizing using ordinal patterns.\nest = EntropyDecomposition(def, PlugIn(Renyi()), CodifyVariables(OrdinalPatterns(m=2)), RelativeAmount())\nassociation(est, x, y, z)","category":"page"},{"location":"examples/examples_associations/#[TEShannon](@ref)","page":"Associations","title":"TEShannon","text":"","category":"section"},{"location":"examples/examples_associations/#example_TEShannon_EntropyDecomposition_TransferOperator","page":"Associations","title":"EntropyDecomposition + TransferOperator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"For transfer entropy examples, we'll construct some time series for which  there is time-delayed forcing between variables.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"\nusing Associations\nusing DynamicalSystemsBase\nusing StableRNGs\nrng = StableRNG(123)\n\nBase.@kwdef struct Logistic4Chain{V, RX, RY, RZ, RW, C1, C2, C3, Σ1, Σ2, Σ3, RNG}\n    xi::V = [0.1, 0.2, 0.3, 0.4]\n    rx::RX = 3.9\n    ry::RY = 3.6\n    rz::RZ = 3.6\n    rw::RW = 3.8\n    c_xy::C1 = 0.4\n    c_yz::C2 = 0.4\n    c_zw::C3 = 0.35\n    σ_xy::Σ1 = 0.05\n    σ_yz::Σ2 = 0.05\n    σ_zw::Σ3 = 0.05\n    rng::RNG = Random.default_rng()\nend\n\nfunction eom_logistic4_chain(u, p::Logistic4Chain, t)\n    (; xi, rx, ry, rz, rw, c_xy, c_yz, c_zw, σ_xy, σ_yz, σ_zw, rng) = p\n    x, y, z, w = u\n    f_xy = (y +  c_xy*(x + σ_xy * rand(rng)) ) / (1 + c_xy*(1+σ_xy))\n    f_yz = (z +  c_yz*(y + σ_yz * rand(rng)) ) / (1 + c_yz*(1+σ_yz))\n    f_zw = (w +  c_zw*(z + σ_zw * rand(rng)) ) / (1 + c_zw*(1+σ_zw))\n    dx = rx * x * (1 - x)\n    dy = ry * (f_xy) * (1 - f_xy)\n    dz = rz * (f_yz) * (1 - f_yz)\n    dw = rw * (f_zw) * (1 - f_zw)\n    return SVector{4}(dx, dy, dz, dw)\nend\n\nfunction system(definition::Logistic4Chain)\n    return DiscreteDynamicalSystem(eom_logistic4_chain, definition.xi, definition)\nend\n\n# An example system where `X → Y → Z → W`.\nsys = system(Logistic4Chain(; rng))\nx, y, z, w = columns(first(trajectory(sys, 300, Ttr = 10000)))\n\nprecise = true # precise bin edges\ndiscretization = CodifyVariables(TransferOperator(RectangularBinning(2, precise))) #\nest_disc_to = EntropyDecomposition(TEShannon(), PlugIn(Shannon()), discretization);\nassociation(est_disc_to, x, y), association(est_disc_to, y, x)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The Shannon-type transfer entropy from x to y is stronger than from y to x, which is what we expect if x drives y.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"association(est_disc_to, x, z), association(est_disc_to, x, z, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The Shannon-type transfer entropy from x to z is stronger than the transfer entropy from x to z given y. This is expected, because x drives z through y, so \"conditioning away\" the effect of y should decrease the estimated  information transfer.","category":"page"},{"location":"examples/examples_associations/#example_TEShannon_CMIDecomposition","page":"Associations","title":"CMIDecomposition","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 1000)\ny = rand(rng, 1000) .+ x\nz = rand(rng, 1000) .+ y\n\n# Estimate transfer entropy by representing it as a CMI and using the `FPVP` estimator.\nest = CMIDecomposition(TEShannon(base = 2), FPVP(k = 3))\nassociation(est, x, z, y) # should be near 0 (and can be negative)","category":"page"},{"location":"examples/examples_associations/#example_TEShannon_SymbolicTransferEntropy","page":"Associations","title":"SymbolicTransferEntropy estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The SymbolicTransferEntropy estimator is just a convenience wrapper which utilizes CodifyVariableswith the OrdinalPatterns outcome space to  discretize the input time series before computing transfer entropy.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We'll use coupled time series from the logistic4 system above, where x → y → z → w. Thus, we expect that the association for the direction x → y is larger than for y → x. We also expect an association x → z, but the association should weaken when conditioning  on the intermediate value y.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing DynamicalSystemsBase\nusing Random; rng = Xoshiro(1234)\nsys = system(Logistic4Chain(; rng))\nx, y, z, w = columns(first(trajectory(sys, 300, Ttr = 10000)))\nest = SymbolicTransferEntropy(m = 5)\nassociation(est, x, y), association(est, y, x), association(est, x, z), association(est, x, z, y)","category":"page"},{"location":"examples/examples_associations/#example_TEShannon_estimator-comparison","page":"Associations","title":"Comparing different estimators ","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Let's reproduce Figure 4 from Zhu et al. (2015), where they test some dedicated transfer entropy estimators on a bivariate autoregressive system. We will test","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The Lindner and Zhu1 dedicated transfer entropy estimators,   which try to eliminate bias.\nThe KraskovStögbauerGrassberger1 estimator, which computes TE naively as a sum of mutual information   terms (without guaranteed cancellation of biases for the total sum).\nThe Kraskov estimator, which computes TE naively as a sum of entropy    terms (without guaranteed cancellation of biases for the total sum).","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\n\nfunction model2(n::Int)\n    𝒩x = Normal(0, 0.1)\n    𝒩y = Normal(0, 0.1)\n    x = zeros(n+2)\n    y = zeros(n+2)\n    x[1] = rand(𝒩x)\n    x[2] = rand(𝒩x)\n    y[1] = rand(𝒩y)\n    y[2] = rand(𝒩y)\n\n    for i = 3:n+2\n        x[i] = 0.45*sqrt(2)*x[i-1] - 0.9*x[i-2] - 0.6*y[i-2] + rand(𝒩x)\n        y[i] = 0.6*x[i-2] - 0.175*sqrt(2)*y[i-1] + 0.55*sqrt(2)*y[i-2] + rand(𝒩y)\n    end\n    return x[3:end], y[3:end]\nend\nte_true = 0.42 # eyeball the theoretical value from their Figure 4.\n\nm = TEShannon(embedding = EmbeddingTE(dT = 2, dS = 2), base = ℯ)\nestimators = [  \n    Zhu1(m, k = 8), \n    Lindner(m, k = 8), \n    MIDecomposition(m, KSG1(k = 8)),\n    EntropyDecomposition(m, Kraskov(k = 8)),\n]\nLs = [floor(Int, 2^i) for i in 8.0:0.5:11]\nnreps = 8\ntes_xy = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\ntes_yx = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\nfor (k, est) in enumerate(estimators)\n    for (i, L) in enumerate(Ls)\n        for j = 1:nreps\n            x, y = model2(L);\n            tes_xy[k][i][j] = association(est, x, y)\n            tes_yx[k][i][j] = association(est, y, x)\n        end\n    end\nend\n\nymin = minimum(map(x -> minimum(Iterators.flatten(Iterators.flatten(x))), (tes_xy, tes_yx)))\nestimator_names = [\"Zhu1\", \"Lindner\", \"KSG1\", \"Kraskov\"]\nls = [:dash, :dot, :dash, :dot]\nmr = [:rect, :hexagon, :xcross, :pentagon]\n\nfig = Figure(resolution = (800, 350))\nax_xy = Axis(fig[1,1], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"x → y\")\nax_yx = Axis(fig[1,2], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"y → x\")\nfor (k, e) in enumerate(estimators)\n    label = estimator_names[k]\n    marker = mr[k]\n    scatterlines!(ax_xy, Ls, mean.(tes_xy[k]); label, marker)\n    scatterlines!(ax_yx, Ls, mean.(tes_yx[k]); label, marker)\n    hlines!(ax_xy, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black) \n    hlines!(ax_yx, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black)\n    linkaxes!(ax_xy, ax_yx)\nend\naxislegend(ax_xy, position = :rb)\n\nfig","category":"page"},{"location":"examples/examples_associations/#Reproducing-Schreiber-(2000)","page":"Associations","title":"Reproducing Schreiber (2000)","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Let's try to reproduce the results from Schreiber's original paper (Schreiber, 2000) where he introduced the transfer entropy. We'll here use the JointProbabilities estimator, discretizing per column of the input data using the CodifyVariables discretization scheme with the ValueBinning outcome space.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing DynamicalSystemsBase\nusing CairoMakie\nusing Statistics\nusing Random; Random.seed!(12234);\n\nfunction ulam_system(dx, x, p, t)\n    f(x) = 2 - x^2\n    ε = p[1]\n    dx[1] = f(ε*x[length(dx)] + (1-ε)*x[1])\n    for i in 2:length(dx)\n        dx[i] = f(ε*x[i-1] + (1-ε)*x[i])\n    end\nend\n\nds = DiscreteDynamicalSystem(ulam_system, rand(100) .- 0.5, [0.04])\nfirst(trajectory(ds, 1000; Ttr = 1000));\n\nεs = 0.02:0.02:1.0\nte_x1x2 = zeros(length(εs)); te_x2x1 = zeros(length(εs))\n# Guess an appropriate bin width of 0.2 for the histogram\ndisc = CodifyVariables(ValueHistogram(0.2))\nest = JointProbabilities(TEShannon(; base = 2), disc)\n\nfor (i, ε) in enumerate(εs)\n    set_parameter!(ds, 1, ε)\n    tr = first(trajectory(ds, 300; Ttr = 5000))\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = association(est, X1, X2)\n    te_x2x1[i] = association(est, X2, X1)\nend\n\nfig = Figure(size = (800, 600))\nax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\nlines!(ax, εs, te_x1x2, label = \"X1 to X2\", color = :black, linewidth = 1.5)\nlines!(ax, εs, te_x2x1, label = \"X2 to X1\", color = :red, linewidth = 1.5)\naxislegend(ax, position = :lt)\nreturn fig","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"As expected, transfer entropy from X1 to X2 is higher than from X2 to X1 across parameter values for ε. But, by our definition of the ulam system, dynamical coupling only occurs from X1 to X2. The results, however, show nonzero transfer entropy in both directions. What does this mean?","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Computing transfer entropy from finite time series introduces bias, and so does any particular choice of entropy estimator used to calculate it. To determine whether a transfer entropy estimate should be trusted, we can employ surrogate testing. We'll generate surrogate using TimeseriesSurrogates.jl. One possible way to do so is to use a SurrogateAssociationTest with independence, but here we'll do the surrogate resampling manually, so we can plot and inspect the results.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"In the example below, we continue with the same time series generated above. However, at each value of ε, we also compute transfer entropy for nsurr = 50 different randomly shuffled (permuted) versions of the source process. If the original transfer entropy exceeds that of some percentile the transfer entropy estimates of the surrogate ensemble, we will take that as \"significant\" transfer entropy.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"nsurr = 25 # in real applications, you should use more surrogates\nbase = 2\nte_x1x2 = zeros(length(εs)); te_x2x1 = zeros(length(εs))\nte_x1x2_surr = zeros(length(εs), nsurr); te_x2x1_surr = zeros(length(εs), nsurr)\n\n# use same bin-width as before\ndisc = CodifyVariables(ValueHistogram(0.2))\nest = JointProbabilities(TEShannon(; base = 2), disc)\n\nfor (i, ε) in enumerate(εs)\n    set_parameter!(ds, 1, ε)\n    tr = first(trajectory(ds, 300; Ttr = 5000))\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = association(est, X1, X2)\n    te_x2x1[i] = association(est, X2, X1)\n    s1 = surrogenerator(X1, RandomShuffle()); s2 = surrogenerator(X2, RandomShuffle())\n\n    for j = 1:nsurr\n        te_x1x2_surr[i, j] = association(est, s1(), X2)\n        te_x2x1_surr[i, j] = association(est, s2(), X1)\n    end\nend\n\n# Compute 95th percentiles of the surrogates for each ε\nqs_x1x2 = [quantile(te_x1x2_surr[i, :], 0.95) for i = 1:length(εs)]\nqs_x2x1 = [quantile(te_x2x1_surr[i, :], 0.95) for i = 1:length(εs)]\n\nfig = with_theme(theme_minimal(), markersize = 2) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\n    scatterlines!(ax, εs, te_x1x2, label = \"X1 to X2\", color = :black, linewidth = 1.5)\n    scatterlines!(ax, εs, qs_x1x2, color = :black, linestyle = :dot, linewidth = 1.5)\n    scatterlines!(ax, εs, te_x2x1, label = \"X2 to X1\", color = :red)\n    scatterlines!(ax, εs, qs_x2x1, color = :red, linestyle = :dot)\n    axislegend(ax, position = :lt)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The plot above shows the original transfer entropies (solid lines) and the 95th percentile transfer entropies of the surrogate ensembles (dotted lines). As expected, using the surrogate test, the transfer entropies from X1 to X2 are mostly significant (solid black line is above dashed black line). The transfer entropies from X2 to X1, on the other hand, are mostly not significant (red solid line is below red dotted line).","category":"page"},{"location":"examples/examples_associations/#[TERenyiJizba](@ref)","page":"Associations","title":"TERenyiJizba","text":"","category":"section"},{"location":"examples/examples_associations/#example_TERenyiJizba_EntropyDecomposition_TransferOperator","page":"Associations","title":"EntropyDecomposition + TransferOperator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We can perform the same type of analysis as above using TERenyiJizba instead of TEShannon.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing DynamicalSystemsBase\nusing StableRNGs; rng = StableRNG(123)\n\n# An example system where `X → Y → Z → W`.\nsys = system(Logistic4Chain(; rng))\nx, y, z, w = columns(first(trajectory(sys, 300, Ttr = 10000)))\n\nprecise = true # precise bin edges\ndiscretization = CodifyVariables(TransferOperator(RectangularBinning(2, precise))) #\nest_disc_to = EntropyDecomposition(TERenyiJizba(), PlugIn(Renyi()), discretization);\nassociation(est_disc_to, x, y), association(est_disc_to, y, x)","category":"page"},{"location":"examples/examples_associations/#[ConvergentCrossMapping](@ref)","page":"Associations","title":"ConvergentCrossMapping","text":"","category":"section"},{"location":"examples/examples_associations/#example_ConvergentCrossMapping_RandomVectors","page":"Associations","title":"RandomVectors estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"When cross-mapping with the RandomVectors estimator, a single random subsample of time indices (i.e. not in any particular order) of length l is drawn for each library size l, and cross mapping is performed using the embedding vectors corresponding to those time indices.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 200), randn(rng, 200)\n\n# We'll draw a single sample at each `l ∈ libsizes`. Sampling with replacement is then\n# necessary, because our 200-pt timeseries will result in embeddings with\n# less than 200 points.\nest = RandomVectors(ConvergentCrossMapping(d = 3); libsizes = 50:25:200, replace = true, rng)\ncrossmap(est, x, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"To generate a distribution of cross-map estimates for each l ∈ libsizes, just call crossmap repeatedly, e.g.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nusing Statistics\n\nx, y = randn(rng, 300), randn(rng, 300)\ndef = ConvergentCrossMapping(d = 3)\nlibsizes = 25:25:200\n\nρs = [[crossmap(RandomVectors(def; libsizes = L, replace = true, rng), x, y) for i = 1:50] for L in libsizes]\n\nusing CairoMakie\nf = Figure(); ax = Axis(f[1, 1]);\nplot!(ax, libsizes, mean.(ρs))\nerrorbars!(ax, libsizes, mean.(ρs), std.(ρs))\nf","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"Now, the k-th element of ρs contains 80 estimates of the correspondence measure ρ at library size libsizes[k].","category":"page"},{"location":"examples/examples_associations/#example_ConvergentCrossMapping_RandomSegment","page":"Associations","title":"RandomSegment estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"When cross-mapping with the RandomSegment estimator, a single random subsample of continguous, ordered time indices of length l is drawn for each library size l, and cross mapping is performed using the embedding vectors corresponding to those time indices.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 200), randn(rng, 200)\n\n# We'll draw a single sample at each `l ∈ libsizes`. We limit the library size to 100, \n# because drawing segments of the data longer than half the available data doesn't make\n# much sense.\nest = RandomSegment(ConvergentCrossMapping(d = 3); libsizes = 50:25:100, rng)\ncrossmap(est, x, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"As above, to generate a distribution of cross-map estimates for each l ∈ libsizes, just call crossmap repeatedly, e.g.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nusing Statistics\n\nx, y = randn(rng, 200), randn(rng, 200)\ndef = ConvergentCrossMapping(d = 3)\nlibsizes = 25:25:100\n\nρs = [[crossmap(RandomSegment(def; libsizes = L, rng), x, y) for i = 1:50] for L in libsizes]\n\nf = Figure(); ax = Axis(f[1, 1]);\nplot!(ax, libsizes, mean.(ρs))\nerrorbars!(ax, libsizes, mean.(ρs), std.(ρs))\nf","category":"page"},{"location":"examples/examples_associations/#[PairwiseAsymmetricInference](@ref)","page":"Associations","title":"PairwiseAsymmetricInference","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"We repeat the analyses above, but here use the pairwise asymmetric inference algorithm instead of the convergent cross map algorithm.","category":"page"},{"location":"examples/examples_associations/#example_PairwiseAsymmetricInference_RandomVectors","page":"Associations","title":"RandomVectors estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 300), randn(rng, 300)\n\n# We'll draw a single sample at each `l ∈ libsizes`. Sampling with replacement is then\n# necessary, because our 200-pt timeseries will result in embeddings with\n# less than 200 points.\nest = RandomVectors(PairwiseAsymmetricInference(d = 3); libsizes = 50:25:200, replace = true, rng)\ncrossmap(est, x, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"To generate a distribution of cross-map estimates for each l ∈ libsizes, just call crossmap repeatedly, e.g.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nusing Statistics\n\nx, y = randn(rng, 300), randn(rng,300)\ndef = PairwiseAsymmetricInference(d = 3)\nlibsizes = 25:25:200\n\nρs = [[crossmap(RandomVectors(def; libsizes = L, replace = true, rng), x, y) for i = 1:50] for L in libsizes]\n\nusing CairoMakie\nf = Figure(); ax = Axis(f[1, 1]);\nplot!(ax, libsizes, mean.(ρs))\nerrorbars!(ax, libsizes, mean.(ρs), std.(ρs))\nf","category":"page"},{"location":"examples/examples_associations/#example_PairwiseAsymmetricInference_RandomSegment","page":"Associations","title":"RandomSegment estimator","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 200), randn(rng, 200)\n\n# We'll draw a single sample at each `l ∈ libsizes`. We limit the library size to 100, \n# because drawing segments of the data longer than half the available data doesn't make\n# much sense.\nest = RandomSegment(PairwiseAsymmetricInference(d = 3); libsizes = 50:25:100, rng)\ncrossmap(est, x, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"As above, to generate a distribution of cross-map estimates for each l ∈ libsizes, just call crossmap repeatedly, e.g.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = MersenneTwister(1234)\nusing Statistics\n\nx, y = randn(rng, 300), randn(rng, 300)\ndef = PairwiseAsymmetricInference(d = 3)\nlibsizes = 25:25:100\n\nρs = [[crossmap(RandomSegment(def; libsizes = L, rng), x, y) for i = 1:50] for L in libsizes]\n\nusing CairoMakie\nf = Figure(); ax = Axis(f[1, 1]);\nplot!(ax, libsizes, mean.(ρs))\nerrorbars!(ax, libsizes, mean.(ρs), std.(ρs))\nf","category":"page"},{"location":"examples/examples_associations/#example_MCR","page":"Associations","title":"MCR","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"To quantify association by the mean conditional probability of recurrence (MCR), we'll create a chain of variables where X drives Y, which in turn drives  Z. We then expect there to be significant detectable association between both X and Y, Y and Z and also X and Z (because Y transfers information from X to Z. We expect the association between X and Z to disappear when conditioning on Y (since we're then \"removing the effect\" of Y).","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234);\nx = rand(rng, 300); y = rand(rng, 300) .* sin.(x); z = rand(rng, 300) .* y;\nest = MCR(r = 0.5)\nassociation(est, x, y), association(est, x, z), association(est, y, z), association(est, x, z, y)","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"The interpretation of the MCR measure is that if two variables are symmetrically coupled, then the conditional recurrence in both directions is equal. Two variables that are uncoupled are symmetrically coupled (i.e. no coupling). We therefore expect the difference in conditional recurrence to be around zero.","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 300)\ny = rand(rng, 300)\nm = MCR(r = 0.5)\nΔ = association(m, x, y) - association(m, y, x)","category":"page"},{"location":"examples/examples_associations/#example_RMCD","page":"Associations","title":"RMCD","text":"","category":"section"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"To quantify association by the recurrence measure of conditional dependence (RMCD), we'll create a chain of variables where X drives Y, which in turn drives  Z. We then expect there to be significant detectable association between both X and Y, Y and Z and also X and Z (because Y transfers information from X to Z. We expect the association between X and Z to disappear when conditioning on Y (since we're then \"removing the effect\" of Y).","category":"page"},{"location":"examples/examples_associations/","page":"Associations","title":"Associations","text":"using Associations\nusing Random; rng = Xoshiro(1234);\nx = rand(rng, 300); y = rand(rng, 300) .* sin.(x); z = rand(rng, 300) .* y;\nest = RMCD(r = 0.5)\nassociation(est, x, y), association(est, x, z), association(est, x, z, y)","category":"page"},{"location":"api/information_single_variable_api/","page":"Single-variable information API","title":"Single-variable information API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/information_single_variable_api/#Single-variable-information-API","page":"Single-variable information API","title":"Single-variable information API","text":"","category":"section"},{"location":"api/information_single_variable_api/","page":"Single-variable information API","title":"Single-variable information API","text":"Below we list some relevant functions types from ComplexityMeasures.jl that  are used for the EntropyDecomposition estimator.","category":"page"},{"location":"api/information_single_variable_api/","page":"Single-variable information API","title":"Single-variable information API","text":"information","category":"page"},{"location":"api/information_single_variable_api/#ComplexityMeasures.information","page":"Single-variable information API","title":"ComplexityMeasures.information","text":"information([die::DiscreteInfoEstimator,] [est::ProbabilitiesEstimator,] o::OutcomeSpace, x) → h::Real\ninformation(o::OutcomeSpace, x) → h::Real\n\nEstimate a discrete information measure from input data x using the provided DiscreteInfoEstimator and ProbabilitiesEstimator over the given OutcomeSpace.\n\nAs an alternative, you can provide an InformationMeasure for the first argument (die) which will default to PlugIn estimation) for the information estimation. You may also skip the first argument (die), in which case Shannon() will be used. You may also skip the second argument (est), which will default to the RelativeAmount probabilities estimator. Note that some information measure estimators (e.g., GeneralizedSchuermann) operate directly on counts and hence ignore est.\n\ninformation([e::DiscreteInfoEstimator,] p::Probabilities) → h::Real\ninformation([e::DiscreteInfoEstimator,] c::Counts) → h::Real\n\nLike above, but estimate the information measure from the pre-computed Probabilities p or Counts. Counts are converted into probabilities using RelativeAmount, unless the estimator e uses counts directly.\n\nSee also: information_maximum, information_normalized for a normalized version.\n\nExamples (naive estimation)\n\nThe simplest way to estimate a discrete measure is to provide the InformationMeasure directly in combination with an OutcomeSpace. This will use the \"naive\" PlugIn estimator for the measure, and the \"naive\" RelativeAmount estimator for the probabilities.\n\nx = randn(100) # some input data\no = ValueBinning(RectangularBinning(5)) # a 5-bin histogram outcome space\nh_s = information(Shannon(), o, x)\n\nHere are some more examples:\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = information(ps) # gives 1, about 1 bit by definition (Shannon entropy by default)\nh = information(Shannon(), ps) # syntactically equivalent to the above\nh = information(Shannon(), UniqueElements(), x) # syntactically equivalent to above\nh = information(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\nh = information(OrdinalPatterns(;m=3), x) # gives about 2, again by definition\n\nExamples (bias-corrected estimation)\n\nIt is known that both PlugIn estimation for information measures and RelativeAmount estimation for probabilities are biased. The scientific literature abounds with estimators that correct for this bias, both on the measure-estimation level and on the probability-estimation level. We thus provide the option to use any DiscreteInfoEstimator in combination with any ProbabilitiesEstimator for improved estimates. Note that custom probabilites estimators will only work with counting-compatible OutcomeSpace.\n\nx = randn(100)\no = ValueBinning(RectangularBinning(5))\n\n# Estimate Shannon entropy estimation using various dedicated estimators\nh_s = information(MillerMadow(Shannon()), RelativeAmount(), o, x)\nh_s = information(HorvitzThompson(Shannon()), Shrinkage(), o, x)\nh_s = information(Schuermann(Shannon()), Shrinkage(), o, x)\n\n# Estimate information measures using the generic `Jackknife` estimator\nh_r = information(Jackknife(Renyi()), Shrinkage(), o, x)\nj_t = information(Jackknife(TsallisExtropy()), BayesianRegularization(), o, x)\nj_r = information(Jackknife(RenyiExtropy()), RelativeAmount(), o, x)\n\n\n\n\n\ninformation(est::DifferentialInfoEstimator, x) → h::Real\n\nEstimate a differential information measure using the provided DifferentialInfoEstimator and input data x.\n\nDescription\n\nThe overwhelming majority of differential estimators estimate the Shannon entropy. If the same estimator can estimate different information measures (e.g. it can estimate both Shannon and Tsallis), then the information measure is provided as an argument to the estimator itself.\n\nSee the table of differential information measure estimators in the docs for all differential information measure estimators.\n\nCurrently, unlike for the discrete information measures, this method doesn't involve explicitly first computing a probability density function and then passing this density to an information measure definition. But in the future, we want to establish a density API similar to the probabilities API.\n\nExamples\n\nTo compute the differential version of a measure, give it as the first argument to a DifferentialInfoEstimator and pass it to information.\n\nx = randn(1000)\nh_sh = information(Kraskov(Shannon()), x)\nh_vc = information(Vasicek(Shannon()), x)\n\nA normal distribution has a base-e Shannon differential entropy of 0.5*log(2π) + 0.5 nats.\n\nest = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.\nh = information(est, randn(2_000_000))\nabs(h - 0.5*log(2π) - 0.5) # ≈ 0.0001\n\n\n\n\n\ninformation(est::MultivariateInformationMeasureEstimator, x...)\n\nEstimate some MultivariateInformationMeasure on input data x..., using the given MultivariateInformationMeasureEstimator.\n\nThis is just a convenience wrapper around association(est, x...).\n\n\n\n\n\n","category":"function"},{"location":"api/information_single_variable_api/#Single-variable-information-measures","page":"Single-variable information API","title":"Single-variable information measures","text":"","category":"section"},{"location":"api/information_single_variable_api/","page":"Single-variable information API","title":"Single-variable information API","text":"Shannon\nRenyi\nTsallis\nKaniadakis","category":"page"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Shannon","page":"Single-variable information API","title":"ComplexityMeasures.Shannon","text":"Shannon <: InformationMeasure\nShannon(; base = 2)\n\nThe Shannon (Shannon, 1948) entropy, used with information to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nThe maximum value of the Shannon entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Renyi","page":"Single-variable information API","title":"ComplexityMeasures.Renyi","text":"Renyi <: InformationMeasure\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe Rényi generalized order-q entropy (Rényi, 1961), used with information to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see Shannon (1948)), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nThe maximum value of the Rényi entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Tsallis","page":"Single-variable information API","title":"ComplexityMeasures.Tsallis","text":"Tsallis <: InformationMeasure\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis generalized order-q entropy (Tsallis, 1988), used with information to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nThe maximum value of the Tsallis entropy is k(L^1 - q - 1)(1 - q), with L the total_outcomes.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Kaniadakis","page":"Single-variable information API","title":"ComplexityMeasures.Kaniadakis","text":"Kaniadakis <: InformationMeasure\nKaniadakis(; κ = 1.0, base = 2.0)\n\nThe Kaniadakis entropy (Tsallis, 2009), used with information to compute\n\nH_K(p) = -sum_i=1^N p_i f_kappa(p_i)\n\nf_kappa (x) = dfracx^kappa - x^-kappa2kappa\n\nwhere if kappa = 0, regular logarithm to the given base is used, and 0 probabilities are skipped.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#Discrete-information-estimators","page":"Single-variable information API","title":"Discrete information estimators","text":"","category":"section"},{"location":"api/information_single_variable_api/","page":"Single-variable information API","title":"Single-variable information API","text":"DiscreteInfoEstimator\nPlugIn\nMillerMadow\nSchuermann\nGeneralizedSchuermann\nJackknife\nHorvitzThompson\nChaoShen","category":"page"},{"location":"api/information_single_variable_api/#ComplexityMeasures.DiscreteInfoEstimator","page":"Single-variable information API","title":"ComplexityMeasures.DiscreteInfoEstimator","text":"DiscreteInfoEstimator\n\nThe supertype of all discrete information measure estimators, which are used in combination with a ProbabilitiesEstimator as input to  information or related functions.\n\nThe first argument to a discrete estimator is always an InformationMeasure (defaults to Shannon).\n\nDescription\n\nA discrete InformationMeasure is a functional of a probability mass function. To estimate such a measure from data, we must first estimate a probability mass function using a ProbabilitiesEstimator from the (encoded/discretized) input data, and then apply the estimator to the estimated probabilities. For example, the Shannon entropy is typically computed using the RelativeAmount estimator to compute probabilities, which are then given to the PlugIn estimator. Many other estimators exist, not only for Shannon entropy, but other information measures as well.\n\nWe provide a library of both generic estimators such as PlugIn or Jackknife (which can be applied to any measure), as well as dedicated estimators such as MillerMadow, which computes Shannon entropy using the Miller-Madow bias correction. The list below gives a complete overview.\n\nImplementations\n\nThe following estimators are generic and can compute any InformationMeasure.\n\nPlugIn. The default, generic plug-in estimator of any information measure.   It computes the measure exactly as stated in the definition, using the computed   probability mass function.\nJackknife. Uses the a combination of the plug-in estimator and the jackknife   principle to estimate the information measure.\n\nShannon entropy estimators\n\nThe following estimators are dedicated Shannon entropy estimators, which provide improvements over the naive PlugIn estimator.\n\nMillerMadow.\nHorvitzThompson.\nSchuermann.\nGeneralizedSchuermann.\nChaoShen.\n\ninfo: Info\nAny of the implemented DiscreteInfoEstimators can be used in combination with any ProbabilitiesEstimator as input to information. What this means is that every estimator actually comes in many different variants - one for each ProbabilitiesEstimator. For example, the MillerMadow estimator of Shannon entropy is typically calculated with RelativeAmount probabilities. But here, you can use for example the BayesianRegularization or the Shrinkage probabilities estimators instead, i.e. information(MillerMadow(), RelativeAmount(outcome_space), x) and information(MillerMadow(), BayesianRegularization(outcomes_space), x) are distinct estimators. This holds for all DiscreteInfoEstimators. Many of these estimators haven't been explored in the literature before, so feel free to explore, and please cite this software if you use it to explore some new estimator combination!\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.PlugIn","page":"Single-variable information API","title":"ComplexityMeasures.PlugIn","text":"PlugIn(e::InformationMeasure) <: DiscreteInfoEstimatorGeneric\n\nThe PlugIn estimator is also called the empirical/naive/\"maximum likelihood\" estimator, and is used with information to any discrete InformationMeasure.\n\nIt computes any quantity exactly as given by its formula. When computing an information measure, which here is defined as a probabilities functional, it computes the quantity directly from a probability mass function, which is derived from maximum-likelihood (RelativeAmount estimates of the probabilities.\n\nBias of plug-in estimates\n\nThe plugin-estimator of Shannon entropy underestimates the true entropy, with a bias that grows with the number of distinct outcomes (Arora et al., 2022)(Arora et al., 2022),\n\nbias(H_S^plugin) = -dfracK-12N + o(N^-1)\n\nwhere K is the number of distinct outcomes, and N is the sample size. Many authors have tried to remedy this by proposing alternative Shannon entropy estimators. For example, the MillerMadow estimator is a simple correction to the plug-in estimator that adds back the bias term above. Many other estimators exist; see DiscreteInfoEstimators for an overview.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.MillerMadow","page":"Single-variable information API","title":"ComplexityMeasures.MillerMadow","text":"MillerMadow <: DiscreteInfoEstimatorShannon\nMillerMadow(measure::Shannon = Shannon())\n\nThe MillerMadow estimator is used with information to compute the discrete Shannon entropy according to Miller (1955).\n\nDescription\n\nThe Miller-Madow estimator of Shannon entropy is given by\n\nH_S^MM = H_S^plugin + dfracm - 12N\n\nwhere H_S^plugin is the Shannon entropy estimated using the PlugIn estimator, m is the number of bins with nonzero probability (as defined in Paninski (2003)), and N is the number of observations.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Schuermann","page":"Single-variable information API","title":"ComplexityMeasures.Schuermann","text":"Schuermann <: DiscreteInfoEstimatorShannon\nSchuermann(definition::Shannon; a = 1.0)\n\nThe Schuermann estimator is used with information to compute the discrete Shannon entropy with the bias-corrected estimator given in Schuermann (2004).\n\nSee detailed description for GeneralizedSchuermann for details.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.GeneralizedSchuermann","page":"Single-variable information API","title":"ComplexityMeasures.GeneralizedSchuermann","text":"GeneralizedSchuermann <: DiscreteInfoEstimatorShannon\nGeneralizedSchuermann(definition = Shannon(); a = 1.0)\n\nThe GeneralizedSchuermann estimator is used with information to compute the discrete Shannon entropy with the bias-corrected estimator given in Grassberger (2022).\n\nThe \"generalized\" part of the name, as opposed to the Schuermann (2004) estimator (Schuermann), is due to the possibility of picking difference parameters a_i for different outcomes. If different parameters are assigned to the different outcomes, a must be a vector of parameters of length length(outcomes), where the outcomes are obtained using outcomes. See Grassberger (2022) for more information. If a is a real number, then a_i = a forall i, and the estimator reduces to the Schuermann estimator.\n\nDescription\n\nFor a set of N observations over M outcomes, the estimator is given by\n\nH_S^opt = varphi(N) - dfrac1N sum_i=1^M n_i G_n_i(a_i)\n\nwhere n_i is the observed frequency of the i-th outcome,\n\nG_n(a) = varphi(n) + (-1)^n int_0^a dfracx^n - 1x + 1 dx\n\nG_n(1) = G_n and G_n(0) = varphi(n), and\n\nG_n = varphi(n) + (-1)^n int_0^1 dfracx^n - 1x + 1 dx\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Jackknife","page":"Single-variable information API","title":"ComplexityMeasures.Jackknife","text":"Jackknife <: DiscreteInfoEstimatorGeneric\nJackknife(definition::InformationMeasure = Shannon())\n\nThe Jackknife estimator is used with information to compute any discrete InformationMeasure.\n\nThe Jackknife estimator uses the generic jackknife principle to reduce bias. Zahl (1977) was the first to apply the jaccknife technique in the context of Shannon entropy estimation. Here, we've generalized his estimator to work with any InformationMeasure.\n\nDescription\n\nAs an example of the jackknife technique, here is the formula for a jackknife estimate of Shannon entropy\n\nH_S^J = N H_S^plugin - dfracN-1N sum_i=1^N H_S^plugin^-i\n\nwhere N is the sample size, H_S^plugin is the plugin estimate of Shannon entropy, and H_S^plugin^-i is the plugin estimate, but computed with the i-th sample left out.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.HorvitzThompson","page":"Single-variable information API","title":"ComplexityMeasures.HorvitzThompson","text":"HorvitzThompson <: DiscreteInfoEstimatorShannon\nHorvitzThompson(measure::Shannon = Shannon())\n\nThe HorvitzThompson estimator is used with information to compute the discrete Shannon entropy according to Horvitz and Thompson (1952).\n\nDescription\n\nThe Horvitz-Thompson estimator of Shannon entropy is given by\n\nH_S^HT = -sum_i=1^M dfracp_i log(p_i) 1 - (1 - p_i)^N\n\nwhere N is the sample size and M is the number of outcomes. Given the true probability p_i of the i-th outcome, 1 - (1 - p_i)^N is the probability that the outcome appears at least once in a sample of size N (Arora et al., 2022). Dividing by this inclusion probability is a form of weighting, and compensates for situations where certain outcomes have so low probabilities that they are not often observed in a sample, for example in power-law distributions.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.ChaoShen","page":"Single-variable information API","title":"ComplexityMeasures.ChaoShen","text":"ChaoShen <: DiscreteInfoEstimatorShannon\nChaoShen(definition::Shannon = Shannon())\n\nThe ChaoShen estimator is used with information to compute the discrete Shannon entropy according to Chao and Shen (2003).\n\nDescription\n\nThis estimator is a modification of the HorvitzThompson estimator that multiplies each plugin probability estimate by an estimate of sample coverage. If f_1 is the number of singletons (outcomes that occur only once) in a sample of length N, then the sample coverage is C = 1 - dfracf_1N. The Chao-Shen estimator of Shannon entropy is then\n\nH_S^CS = -sum_i=1^M left( dfracC p_i log(C p_i)1 - (1 - C p_i)^N right)\n\nwhere N is the sample size and M is the number of outcomes. If f_1 = N, then f_1 is set to f_1 = N - 1 to ensure positive entropy (Arora et al., 2022).\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#Differential-information-estimators","page":"Single-variable information API","title":"Differential information estimators","text":"","category":"section"},{"location":"api/information_single_variable_api/","page":"Single-variable information API","title":"Single-variable information API","text":"DifferentialInfoEstimator\nKraskov\nKozachenkoLeonenko\nZhu\nZhuSingh\nGao\nGoria\nLord\nLeonenkoProzantoSavani\nVasicek\nAlizadehArghami\nEbrahimi\nCorrea","category":"page"},{"location":"api/information_single_variable_api/#ComplexityMeasures.DifferentialInfoEstimator","page":"Single-variable information API","title":"ComplexityMeasures.DifferentialInfoEstimator","text":"DifferentialInfoEstimator\n\nThe supertype of all differential information measure estimators. These estimators compute an information measure in various ways that do not involve explicitly estimating a probability distribution.\n\nEach DifferentialInfoEstimators uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of information measures. For example, Kraskov estimates the Shannon entropy.\n\nSee information for usage.\n\nImplementations\n\nKozachenkoLeonenko.\nKraskov.\nGoria.\nGao.\nZhu\nZhuSingh.\nLord.\nAlizadehArghami.\nCorrea.\nVasicek.\nEbrahimi.\nLeonenkoProzantoSavani.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Kraskov","page":"Single-variable information API","title":"ComplexityMeasures.Kraskov","text":"Kraskov <: DifferentialInfoEstimator\nKraskov(definition = Shannon(); k::Int = 1, w::Int = 0)\n\nThe Kraskov estimator computes the Shannon differential information of a multi-dimensional StateSpaceSet using the k-th nearest neighbor searches method from Kraskov et al. (2004), with logarithms to the base specified in definition.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Kraskov estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSee also: information, KozachenkoLeonenko, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.KozachenkoLeonenko","page":"Single-variable information API","title":"ComplexityMeasures.KozachenkoLeonenko","text":"KozachenkoLeonenko <: DifferentialInfoEstimator\nKozachenkoLeonenko(definition = Shannon(); w::Int = 0)\n\nThe KozachenkoLeonenko estimator (Kozachenko and Leonenko, 1987) computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nusing the nearest neighbor method from Kozachenko and Leonenko (1987), as described in Charzyńska and Gambin (2016).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\nSee also: information, Kraskov, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Zhu","page":"Single-variable information API","title":"ComplexityMeasures.Zhu","text":"Zhu <: DifferentialInfoEstimator\nZhu(; definition = Shannon(), k = 1, w = 0)\n\nThe Zhu estimator (Zhu et al., 2015) is an extension to KozachenkoLeonenko, and computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Zhu estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby approximating densities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: information, KozachenkoLeonenko, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.ZhuSingh","page":"Single-variable information API","title":"ComplexityMeasures.ZhuSingh","text":"ZhuSingh <: DifferentialInfoEstimator\nZhuSingh(definition = Shannon(); k = 1, w = 0)\n\nThe ZhuSingh estimator (Zhu et al., 2015) computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. ZhuSingh estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: information, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Gao","page":"Single-variable information API","title":"ComplexityMeasures.Gao","text":"Gao <: DifferentialInfoEstimator\nGao(definition = Shannon(); k = 1, w = 0, corrected = true)\n\nThe Gao estimator (Gao et al., 09–12 May 2015) computes the Shannon differential information, using a k-th nearest-neighbor approach based on Singh et al. (2003), with logarithms to the base specified in definition.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nGao et al. (09–12 May 2015) give two variants of this estimator. If corrected == false, then the uncorrected version is used. If corrected == true, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Goria","page":"Single-variable information API","title":"ComplexityMeasures.Goria","text":"Goria <: DifferentialInfoEstimator\nGoria(measure = Shannon(); k = 1, w = 0)\n\nThe Goria estimator (Goria et al., 2005) computes the Shannon differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Goria estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSpecifically, let bfn_1 bfn_2 ldots bfn_N be the distance of the samples bfx_1 bfx_2 ldots bfx_N  to their k-th nearest neighbors. Next, let the geometric mean of the distances be\n\nhatrho_k = left( prod_i=1^N right)^dfrac1N\n\nGoria et al. (2005)'s estimate of Shannon differential entropy is then\n\nhatH = mhatrho_k + log(N - 1) - psi(k) + log c_1(m)\n\nwhere c_1(m) = dfrac2pi^fracm2m Gamma(m2) and psi is the digamma function.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Lord","page":"Single-variable information API","title":"ComplexityMeasures.Lord","text":"Lord <: DifferentialInfoEstimator\nLord(measure = Shannon(); k = 10, w = 0)\n\nThe Lord estimator (Lord et al., 2018) estimates the Shannon differential information using a nearest neighbor approach with a local nonuniformity correction (LNC), with logarithms to the base specified in definition.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples barX = bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density function f  mathbbR^d to mathbbR. Lord estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby using the resubstitution formula\n\nhatbarX k = -mathbbElog(f(X))\napprox sum_i = 1^N log(hatf(bfx_i))\n\nwhere hatf(bfx_i) is an estimate of the density at bfx_i constructed in a manner such that hatf(bfx_i) propto dfrack(x_i)  NV_i, where k(x_i) is the number of points in the neighborhood of bfx_i, and V_i is the volume of that neighborhood.\n\nWhile most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities hatf(bfx_i), the Lord estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point xᵢ, estimated using singular value decomposition (SVD) on the k-th nearest neighbors of xᵢ. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes Lord a well-suited entropy estimator for a wide range of systems.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.LeonenkoProzantoSavani","page":"Single-variable information API","title":"ComplexityMeasures.LeonenkoProzantoSavani","text":"LeonenkoProzantoSavani <: DifferentialInfoEstimator\nLeonenkoProzantoSavani(definition = Shannon(); k = 1, w = 0)\n\nThe LeonenkoProzantoSavani estimator (Leonenko et al., 2008) computes the  Shannon, Renyi, or Tsallis differential information of a multi-dimensional StateSpaceSet, with logarithms to the base specified in definition.\n\nDescription\n\nThe estimator uses k-th nearest-neighbor searches.  w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nFor details, see Leonenko et al. (2008).\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Vasicek","page":"Single-variable information API","title":"ComplexityMeasures.Vasicek","text":"Vasicek <: DifferentialInfoEstimator\nVasicek(definition = Shannon(); m::Int = 1)\n\nThe Vasicek estimator computes the Shannon differential information of a timeseries using the method from Vasicek (1976), with logarithms to the base specified in definition.\n\nThe Vasicek estimator belongs to a class of differential entropy estimators based on order statistics, of which Vasicek (1976) was the first. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Vasicek estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Vasicek Shannon differential entropy estimate is then\n\nhatH_V(barX m) =\ndfrac1n\nsum_i = 1^n log left dfracn2m (barX_(i+m) - barX_(i-m)) right\n\nUsage\n\nIn practice, choice of m influences how fast the entropy converges to the true value. For small value of m, convergence is slow, so we recommend to scale m according to the time series length n and use m >= n/100 (this is just a heuristic based on the tests written for this package).\n\nSee also: information, Correa, AlizadehArghami, Ebrahimi, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.AlizadehArghami","page":"Single-variable information API","title":"ComplexityMeasures.AlizadehArghami","text":"AlizadehArghami <: DifferentialInfoEstimator\nAlizadehArghami(definition = Shannon(); m::Int = 1)\n\nThe AlizadehArghami estimator computes the Shannon differential information of a timeseries using the method from Alizadeh and Arghami (2010), with logarithms to the base specified in definition.\n\nThe AlizadehArghami estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. AlizadehArghami estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X:\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The AlizadehArghami Shannon differential entropy estimate is then the the Vasicek estimate hatH_V(barX m n), plus a correction factor\n\nhatH_A(barX m n) = hatH_V(barX m n) +\ndfrac2nleft(m log(2) right)\n\nSee also: information, Correa, Ebrahimi, Vasicek, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Ebrahimi","page":"Single-variable information API","title":"ComplexityMeasures.Ebrahimi","text":"Ebrahimi <: DifferentialInfoEstimator\nEbrahimi(definition = Shannon(); m::Int = 1)\n\nThe Ebrahimi estimator computes the Shannon information of a timeseries using the method from Ebrahimi et al. (1994), with logarithms to the base specified in definition.\n\nThe Ebrahimi estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Ebrahimi estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, it makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n). The Ebrahimi Shannon differential entropy estimate is then\n\nhatH_E(barX m) =\ndfrac1n sum_i = 1^n log\nleft dfracnc_i m (barX_(i+m) - barX_(i-m)) right\n\nwhere\n\nc_i =\nbegincases\n    1 + fraci - 1m  1 geq i geq m \n    2                     m + 1 geq i geq n - m \n    1 + fracn - im  n - m + 1 geq i geq n\nendcases\n\nSee also: information, Correa, AlizadehArghami, Vasicek, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/information_single_variable_api/#ComplexityMeasures.Correa","page":"Single-variable information API","title":"ComplexityMeasures.Correa","text":"Correa <: DifferentialInfoEstimator\nCorrea(definition = Shannon(); m::Int = 1)\n\nThe Correa estimator computes the Shannon differential information of a timeseries using the method from Correa (1995), with logarithms to the base specified in definition.\n\nThe Correa estimator belongs to a class of differential entropy estimators based on order statistics. It only works for timeseries input.\n\nDescription\n\nAssume we have samples barX = x_1 x_2 ldots x_N  from a continuous random variable X in mathbbR with support mathcalX and density functionf  mathbbR to mathbbR. Correa estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nHowever, instead of estimating the above integral directly, Correa makes use of the equivalent integral, where F is the distribution function for X,\n\nH(X) = int_0^1 log left(dfracddpF^-1(p) right) dp\n\nThis integral is approximated by first computing the order statistics of barX (the input timeseries), i.e. x_(1) leq x_(2) leq cdots leq x_(n), ensuring that end points are included. The Correa estimate of Shannon differential entropy is then\n\nH_C(barX m n) =\ndfrac1n sum_i = 1^n log\nleft dfrac sum_j=i-m^i+m(barX_(j) -\ntildeX_(i))(j - i)n sum_j=i-m^i+m (barX_(j) - tildeX_(i))^2\nright\n\nwhere\n\ntildeX_(i) = dfrac12m + 1 sum_j = i - m^i + m X_(j)\n\nSee also: information, AlizadehArghami, Ebrahimi, Vasicek, DifferentialInfoEstimator.\n\n\n\n\n\n","category":"type"},{"location":"independence/","page":"Independence","title":"Independence","text":"CollapsedDocStrings = true","category":"page"},{"location":"independence/#independence_testing","page":"Independence","title":"Independence testing","text":"","category":"section"},{"location":"independence/","page":"Independence","title":"Independence","text":"For practical applications, it is often useful to determine whether variables are independent, possible conditioned upon  another set of variables. One way of doing so is to utilize an  association measure, and perform some sort of randomization-based independence testing.","category":"page"},{"location":"independence/","page":"Independence","title":"Independence","text":"For example, to test the dependence between time series, time series surrogates testing is used. Many other frameworks for independence exist too. Here, we've collected some independence testing frameworks, and made sure that they are compatible with as many of the implemented association measures as possible.","category":"page"},{"location":"independence/#Independence-testing-API","page":"Independence","title":"Independence testing API","text":"","category":"section"},{"location":"independence/","page":"Independence","title":"Independence","text":"The independence test API is defined by","category":"page"},{"location":"independence/","page":"Independence","title":"Independence","text":"independence\nIndependenceTest","category":"page"},{"location":"independence/","page":"Independence","title":"Independence","text":"independence\nIndependenceTest","category":"page"},{"location":"independence/#Associations.independence","page":"Independence","title":"Associations.independence","text":"independence(test::IndependenceTest, x, y, [z]) → summary\n\nPerform the given IndependenceTest test on data x, y and z. If only x and y are given, test must provide a bivariate association measure. If z is given too, then test must provide a conditional association measure.\n\nReturns a test summary, whose type depends on test.\n\nCompatible independence tests\n\nSurrogateAssociationTest\nLocalPermutationTest\nJointDistanceDistributionTest\nCorrTest\n\n\n\n\n\n","category":"function"},{"location":"independence/#Associations.IndependenceTest","page":"Independence","title":"Associations.IndependenceTest","text":"IndependenceTest <: IndependenceTest\n\nThe supertype for all independence tests.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[SurrogateAssociationTest](@ref)","page":"Independence","title":"SurrogateAssociationTest","text":"","category":"section"},{"location":"independence/","page":"Independence","title":"Independence","text":"SurrogateAssociationTest\nSurrogateAssociationTestResult","category":"page"},{"location":"independence/#Associations.SurrogateAssociationTest","page":"Independence","title":"Associations.SurrogateAssociationTest","text":"SurrogateAssociationTest <: IndependenceTest\nSurrogateAssociationTest(est_or_measure;\n    nshuffles::Int = 100,\n    surrogate = RandomShuffle(),\n    rng = Random.default_rng(),\n    show_progress = false,\n)\n\nA surrogate-data based generic (conditional) independence test for assessing whether the  association between variables X and Y are independent, potentially conditioned on a  third variable Z.\n\nCompatible estimators and measures\n\nCompatible with AssociationMeasures that measure some sort of pairwise or conditional association.\n\nnote: Note\nYou must yourself determine whether using a particular measure is meaningful, and what it means.\n\nnote: Note\nIf used with a TransferEntropy measure such as TEShannon, then the source variable is always shuffled, and the target and conditional variable are left unshuffled.\n\nUsage\n\nUse with independence to perform a surrogate test with input data. This will   return a SurrogateAssociationTestResult.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether x and y are independent (given z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between the input variables.\n\nThe test first estimates the desired statistic using est_or_measure on the input data.  Then, the first input variable is shuffled nshuffled times according to the given  surrogate method (each type of surrogate represents a distinct null hypothesis). For each shuffle, est_or_measure is recomputed and the results are stored. \n\nExamples\n\nExample 1:    SMeasure test for pairwise independence.\nExample 2:    DistanceCorrelation test for pairwise independence.\nExample 3:   PartialCorrelation test for conditional independence.\nExample 4:   MIShannon test for pairwise independence on categorical data.\nExample 5:   CMIShannon test for conditional independence on categorical data.  \nExample 6: MCR test for    pairwise and conditional independence.  \n\n\n\n\n\n","category":"type"},{"location":"independence/#Associations.SurrogateAssociationTestResult","page":"Independence","title":"Associations.SurrogateAssociationTestResult","text":"SurrogateAssociationTestResult(m, m_surr, pvalue)\n\nHolds the result of a SurrogateAssociationTest. m is the measure computed on the original data. m_surr is a vector of the measure computed on permuted data, where m_surr[i] is the measure compute on the i-th permutation. pvalue is the one-sided p-value for the test.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[LocalPermutationTest](@ref)","page":"Independence","title":"LocalPermutationTest","text":"","category":"section"},{"location":"independence/","page":"Independence","title":"Independence","text":"LocalPermutationTest\nLocalPermutationTestResult","category":"page"},{"location":"independence/#Associations.LocalPermutationTest","page":"Independence","title":"Associations.LocalPermutationTest","text":"LocalPermutationTest <: IndependenceTest\nLocalPermutationTest(measure, [est];\n    kperm::Int = 5,\n    nshuffles::Int = 100,\n    rng = Random.default_rng(),\n    replace = true,\n    w::Int = 0,\n    show_progress = false)\n\nLocalPermutationTest is a generic conditional independence test (Runge, 09–11 Apr 2018) for assessing whether two variables X and Y are conditionally independendent given a third variable Z (all of which may be multivariate).\n\nWhen used with independence, a LocalPermutationTestResult is returned.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether X and Y are independent (given Z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between x and y, but preserve dependencies between x and z.\n\nThe algorithm is as follows:\n\nCompute the original conditional independence statistic I(X; Y | Z).\nAllocate a scalar valued vector Î with space for nshuffles elements.\nFor k ∈ [1, 2, …, nshuffles], repeat\nFor each zᵢ ∈ Y, let nᵢ be time indices of the kperm nearest neighbors of zᵢ,   excluding the w nearest neighbors of zᵢ from the neighbor query (i.e w is   the Theiler window).\nLet xᵢ⋆ = X[j], where j is randomly sampled from nᵢ with replacement.   This way, xᵢ is replaced with xⱼ only if zᵢ ≈ zⱼ (zᵢ and zⱼ are close).   Repeat for i = 1, 2, …, n and obtain the shuffled X̂ = [x̂₁, x̂₂, …, x̂ₙ].\nCompute the conditional independence statistic Iₖ(X̂; Y | Z).\nLet Î[k] = Iₖ(X̂; Y | Z).\nCompute the p-value as count(Î[k] .<= I) / nshuffles).\n\nIn additional to the conditional variant from Runge (2018), we also provide a pairwise version, where the shuffling procedure is identical, except neighbors in Y are used instead of Z and we I(X; Y) and Iₖ(X̂; Y) instead of I(X; Y | Z) and Iₖ(X̂; Y | Z).\n\nCompatible measures\n\nMeasure Pairwise Conditional Requires est Note\nPartialCorrelation ✖ ✓ No \nDistanceCorrelation ✖ ✓ No \nCMIShannon ✖ ✓ Yes \nTEShannon ✓ ✓ Yes Pairwise tests not possible with TransferEntropyEstimators, only lower-level estimators, e.g. FPVP, GaussianMI or Kraskov\nPartialMutualInformation ✖ ✓ Yes \n\nThe LocalPermutationTest is only defined for conditional independence testing. Exceptions are for measures like TEShannon, which use conditional measures under the hood even for their pairwise variants, and are therefore compatible with LocalPermutationTest.\n\nThe nearest-neighbor approach in Runge (2018) can be reproduced by using the CMIShannon measure with the FPVP estimator.\n\nExamples\n\nExample 1:   Conditional independence test using CMIShannon\nExample 2):    Conditional independence test using TEShannon\n\n\n\n\n\n","category":"type"},{"location":"independence/#Associations.LocalPermutationTestResult","page":"Independence","title":"Associations.LocalPermutationTestResult","text":"LocalPermutationTestResult(m, m_surr, pvalue)\n\nHolds the result of a LocalPermutationTest. m is the measure computed on the original data. m_surr is a vector of the measure computed on permuted data, where m_surr[i] is the measure compute on the i-th permutation. pvalue is the one-sided p-value for the test.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[JointDistanceDistributionTest](@ref)","page":"Independence","title":"JointDistanceDistributionTest","text":"","category":"section"},{"location":"independence/","page":"Independence","title":"Independence","text":"JointDistanceDistributionTest\nJDDTestResult","category":"page"},{"location":"independence/#Associations.JointDistanceDistributionTest","page":"Independence","title":"Associations.JointDistanceDistributionTest","text":"JointDistanceDistributionTest <: IndependenceTest\nJointDistanceDistributionTest(measure::JointDistanceDistribution; rng = Random.default_rng())\n\nAn independence test for two variables based on the JointDistanceDistribution (Amigó and Hirata, 2018).\n\nWhen used with independence, a JDDTestResult is returned.\n\nDescription\n\nThe joint distance distribution (labelled Δ in their paper) is used by Amigó & Hirata (2018) to detect directional couplings of the form X to Y or Y to X. JointDistanceDistributionTest formulates their method as an independence test.\n\nFormally, we test the hypothesis H_0 (the variables are independent) against H_1 (there is directional coupling between the variables). To do so, we use a right-sided/upper-tailed t-test to check mean of Δ is skewed towards positive value, i.e.\n\nH_0 = mu(Delta) = 0\nH_1 = mu(Delta)  0.\n\nWhen used with independence, a JDDTestResult is returned, which contains the joint distance distribution and a p-value. If you only need Δ, use association  with a JointDistanceDistribution instance directly.\n\nExamples\n\nExample 1. Detecting (in)dependence   in bidirectionally coupled logistic maps.\n\n\n\n\n\n","category":"type"},{"location":"independence/#Associations.JDDTestResult","page":"Independence","title":"Associations.JDDTestResult","text":"JDDTestResult(Δjdd, hypothetical_μ, pvalue)\n\nHolds the results of JointDistanceDistributionTest. Δjdd is the Δ-distribution, hypothetical_μ is the hypothetical mean of the Δ-distribution under the null, and pvalue is the p-value for the one-sided t-test.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[CorrTest](@ref)","page":"Independence","title":"CorrTest","text":"","category":"section"},{"location":"independence/","page":"Independence","title":"Independence","text":"CorrTest\nCorrTestResult","category":"page"},{"location":"independence/#Associations.CorrTest","page":"Independence","title":"Associations.CorrTest","text":"CorrTest <: IndependenceTest\nCorrTest()\n\nAn independence test based correlation (for two variables) and partial correlation (for three variables) (Levy and Narula, 1978); as described in Schmidt et al. (2018).\n\nUses PearsonCorrelation and PartialCorrelation internally.\n\nAssumes that the input data are (multivariate) normally distributed. Then ρ(X, Y) = 0 implies X ⫫ Y and ρ(X, Y | 𝐙) = 0 implies X ⫫ Y | 𝐙.\n\nDescription\n\nThe null hypothesis is H₀ := ρ(X, Y | 𝐙) = 0. We use the approach in Levy & Narula (1978)(Levy and Narula, 1978) and compute the Z-transformation of the observed (partial) correlation coefficient hatrho_XYbfZ:\n\nZ(hatrho_XYbfZ) =\nlogdfrac1 + hatrho_XYbfZ1 - hatrho_XYbfZ\n\nTo test the null hypothesis against the alternative hypothesis H₁ := ρ(X, Y | 𝐙) > 0, calculate\n\nhatZ = dfrac12dfracZ(hatrho_XYbfZ) - Z(0)sqrt1(n - d - 3)\n\nand compute the two-sided p-value (Schmidt et al., 2018)\n\np(X Y  bfZ) = 2(1 - phi(sqrtn - d - 3Z(hatrho_XYbfZ)))\n\nwhere d is the dimension of bfZ and n is the number of samples. For the pairwise case, the procedure is identical, but set bfZ = emptyset.\n\nExamples\n\nExample 1. Pairwise and conditional tests for independence   on coupled noise processes.\n\n\n\n\n\n","category":"type"},{"location":"independence/#Associations.CorrTestResult","page":"Independence","title":"Associations.CorrTestResult","text":"CorrTestResult(pvalue, ρ, z)\n\nA simple struct that holds the results of a CorrTest test: the (partial) correlation coefficient ρ, Fisher's z, and pvalue - the two-sided p-value for the test.\n\n\n\n\n\n","category":"type"},{"location":"examples/examples_independence/#examples_independence","page":"Independence testing","title":"Examples of independence testing","text":"","category":"section"},{"location":"examples/examples_independence/#example_CorrTest","page":"Independence testing","title":"CorrTest","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n\n# Some normally distributed data\nX = randn(rng, 1000) \nY = 0.5*randn(rng, 1000) .+ X\nZ = 0.5*randn(rng, 1000) .+ Y\nW = randn(rng, 1000);","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Let's test a few independence relationships. For example, we expect that X ⫫ W. We also expect dependence X !⫫ Z, but this dependence should vanish when conditioning on the intermediate variable, so we expect X ⫫ Z | Y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(CorrTest(), X, W)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the outcome is that we can't reject the null hypothesis that X ⫫ W.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(CorrTest(), X, Z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"However, we can reject the  null hypothesis that X ⫫ Z, so the evidence favors the alternative hypothesis X !⫫ Z.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(CorrTest(), X, Z, Y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the correlation between X and Z significantly vanishes when conditioning on Y, because Y is solely responsible for the observed correlation between X and Y.","category":"page"},{"location":"examples/examples_independence/#examples_independence_JointDistanceDistributionTest","page":"Independence testing","title":"JointDistanceDistributionTest","text":"","category":"section"},{"location":"examples/examples_independence/#Bidirectionally-coupled-logistic-maps","page":"Independence testing","title":"Bidirectionally coupled logistic maps","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Let's use the built-in logistic2_bidir discrete dynamical system to create a pair of bidirectionally coupled time series and use the JointDistanceDistributionTest to see if we can confirm from observed time series that these variables are bidirectionally coupled. We'll use a significance level of 1 - α = 0.99, i.e. α = 0.01.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing DynamicalSystemsBase\nusing Random; rng = Xoshiro(1234)\nBase.@kwdef struct Logistic2Bidir{V, C1, C2, R1, R2, Σx, Σy, R}\n    xi::V = [0.5, 0.5]\n    c_xy::C1 = 0.1\n    c_yx::C2 = 0.1\n    r₁::R1 = 3.78\n    r₂::R2 = 3.66\n    σ_xy::Σx = 0.05\n    σ_yx::Σy = 0.05\n    rng::R = Random.default_rng()\nend\n\nfunction system(definition::Logistic2Bidir)\n    return DiscreteDynamicalSystem(eom_logistic2bidir, definition.xi, definition)\nend\n\nfunction eom_logistic2bidir(u, p::Logistic2Bidir, t)\n    (; xi, c_xy, c_yx, r₁, r₂, σ_xy, σ_yx, rng) = p\n    x, y = u\n    f_xy = (y +  c_xy*(x + σ_xy * rand(rng)) ) / (1 + c_xy*(1+σ_xy))\n    f_yx = (x +  c_yx*(y + σ_yx * rand(rng)) ) / (1 + c_yx*(1+σ_yx))\n    dx = r₁ * (f_yx) * (1 - f_yx)\n    dy = r₂ * (f_xy) * (1 - f_xy)\n    return SVector{2}(dx, dy)\nend","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We start by generating some time series and configuring the test.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nsys = system(Logistic2Bidir(c_xy = 0.5, c_yx = 0.4))\nx, y = columns(first(trajectory(sys, 2000, Ttr = 10000)))\nmeasure = JointDistanceDistribution(D = 5, B = 5)\ntest = JointDistanceDistributionTest(measure)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Now, we test for independence in both directions.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, y, x)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the null hypothesis is rejected in both directions at the pre-determined  significance level, and hence we detect directional coupling in both directions.","category":"page"},{"location":"examples/examples_independence/#Non-coupled-logistic-maps","page":"Independence testing","title":"Non-coupled logistic maps","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"What happens in the example above if there is no coupling?","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"sys = system(Logistic2Bidir(c_xy = 0.00, c_yx = 0.0))\nx, y = columns(first(trajectory(sys, 1000, Ttr = 10000)));\nrxy = independence(test, x, y)\nryx = independence(test, y, x)\npvalue(rxy), pvalue(ryx)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"At significance level 0.99, we can't reject the null in either direction, hence there's not enough evidence in the data to suggest directional coupling.","category":"page"},{"location":"examples/examples_independence/#examples_surrogatetest","page":"Independence testing","title":"SurrogateAssociationTest","text":"","category":"section"},{"location":"examples/examples_independence/#example_SurrogateAssociationTest_DistanceCorrelation","page":"Independence testing","title":"Distance correlation","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nx = randn(1000)\ny = randn(1000) .+ 0.5x\nindependence(SurrogateAssociationTest(DistanceCorrelation()), x, y)","category":"page"},{"location":"examples/examples_independence/#example_SurrogateAssociationTest_PartialCorrelation","page":"Independence testing","title":"Partial correlation","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nx = randn(1000)\ny = randn(1000) .+ 0.5x\nz = randn(1000) .+ 0.8y\nindependence(SurrogateAssociationTest(PartialCorrelation()), x, z, y)","category":"page"},{"location":"examples/examples_independence/#example_SurrogateAssociationTest_SMeasure","page":"Independence testing","title":"SMeasure","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nx, y = randn(1000), randn(1000)\nmeasure = SMeasure(dx = 4, dy = 3)\ns = association(measure,     x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The s statistic is larger when there is stronger coupling and smaller when there is weaker coupling. To check whether s is significant (i.e. large enough to claim directional dependence), we can use a SurrogateAssociationTest.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test = SurrogateAssociationTest(measure)\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The p-value is high, and we can't reject the null at any reasonable significance level. Hence, there isn't evidence in the data to support directional coupling from x to y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"What happens if we use coupled variables?","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"z = x .+ 0.1y\nindependence(test, x, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Now we can confidently reject the null (independence), and conclude that there is evidence in the data to support directional dependence from x to z.","category":"page"},{"location":"examples/examples_independence/#example_SurrogateAssociationTest_MIShannon_categorical","page":"Independence testing","title":"MIShannon, categorical","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"In this example, we expect the preference and the food variables to be independent.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n# Simulate \nn = 1000\npreference = rand(rng, [\"yes\", \"no\"], n)\nfood = rand(rng, [\"veggies\", \"meat\", \"fish\"], n)\nest = JointProbabilities(MIShannon(), CodifyVariables(UniqueElements()))\ntest = SurrogateAssociationTest(est)\nindependence(test, preference, food)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, there's not enough evidence to reject the null hypothesis that the variables are independent.","category":"page"},{"location":"examples/examples_independence/#example_SurrogateAssociationTest_CMIShannon_categorical","page":"Independence testing","title":"CMIShannon, categorical","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we simulate a survey at a ski resort. The data are such that the place a person grew up is associated with how many times they fell while going skiing. The control happens through an intermediate variable preferred_equipment, which indicates what type of physical activity the person has engaged with in the past. Some activities like skateboarding leads to better overall balance, so people that are good on a skateboard also don't fall, and people that to less challenging activities fall more often.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We should be able to reject places ⫫ experience, but not reject places ⫫ experience | preferred_equipment.  Let's see if we can detect these relationships using (conditional) mutual information.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 1000\n\nplaces = rand(rng, [\"city\", \"countryside\", \"under a rock\"], n);\npreferred_equipment = map(places) do place\n    if cmp(place, \"city\") == 1\n        return rand(rng, [\"skateboard\", \"bmx bike\"])\n    elseif cmp(place, \"countryside\") == 1\n        return rand(rng, [\"sled\", \"snowcarpet\"])\n    else\n        return rand(rng, [\"private jet\", \"ferris wheel\"])\n    end\nend;\nexperience = map(preferred_equipment) do equipment\n    if equipment ∈ [\"skateboard\", \"bmx bike\"]\n        return \"didn't fall\"\n    elseif equipment ∈ [\"sled\", \"snowcarpet\"]\n        return \"fell 3 times or less\"\n    else\n        return \"fell uncontably many times\"\n    end\nend;\nest_mi = JointProbabilities(MIShannon(), CodifyVariables(UniqueElements()))\ntest = SurrogateAssociationTest(est_mi)\nindependence(test, places, experience)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the evidence favors the alternative hypothesis that places and  experience are dependent.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"est_cmi = JointProbabilities(CMIShannon(), CodifyVariables(UniqueElements()))\ntest = SurrogateAssociationTest(est_cmi)\nindependence(test, places, experience, preferred_equipment)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Again, as expected, when conditioning on the mediating variable, the dependence disappears, and we can't reject the null hypothesis of independence.","category":"page"},{"location":"examples/examples_independence/#example_independence_MCR","page":"Independence testing","title":"MCR","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing Random; rng = Xoshiro(1234)\n\nx = rand(rng, 300)\ny = rand(rng, 300)\ntest = SurrogateAssociationTest(MCR(r = 0.5); rng, nshuffles = 100, surrogate = RandomShuffle())\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we can't reject independence. What happens if two variables are coupled?","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nx = rand(rng, 300)\nz = x .+ rand(rng, 300)\ntest = SurrogateAssociationTest(MCR(r = 0.5); rng, nshuffles = 100, surrogate = RandomShuffle())\nindependence(test, x, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Now, because the variables are coupled, the evidence in the data support dependence.","category":"page"},{"location":"examples/examples_independence/#example_LocalPermutationTest","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"To demonstrate the local permutation test for independence, we'll again use the  chain of unidirectionally coupled logistic maps.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We'll implement a set of chained logistic maps with unidirectional coupling.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using DynamicalSystemsBase\nBase.@kwdef struct Logistic4Chain{V, RX, RY, RZ, RW, C1, C2, C3, Σ1, Σ2, Σ3, RNG}\n    xi::V = [0.1, 0.2, 0.3, 0.4]\n    rx::RX = 3.9\n    ry::RY = 3.6\n    rz::RZ = 3.6\n    rw::RW = 3.8\n    c_xy::C1 = 0.4\n    c_yz::C2 = 0.4\n    c_zw::C3 = 0.35\n    σ_xy::Σ1 = 0.05\n    σ_yz::Σ2 = 0.05\n    σ_zw::Σ3 = 0.05\n    rng::RNG = Random.default_rng()\nend\n\nfunction eom_logistic4_chain(u, p::Logistic4Chain, t)\n    (; xi, rx, ry, rz, rw, c_xy, c_yz, c_zw, σ_xy, σ_yz, σ_zw, rng) = p\n    x, y, z, w = u\n    f_xy = (y +  c_xy*(x + σ_xy * rand(rng)) ) / (1 + c_xy*(1+σ_xy))\n    f_yz = (z +  c_yz*(y + σ_yz * rand(rng)) ) / (1 + c_yz*(1+σ_yz))\n    f_zw = (w +  c_zw*(z + σ_zw * rand(rng)) ) / (1 + c_zw*(1+σ_zw))\n    dx = rx * x * (1 - x)\n    dy = ry * (f_xy) * (1 - f_xy)\n    dz = rz * (f_yz) * (1 - f_yz)\n    dw = rw * (f_zw) * (1 - f_zw)\n    return SVector{4}(dx, dy, dz, dw)\nend\n\n\nfunction system(definition::Logistic4Chain)\n    return DiscreteDynamicalSystem(eom_logistic4_chain, definition.xi, definition)\nend","category":"page"},{"location":"examples/examples_independence/#example_LocalPermutationTest_CMIShannon","page":"Independence testing","title":"CMIShannon","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"To estimate CMI, we'll use the Kraskov differential entropy estimator, which naively computes CMI as a sum of entropy terms without guaranteed bias cancellation.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing Random; rng = Xoshiro(1234)\nn = 100\nX = randn(rng, n)\nY = X .+ randn(rng, n) .* 0.4\nZ = randn(rng, n) .+ Y\nx, y, z = StateSpaceSet.((X, Y, Z))\ntest = LocalPermutationTest(FPVP(CMIShannon()), nshuffles = 19)\nindependence(test, x, y, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We expect there to be a detectable influence from X to Y, if we condition on Z or not, because Z doesn't influence neither X nor Y. The null hypothesis is that the first two variables are conditionally independent given the third, which we reject with a very low p-value. Hence, we accept the alternative hypothesis that the first two variables X and Y. are conditionally dependent given Z.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, x, z, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we cannot reject the null hypothesis that X and Z are conditionally independent given Y, because Y is the variable that transmits information from X to Z.","category":"page"},{"location":"examples/examples_independence/#example_LocalPermutationTest_TEShannon","page":"Independence testing","title":"TEShannon","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we demonstrate LocalPermutationTest with the TEShannon measure with default parameters and the FPVP estimator. We'll use the system of four coupled logistic maps that are linked X → Y → Z → W defined above.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We should expect the transfer entropy X → Z to be non-significant when conditioning on Y, because all information from X to Z is transferred through Y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using Associations\nusing Random; rng = Random.default_rng()\nn = 300\nsys = system(Logistic4Chain(; xi = rand(rng, 4), rng))\nx, y, z, w = columns(trajectory(sys, n) |> first)\nest = CMIDecomposition(TEShannon(), FPVP(k = 10))\ntest = LocalPermutationTest(est, nshuffles = 19)\nindependence(test, x, z, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we cannot reject the null hypothesis that X and Z are conditionally independent given Y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The same goes for variables one step up the chain:","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, y, w, z)","category":"page"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"CollapsedDocStrings = true","category":"page"},{"location":"causal_graphs/#causal_graphs","page":"Network/graph inference","title":"Inferring causal graphs","text":"","category":"section"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"Directed causal graphical models, estimated on observed data, is an incredibly useful framework for causal inference. There exists a plethora of methods for estimating such models.","category":"page"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"Useful reading:","category":"page"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"Pearl, J. Glymour, M., & Jewell, N. P. (2016). Causal inference in statistics:   A primer. John Wiley & Sons. An excellent introductory book, suitable for anyone   interested, from a beginners to experts.\nGlymour, C., Zhang, K., & Spirtes, P. (2019). Review of causal discovery methods   based on graphical models. Frontiers in genetics, 10, 524. The authoritative   overview of causal discovery from graphical models. Many more methods have also emerged   since this paper.","category":"page"},{"location":"causal_graphs/#Causal-graph-API","page":"Network/graph inference","title":"Causal graph API","text":"","category":"section"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"The API for inferring causal graphs is defined by:","category":"page"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"infer_graph\nGraphAlgorithm, and its subtypes","category":"page"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"infer_graph\nGraphAlgorithm","category":"page"},{"location":"causal_graphs/#Associations.infer_graph","page":"Network/graph inference","title":"Associations.infer_graph","text":"infer_graph(algorithm::GraphAlgorithm, x) → g\n\nInfer graph from input data x using the given algorithm.\n\nReturns g, whose type depends on algorithm.\n\n\n\n\n\n","category":"function"},{"location":"causal_graphs/#Associations.GraphAlgorithm","page":"Network/graph inference","title":"Associations.GraphAlgorithm","text":"GraphAlgorithm\n\nThe supertype of all causal graph inference algorithms.\n\nConcrete implementations\n\nOCE. The optimal causation entropy algorithm for time series graphs.\nPC.\n\n\n\n\n\n","category":"type"},{"location":"causal_graphs/#docs_OCE","page":"Network/graph inference","title":"Optimal causation entropy","text":"","category":"section"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"OCE\nOCESelectedParents","category":"page"},{"location":"causal_graphs/#Associations.OCE","page":"Network/graph inference","title":"Associations.OCE","text":"OCE <: GraphAlgorithm\nOCE(; utest::IndependenceTest = SurrogateAssociationTest(MIShannon(), KSG2(k = 3, w = 3)),\n      ctest::C = LocalPermutationTest(CMIShannon(), MesnerShalizi(k = 3, w = 3)),\n      τmax::T = 1, α = 0.05)\n\nThe optimal causation entropy (OCE) algorithm for causal discovery Sun et al. (2015).\n\nDescription\n\nThe OCE algorithm has three steps to determine the parents of a variable xᵢ.\n\nPerform pairwise independence tests using utest and select the variable xⱼ(-τ)  that has the highest significant (i.e. with associated p-value below α)  association with xᵢ(0). Assign it to the set of selected parents P.\nPerform conditional independence tests using ctest, finding the parent  Pₖ that has the highest association with xᵢ given the already selected parents,  and add it to P.  Repeat until no more variables with significant association are found.\nBackwards elimination of parents Pₖ of xᵢ(0) for which xᵢ(0) ⫫ Pₖ | P - {Pₖ},  where P is the set of parent nodes found in the previous steps.\n\nτmax indicates the maximum lag τ between the target variable xᵢ(0) and its potential parents xⱼ(-τ). Sun et al. 2015's method is based on τmax = 1.\n\nReturns\n\nWhen used with infer_graph, it returns a vector p, where p[i] are the parents for each input variable. This result can be converted to a SimpleDiGraph from Graphs.jl (see example).\n\nUsage\n\nOCE is used with infer_graph to infer the parents of the input data. Input data must either be a Vector{Vector{<:Real}}, or a StateSpaceSet.\n\nExamples\n\nInferring time series graph from a chain of logistic maps\n\n\n\n\n\n","category":"type"},{"location":"causal_graphs/#Associations.OCESelectedParents","page":"Network/graph inference","title":"Associations.OCESelectedParents","text":"OCESelectedParents\n\nA simple struct for storing the parents of a single variable xᵢ inferred by the OCE algorithm. When using OCE with infer_graph, a Vector{OCESelectedParents} is returned - one per variable in the input data.\n\nAssumptions and notation\n\nAssumes the input x is a Vector{Vector{<:Real}} or a StateSpaceSet (for which each column is treated as a variable). It contains the following fields, where we use the notation xₖ(τ) to indicate the k-th variable lagged by time-lag τ. For example, x₂(-3) is the variable x[2] lagged by 3 time steps.\n\nFields\n\ni: The index of the target variable (i.e. xᵢ(0) is the target).\nall_idxs: The possible variable indices of parent variables (i.e. 1:M,   where M is the number of input variables).\nparents_js: The variable indices of the selected parent variables –- one per selected   parent.\nparents_τs: The lags for the selected parent variables –- one per selected parent.\nparents: A vector containing the raw, time-lagged data for each selected parent   variables. Let τ = parents_τs[k] and j = parents_js[k]. Then parents[k] is   the raw data for the variable xⱼ(-τ).\n\n\n\n\n\n","category":"type"},{"location":"causal_graphs/#[PC](@ref)","page":"Network/graph inference","title":"PC","text":"","category":"section"},{"location":"causal_graphs/","page":"Network/graph inference","title":"Network/graph inference","text":"PC","category":"page"},{"location":"causal_graphs/#Associations.PC","page":"Network/graph inference","title":"Associations.PC","text":"PC <: GraphAlgorithm\nPC(pairwise_test, conditional_test;\n    α = 0.05, max_depth = Inf, maxiters_orient = Inf)\n\nThe PC algorithm (Spirtes et al., 2000), which is named named after the first names of the authors, Peter Spirtes and Clark Glymour, which is implemented as described in Kalisch and Bühlmann (2008).\n\nArguments\n\npairwise_test: An IndependenceTest that uses a pairwise,   nondirectional AssociationMeasure measure (e.g. a parametric   CorrTest, or SurrogateAssociationTest with the MIShannon measure).\nconditional_test: An IndependenceTest that uses a conditional,   nondirectional AssociationMeasure (e.g. CorrTest,   or SurrogateAssociationTest with the CMIShannon measure).\n\nKeyword arguments\n\nα::Real. The significance level of the test.\nmax_depth. The maximum level of conditional indendence tests to be   performed. By default, there is no limit (i.e. max_depth = Inf), meaning that   maximum depth is N - 2, where N is the number of input variables.\nmaxiters_orient::Real. The maximum number of times to apply the orientation   rules. By default, there is not limit (i.e. maxiters_orient = Inf).\n\ninfo: Directional measures will not give meaningful answers\nDuring the skeleton search phase, if a significance association between two nodes are is found, then a bidirectional edge is drawn between them. The generic implementation of PC therefore doesn't currently handle directional measures such as TEShannon. The reason is that if a  directional relationship X → Y exists between two nodes X and Y, then the algorithm would first draw a bidirectional arrow between X and Y when analysing the direction X → Y, and then removing it again when analysing in the direction Y → X (a similar situation would also occur for the conditional stage). This will be fixed in a future release. For now, use nondirectional measures, e.g. MIShannon and CMIShannon!\n\nDescription\n\nWhen used with infer_graph on some input data x, the PC algorithm performs the following steps:\n\nInitialize an empty fully connected graph g with N nodes, where N is the number  of variables and x[i] is the data for the i-th node.\nReduce the fully connected g to a skeleton graph by performing pairwise  independence tests between all vertices using pairwise_test. Remove  any edges where adjacent vertices are found to be independent according to the test  (i.e. the null hypothesis of independence cannot be rejected at significance level  1 - α).\nThin the skeleton g by conditional independence testing. If  x[i] ⫫ x[j] | x[Z] for some set of variables Z (not including i and j)  according to conditional_test (i.e. the null hypothesis of conditional independence  cannot be rejected at significance level 1 - α), then the edge between i and j is  removed, and we record the separating set S(i, j) = Z. Independence tests are first  performed for conditioning sets of size 1, and repeated for conditioning sets of  increasing size, which in most cases limits the number of tests needed.  The separating  sets S(i, j), which records which variables were in the conditioning set that  rendered variables i and j independent, are recorded.  If max_depth is an integer, then this procedure is performed on conditioning  sets of sizes 1:max_depth, and if max_depth == nothing, then all possible  conditioning set sizes are potentially used.\nCreate a directed graph dg from g by replacing every  undirected edge X - Y in g by the bidirectional edge X ↔ Y (i.e.  construct two directional edges X → Y and Y → X). Orientiation rules 0-3  are then repeatedly applied to dg until no more edges can be oriented:\nRule 0 (orients v-structures): X ↔ Y ↔ Z becomes X → Y ← Z if Y is not in the   separating set S(X, Z).\nRule 1 (prevents new v-structures): X → Y ↔ Z becomes X → Y → Z if X and Z   are not adjacent.\nRule 2 (avoids cycles): X → Y → Z ↔ X becomes X → Y → Z ← X.\nRule 3: To avoid creating cycles or new v-structures, whenever X - Y → Z,   X - W → Z, and X - Z but there is no edge between Y and W, turn the   undirected X - Z edge into the directed edge X → Z.\n\nThe resulting directed graph (a SimpleDiGraph from Graphs.jl) is then returned.\n\nExamples\n\nPC algorithm with parametric independence tests\nPC algorithm with nonparametric independence tests\n\n\n\n\n\n","category":"type"},{"location":"#Associations.jl","page":"Associations.jl","title":"Associations.jl","text":"","category":"section"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Associations","category":"page"},{"location":"#Associations","page":"Associations.jl","title":"Associations","text":"Associations\n\n(Image: CI) (Image: ) (Image: ) (Image: codecov) (Image: DOI)\n\nAssociations.jl is a package for quantifying associations, independence testing and causal inference.\n\nAll further information is provided in the documentation, which you can either find online or build locally by running the docs/make.jl file.\n\nKey features\n\nAssociation API: includes measures and their estimators for pairwise, conditional and other forms of    association from conventional statistics, from dynamical systems theory, and from information theory: partial correlation, distance correlation, (conditional) mutual information, transfer entropy, convergent cross mapping and a lot more!\nIndependence testing API, which is automatically compatible with   every association measure estimator implemented in the package. \nCausal (network) inference API integrating the association measures and independence testing framework.\n\nAddititional features\n\nExtending on features from ComplexityMeasures.jl, we also offer \n\nDiscretization API for multiple (multivariate) input datasets.\nMultivariate counting and probability estimation API.\nMultivariate information measure API\n\nInstallation\n\nTo install the package, run import Pkg; Pkg.add(\"Associations\").\n\nPreviously, this package was called CausalityTools.jl.\n\n\n\n\n\n","category":"module"},{"location":"#Latest-news","page":"Associations.jl","title":"Latest news","text":"","category":"section"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"info: Package rename\nThe package has been renamed from CausalityTools.jl to Associations.jl.","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Associations.jl has been updated to v4! ","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"This update includes a number of breaking changes, several of which are not backwards compatible. These are done to ensure compatibility with  ComplexityMeasures.jl v3, which provides discretization functionality that we use here.","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Important changes are:","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Convenience methods have been removed completely. Use association instead.\nExample systems have been removed.\nThe syntax for computing an association has changed. Estimators now always contain the definition it estimates. For example, association(MIShannon(), KSG1(), x, y) is now association(KSG1(MIShannon()), x, y).\nSurrogateTest has been renamed to SurrogateAssociationTest. \nSee the CHANGELOG.md for a complete list of changes.","category":"page"},{"location":"#Getting-started","page":"Associations.jl","title":"Getting started","text":"","category":"section"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"The quickest way to get going with the package is to check out the examples in the left-hand menu.","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"info: Info\nTo make it easier to navigate the extensive documentation, all documentation strings are  collapsed by default. Click the arrow icon in  the top toolbar to expand/collapse the docstrings in a page.","category":"page"},{"location":"#Documentation-content","page":"Associations.jl","title":"Documentation content","text":"","category":"section"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Association measures lists all implemented association measures and their estimators.\nIndependence testing lists all implemented ways of determining if an association between datasets is \"significant\".\nCausal inference lists all methods of inferring association networks (also called \"network graphs\" and \"causal graphs\") between multiple variables.\nNumerous examples for association measure estimation,  independence testing, and  network inference.","category":"page"},{"location":"#Input-data-for-Associations.jl","page":"Associations.jl","title":"Input data for Associations.jl","text":"","category":"section"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Input data for Associations.jl are given as:","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Univariate timeseries, which are given as standard Julia Vectors.\nMultivariate timeseries, StateSpaceSets, or state space sets, which are given as   StateSpaceSets. Many methods convert timeseries inputs to StateSpaceSet   for faster internal computations.\nCategorical data can be used with JointProbabilities to compute various   information theoretic measures and is represented using any iterable whose elements   can be any arbitrarily complex data type (as long as it's hashable), for example   Vector{String}, {Vector{Int}}, or Vector{Tuple{Int, String}}.","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"StateSpaceSets.StateSpaceSet","category":"page"},{"location":"#StateSpaceSets.StateSpaceSet","page":"Associations.jl","title":"StateSpaceSets.StateSpaceSet","text":"StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T}\n\nA dedicated interface for sets in a state space. It is an ordered container of equally-sized points of length D. Each point is represented by SVector{D, T}. The data are a standard Julia Vector{SVector}, and can be obtained with vec(ssset::StateSpaceSet). Typically the order of points in the set is the time direction, but it doesn't have to be.\n\nWhen indexed with 1 index, StateSpaceSet is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more.\n\nStateSpaceSet also supports almost all sensible vector operations like append!, push!, hcat, eachrow, among others.\n\nDescription of indexing\n\nIn the following let i, j be integers, typeof(X) <: AbstractStateSpaceSet and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges, and for performance benefits make v2 an SVector{Int}).\n\nX[i] == X[i, :] gives the ith point (returns an SVector)\nX[v1] == X[v1, :], returns a StateSpaceSet with the points in those indices.\nX[:, j] gives the jth variable timeseries (or collection), as Vector\nX[v1, v2], X[:, v2] returns a StateSpaceSet with the appropriate entries (first indices being \"time\"/point index, while second being variables)\nX[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(ssset) or StateSpaceSet(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like StateSpaceSet(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"#Maintainers-and-contributors","page":"Associations.jl","title":"Maintainers and contributors","text":"","category":"section"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"The Associations.jl software is maintained by Kristian Agasøster Haaga, who also curates and writes this documentation. Significant contributions to the API and documentation design has been made by George Datseris, which also co-authors ComplexityMeasures.jl, which we develop in tandem with this package.","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"A complete list of contributors to this repo are listed on the main Github page. Some important contributions are:","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Norbert Genera contributed bug reports and   investigations that led to subsequent improvements for the pairwise asymmetric   inference algorithm and an improved cross mapping API.\nDavid Diego's contributions were   invaluable in the initial stages of development. His MATLAB code provided the basis   for several transfer entropy methods and binning-related code.\nGeorge Datseris also ported KSG1 and KSG2 mutual   information estimators to Neighborhood.jl.\nBjarte Hannisdal provided tutorials for mutual information.\nTor Einar Møller contributed to cross-mapping methods in initial stages of development.","category":"page"},{"location":"","page":"Associations.jl","title":"Associations.jl","text":"Many individuals has contributed code to other packages in the JuliaDynamics ecosystem which we use here. Contributors are listed in the respective GitHub repos and webpages.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/cross_map_api/#Cross-map-API","page":"Cross-map API","title":"Cross-map API","text":"","category":"section"},{"location":"api/cross_map_api/#Estimators","page":"Cross-map API","title":"Estimators","text":"","category":"section"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"CrossmapEstimator\nRandomVectors\nRandomSegment\nExpandingSegment\nEnsemble","category":"page"},{"location":"api/cross_map_api/#Associations.CrossmapEstimator","page":"Cross-map API","title":"Associations.CrossmapEstimator","text":"CrossmapEstimator{M<:CrossmapMeasure, LIBSIZES, RNG}\n\nThe abstract supertype for all cross-map estimators.\n\nConcrete subtypes\n\nRandomVectors\nRandomSegment\nExpandingSegment\n\nDescription\n\nBecause the type of the library may differ between estimators, and because RNGs from different packages may be used, subtypes must implement the LIBSIZES and RNG type parameters.\n\nFor efficiency purposes, subtypes may contain mutable containers that can be re-used for ensemble analysis (see Ensemble).\n\ninfo: Libraries\nA cross-map estimator uses the concept of \"libraries\". A library is essentially just a reference to a set of points, and usually, a library refers to indices of points, not the actual points themselves.For example, for timeseries, RandomVectors(libsizes = 50:25:100) produces three separate libraries, where the first contains 50 randomly selected time indices, the second contains 75 randomly selected time indices, and the third contains 100 randomly selected time indices. This of course assumes that all quantities involved can be indexed using the same time indices, meaning that the concept of \"library\" only makes sense after relevant quantities have been jointly embedded, so that they can be jointly indexed. For non-instantaneous prediction, the maximum possible library size shrinks with the magnitude of the index/time-offset for the prediction.For spatial analyses (not yet implemented), indices could be more complex and involve multi-indices.\n\n\n\n\n\n","category":"type"},{"location":"api/cross_map_api/#Associations.RandomVectors","page":"Cross-map API","title":"Associations.RandomVectors","text":"RandomVectors <: CrossmapEstimator\nRandomVectors(definition::CrossmapMeasure; libsizes, replace = false, \n    rng = Random.default_rng())\n\nCross map once over  N = length(libsizes) different \"point libraries\", where  point indices are selected randomly (not considering time ordering). \n\nThis is method 3 from Luo et al. (2015). See CrossmapEstimator for an in-depth  explanation of what \"library\" means in this context.\n\nDescription\n\nThe cardinality of the point libraries are given by libsizes. One set of  random point indices is selected per L ∈ libsizes, and the i-th  library has cardinality k = libsizes[i]. \n\nPoint indices within each library are randomly selected, independently of other libraries. A user-specified rng may be specified for reproducibility. The replace argument controls whether sampling is done with or without replacement. If the time series you're cross mapping between have length M, and Lᵢ < M for any Lᵢ ∈ libsizes, then you must set replace = true.\n\nReturns\n\nThe return type when used with association depends on the type of libsizes.\n\nIf libsizes is an Int (a single library), then a single cross-map estimate is returned.\nIf libsizes is an AbstractVector{Int} (multiple libraries), then a vector of cross-map   estimates is returned –- one per library.\n\nSee also: CrossmapEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/cross_map_api/#Associations.RandomSegment","page":"Cross-map API","title":"Associations.RandomSegment","text":"RandomSegment <: CrossmapEstimator\nRandomSegment(definition::CrossmapMeasure; libsizes::Int, rng = Random.default_rng())\n\nCross map once over N = length(libsizes) different \"point libraries\", where  point indices are selected as time-contiguous segments with random starting points.\n\nThis is method 2 from Luo et al. (2015). See CrossmapEstimator for an in-depth  explanation of what \"library\" means in this context.\n\nDescription\n\nThe cardinality of the point index segments are given by libsizes. One segment  with a randomly selected starting point is picked per L ∈ libsizes, and the i-th  point index segment has cardinality k = libsizes[i]. \n\nThe starting point for each library is selected independently of other libraries. A user-specified rng may be specified for reproducibility. If the time series you're cross mapping between have length M, and Lᵢ < M for any Lᵢ ∈ libsizes, then an error will be thrown.\n\nA user-specified rng may be specified for reproducibility.\n\nReturns\n\nThe return type when used with association depends on the type of libsizes.\n\nIf libsizes is an Int (a single library), then a single cross-map estimate is returned.\nIf libsizes is an AbstractVector{Int} (multiple libraries), then a vector of cross-map   estimates is returned –- one per library.\n\nSee also: CrossmapEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/cross_map_api/#Associations.ExpandingSegment","page":"Cross-map API","title":"Associations.ExpandingSegment","text":"ExpandingSegment <: CrossmapEstimator\nExpandingSegment(definition::CrossmapMeasure; libsizes, rng = Random.default_rng())\n\nCross map once over N = length(libsizes) different \"point libraries\", where  point indices are selected as time-contiguous segments/windows.\n\nThis is the method from (Sugihara et al., 2012). See CrossmapEstimator for an in-depth  explanation of what \"library\" means in this context.\n\nDescription\n\nPoint index segments are selected as first available data point index, up to the Lth data point index. This results in one library of contiguous time indices per L ∈ libsizes.\n\nIf used in an ensemble setting, the estimator is applied to time indices Lmin:step:Lmax of the joint embedding.\n\nReturns\n\nThe return type when used with association depends on the type of libsizes.\n\nIf libsizes is an Int (a single library), then a single cross-map estimate is returned.\nIf libsizes is an AbstractVector{Int} (multiple libraries), then a vector of cross-map   estimates is returned –- one per library.\n\n\n\n\n\n","category":"type"},{"location":"api/cross_map_api/#Associations.Ensemble","page":"Cross-map API","title":"Associations.Ensemble","text":"Ensemble(est::CrossmapEstimator{<:CrossmapMeasure}, nreps::Int = 100)\n\nA directive to compute an ensemble analysis, where measure (e.g. ConvergentCrossMapping) is computed using the given estimator est (e.g. RandomVectors)\n\nExamples\n\nExample 1:    Reproducing Figure 3A from Sugihara et al. (2012).\n\n\n\n\n\n","category":"type"},{"location":"api/cross_map_api/#Advanced-utility-methods","page":"Cross-map API","title":"Advanced utility methods","text":"","category":"section"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"For most use cases, it is sufficient to provide a CrossmapEstimator to  association to compute a cross map measure. However, in some cases it  can be useful to have more fine-grained controls. We offer a few utility functions for this purpose.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"These functions are used in the examples below, where we reproduce Figures 3C and 3D of Sugihara et al. (2012) and reproduce figures from McCracken and Weigel (2014).","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"predict\ncrossmap","category":"page"},{"location":"api/cross_map_api/#Associations.predict","page":"Cross-map API","title":"Associations.predict","text":"predict(measure::CrossmapEstimator, t::AbstractVector, s::AbstractVector) → t̂ₛ, t̄, ρ\npredict(measure::CrossmapEstimator, t̄::AbstractVector, S̄::AbstractStateSpaceSet) → t̂ₛ\n\nPerform point-wise cross mappings between source embeddings and target time series according to the algorithm specified by the given cross-map measure (e.g. ConvergentCrossMapping or PairwiseAsymmetricInference).\n\nFirst method: Jointly embeds the target t and source s time series (according to   measure) to obtain time-index aligned target timeseries t̄ and source embedding   S̄ (which is now a StateSpaceSet).   Then calls predict(measure, t̄, S̄) (the first method), and returns both the   predictions t̂ₛ, observations t̄ and their correspondence ρ according to measure.\nSecond method: Returns a vector of predictions t̂ₛ (t̂ₛ := \"predictions of t̄ based   on source embedding S̄\"), where t̂ₛ[i] is the prediction for t̄[i]. It assumes   pre-embedded data which have been correctly time-aligned using a joint embedding, i.e.    such that t̄[i] and S̄[i] correspond to the same time index.\n\nDescription\n\nFor each i ∈ {1, 2, …, N} where N = length(t) == length(s), we make the prediction t̂[i] (an estimate of t[i]) based on a linear combination of D + 1 other points in t, where the selection of points and weights for the linear combination are determined by the D+1 nearest neighbors of the point S̄[i]. The details of point selection and weights depend on measure.\n\nNote: Some CrossmapMeasures may define more general mapping procedures. If so, the algorithm is described in their docstring.\n\n\n\n\n\n","category":"function"},{"location":"api/cross_map_api/#Associations.crossmap","page":"Cross-map API","title":"Associations.crossmap","text":"crossmap(measure::CrossmapEstimator, t::AbstractVector, s::AbstractVector) → ρ::Real\ncrossmap(measure::CrossmapEstimator, est, t::AbstractVector, s::AbstractVector) → ρ::Vector\ncrossmap(measure::CrossmapEstimator, t̄::AbstractVector, S̄::AbstractStateSpaceSet) → ρ\n\nCompute the cross map estimates between between raw time series t and s (and return the real-valued cross-map statistic ρ). If a CrossmapEstimator est is provided, cross mapping is done on random subsamples of the data, where subsampling is dictated by est (a vector of values for ρ is returned).\n\nAlternatively, cross-map between time-aligned time series t̄ and source embedding S̄ that have been constructed by jointly (pre-embedding) some input data.\n\nThis is just a wrapper around predict that simply returns the correspondence measure between the source and the target.\n\n\n\n\n\n","category":"function"},{"location":"api/cross_map_api/#example_ConvergentCrossMapping_reproducing_sugihara","page":"Cross-map API","title":"Example: reproducing Sugihara et al. (2012)","text":"","category":"section"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"note: Run blocks consecutively\nIf copying these examples and running them locally, make sure the relevant packages (given in the first block) are loaded first.","category":"page"},{"location":"api/cross_map_api/#Figure-3A","page":"Cross-map API","title":"Figure 3A","text":"","category":"section"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"Let's reproduce figure 3A too, focusing only on ConvergentCrossMapping this time. In this figure, they compute the cross mapping for libraries of increasing size, always starting at time index 1. This approach - which we here call the ExpandingSegment estimator - is one of many ways of estimating the correspondence between observed and predicted value.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"For this example, they use a bidirectional system with asymmetrical coupling strength.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"using Associations\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n\nfunction eom_logistic_sugi(u, p, t)\n    (; rx, ry, βxy, βyx) = p\n    (; x, y) = u\n\n    dx = x*(rx - rx*x - βxy*y)\n    dy = y*(ry - ry*y - βyx*x)\n    return SVector{2}(dx, dy)\nend\n\n# βxy := effect on x of y\n# βyx := effect on y of x\nfunction logistic_sugi(; u0 = rand(2), rx, ry, βxy, βyx)\n    p = @LArray [rx, ry, βxy, βyx] (:rx, :ry, :βxy, :βyx)\n    DiscreteDynamicalSystem(eom_logistic_sugi, u0, p)\nend\n\n# Used in `reproduce_figure_3A_naive`, and `reproduce_figure_3A_ensemble` below.\nfunction add_to_fig!(fig_pos, libsizes, ρs_x̂y, ρs_ŷx; title = \"\", quantiles = false)\n    ax = Axis(fig_pos; title, aspect = 1,\n        xlabel = \"Library size\", ylabel = \"Correlation (ρ)\")\n    ylims!(ax, (-1, 1))\n    hlines!([0], linestyle = :dash, alpha = 0.5, color = :grey)\n    scatterlines!(libsizes, median.(ρs_x̂y), label = \"x̂|y\", color = :blue)\n    scatterlines!(libsizes, median.(ρs_ŷx), label = \"ŷ|x\", color = :red)\n    if quantiles\n        band!(libsizes, quantile.(ρs_x̂y, 0.05), quantile.(ρs_x̂y, 0.95), color = (:blue, 0.5))\n        band!(libsizes, quantile.(ρs_ŷx, 0.05), quantile.(ρs_ŷx, 0.95), color = (:red, 0.5))\n    end\n    axislegend(ax, position = :rb)\nend\n\nfunction reproduce_figure_3A_naive(definition::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, βxy = 0.02, βyx = 0.32);\n    x, y = columns(first(trajectory(sys_bidir, 3100, Ttr = 10000)));\n    libsizes = [20:2:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:3000]\n    est = ExpandingSegment(definition; libsizes);\n    ρs_x̂y = crossmap(est, x, y)\n    ρs_ŷx = crossmap(est, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, ρs_x̂y, ρs_ŷx; title = \"`ExpandingSegment`\")\n        fig\n    end\nend\n\nreproduce_figure_3A_naive(ConvergentCrossMapping(d = 3))","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"Hm. This looks a bit like the paper, but the curve is not smooth. We can do better!","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"It is not clear from the paper exactly what they plot in their Figure 3A, if they plot an average of some kind, or precisely what parameters and initial conditions they use. However, we can get a smoother plot by using a Ensemble. Combined with a CrossmapEstimator, it uses Monte Carlo resampling on subsets of the input data to compute an ensemble of ρs that we here use to compute the median and 90-th percentile range for each library size.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"function reproduce_figure_3A_ensemble(definition::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.4, 0.2], rx = 3.8, ry = 3.5, βxy = 0.02, βyx = 0.1);\n    x, y = columns(first(trajectory(sys_bidir, 5000, Ttr = 10000)));\n    # Note: our time series are 1000 points long. When embedding, some points are\n    # lost, so we must use slightly less points for the segments than \n    # there are points in the original time series.\n    libsizes = [20:5:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:2000]\n    # No point in doing more than one rep, because there data are always the same\n    # for `ExpandingSegment.`\n    ensemble_ev = Ensemble(ExpandingSegment(definition; libsizes); nreps = 1)\n    ensemble_rs = Ensemble(RandomSegment(definition; libsizes); nreps = 30)\n    ensemble_rv = Ensemble(RandomVectors(definition; libsizes); nreps = 30)\n    ρs_x̂y_es = crossmap(ensemble_ev, x, y)\n    ρs_ŷx_es = crossmap(ensemble_ev, y, x)\n    ρs_x̂y_rs = crossmap(ensemble_rs, x, y)\n    ρs_ŷx_rs = crossmap(ensemble_rs, y, x)\n    ρs_x̂y_rv = crossmap(ensemble_rv, x, y)\n    ρs_ŷx_rv = crossmap(ensemble_rv, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, ρs_x̂y_es, ρs_ŷx_es; title = \"`ExpandingSegment`\", quantiles = false) # quantiles make no sense for `ExpandingSegment`\n        add_to_fig!(fig[1, 2], libsizes, ρs_x̂y_rs, ρs_ŷx_rs; title = \"`RandomSegment`\", quantiles = true)\n        add_to_fig!(fig[1, 3], libsizes, ρs_x̂y_rv, ρs_ŷx_rv; title = \"`RandomVector`\", quantiles = true)\n        fig\n    end\nend\n\nreproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, τ = -1))","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"With the RandomVectors estimator, the mean of our ensemble ρs seem to look pretty much identical to Figure 3A in Sugihara et al. The RandomSegment estimator also performs pretty well, but since subsampled segments are contiguous, there are probably some autocorrelation effects at play.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"We can avoid the autocorrelation issue by tuning the w parameter of the ConvergentCrossMapping measure, which is the  Theiler window. Setting the Theiler window to w > 0, we can exclude neighbors of a query point p that are close to p in time, and thus deal with autocorrelation issues that way (the default w = 0 excludes only the point itself). Let's re-do the analysis with w = 5, just for fun.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"reproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, τ = -1, w = 5))","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"There wasn't really that much of a difference, since for the logistic map, the autocorrelation function flips sign for every lag increase. However, for examples from other systems, tuning w may be important.","category":"page"},{"location":"api/cross_map_api/#Figure-3B","page":"Cross-map API","title":"Figure 3B","text":"","category":"section"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"What about figure 3B? Here they generate time series of length 400 for a range of values for both coupling parameters, and plot the dominant direction Delta = rho(hatx  y) - rho(haty  x).","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"In the paper, they use a 1000 different parameterizations for the logistic map parameters, but don't state what is summarized in the plot. For simplicity, we'll therefore just stick to rx = ry = 3.7, as in the examples above, and just loop over the coupling strengths in either direction.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"function reproduce_figure_3B()\n    βxys = 0.0:0.02:0.4\n    βyxs = 0.0:0.02:0.4\n    ρx̂ys = zeros(length(βxys), length(βyxs))\n    ρŷxs = zeros(length(βxys), length(βyxs))\n\n    for (i, βxy) in enumerate(βxys)\n        for (j, βyx) in enumerate(βyxs)\n            sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.7, βxy, βyx);\n            # Generate 1000 points. Randomly select a 400-pt long segment.\n            x, y = columns(first(trajectory(sys_bidir, 400, Ttr = 10000)));\n            definition = CCM(d = 3, w = 5, τ = -1)\n            ensemble = Ensemble(RandomVectors(definition; libsizes = 100), nreps = 50)\n            ρx̂ys[i, j] = mean(crossmap(ensemble, x, y))\n            ρŷxs[i, j] = mean(crossmap(ensemble, y, x))\n        end\n    end\n    Δ = ρŷxs .- ρx̂ys\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure();\n        ax = Axis(fig[1, 1], xlabel = \"βxy\", ylabel = \"βyx\")\n        cont = contourf!(ax, Δ, levels = range(-1, 1, length = 10),\n            colormap = :curl)\n        ax.xticks = 1:length(βxys), string.([i % 2 == 0 ? βxys[i] : \"\" for i in 1:length(βxys)])\n        ax.yticks = 1:length(βyxs), string.([i % 2 == 0 ? βyxs[i] : \"\" for i in 1:length(βyxs)])\n        Colorbar(fig[1 ,2], cont, label = \"Δ (ρ(ŷ|x) - ρ(x̂|y))\")\n        tightlimits!(ax)\n        fig\n    end\nend\n\nreproduce_figure_3B()","category":"page"},{"location":"api/cross_map_api/#example_sugihara_figs3Cand3D","page":"Cross-map API","title":"Figures 3C and 3D","text":"","category":"section"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"Let's reproduce figures 3C and 3D in Sugihara et al. (2012), which introduced the ConvergentCrossMapping measure. Equations and parameters can be found in their supplementary material. Simulatenously, we also compute the PairwiseAsymmetricInference measure from McCracken and Weigel (2014), which is a related method, but uses a slightly different embedding.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"using Associations\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n\n# -----------------------------------------------------------------------------------------\n# Create 500-point long time series for Sugihara et al. (2012)'s example for figure 3.\n# -----------------------------------------------------------------------------------------\nsys_unidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, βxy = 0.00, βyx = 0.32);\nx, y = columns(first(trajectory(sys_unidir, 500, Ttr = 10000)));\n\n# -----------------------------------------------------------------------------------------\n# Cross map.\n# -----------------------------------------------------------------------------------------\nm_ccm = ConvergentCrossMapping(d = 2)\nm_pai = PairwiseAsymmetricInference(d = 2)\n# Make predictions x̂y, i.e. predictions `x̂` made from embedding of y (AND x, if PAI)\nt̂ccm_x̂y, tccm_x̂y, ρccm_x̂y = predict(m_ccm, x, y)\nt̂pai_x̂y, tpai_x̂y, ρpai_x̂y = predict(m_pai, x, y);\n# Make predictions ŷx, i.e. predictions `ŷ` made from embedding of x (AND y, if PAI)\nt̂ccm_ŷx, tccm_ŷx, ρccm_ŷx = predict(m_ccm, y, x)\nt̂pai_ŷx, tpai_ŷx, ρpai_ŷx = predict(m_pai, y, x);\n\n# -----------------------------------------------------------------------------------------\n# Plot results\n# -----------------------------------------------------------------------------------------\nρs = (ρccm_x̂y, ρpai_x̂y, ρccm_ŷx, ρpai_ŷx)\nsccm_x̂y, spai_x̂y, sccm_ŷx, spai_ŷx = (map(ρ -> (@sprintf \"%.3f\" ρ), ρs)...,)\n\nρs = (ρccm_x̂y, ρpai_x̂y, ρccm_ŷx, ρpai_ŷx)\nsccm_x̂y, spai_x̂y, sccm_ŷx, spai_ŷx = (map(ρ -> (@sprintf \"%.3f\" ρ), ρs)...,)\n\nwith_theme(theme_minimal(),\n    markersize = 5) do\n    fig = Figure();\n    ax_ŷx = Axis(fig[2,1], aspect = 1, xlabel = \"y(t) (observed)\", ylabel = \"ŷ(t) | x (predicted)\")\n    ax_x̂y = Axis(fig[2,2], aspect = 1, xlabel = \"x(t) (observed)\", ylabel = \"x̂(t) | y (predicted)\")\n    xlims!(ax_ŷx, (0, 1)), ylims!(ax_ŷx, (0, 1))\n    xlims!(ax_x̂y, (0, 1)), ylims!(ax_x̂y, (0, 1))\n    ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n    scatterlines!(ax_ts, x[1:300], label = \"x\")\n    scatterlines!(ax_ts, y[1:300], label = \"y\")\n    axislegend()\n    scatter!(ax_ŷx, tccm_ŷx, t̂ccm_ŷx, label = \"CCM (ρ = $sccm_ŷx)\", color = :black)\n    scatter!(ax_ŷx, tpai_ŷx, t̂pai_ŷx, label = \"PAI (ρ = $spai_ŷx)\", color = :red)\n    axislegend(ax_ŷx, position = :lt)\n    scatter!(ax_x̂y, tccm_x̂y, t̂ccm_x̂y, label = \"CCM (ρ = $sccm_x̂y)\", color = :black)\n    scatter!(ax_x̂y, tpai_x̂y, t̂pai_x̂y, label = \"PAI (ρ = $spai_x̂y)\", color = :red)\n    axislegend(ax_x̂y, position = :lt)\n    fig\nend","category":"page"},{"location":"api/cross_map_api/#example_PairwiseAsymmetricInference_reproduce_mccracken","page":"Cross-map API","title":"Example: reproducing McCracken & Weigel (2014)","text":"","category":"section"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"Let's try to reproduce figure 8 from McCracken and Weigel (2014)'s paper on PairwiseAsymmetricInference (PAI). We'll start by defining the their example B (equations 6-7). This system consists of two variables X and Y, where X drives Y.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"After we have computed the PAI in both directions, we define a measure of directionality as the difference between PAI in the X to Y direction and in the Y to X direction, so that if X drives Y, then Delta  0.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"using Associations\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\nusing Distributions: Normal\nusing Statistics: mean, std\n\nfunction eom_nonlinear_sindriver(dx, x, p, n)\n    a, b, c, t, Δt = (p...,)\n    x, y = x[1], x[2]\n    𝒩 = Normal(0, 1)\n    \n    dx[1] = sin(t)\n    dx[2] = a*x * (1 - b*x) + c* rand(𝒩)\n    p[end-1] += 1 # update t\n\n    return\nend\n\nfunction nonlinear_sindriver(;u₀ = rand(2), a = 1.0, b = 1.0, c = 2.0, Δt = 1)\n    DiscreteDynamicalSystem(eom_nonlinear_sindriver, u₀, [a, b, c, 0, Δt])\nend\n\nfunction reproduce_figure_8_mccraken(; \n        c = 2.0, Δt = 0.2,\n        as = 0.5:0.5:5.0,\n        bs = 0.5:0.5:5.0)\n    # -----------------------------------------------------------------------------------------\n    # Generate many time series for many different values of the parameters `a` and `b`,\n    # and compute PAI. This will replicate the upper right panel of \n    # figure 8 in McCracken & Weigel (2014).\n    # -----------------------------------------------------------------------------------------\n    \n    measure = PairwiseAsymmetricInference(d = 3)\n\n    # Manually resample `nreps` length-`L` time series and use mean ρ(x̂|X̄y) - ρ(ŷ|Ȳx)\n    # for each parameter combination.\n    nreps = 50\n    L = 200 # length of timeseries\n    Δ = zeros(length(as), length(bs))\n    for (i, a) in enumerate(as)\n        for (j, b) in enumerate(bs)\n            s = nonlinear_sindriver(; a, b, c,  Δt)\n            x, y = columns(first(trajectory(s, 1000, Ttr = 10000)))\n            Δreps = zeros(nreps)\n            for i = 1:nreps\n                # Ensure we're subsampling at the same time indices. \n                ind_start = rand(1:(1000-L))\n                r = ind_start:(ind_start + L)\n                Δreps[i] = @views crossmap(measure, y[r], x[r]) - \n                    crossmap(measure, x[r], y[r])\n            end\n            Δ[i, j] = mean(Δreps)\n        end\n    end\n\n    # -----------------------------------------------------------------------------------------\n    # An example time series for plotting.\n    # -----------------------------------------------------------------------------------------\n    sys = nonlinear_sindriver(; a = 1.0, b = 1.0, c, Δt)\n    npts = 500\n    orbit = first(trajectory(sys, npts, Ttr = 10000))\n    x, y = columns(orbit)\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        \n        X = x[1:300]\n        Y = y[1:300]\n        fig = Figure();\n        ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n        scatterlines!(ax_ts, (X .- mean(X)) ./ std(X), label = \"x\")\n        scatterlines!(ax_ts, (Y .- mean(Y)) ./ std(Y), label = \"y\")\n        axislegend()\n\n        ax_hm = Axis(fig[2, 1:2], xlabel = \"a\", ylabel = \"b\")\n        ax_hm.yticks = (1:length(as), string.([i % 2 == 0 ? as[i] : \"\" for i = 1:length(as)]))\n        ax_hm.xticks = (1:length(bs), string.([i % 2 == 0 ? bs[i] : \"\" for i = 1:length(bs)]))\n        hm = heatmap!(ax_hm, Δ,  colormap = :viridis)\n        Colorbar(fig[2, 3], hm; label = \"Δ' = ρ(ŷ | yx) - ρ(x̂ | xy)\")\n        fig\n    end\nend\n\nreproduce_figure_8_mccraken()","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"We haven't used as many parameter combinations as McCracken and Weigel (2014) did,  but we get a figure that looks roughly similar to theirs.","category":"page"},{"location":"api/cross_map_api/","page":"Cross-map API","title":"Cross-map API","text":"As expected, Delta  0 for all parameter combinations, implying that X \"PAI drives\" Y.","category":"page"}]
}
