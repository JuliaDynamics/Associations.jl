{
    "docs": [
        {
            "location": "/", 
            "text": "CausalityTools.jl\n\n\nCausalityTools\n is a Julia package providing algorithms for detecting causal relations in complex systems based on time series data.\n\n\n\n\nGoals\n\n\n\n\nProvide a comprehensive, \neasy-to-use framework\n for the detection of directional causal influences in complex dynamical systems from time series.\n\n\nFunctional and efficient \nimplementations\n of causality detection algorithms, with thorough documentation and references to primary literature.\n\n\nIntegration with UncertainDatasets.jl\n, which greatly simplifies working with uncertain data.\n\n\nIntegration with DynamicalSystems.jl\n, for quick analysis of time series from systems where the governing equations are known.\n\n\nLibrary of example dynamical systems\n for testing algorithm performance.\n\n\nSurrogate data methods\n for null-hypothesis testing. In the future,   surrogate methods will be provided as part of resampling schemes.\n\n\nWorked examples\n for the algorithms.\n\n\n\n\n\n\nStatus\n\n\nThe package and documentation is under active development. \nBreaking changes may occur in CausalityTools and its dependencies until the 1.0 release\n.\n\n\n\n\nPackage structure\n\n\nCausalityTools.jl\n brings together the following packages into one environment:\n\n\n\n\n\n\n\n\npackage\n\n\nfunctionality\n\n\nversion\n\n\nbuild\n\n\n\n\n\n\n\n\n\n\nCausalityToolsBase.jl\n\n\nBasic functionality for the CausalityTools ecosystem.\n\n\n0.6.0\n\n\n\n\n\n\n\n\nStateSpaceReconstruction.jl\n\n\nFully flexible state space reconstructions (embeddings), partitioning routines (variable-width rectangular, and triangulations), and partition refinement (equal-volume splitting of  simplices).\n\n\n0.4.2\n\n\n\n\n\n\n\n\nTimeseriesSurrogates.jl\n\n\nGenerate surrogate data from time series.\n\n\n0.3.1\n\n\n\n\n\n\n\n\nTransferEntropy.jl\n\n\nTransfer entropy estimators.\n\n\n0.4.3\n\n\n\n\n\n\n\n\nPerronFrobenius.jl\n\n\nTransfer (Perron-Frobenius) operator estimators.\n\n\n0.6.0\n\n\n\n\n\n\n\n\nSimplices.jl\n\n\nExact simplex intersections in N dimensions.\n\n\n0.4.1\n\n\n\n\n\n\n\n\nCrossMappings.jl\n\n\nExact simplex intersections in N dimensions.\n\n\n0.3.4\n\n\n\n\n\n\n\n\n\n\n\n\nContributors\n\n\n\n\nKristian Agas\u00f8ster Haaga (\n@kahaaga\n)\n\n\nDavid Diego (\n@susydiegolas\n)\n\n\nTor Einar M\u00f8ller (\n@tormolle\n)\n\n\n\n\n\n\nRelated software\n\n\n\n\nDynamicalSystems.jl\n provides a range of tools for exploring nonlinear dynamics and chaos, both for synthetic and observed systems. We provide seamless interaction with \nDataset\n outputs from DynamicalSystems.  Most of our example systems are also implemented as \nDiscreteDynamicalSystem\ns or \nContinuousDynamicalSystems\n from DynamicalSystems.", 
            "title": "Home"
        }, 
        {
            "location": "/#causalitytoolsjl", 
            "text": "CausalityTools  is a Julia package providing algorithms for detecting causal relations in complex systems based on time series data.", 
            "title": "CausalityTools.jl"
        }, 
        {
            "location": "/#goals", 
            "text": "Provide a comprehensive,  easy-to-use framework  for the detection of directional causal influences in complex dynamical systems from time series.  Functional and efficient  implementations  of causality detection algorithms, with thorough documentation and references to primary literature.  Integration with UncertainDatasets.jl , which greatly simplifies working with uncertain data.  Integration with DynamicalSystems.jl , for quick analysis of time series from systems where the governing equations are known.  Library of example dynamical systems  for testing algorithm performance.  Surrogate data methods  for null-hypothesis testing. In the future,   surrogate methods will be provided as part of resampling schemes.  Worked examples  for the algorithms.", 
            "title": "Goals"
        }, 
        {
            "location": "/#status", 
            "text": "The package and documentation is under active development.  Breaking changes may occur in CausalityTools and its dependencies until the 1.0 release .", 
            "title": "Status"
        }, 
        {
            "location": "/#package-structure", 
            "text": "CausalityTools.jl  brings together the following packages into one environment:     package  functionality  version  build      CausalityToolsBase.jl  Basic functionality for the CausalityTools ecosystem.  0.6.0     StateSpaceReconstruction.jl  Fully flexible state space reconstructions (embeddings), partitioning routines (variable-width rectangular, and triangulations), and partition refinement (equal-volume splitting of  simplices).  0.4.2     TimeseriesSurrogates.jl  Generate surrogate data from time series.  0.3.1     TransferEntropy.jl  Transfer entropy estimators.  0.4.3     PerronFrobenius.jl  Transfer (Perron-Frobenius) operator estimators.  0.6.0     Simplices.jl  Exact simplex intersections in N dimensions.  0.4.1     CrossMappings.jl  Exact simplex intersections in N dimensions.  0.3.4", 
            "title": "Package structure"
        }, 
        {
            "location": "/#contributors", 
            "text": "Kristian Agas\u00f8ster Haaga ( @kahaaga )  David Diego ( @susydiegolas )  Tor Einar M\u00f8ller ( @tormolle )", 
            "title": "Contributors"
        }, 
        {
            "location": "/#related-software", 
            "text": "DynamicalSystems.jl  provides a range of tools for exploring nonlinear dynamics and chaos, both for synthetic and observed systems. We provide seamless interaction with  Dataset  outputs from DynamicalSystems.  Most of our example systems are also implemented as  DiscreteDynamicalSystem s or  ContinuousDynamicalSystems  from DynamicalSystems.", 
            "title": "Related software"
        }, 
        {
            "location": "/causalitytests/causality_from_time_series/", 
            "text": "Causality from time series\n\n\nThe \ncausality\n function and its methods provide a common interface for testing causal hypotheses. For analysing time series, all you need to do is provide a \nsource\n and a \ntarget\n. Then, choose  one of the \navailable causality tests\n to quantify the (directional) dynamical dependence between \nsource\n and \ntarget\n.\n\n\nFor data with uncertainties, see \nuncertainty handling\n.\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nsource\n::\nAbstractVector\n,\n \nAbstractVector\n,\n \ntest\n::\nCausalityTest\n)\n\n\n\n\n\nTest for a causal influence from \nsource\n to \ntarget\n using the provided causality \ntest\n.\n\n\nExamples\n\n\nx\n,\n \ny\n \n=\n \nrand\n(\n300\n),\n \nrand\n(\n300\n)\n\n\n\n# Define some causality tests and apply them to `x` and `y`.\n\n\ntest_ccm\n \n=\n \nConvergentCrossMappingTest\n(\ntimeseries_lengths\n \n=\n \n[\n45\n,\n \n50\n],\n \nn_reps\n \n=\n \n20\n)\n\n\ntest_cm\n \n=\n \nCrossMappingTest\n(\nn_reps\n \n=\n \n10\n)\n\n\ntest_vf\n \n=\n \nVisitationFrequencyTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\n5\n),\n \n\u03b7s\n \n=\n \n1\n:\n5\n)\n\n\ntest_tog\n \n=\n \nTransferOperatorGridTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\n5\n),\n \n\u03b7s\n \n=\n \n1\n:\n5\n)\n\n\ntest_jdd\n \n=\n \nJointDistanceDistributionTest\n()\n\n\ntest_jddt\n \n=\n \nJointDistanceDistributionTTest\n()\n\n\npredtest\n \n=\n \nVisitationFrequencyTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\n5\n),\n \n\u03b7s\n \n=\n \n-\n5\n:\n5\n)\n\n\ntest_pa\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \npredtest\n)\n\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest_ccm\n)\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest_cm\n)\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest_vf\n)\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest_tog\n)\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest_jdd\n)\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest_jddt\n)\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest_pa\n)\n\n\n\n\n\nsource", 
            "title": "Causality from time series"
        }, 
        {
            "location": "/causalitytests/causality_from_time_series/#causality-from-time-series", 
            "text": "The  causality  function and its methods provide a common interface for testing causal hypotheses. For analysing time series, all you need to do is provide a  source  and a  target . Then, choose  one of the  available causality tests  to quantify the (directional) dynamical dependence between  source  and  target .  For data with uncertainties, see  uncertainty handling .  #  CausalityToolsBase.causality     Method .  causality ( source :: AbstractVector ,   AbstractVector ,   test :: CausalityTest )   Test for a causal influence from  source  to  target  using the provided causality  test .  Examples  x ,   y   =   rand ( 300 ),   rand ( 300 )  # Define some causality tests and apply them to `x` and `y`.  test_ccm   =   ConvergentCrossMappingTest ( timeseries_lengths   =   [ 45 ,   50 ],   n_reps   =   20 )  test_cm   =   CrossMappingTest ( n_reps   =   10 )  test_vf   =   VisitationFrequencyTest ( binning   =   RectangularBinning ( 5 ),   \u03b7s   =   1 : 5 )  test_tog   =   TransferOperatorGridTest ( binning   =   RectangularBinning ( 5 ),   \u03b7s   =   1 : 5 )  test_jdd   =   JointDistanceDistributionTest ()  test_jddt   =   JointDistanceDistributionTTest ()  predtest   =   VisitationFrequencyTest ( binning   =   RectangularBinning ( 5 ),   \u03b7s   =   - 5 : 5 )  test_pa   =   PredictiveAsymmetryTest ( predictive_test   =   predtest )  causality ( x ,   y ,   test_ccm )  causality ( x ,   y ,   test_cm )  causality ( x ,   y ,   test_vf )  causality ( x ,   y ,   test_tog )  causality ( x ,   y ,   test_jdd )  causality ( x ,   y ,   test_jddt )  causality ( x ,   y ,   test_pa )   source", 
            "title": "Causality from time series"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data/", 
            "text": "Uncertainty handling\n\n\nAll high-level causality test are integrated with the uncertainty handling machinery in  \nUncertainData.jl\n. Any combination of real-valued vectors, \nVector{\n:AbstractUncertainValue}\n, or \nAbstractUncertainValueDataset\n are accepted as inputs to \ncausality\n, making uncertainty quantification on the causality statistics a breeze.\n\n\nFor more fine grained control over the analysis, check out the \nsyntax overview\n for low-level estimators.\n\n\n\n\nList of \ncausality\n methods for uncertain data\n\n\nIf both indices and values have uncertainties, use one of the following methods:\n\n\n\n\nNaive resampling\n\n\nNaive constrained resampling\n\n\nBinned resampling\n\n\nStrictly increasing, interpolated resampling", 
            "title": "Methods"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data/#uncertainty-handling", 
            "text": "All high-level causality test are integrated with the uncertainty handling machinery in   UncertainData.jl . Any combination of real-valued vectors,  Vector{ :AbstractUncertainValue} , or  AbstractUncertainValueDataset  are accepted as inputs to  causality , making uncertainty quantification on the causality statistics a breeze.  For more fine grained control over the analysis, check out the  syntax overview  for low-level estimators.", 
            "title": "Uncertainty handling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data/#list-of-causality-methods-for-uncertain-data", 
            "text": "If both indices and values have uncertainties, use one of the following methods:   Naive resampling  Naive constrained resampling  Binned resampling  Strictly increasing, interpolated resampling", 
            "title": "List of causality methods for uncertain data"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_naive/", 
            "text": "Naive resampling\n\n\n\n\nUncertainties only in values\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nx\n,\n \ny\n,\n \ntest\n::\nCausalityTest\n)\n\n\n\n\n\nTest for a causal influence from \nsource\n to \ntarget\n using the provided causality \ntest\n.\n\n\nBoth \nx\n and \ny\n can be a real-valued vector, \nVector{\n:AbstractUncertainValue}\n  or a \nAbstractUncertainValueDataset\n. If either \nx\n, \ny\n or both are uncertain, then the test  is applied to a single draw from the uncertain data. \n\n\nExamples\n\n\nGenerate some example data series x and y, where x influences y:\n\n\nn_pts\n \n=\n \n300\n\n\na\u2081\n,\n \na\u2082\n,\n \nb\u2081\n,\n \nb\u2082\n,\n \n\u03be\u2081\n,\n \n\u03be\u2082\n,\n \nC\u2081\u2082\n \n=\n \n0.7\n,\n \n0.1\n,\n \n0.75\n,\n \n0.2\n,\n \n0.3\n,\n \n0.3\n,\n \n0.5\n\n\n\nD\n \n=\n \nrand\n(\nn_pts\n,\n \n2\n)\n \n\nfor\n \nt\n \nin\n \n5\n:\nn_pts\n\n    \nD\n[\nt\n,\n1\n]\n \n=\n \na\u2081\n*\nD\n[\nt\n-\n1\n,\n1\n]\n \n-\n \na\u2082\n*\nD\n[\nt\n-\n3\n,\n1\n]\n \n+\n                \n\u03be\u2081\n*\nrand\n(\nNormal\n(\n0\n,\n \n1\n))\n\n    \nD\n[\nt\n,\n2\n]\n \n=\n \nb\u2081\n*\nD\n[\nt\n-\n1\n,\n2\n]\n \n-\n \nb\u2082\n*\nD\n[\nt\n-\n2\n,\n2\n]\n \n+\n \nC\u2081\u2082\n*\nD\n[\nt\n-\n1\n,\n1\n]\n \n+\n \n\u03be\u2082\n*\nrand\n(\nNormal\n(\n0\n,\n \n1\n))\n\n\nend\n\n\n\n\n\nGather time series and add some uncertainties to them:\n\n\nx\n,\n \ny\n \n=\n \nD\n[\n:\n,\n \n1\n],\n \nD\n[\n:\n,\n \n2\n]\n\n\nuvalx\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\nx\n,\n \nrand\n()))\n\n\nuvaly\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\ny\n,\n \nrand\n()))\n\n\n\nxd\n \n=\n \nUncertainValueDataset\n(\nuvalx\n)\n\n\nyd\n \n=\n \nUncertainValueDataset\n(\nuvaly\n)\n\n\n\n\n\nAny combination of certain and uncertain values will work:\n\n\ncausality\n(\nx\n,\n \ny\n,\n \npa_test\n)\n\n\ncausality\n(\nx\n,\n \nyd\n,\n \npa_test\n)\n\n\ncausality\n(\nxd\n,\n \nyd\n,\n \npa_test\n)\n\n\ncausality\n(\nx\n,\n \nuvaly\n,\n \npa_test\n)\n\n\ncausality\n(\nuvalx\n,\n \nuvaly\n,\n \npa_test\n)\n\n\n\n\n\nOn multiple realisations of the uncertain \nyd\n, but fixing \nx\n:\n\n\n[\ncausality\n(\nx\n,\n \nyd\n,\n \npa_test\n)\n \nfor\n \ni\n \n=\n \n1\n:\n100\n]\n\n\n\n\n\nsource\n\n\n\n\nUncertainties in both indices and values\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nsource\n::\nAbstractUncertainIndexValueDataset\n,\n \n    \ntarget\n::\nAbstractUncertainIndexValueDataset\n,\n \n    \ntest\n::\nCausalityTest\n)\n\n\n\n\n\nTest for a causal influence from \nsource\n to \ntarget\n using the provided causality \ntest\n.\n\n\nThe test is performed on a single draw of the values of \nsource\n and \ntarget\n, disregarding the ordering resulting from resampling, but respecting the order of the points in the dataset.\n\n\nNote: if the uncertain values furnishing the indices have overlapping supports, you might  mess up the index-ordering (e.g. time-ordering) of the data points\n.\n\n\nExample\n\n\nGenerate some example data series x and y, where x influences y:\n\n\nn_pts\n \n=\n \n300\n\n\na\u2081\n,\n \na\u2082\n,\n \nb\u2081\n,\n \nb\u2082\n,\n \n\u03be\u2081\n,\n \n\u03be\u2082\n,\n \nC\u2081\u2082\n \n=\n \n0.7\n,\n \n0.1\n,\n \n0.75\n,\n \n0.2\n,\n \n0.3\n,\n \n0.3\n,\n \n0.5\n\n\n\nD\n \n=\n \nrand\n(\nn_pts\n,\n \n2\n)\n \n\nfor\n \nt\n \nin\n \n5\n:\nn_pts\n\n    \nD\n[\nt\n,\n1\n]\n \n=\n \na\u2081\n*\nD\n[\nt\n-\n1\n,\n1\n]\n \n-\n \na\u2082\n*\nD\n[\nt\n-\n4\n,\n1\n]\n \n+\n                \n\u03be\u2081\n*\nrand\n(\nNormal\n(\n0\n,\n \n1\n))\n\n    \nD\n[\nt\n,\n2\n]\n \n=\n \nb\u2081\n*\nD\n[\nt\n-\n1\n,\n2\n]\n \n-\n \nb\u2082\n*\nD\n[\nt\n-\n4\n,\n2\n]\n \n+\n \nC\u2081\u2082\n*\nD\n[\nt\n-\n1\n,\n1\n]\n \n+\n \n\u03be\u2082\n*\nrand\n(\nNormal\n(\n0\n,\n \n1\n))\n\n\nend\n\n\n\nx\n,\n \ny\n \n=\n \nD\n[\n:\n,\n \n1\n],\n \nD\n[\n:\n,\n \n2\n]\n\n\n\n\n\nAdd some uncertainties and gather in an \nUncertainIndexValueDataset\n\n\nt\n \n=\n \n1\n:\nn_pts\n\n\ntu\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\nt\n,\n \nrand\n()))\n\n\nxu\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\nx\n,\n \nrand\n()))\n\n\nyu\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\ny\n,\n \nrand\n()))\n\n\nX\n \n=\n \nUncertainIndexValueDataset\n(\ntu\n,\n \nxu\n)\n\n\nY\n \n=\n \nUncertainIndexValueDataset\n(\ntu\n,\n \nyu\n)\n\n\n\n\n\nDefine a causality test, for example the predictive asymmetry test:\n\n\n# Define causality test \n\n\nk\n,\n \nl\n,\n \nm\n \n=\n \n1\n,\n \n1\n,\n \n1\n\n\n\u03b7s\n \n=\n \n-\n8\n:\n8\n\n\nn_subdivs\n \n=\n \nfloor\n(\nInt\n,\n \nn_pts\n^\n(\n1\n/\n(\nk\n+\nl\n+\nm\n+\n1\n)))\n\n\nbin\n \n=\n \nRectangularBinning\n(\nn_subdivs\n)\n\n\nte_test\n \n=\n \nVisitationFrequencyTest\n(\nk\n \n=\n \nk\n,\n \nl\n \n=\n \nl\n,\n \nm\n \n=\n \nm\n,\n \nbinning\n \n=\n \nbin\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\npa_test\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \nte_test\n)\n\n\n\n\n\nRun the causality test on a single draw of \nX\n and a single draw of \nY\n:\n\n\npa_XY = causality(X, Y, pa_test)\npa_YX = causality(Y, X, pa_test)\n\n\n\n\nRepeat the test on multiple draws:\n\n\npa_XY\n \n=\n \n[\ncausality\n(\nX\n,\n \nY\n,\n \npa_test\n)\n \nfor\n \ni\n \n=\n \n1\n:\n100\n]\n\n\npa_YX\n \n=\n \n[\ncausality\n(\nY\n,\n \nX\n,\n \npa_test\n)\n \nfor\n \ni\n \n=\n \n1\n:\n100\n]\n\n\n\n\n\nsource", 
            "title": "Naive resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_naive/#naive-resampling", 
            "text": "", 
            "title": "Naive resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_naive/#uncertainties-only-in-values", 
            "text": "#  CausalityToolsBase.causality     Method .  causality ( x ,   y ,   test :: CausalityTest )   Test for a causal influence from  source  to  target  using the provided causality  test .  Both  x  and  y  can be a real-valued vector,  Vector{ :AbstractUncertainValue}   or a  AbstractUncertainValueDataset . If either  x ,  y  or both are uncertain, then the test  is applied to a single draw from the uncertain data.   Examples  Generate some example data series x and y, where x influences y:  n_pts   =   300  a\u2081 ,   a\u2082 ,   b\u2081 ,   b\u2082 ,   \u03be\u2081 ,   \u03be\u2082 ,   C\u2081\u2082   =   0.7 ,   0.1 ,   0.75 ,   0.2 ,   0.3 ,   0.3 ,   0.5  D   =   rand ( n_pts ,   2 )   for   t   in   5 : n_pts \n     D [ t , 1 ]   =   a\u2081 * D [ t - 1 , 1 ]   -   a\u2082 * D [ t - 3 , 1 ]   +                  \u03be\u2081 * rand ( Normal ( 0 ,   1 )) \n     D [ t , 2 ]   =   b\u2081 * D [ t - 1 , 2 ]   -   b\u2082 * D [ t - 2 , 2 ]   +   C\u2081\u2082 * D [ t - 1 , 1 ]   +   \u03be\u2082 * rand ( Normal ( 0 ,   1 ))  end   Gather time series and add some uncertainties to them:  x ,   y   =   D [ : ,   1 ],   D [ : ,   2 ]  uvalx   =   UncertainValue . ( Normal . ( x ,   rand ()))  uvaly   =   UncertainValue . ( Normal . ( y ,   rand ()))  xd   =   UncertainValueDataset ( uvalx )  yd   =   UncertainValueDataset ( uvaly )   Any combination of certain and uncertain values will work:  causality ( x ,   y ,   pa_test )  causality ( x ,   yd ,   pa_test )  causality ( xd ,   yd ,   pa_test )  causality ( x ,   uvaly ,   pa_test )  causality ( uvalx ,   uvaly ,   pa_test )   On multiple realisations of the uncertain  yd , but fixing  x :  [ causality ( x ,   yd ,   pa_test )   for   i   =   1 : 100 ]   source", 
            "title": "Uncertainties only in values"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_naive/#uncertainties-in-both-indices-and-values", 
            "text": "#  CausalityToolsBase.causality     Method .  causality ( source :: AbstractUncertainIndexValueDataset ,  \n     target :: AbstractUncertainIndexValueDataset ,  \n     test :: CausalityTest )   Test for a causal influence from  source  to  target  using the provided causality  test .  The test is performed on a single draw of the values of  source  and  target , disregarding the ordering resulting from resampling, but respecting the order of the points in the dataset.  Note: if the uncertain values furnishing the indices have overlapping supports, you might  mess up the index-ordering (e.g. time-ordering) of the data points .  Example  Generate some example data series x and y, where x influences y:  n_pts   =   300  a\u2081 ,   a\u2082 ,   b\u2081 ,   b\u2082 ,   \u03be\u2081 ,   \u03be\u2082 ,   C\u2081\u2082   =   0.7 ,   0.1 ,   0.75 ,   0.2 ,   0.3 ,   0.3 ,   0.5  D   =   rand ( n_pts ,   2 )   for   t   in   5 : n_pts \n     D [ t , 1 ]   =   a\u2081 * D [ t - 1 , 1 ]   -   a\u2082 * D [ t - 4 , 1 ]   +                  \u03be\u2081 * rand ( Normal ( 0 ,   1 )) \n     D [ t , 2 ]   =   b\u2081 * D [ t - 1 , 2 ]   -   b\u2082 * D [ t - 4 , 2 ]   +   C\u2081\u2082 * D [ t - 1 , 1 ]   +   \u03be\u2082 * rand ( Normal ( 0 ,   1 ))  end  x ,   y   =   D [ : ,   1 ],   D [ : ,   2 ]   Add some uncertainties and gather in an  UncertainIndexValueDataset  t   =   1 : n_pts  tu   =   UncertainValue . ( Normal . ( t ,   rand ()))  xu   =   UncertainValue . ( Normal . ( x ,   rand ()))  yu   =   UncertainValue . ( Normal . ( y ,   rand ()))  X   =   UncertainIndexValueDataset ( tu ,   xu )  Y   =   UncertainIndexValueDataset ( tu ,   yu )   Define a causality test, for example the predictive asymmetry test:  # Define causality test   k ,   l ,   m   =   1 ,   1 ,   1  \u03b7s   =   - 8 : 8  n_subdivs   =   floor ( Int ,   n_pts ^ ( 1 / ( k + l + m + 1 )))  bin   =   RectangularBinning ( n_subdivs )  te_test   =   VisitationFrequencyTest ( k   =   k ,   l   =   l ,   m   =   m ,   binning   =   bin ,   \u03b7s   =   \u03b7s )  pa_test   =   PredictiveAsymmetryTest ( predictive_test   =   te_test )   Run the causality test on a single draw of  X  and a single draw of  Y :  pa_XY = causality(X, Y, pa_test)\npa_YX = causality(Y, X, pa_test)  Repeat the test on multiple draws:  pa_XY   =   [ causality ( X ,   Y ,   pa_test )   for   i   =   1 : 100 ]  pa_YX   =   [ causality ( Y ,   X ,   pa_test )   for   i   =   1 : 100 ]   source", 
            "title": "Uncertainties in both indices and values"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_naive_constrained/", 
            "text": "Naive, constrained resampling\n\n\nIf you need to truncate the furnishing distributions of your uncertain datasets before  applying a causality test, use the following method.\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nsource\n::\nAbstractUncertainIndexValueDataset\n,\n \n    \ntarget\n::\nAbstractUncertainIndexValueDataset\n,\n \n    \ntest\n::\nConstrainedTest\n{\nCT\n,\n \nCR\n})\n \nwhere\n \n{\nCT\n,\n \nCR\n}\n\n\n\n\n\nApply a causality test of type \nCT\n to \ntest.n\n independent  realisations of \nsource\n and \ntarget\n, after first constraining  the supports of the uncertain values furnishing the datasets.\n\n\nSee also \nConstrainedTest\n.\n\n\nsource\n\n\n\n\nConstrainedTest\n\n\n#\n\n\nCausalityTools.CausalityTests.ConstrainedTest\n \n \nType\n.\n\n\nConstrainedTest\n(\ntest\n::\nCausalityTest\n,\n \nconstraints\n::\nConstrainedResampling\n,\n \nn\n::\nInt\n)\n\n\nConstrainedTest\n(\ntest\n::\nCausalityTest\n,\n \nconstraints\n::\nConstrainedResampling\n)\n\n\n\n\n\nA causality test where the supports of the uncertain values furnishing the  uncertain values in the datasets are truncated (according to the provided  \nconstraints\n) prior to performing the \ntest\n. \n\n\nn\n controls the number of independent resamplings over which the test  is performed (if not provided, \nn\n is set to \n1\n by default).\n\n\nExamples\n\n\nAssume we want to apply a causality test to two uncertain datasets \nX\n and \nY\n, but restricting the ranges of values their elements can take while resampling. \n\n\n# Constraints for X.indices and X.values\n\n\ncx\n \n=\n \n(\nTruncateQuantiles\n(\n0.3\n,\n \n0.7\n),\n \nTruncateStd\n(\n1.5\n))\n\n\n\n# Constraints for Y.indices and Y.values\n\n\ncy\n \n=\n \n(\nTruncateQuantiles\n(\n0.3\n,\n \n0.7\n),\n \nTruncateStd\n(\n1.5\n))\n\n\n\n# Need to gather to feed to the ConstrainedTest constructor.\n\n\ncs\n \n=\n \nConstrainedIndexValueResampling\n(\ncx\n,\n \ncy\n)\n\n\n\n# A cross mapping test applied to 100 independent realisations\n\n\n# of `X` and `Y`, resampled after constraining the data.\n\n\nctest\n \n=\n \nConstrainedTest\n(\nCrossMappingTest\n(),\n \ncs\n,\n \n100\n)\n\n\n\n\n\nsource", 
            "title": "Constrained resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_naive_constrained/#naive-constrained-resampling", 
            "text": "If you need to truncate the furnishing distributions of your uncertain datasets before  applying a causality test, use the following method.  #  CausalityToolsBase.causality     Method .  causality ( source :: AbstractUncertainIndexValueDataset ,  \n     target :: AbstractUncertainIndexValueDataset ,  \n     test :: ConstrainedTest { CT ,   CR })   where   { CT ,   CR }   Apply a causality test of type  CT  to  test.n  independent  realisations of  source  and  target , after first constraining  the supports of the uncertain values furnishing the datasets.  See also  ConstrainedTest .  source", 
            "title": "Naive, constrained resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_naive_constrained/#constrainedtest", 
            "text": "#  CausalityTools.CausalityTests.ConstrainedTest     Type .  ConstrainedTest ( test :: CausalityTest ,   constraints :: ConstrainedResampling ,   n :: Int )  ConstrainedTest ( test :: CausalityTest ,   constraints :: ConstrainedResampling )   A causality test where the supports of the uncertain values furnishing the  uncertain values in the datasets are truncated (according to the provided   constraints ) prior to performing the  test .   n  controls the number of independent resamplings over which the test  is performed (if not provided,  n  is set to  1  by default).  Examples  Assume we want to apply a causality test to two uncertain datasets  X  and  Y , but restricting the ranges of values their elements can take while resampling.   # Constraints for X.indices and X.values  cx   =   ( TruncateQuantiles ( 0.3 ,   0.7 ),   TruncateStd ( 1.5 ))  # Constraints for Y.indices and Y.values  cy   =   ( TruncateQuantiles ( 0.3 ,   0.7 ),   TruncateStd ( 1.5 ))  # Need to gather to feed to the ConstrainedTest constructor.  cs   =   ConstrainedIndexValueResampling ( cx ,   cy )  # A cross mapping test applied to 100 independent realisations  # of `X` and `Y`, resampled after constraining the data.  ctest   =   ConstrainedTest ( CrossMappingTest (),   cs ,   100 )   source", 
            "title": "ConstrainedTest"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_binneddatacausalitytest/", 
            "text": "Binned resampling\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nx\n::\nAbstractUncertainIndexValueDataset\n,\n\n    \ny\n::\nAbstractUncertainIndexValueDataset\n,\n\n    \ntest\n::\nBinnedDataCausalityTest\n)\n\n\n\n\n\nApply a causality test to \nx\n and \ny\n, which are both data series  with uncertainties in both indices and values. To get the data  on an equally-spaced temporal grid, the data are first binned according to the instructions in \ntest.binning\n. \n\n\nBinning methods\n\n\n\n\nIf \ntest.binning\n results in an uncertain value for each bin, then the causality\n\n\n\n\ntest is applied \ntest.n_realizations\n times. Examples are \nUncertainData.BinnedResampling\n  and \nUncertainData.BinnedWeightedResampling\n, which both return a KDE estimate to the  distribution of values in each bin.\n\n\n\n\nIf \ntest.binning\n returns a summary statistic for each bin, then the causality test is\n\n\n\n\napplied to the summarised time series. Examples are \nUncertainData.BinnedMeanResampling\n  and \nUncertainData.BinnedMeanWeightedResampling\n, which both return the mean of each bin.\n\n\nExample\n\n\nSome example data\n\n\nLet's say we have the following datasets \nX\n and \nY\n: \n\n\nusing\n \nCausalityTools\n,\n \nUncertainData\n \n\n\nsys\n \n=\n \nar1_unidir\n(\nu\u1d62\n \n=\n \n[\n0.1\n,\n \n0.1\n],\n \nc_xy\n \n=\n \n0.41\n)\n\n\nvars\n \n=\n \n(\n1\n,\n \n2\n)\n \n# ar1_unidir has only two variables, X and Y\n\n\nn_steps\n \n=\n \n100\n \n# the number of points in the time series\n\n\ntstep\n \n=\n \n10\n \n# the mean of each time value is stepped by `tstep`\n\n\n\nX\n,\n \nY\n \n=\n \nexample_uncertain_indexvalue_datasets\n(\nsys\n,\n \nn_steps\n,\n \nvars\n,\n \ntstep\n \n=\n \n10\n,\n \n\nd_xind\n \n=\n \nUniform\n(\n7.5\n,\n \n15.5\n),\n \n\nd_yind\n \n=\n \nUniform\n(\n5.5\n,\n \n15.5\n),\n \n\nd_xval\n \n=\n \nUniform\n(\n0.1\n,\n \n0.5\n));\n\n\n\n\n\nNow we can perform the causality tests on our uncertain data, either over  the bin means, or over an ensemble of independent realisations of  the dataset given the distribution of points in each bin.\n\n\nSummary statistic for each bin\n\n\nPredictiveAsymmetryTest\n\n\nIf the binning method returns a summary statistic (e.g. the mean) for each bin,  then the causality test is applied exactly once to the bin summaries.\n\n\n# Bin the data by drawing n_draws = 5000 realizations of each uncertain data \n\n\n# point, then assign the draws to the correct bins and get a distribution \n\n\n# of values in each bin.\n\n\nbinning\n \n=\n \nBinnedResampling\n(\n0\n:\n10\n:\n1000\n,\n \n5000\n)\n\n\n\n# A predictive asymmetry test. For the predictions, we\nll use a transfer entropy \n\n\n# test, which uses the visitation frequency estimator. We\nll predict 5 time steps \n\n\n# forwards and backwards in time (\u03b7s = -5:5). For the state space binning, we\nll\n\n\n# determine how many intervals to split each axis into from the number of points\n\n\n# in the time series.\n\n\nk\n,\n \nl\n,\n \nm\n \n=\n \n1\n,\n \n1\n,\n \n1\n \n# embedding parameters\n\n\nn_subdivisions\n \n=\n \nfloor\n(\nInt\n,\n \nlength\n(\n0\n:\n10\n:\n1000\n)\n^\n(\n1\n/\n(\nk\n \n+\n \nl\n \n+\n \nm\n \n+\n \n1\n)))\n\n\nstate_space_binning\n \n=\n \nRectangularBinning\n(\nn_subdivisions\n)\n\n\n\u03b7s\n \n=\n \n-\n5\n:\n5\n\n\nte_test\n \n=\n \nVisitationFrequencyTest\n(\nk\n \n=\n \nk\n,\n \nl\n \n=\n \nl\n,\n \nm\n \n=\n \nm\n,\n\n\nbinning\n \n=\n \nstate_space_binning\n,\n \n\n\u03b7s\n \n=\n \n\u03b7s\n,\n \nb\n \n=\n \n2\n)\n \n# use base-2 logarithms\n\n\npa_test\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \nte_test\n)\n\n\n\n# Compute transfer entropy in both directions for the bin means.\n\n\n# We still have to specify the number of realisations for the test, \n\n\n# but it is ignored, so it doesn\nt matter what number we input here.\n\n\ntest\n \n=\n \nBinnedDataCausalityTest\n(\npa_test\n,\n \nbinning\n,\n \n1\n)\n\n\n\n# Perform the tests\n\n\ntes_xy\n \n=\n \ncausality\n(\nX\n,\n \nY\n,\n \ntest\n)\n\n\ntes_yx\n \n=\n \ncausality\n(\nY\n,\n \nX\n,\n \ntest\n)\n\n\n\n# Plot the results\n\n\nplot\n(\nxlabel\n \n=\n \nPrediction lag (eta)\n,\n \nylabel\n \n=\n \nPredictive asymmetry (bits)\n)\n\n\nplot!\n(\n\u03b7s\n[\n\u03b7s\n \n.\n \n0\n],\n \ntes_xy\n,\n \nlabel\n \n=\n \nx -\n y (binned)\n,\n \nc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\n\u03b7s\n[\n\u03b7s\n \n.\n \n0\n],\n \ntes_yx\n,\n \nlabel\n \n=\n \ny -\n x (binned)\n,\n \nc\n \n=\n \n:\nred\n)\n\n\nhline!\n([\n0\n],\n \nlw\n \n=\n \n2\n,\n \nls\n \n=\n \n:\ndot\n,\n \n\u03b1\n \n=\n \n0.5\n,\n \nlabel\n \n=\n \n,\n \nc\n \n=\n \n:\ngrey\n)\n\n\n\n\n\nUncertain values representing each bin\n\n\nIf the binning method returns an uncertain value for each bin, then the  causality test is applied to \ntest.n_realizations\n independent draws  of the binned dataset.\n\n\nPredictive asymmetry test\n\n\n# Bin the data by drawing n_draws = 5000 realizations of each uncertain data \n\n\n# point, then assign the draws to the correct bins and get a distribution \n\n\n# of values in each bin.\n\n\ngrid\n \n=\n \n0\n:\n5\n:\n1000\n\n\nbinning\n \n=\n \nBinnedResampling\n(\ngrid\n,\n \n5000\n)\n\n\n\n# A predictive asymmetry test. For the predictions, we\nll use a transfer entropy \n\n\n# test, which uses the visitation frequency estimator. We\nll predict 5 time steps \n\n\n# forwards and backwards in time (\u03b7s = -5:5). For the state space binning, we\nll\n\n\n# determine how many intervals to split each axis into from the number of points\n\n\n# in the time series.\n\n\nk\n,\n \nl\n,\n \nm\n \n=\n \n1\n,\n \n1\n,\n \n1\n \n# embedding parameters\n\n\nn_subdivisions\n \n=\n \nfloor\n(\nInt\n,\n \nlength\n(\n0\n:\n10\n:\n1000\n)\n^\n(\n1\n/\n(\nk\n \n+\n \nl\n \n+\n \nm\n \n+\n \n1\n)))\n\n\nstate_space_binning\n \n=\n \nRectangularBinning\n(\nn_subdivisions\n)\n\n\n\u03b7s\n \n=\n \n-\n5\n:\n5\n\n\nte_test\n \n=\n \nVisitationFrequencyTest\n(\nk\n \n=\n \nk\n,\n \nl\n \n=\n \nl\n,\n \nm\n \n=\n \nm\n,\n\n\nbinning\n \n=\n \nstate_space_binning\n,\n \n\n\u03b7s\n \n=\n \n\u03b7s\n,\n \nb\n \n=\n \n2\n)\n \n# use base-2 logarithms\n\n\npa_test\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \nte_test\n)\n\n\n\n# Compute transfer entropy in both directions over 50 independent realizations \n\n\n# of the binned dataset.\n\n\nn_realizations\n \n=\n \n50\n\n\ntest\n \n=\n \nBinnedDataCausalityTest\n(\npa_test\n,\n \nbinning\n,\n \nn_realizations\n)\n\n\n\n# Perform the tests on the bin means. `tes_xy` now contains 50 independent \n\n\n# computation of the predictive asymmetry at predictive lags 1 to 5, \n\n\n# and `tes_yx` contains the same but for the other direction\n\n\ntes_xy\n \n=\n \ncausality\n(\nX\n,\n \nY\n,\n \ntest\n)\n\n\ntes_yx\n \n=\n \ncausality\n(\nY\n,\n \nX\n,\n \ntest\n)\n\n\n\n# Gather results in a matrix and compute means and standard deviations \n\n\n# for the predictive asymmetries at each prediction lag\n\n\nM_xy\n \n=\n \nhcat\n(\ntes_xy\n...\n,)\n\n\nM_yx\n \n=\n \nhcat\n(\ntes_yx\n...\n,)\n\n\n\nmeans_xy\n \n=\n \nmean\n(\nM_xy\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n]\n\n\nmeans_yx\n \n=\n \nmean\n(\nM_yx\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n]\n\n\nstdevs_xy\n \n=\n \nstd\n(\nM_yx\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n]\n\n\nstdevs_yx\n \n=\n \nstd\n(\nM_yx\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n]\n\n\n\n# Plot the predictive asymmetry as a function of prediction lag\n\n\nplot\n(\nxlabel\n \n=\n \nPrediction lag (eta)\n,\n \nylabel\n \n=\n \nPredictive asymmetry (bits)\n)\n\n\nplot!\n(\n\u03b7s\n[\n\u03b7s\n \n.\n \n0\n],\n \nmeans_xy\n,\n \nribbon\n \n=\n \nstdevs_xy\n,\n \nlabel\n \n=\n \nx -\n y (binned)\n,\n \nc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\n\u03b7s\n[\n\u03b7s\n \n.\n \n0\n],\n \nmeans_yx\n,\n \nribbon\n \n=\n \nstdevs_yx\n,\n \nlabel\n \n=\n \ny -\n x (binned)\n,\n \nc\n \n=\n \n:\nred\n)\n\n\nhline!\n([\n0\n],\n \nlw\n \n=\n \n2\n,\n \nls\n \n=\n \n:\ndot\n,\n \n\u03b1\n \n=\n \n0.5\n,\n \nlabel\n \n=\n \n,\n \nc\n \n=\n \n:\ngrey\n)\n\n\n\n\n\nConvergent cross mapping test\n\n\nusing\n \nPlots\n \n\n\n# Bin the data by drawing n_draws = 5000 realizations of each uncertain data \n\n\n# point, then assign the draws to the correct bins and get a distribution \n\n\n# of values in each bin.\n\n\nbinning\n \n=\n \nBinnedResampling\n(\n0\n:\n10\n:\n1000\n,\n \n5000\n)\n\n\n\n# A convergent cross mapping causality test\n\n\nts_lengths\n \n=\n \n15\n:\n5\n:\n100\n \n|\n \ncollect\n\n\ncausalitytest\n \n=\n \nConvergentCrossMappingTest\n(\n\n\ntimeseries_lengths\n \n=\n \nts_lengths\n,\n \n\nn_reps\n \n=\n \n100\n,\n \n\nlibsize\n \n=\n \n100\n,\n \n\n\u03c4\n \n=\n \n1\n,\n \n\nreplace\n \n=\n \ntrue\n)\n\n\n\n# Run ccm in both directions over 50 independent realizations of the binned datasets\n\n\nn_realizations\n \n=\n \n50\n\n\ntest\n \n=\n \nBinnedDataCausalityTest\n(\ncausalitytest\n,\n \nbinning\n,\n \nn_realizations\n)\n\n\n\nccms_xy\n \n=\n \ncausality\n(\nX\n,\n \nY\n,\n \ntest\n);\n\n\nccms_yx\n \n=\n \ncausality\n(\nY\n,\n \nX\n,\n \ntest\n);\n\n\n\nrealization_means_xy\n \n=\n \n[\nmean\n.\n(\nrealization\n)\n \nfor\n \nrealization\n \nin\n \nccms_xy\n]\n\n\nrealization_means_yx\n \n=\n \n[\nmean\n.\n(\nrealization\n)\n \nfor\n \nrealization\n \nin\n \nccms_yx\n]\n\n\nM_xy\n \n=\n \nhcat\n(\nrealization_means_xy\n...\n,)\n\n\nM_yx\n \n=\n \nhcat\n(\nrealization_means_yx\n...\n,)\n\n\n\nplot\n(\nxlabel\n \n=\n \nTime series length\n,\n \nylabel\n \n=\n \nCross map skill\n)\n\n\nplot!\n(\nts_lengths\n,\n \nmean\n(\nM_xy\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n],\n \n\nribbon\n \n=\n \nstd\n(\nM_xy\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n],\n \n\nlabel\n \n=\n \nx -\n y (binned)\n)\n\n\nplot!\n(\nts_lengths\n,\n \nmean\n(\nM_yx\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n],\n \n\nribbon\n \n=\n \nstd\n(\nM_yx\n,\n \ndims\n \n=\n \n2\n)[\n:\n,\n \n1\n],\n \n\nlabel\n \n=\n \ny -\n x (binned)\n)\n\n\n\n\n\nsource", 
            "title": "Binned resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_binneddatacausalitytest/#binned-resampling", 
            "text": "#  CausalityToolsBase.causality     Method .  causality ( x :: AbstractUncertainIndexValueDataset , \n     y :: AbstractUncertainIndexValueDataset , \n     test :: BinnedDataCausalityTest )   Apply a causality test to  x  and  y , which are both data series  with uncertainties in both indices and values. To get the data  on an equally-spaced temporal grid, the data are first binned according to the instructions in  test.binning .   Binning methods   If  test.binning  results in an uncertain value for each bin, then the causality   test is applied  test.n_realizations  times. Examples are  UncertainData.BinnedResampling   and  UncertainData.BinnedWeightedResampling , which both return a KDE estimate to the  distribution of values in each bin.   If  test.binning  returns a summary statistic for each bin, then the causality test is   applied to the summarised time series. Examples are  UncertainData.BinnedMeanResampling   and  UncertainData.BinnedMeanWeightedResampling , which both return the mean of each bin.  Example  Some example data  Let's say we have the following datasets  X  and  Y :   using   CausalityTools ,   UncertainData   sys   =   ar1_unidir ( u\u1d62   =   [ 0.1 ,   0.1 ],   c_xy   =   0.41 )  vars   =   ( 1 ,   2 )   # ar1_unidir has only two variables, X and Y  n_steps   =   100   # the number of points in the time series  tstep   =   10   # the mean of each time value is stepped by `tstep`  X ,   Y   =   example_uncertain_indexvalue_datasets ( sys ,   n_steps ,   vars ,   tstep   =   10 ,   d_xind   =   Uniform ( 7.5 ,   15.5 ),   d_yind   =   Uniform ( 5.5 ,   15.5 ),   d_xval   =   Uniform ( 0.1 ,   0.5 ));   Now we can perform the causality tests on our uncertain data, either over  the bin means, or over an ensemble of independent realisations of  the dataset given the distribution of points in each bin.  Summary statistic for each bin  PredictiveAsymmetryTest  If the binning method returns a summary statistic (e.g. the mean) for each bin,  then the causality test is applied exactly once to the bin summaries.  # Bin the data by drawing n_draws = 5000 realizations of each uncertain data   # point, then assign the draws to the correct bins and get a distribution   # of values in each bin.  binning   =   BinnedResampling ( 0 : 10 : 1000 ,   5000 )  # A predictive asymmetry test. For the predictions, we ll use a transfer entropy   # test, which uses the visitation frequency estimator. We ll predict 5 time steps   # forwards and backwards in time (\u03b7s = -5:5). For the state space binning, we ll  # determine how many intervals to split each axis into from the number of points  # in the time series.  k ,   l ,   m   =   1 ,   1 ,   1   # embedding parameters  n_subdivisions   =   floor ( Int ,   length ( 0 : 10 : 1000 ) ^ ( 1 / ( k   +   l   +   m   +   1 )))  state_space_binning   =   RectangularBinning ( n_subdivisions )  \u03b7s   =   - 5 : 5  te_test   =   VisitationFrequencyTest ( k   =   k ,   l   =   l ,   m   =   m ,  binning   =   state_space_binning ,   \u03b7s   =   \u03b7s ,   b   =   2 )   # use base-2 logarithms  pa_test   =   PredictiveAsymmetryTest ( predictive_test   =   te_test )  # Compute transfer entropy in both directions for the bin means.  # We still have to specify the number of realisations for the test,   # but it is ignored, so it doesn t matter what number we input here.  test   =   BinnedDataCausalityTest ( pa_test ,   binning ,   1 )  # Perform the tests  tes_xy   =   causality ( X ,   Y ,   test )  tes_yx   =   causality ( Y ,   X ,   test )  # Plot the results  plot ( xlabel   =   Prediction lag (eta) ,   ylabel   =   Predictive asymmetry (bits) )  plot! ( \u03b7s [ \u03b7s   .   0 ],   tes_xy ,   label   =   x -  y (binned) ,   c   =   : black )  plot! ( \u03b7s [ \u03b7s   .   0 ],   tes_yx ,   label   =   y -  x (binned) ,   c   =   : red )  hline! ([ 0 ],   lw   =   2 ,   ls   =   : dot ,   \u03b1   =   0.5 ,   label   =   ,   c   =   : grey )   Uncertain values representing each bin  If the binning method returns an uncertain value for each bin, then the  causality test is applied to  test.n_realizations  independent draws  of the binned dataset.  Predictive asymmetry test  # Bin the data by drawing n_draws = 5000 realizations of each uncertain data   # point, then assign the draws to the correct bins and get a distribution   # of values in each bin.  grid   =   0 : 5 : 1000  binning   =   BinnedResampling ( grid ,   5000 )  # A predictive asymmetry test. For the predictions, we ll use a transfer entropy   # test, which uses the visitation frequency estimator. We ll predict 5 time steps   # forwards and backwards in time (\u03b7s = -5:5). For the state space binning, we ll  # determine how many intervals to split each axis into from the number of points  # in the time series.  k ,   l ,   m   =   1 ,   1 ,   1   # embedding parameters  n_subdivisions   =   floor ( Int ,   length ( 0 : 10 : 1000 ) ^ ( 1 / ( k   +   l   +   m   +   1 )))  state_space_binning   =   RectangularBinning ( n_subdivisions )  \u03b7s   =   - 5 : 5  te_test   =   VisitationFrequencyTest ( k   =   k ,   l   =   l ,   m   =   m ,  binning   =   state_space_binning ,   \u03b7s   =   \u03b7s ,   b   =   2 )   # use base-2 logarithms  pa_test   =   PredictiveAsymmetryTest ( predictive_test   =   te_test )  # Compute transfer entropy in both directions over 50 independent realizations   # of the binned dataset.  n_realizations   =   50  test   =   BinnedDataCausalityTest ( pa_test ,   binning ,   n_realizations )  # Perform the tests on the bin means. `tes_xy` now contains 50 independent   # computation of the predictive asymmetry at predictive lags 1 to 5,   # and `tes_yx` contains the same but for the other direction  tes_xy   =   causality ( X ,   Y ,   test )  tes_yx   =   causality ( Y ,   X ,   test )  # Gather results in a matrix and compute means and standard deviations   # for the predictive asymmetries at each prediction lag  M_xy   =   hcat ( tes_xy ... ,)  M_yx   =   hcat ( tes_yx ... ,)  means_xy   =   mean ( M_xy ,   dims   =   2 )[ : ,   1 ]  means_yx   =   mean ( M_yx ,   dims   =   2 )[ : ,   1 ]  stdevs_xy   =   std ( M_yx ,   dims   =   2 )[ : ,   1 ]  stdevs_yx   =   std ( M_yx ,   dims   =   2 )[ : ,   1 ]  # Plot the predictive asymmetry as a function of prediction lag  plot ( xlabel   =   Prediction lag (eta) ,   ylabel   =   Predictive asymmetry (bits) )  plot! ( \u03b7s [ \u03b7s   .   0 ],   means_xy ,   ribbon   =   stdevs_xy ,   label   =   x -  y (binned) ,   c   =   : black )  plot! ( \u03b7s [ \u03b7s   .   0 ],   means_yx ,   ribbon   =   stdevs_yx ,   label   =   y -  x (binned) ,   c   =   : red )  hline! ([ 0 ],   lw   =   2 ,   ls   =   : dot ,   \u03b1   =   0.5 ,   label   =   ,   c   =   : grey )   Convergent cross mapping test  using   Plots   # Bin the data by drawing n_draws = 5000 realizations of each uncertain data   # point, then assign the draws to the correct bins and get a distribution   # of values in each bin.  binning   =   BinnedResampling ( 0 : 10 : 1000 ,   5000 )  # A convergent cross mapping causality test  ts_lengths   =   15 : 5 : 100   |   collect  causalitytest   =   ConvergentCrossMappingTest (  timeseries_lengths   =   ts_lengths ,   n_reps   =   100 ,   libsize   =   100 ,   \u03c4   =   1 ,   replace   =   true )  # Run ccm in both directions over 50 independent realizations of the binned datasets  n_realizations   =   50  test   =   BinnedDataCausalityTest ( causalitytest ,   binning ,   n_realizations )  ccms_xy   =   causality ( X ,   Y ,   test );  ccms_yx   =   causality ( Y ,   X ,   test );  realization_means_xy   =   [ mean . ( realization )   for   realization   in   ccms_xy ]  realization_means_yx   =   [ mean . ( realization )   for   realization   in   ccms_yx ]  M_xy   =   hcat ( realization_means_xy ... ,)  M_yx   =   hcat ( realization_means_yx ... ,)  plot ( xlabel   =   Time series length ,   ylabel   =   Cross map skill )  plot! ( ts_lengths ,   mean ( M_xy ,   dims   =   2 )[ : ,   1 ],   ribbon   =   std ( M_xy ,   dims   =   2 )[ : ,   1 ],   label   =   x -  y (binned) )  plot! ( ts_lengths ,   mean ( M_yx ,   dims   =   2 )[ : ,   1 ],   ribbon   =   std ( M_yx ,   dims   =   2 )[ : ,   1 ],   label   =   y -  x (binned) )   source", 
            "title": "Binned resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_strictlyincreasing_interpolated/", 
            "text": "Strictly increasing, interpolated resampling\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nsource\n::\nAbstractUncertainIndexValueDataset\n,\n \n    \ntarget\n::\nAbstractUncertainIndexValueDataset\n,\n \n    \ntest\n::\nCausalityTest\n,\n \nconstraint\n::\nSequentialSamplingConstraint\n,\n \ngrid\n::\nRegularGrid\n)\n\n\n\n\n\nTest for a causal influence from \nsource\n to \ntarget\n using the provided causality \ntest\n on  single draws of \nsource\n and \ntarget\n that have been generated according to the provided  \nsequential\n sampling constraint. After interpolating both \nsource\n and \ntarget\n  to the provided regular \ngrid\n, the causality test is performed on the interpolated data.\n\n\nExample\n\n\nFirst, generate some example data. \n\n\nN\n \n=\n \n300\n\n\na\u2081\n,\n \na\u2082\n,\n \nb\u2081\n,\n \nb\u2082\n,\n \n\u03be\u2081\n,\n \n\u03be\u2082\n,\n \nC\u2081\u2082\n \n=\n \n0.7\n,\n \n0.1\n,\n \n0.75\n,\n \n0.2\n,\n \n0.3\n,\n \n0.3\n,\n \n0.5\n\n\n\nD\n \n=\n \nrand\n(\nN\n,\n \n2\n)\n \n\nfor\n \nt\n \nin\n \n5\n:\nN\n\n    \nD\n[\nt\n,\n1\n]\n \n=\n \na\u2081\n*\nD\n[\nt\n-\n1\n,\n1\n]\n \n-\n \na\u2082\n*\nD\n[\nt\n-\n3\n,\n1\n]\n \n+\n                \n\u03be\u2081\n*\nrand\n(\nNormal\n(\n0\n,\n \n1\n))\n\n    \nD\n[\nt\n,\n2\n]\n \n=\n \nb\u2081\n*\nD\n[\nt\n-\n1\n,\n2\n]\n \n-\n \nb\u2082\n*\nD\n[\nt\n-\n2\n,\n2\n]\n \n+\n \nC\u2081\u2082\n*\nD\n[\nt\n-\n1\n,\n1\n]\n \n+\n \n\u03be\u2082\n*\nrand\n(\nNormal\n(\n0\n,\n \n1\n))\n\n\nend\n\n\n\n\n\nGather time series and add some uncertainties to them:\n\n\nts\n \n=\n \ncollect\n(\n1\n:\nN\n)\n\n\nx\n,\n \ny\n \n=\n \nD\n[\n:\n,\n \n1\n],\n \nD\n[\n:\n,\n \n2\n]\n\n\nt\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\nts\n \n.+\n \n0.2\n \n.*\n \nrand\n(\nN\n),\n \nrand\n(\nN\n)))\n\n\nuvalx\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\nx\n,\n \nrand\n()))\n\n\nuvaly\n \n=\n \nUncertainValue\n.\n(\nNormal\n.\n(\ny\n,\n \nrand\n()))\n\n\n\nX\n \n=\n \nUncertainIndexValueDataset\n(\nt\n,\n \nuvalx\n)\n\n\nY\n \n=\n \nUncertainIndexValueDataset\n(\nt\n,\n \nuvaly\n)\n\n\n\n\n\nNow, resample both the indices and values of \nX\n and \nY\n in a strictly  increasing manner according to the indices, interpolate both  to the same regular grid, and perform a causality test.\n\n\nn_bins\n \n=\n \nceil\n(\nInt\n,\n \nN\n^\n(\n1\n/\n4\n))\n\n\nte_test\n \n=\n \nVisitationFrequencyTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\nn_bins\n),\n \n\u03b7s\n \n=\n \n-\n5\n:\n5\n)\n\n\npa_test\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \nte_test\n)\n\n\n\n# Sample a strictly increasing realisation of the indices, interpolate,\n\n\n# then perform the causality test on the interpolated data.\n\n\ncausality\n(\nX\n,\n \nY\n,\n \npa_test\n,\n \nStrictlyIncreasing\n(),\n \nRegularGrid\n(\n1\n:\n1\n:\nN\n))\n\n\n\n\n\nsource", 
            "title": "Strictly increasing and interpolated resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_uncertain_data_strictlyincreasing_interpolated/#strictly-increasing-interpolated-resampling", 
            "text": "#  CausalityToolsBase.causality     Method .  causality ( source :: AbstractUncertainIndexValueDataset ,  \n     target :: AbstractUncertainIndexValueDataset ,  \n     test :: CausalityTest ,   constraint :: SequentialSamplingConstraint ,   grid :: RegularGrid )   Test for a causal influence from  source  to  target  using the provided causality  test  on  single draws of  source  and  target  that have been generated according to the provided   sequential  sampling constraint. After interpolating both  source  and  target   to the provided regular  grid , the causality test is performed on the interpolated data.  Example  First, generate some example data.   N   =   300  a\u2081 ,   a\u2082 ,   b\u2081 ,   b\u2082 ,   \u03be\u2081 ,   \u03be\u2082 ,   C\u2081\u2082   =   0.7 ,   0.1 ,   0.75 ,   0.2 ,   0.3 ,   0.3 ,   0.5  D   =   rand ( N ,   2 )   for   t   in   5 : N \n     D [ t , 1 ]   =   a\u2081 * D [ t - 1 , 1 ]   -   a\u2082 * D [ t - 3 , 1 ]   +                  \u03be\u2081 * rand ( Normal ( 0 ,   1 )) \n     D [ t , 2 ]   =   b\u2081 * D [ t - 1 , 2 ]   -   b\u2082 * D [ t - 2 , 2 ]   +   C\u2081\u2082 * D [ t - 1 , 1 ]   +   \u03be\u2082 * rand ( Normal ( 0 ,   1 ))  end   Gather time series and add some uncertainties to them:  ts   =   collect ( 1 : N )  x ,   y   =   D [ : ,   1 ],   D [ : ,   2 ]  t   =   UncertainValue . ( Normal . ( ts   .+   0.2   .*   rand ( N ),   rand ( N )))  uvalx   =   UncertainValue . ( Normal . ( x ,   rand ()))  uvaly   =   UncertainValue . ( Normal . ( y ,   rand ()))  X   =   UncertainIndexValueDataset ( t ,   uvalx )  Y   =   UncertainIndexValueDataset ( t ,   uvaly )   Now, resample both the indices and values of  X  and  Y  in a strictly  increasing manner according to the indices, interpolate both  to the same regular grid, and perform a causality test.  n_bins   =   ceil ( Int ,   N ^ ( 1 / 4 ))  te_test   =   VisitationFrequencyTest ( binning   =   RectangularBinning ( n_bins ),   \u03b7s   =   - 5 : 5 )  pa_test   =   PredictiveAsymmetryTest ( predictive_test   =   te_test )  # Sample a strictly increasing realisation of the indices, interpolate,  # then perform the causality test on the interpolated data.  causality ( X ,   Y ,   pa_test ,   StrictlyIncreasing (),   RegularGrid ( 1 : 1 : N ))   source", 
            "title": "Strictly increasing, interpolated resampling"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/", 
            "text": "Causality in dynamical systems\n\n\n\n\nGeneral syntax\n\n\nThe syntax for quantifying directional causality between variables of dynamical systems is \n\n\ncausality\n(\nsystem\n::\nDynamicalSystem\n,\n \nsetup\n::\nDynamicalSystemSetup\n,\n \ntest\n::\nCausalityTest\n)\n\n\n\n\n\nHere,\n\n\n\n\nsystem\n is either a \nDiscreteDynamicalSystem\n or \nContinuousDynamicalSystem\n instance.    See \nDynamicalSystems.jl\n for    more info on how to construct and initialise dynamical systems,\n\n\nsetup\n is an instance of either \nContinuousSystemSetup\n or    \nDiscreteSystemSetup\n. This gives the information on trajectory lengths, sampling    steps and solver options. It also instructs the causality estimators about which    variables of the dynamical system from which the time series for \nsource\n and \ntarget\n    are to be sampled,\n\n\ntest\n is an instance of a \ncausality test\n.\n\n\n\n\nWhat happens under the hood is that the dynamical system you provided is iterated  using the provided \nsetup\n parameters. A trajectory for the system is then recorded and  subsampled according to your specifications in \nsetup\n. Then, the causality \ntest\n is  applied to the time series for the variables you indicated in the \nsetup\n.\n\n\n\n\nContinuous systems\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nsystem\n::\nContinuousDynamicalSystem\n,\n \nsetup\n::\nContinuousSystemSetup\n,\n \n    \ntest\n::\nCausalityTest\n)\n\n\n\n\n\nApply the causality \ntest\n to the given continuous \nsystem\n using the provided \nsetup\n parameters.\n\n\nExample\n\n\nLet's re-create the \nlorenz_lorenz\n system that ships with \nCausalityTools\n and analyse the  dynamical influence between some of its components.\n\n\nfunction\n \neom_lorenz_lorenz\n(\nu\n,\n \np\n,\n \nt\n)\n\n    \nc_xy\n,\n \nc_yx\n,\n \na\u2081\n,\n \na\u2082\n,\n \na\u2083\n,\n \nb\u2081\n,\n \nb\u2082\n,\n \nb\u2083\n \n=\n \n(\np\n...\n,)\n\n    \nx1\n,\n \nx2\n,\n \nx3\n,\n \ny1\n,\n \ny2\n,\n \ny3\n \n=\n \n(\nu\n...\n,)\n\n\n    \ndx1\n \n=\n \n-\na\u2081\n*\n(\nx1\n \n-\n \nx2\n)\n \n+\n \nc_yx\n*\n(\ny1\n \n-\n \nx1\n)\n\n    \ndx2\n \n=\n \n-\nx1\n*\nx3\n \n+\n \na\u2082\n*\nx1\n \n-\n \nx2\n\n    \ndx3\n \n=\n \nx1\n*\nx2\n \n-\n \na\u2083\n*\nx3\n\n    \ndy1\n \n=\n \n-\nb\u2081\n*\n(\ny1\n \n-\n \ny2\n)\n \n+\n \nc_xy\n*\n(\nx1\n \n-\n \ny1\n)\n\n    \ndy2\n \n=\n \n-\ny1\n*\ny3\n \n+\n \nb\u2082\n*\ny1\n \n-\n \ny2\n\n    \ndy3\n \n=\n \ny1\n*\ny2\n \n-\n \nb\u2083\n*\ny3\n\n\n    \nreturn\n \nSVector\n{\n6\n}(\ndx1\n,\n \ndx2\n,\n \ndx3\n,\n \ndy1\n,\n \ndy2\n,\n \ndy3\n)\n\n\nend\n\n\n\nfunction\n \nlorenz_lorenz\n(;\n \nu0\n \n=\n \nrand\n(\n6\n),\n \n        \nc_xy\n \n=\n \n0.2\n,\n \nc_yx\n \n=\n \n0.0\n,\n \n        \na\u2081\n \n=\n \n10\n,\n \na\u2082\n \n=\n \n28\n,\n \na\u2083\n \n=\n \n8\n/\n3\n,\n \n        \nb\u2081\n \n=\n \n10\n,\n \nb\u2082\n \n=\n \n28\n,\n \nb\u2083\n \n=\n \n9\n/\n3\n)\n\n    \nContinuousDynamicalSystem\n(\neom_lorenz_lorenz\n,\n \nu0\n,\n \n[\nc_xy\n,\n \nc_yx\n,\n \na\u2081\n,\n \na\u2082\n,\n \na\u2083\n,\n \nb\u2081\n,\n \nb\u2082\n,\n \nb\u2083\n])\n\n\nend\n \n\n\n# Create an instance of the system with default parameters \n\n\nsys\n \n=\n \nlorenz_lorenz\n()\n\n\n\n# Analysis setup. We\nll check for an influence from variable x1 to y1, which are the \n\n\n# 1st and 4st components of the orbit. We\nll generate orbits consisting of 500 points.\n\n\nsetup\n \n=\n \nContinuousSystemSetup\n(\nsource\n \n=\n \n1\n,\n \ntarget\n \n=\n \n4\n,\n \nn_pts\n \n=\n \n500\n)\n\n\n\n# Predictive asymmetry causality test \n\n\npredtest\n \n=\n \nTransferOperatorGridTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\n5\n),\n \n\u03b7s\n \n=\n \n-\n5\n:\n5\n)\n\n\ntest_pa\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \npredtest\n)\n\n\n\ncausality\n(\nsys\n,\n \nsetup\n,\n \ntest_pa\n)\n\n\n\n\n\nsource\n\n\n\n\nDiscrete systems\n\n\n#\n\n\nCausalityToolsBase.causality\n \n \nMethod\n.\n\n\ncausality\n(\nsystem\n::\nDiscreteDynamicalSystem\n,\n \nsetup\n::\nDiscreteSystemSetup\n,\n \n    \ntest\n::\nCausalityTest\n)\n\n\n\n\n\nApply the causality \ntest\n to the given discrete \nsystem\n using the provided \nsetup\n parameters.\n\n\nExample\n\n\n# Use one of the built-in dynamical systems that ship with CausalityTools, \n\n\n# `logistic2_unidir`, which is a system of two chaotic logistic maps `x` and `y` with \n\n\n# coupling `x` to `y`, with `c_xy` controlling the coupling strength.\n\n\nsys\n \n=\n \nlogistic2_unidir\n(\nc_xy\n \n=\n \n1.0\n)\n\n\n\n# We treat the first variable as the source and the second variable as the target.\n\n\n# Iterate the system until we have time series with 200 observations. \n\n\nsetup\n \n=\n \nDiscreteSystemSetup\n(\nsource\n \n=\n \n1\n,\n \ntarget\n \n=\n \n2\n,\n \nn_pts\n \n=\n \n200\n)\n\n\n\n# Define some causality tests (with default values, you\nd want to tweak the \n\n\n# parameters to your needs)\n\n\ntest_ccm\n \n=\n \nConvergentCrossMappingTest\n(\ntimeseries_lengths\n \n=\n \n[\n45\n,\n \n50\n],\n \nn_reps\n \n=\n \n20\n)\n\n\ntest_cm\n \n=\n \nCrossMappingTest\n(\nn_reps\n \n=\n \n10\n)\n\n\ntest_vf\n \n=\n \nVisitationFrequencyTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\n5\n),\n \n\u03b7s\n \n=\n \n1\n:\n5\n)\n\n\ntest_tog\n \n=\n \nTransferOperatorGridTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\n5\n),\n \n\u03b7s\n \n=\n \n1\n:\n5\n)\n\n\ntest_jdd\n \n=\n \nJointDistanceDistributionTest\n()\n\n\ntest_jddt\n \n=\n \nJointDistanceDistributionTTest\n()\n\n\npredtest\n \n=\n \nVisitationFrequencyTest\n(\nbinning\n \n=\n \nRectangularBinning\n(\n5\n),\n \n\u03b7s\n \n=\n \n-\n5\n:\n5\n)\n\n\ntest_pa\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \npredtest\n)\n\n\n\n# Compute the causality statistics using the different tests\n\n\ncausality\n(\nsys\n,\n \nsetup\n,\n \ntest_ccm\n)\n\n\ncausality\n(\nsys\n,\n \nsetup\n,\n \ntest_cm\n)\n\n\ncausality\n(\nsys\n,\n \nsetup\n,\n \ntest_vf\n)\n\n\ncausality\n(\nsys\n,\n \nsetup\n,\n \ntest_tog\n)\n\n\ncausality\n(\nsys\n,\n \nsetup\n,\n \ntest_jdd\n)\n\n\ncausality\n(\nsys\n,\n \nsetup\n,\n \ntest_jddt\n)\n\n\n\n\n\nsource\n\n\n\n\nSetting up the analysis\n\n\nThe setup for continuous and discrete systems is slightly different, so there are two separate  types where you can give instructions about step length, time series length, etc, to the solvers.\n\n\n\n\nContinuous system setup\n\n\n#\n\n\nCausalityTools.IntegrationDynamicalSystems.ContinuousSystemSetup\n \n \nType\n.\n\n\nContinuousSystemSetup\n(\nresampling\n \n=\n \nNoResampling\n,\n \ndt\n::\nInt\n \n=\n \n0.01\n,\n \nTtr\n::\nInt\n \n=\n \n0\n,\n \n    \nsample_step\n::\nInt\n \n=\n \n1\n,\n \nn_pts\n::\nInt\n \n=\n \n100\n,\n\n    \ndiffeq\n \n=\n \n(\nalg\n \n=\n \nSimpleDiffEq\n.\nSimpleATsit5\n(),\n \nabstol\n \n=\n \n1.0e-6\n,\n \nreltol\n \n=\n \n1.0e-6\n),\n\n    \nsource\n,\n \ntarget\n)\n\n\n\n\n\nSetup for causality analysis of continuous dynamical systems.\n\n\nMandatory keyword arguments\n\n\n\n\nsource\n:  The variable(s) to use as \nsource\n when calling \ncausality\n. Usually integer(s).\n\n\ntarget\n:  The variable(s) to use as \ntarget\n when calling \ncausality\n. Usually integer(s).\n\n\n\n\nOptional keywords\n\n\n\n\nresampling\n: An instance of a resampling scheme. Defaults to \nNoResampling()\n.\n\n\ndt::Int\n: The time step when iterating the system. Defaults to 0.01.\n\n\nTtr::Int\n: The number of transient iterations before starting sampling. Defaults to 0.\n\n\nsample_step::Int\n: The sampling step in the final orbit. If \nsample_step\n \n 1, then the    orbit is generated until time \nT*sample_step\n, after which each \nsample_step\nth point is    drawn to generate the final orbit.\n\n\ndiffeq\n: Arguments propagated to init DifferentialEquations.jl. Defaults to   \ndiffeq = (alg = SimpleDiffEq.SimpleATsit5(), abstol = 1.0e-6, reltol = 1.0e-6)\n.\n\n\nn_pts::Int\n: The number of points in the orbit. Defaults to 100.\n\n\n\n\nsource\n\n\n\n\nDiscrete system setup\n\n\n#\n\n\nCausalityTools.IntegrationDynamicalSystems.DiscreteSystemSetup\n \n \nType\n.\n\n\nDiscreteSystemSetup\n(\nresampling\n \n=\n \nNoResampling\n,\n \ndt\n::\nInt\n \n=\n \n1\n,\n \nTtr\n::\nInt\n \n=\n \n0\n,\n \n    \ndiffeq\n \n=\n \n(\nalg\n \n=\n \nSimpleDiffEq\n.\nSimpleATsit5\n(),\n \nabstol\n \n=\n \n1.0e-6\n,\n \nreltol\n \n=\n \n1.0e-6\n),\n\n    \nn_pts\n \n=\n \n100\n,\n \nsource\n,\n \ntarget\n)\n\n\n\n\n\nSetup for causality analysis of discrete dynamical systems.\n\n\nMandatory keyword arguments\n\n\n\n\nsource\n:  The variable(s) to use as \nsource\n when calling \ncausality\n. Usually integer(s).\n\n\ntarget\n:  The variable(s) to use as \ntarget\n when calling \ncausality\n. Usually integer(s).\n\n\n\n\nOptional keywords\n\n\n\n\nresampling\n: An instance of a resampling scheme. Defaults to \nNoResampling()\n.\n\n\ndt::Int\n: The time step when iterating the system.\n\n\nTtr::Int\n: The number of transient iterations before starting sampling.\n\n\ndiffeq\n: Arguments propagated to init DifferentialEquations.jl. Defaults to   \ndiffeq = (alg = SimpleDiffEq.SimpleATsit5(), abstol = 1.0e-6, reltol = 1.0e-6)\n.\n\n\nn_pts::Int\n: The number of points in the orbit.\n\n\n\n\nsource", 
            "title": "Causality from dynamical systems"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/#causality-in-dynamical-systems", 
            "text": "", 
            "title": "Causality in dynamical systems"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/#general-syntax", 
            "text": "The syntax for quantifying directional causality between variables of dynamical systems is   causality ( system :: DynamicalSystem ,   setup :: DynamicalSystemSetup ,   test :: CausalityTest )   Here,   system  is either a  DiscreteDynamicalSystem  or  ContinuousDynamicalSystem  instance.    See  DynamicalSystems.jl  for    more info on how to construct and initialise dynamical systems,  setup  is an instance of either  ContinuousSystemSetup  or     DiscreteSystemSetup . This gives the information on trajectory lengths, sampling    steps and solver options. It also instructs the causality estimators about which    variables of the dynamical system from which the time series for  source  and  target     are to be sampled,  test  is an instance of a  causality test .   What happens under the hood is that the dynamical system you provided is iterated  using the provided  setup  parameters. A trajectory for the system is then recorded and  subsampled according to your specifications in  setup . Then, the causality  test  is  applied to the time series for the variables you indicated in the  setup .", 
            "title": "General syntax"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/#continuous-systems", 
            "text": "#  CausalityToolsBase.causality     Method .  causality ( system :: ContinuousDynamicalSystem ,   setup :: ContinuousSystemSetup ,  \n     test :: CausalityTest )   Apply the causality  test  to the given continuous  system  using the provided  setup  parameters.  Example  Let's re-create the  lorenz_lorenz  system that ships with  CausalityTools  and analyse the  dynamical influence between some of its components.  function   eom_lorenz_lorenz ( u ,   p ,   t ) \n     c_xy ,   c_yx ,   a\u2081 ,   a\u2082 ,   a\u2083 ,   b\u2081 ,   b\u2082 ,   b\u2083   =   ( p ... ,) \n     x1 ,   x2 ,   x3 ,   y1 ,   y2 ,   y3   =   ( u ... ,) \n\n     dx1   =   - a\u2081 * ( x1   -   x2 )   +   c_yx * ( y1   -   x1 ) \n     dx2   =   - x1 * x3   +   a\u2082 * x1   -   x2 \n     dx3   =   x1 * x2   -   a\u2083 * x3 \n     dy1   =   - b\u2081 * ( y1   -   y2 )   +   c_xy * ( x1   -   y1 ) \n     dy2   =   - y1 * y3   +   b\u2082 * y1   -   y2 \n     dy3   =   y1 * y2   -   b\u2083 * y3 \n\n     return   SVector { 6 }( dx1 ,   dx2 ,   dx3 ,   dy1 ,   dy2 ,   dy3 )  end  function   lorenz_lorenz (;   u0   =   rand ( 6 ),  \n         c_xy   =   0.2 ,   c_yx   =   0.0 ,  \n         a\u2081   =   10 ,   a\u2082   =   28 ,   a\u2083   =   8 / 3 ,  \n         b\u2081   =   10 ,   b\u2082   =   28 ,   b\u2083   =   9 / 3 ) \n     ContinuousDynamicalSystem ( eom_lorenz_lorenz ,   u0 ,   [ c_xy ,   c_yx ,   a\u2081 ,   a\u2082 ,   a\u2083 ,   b\u2081 ,   b\u2082 ,   b\u2083 ])  end   # Create an instance of the system with default parameters   sys   =   lorenz_lorenz ()  # Analysis setup. We ll check for an influence from variable x1 to y1, which are the   # 1st and 4st components of the orbit. We ll generate orbits consisting of 500 points.  setup   =   ContinuousSystemSetup ( source   =   1 ,   target   =   4 ,   n_pts   =   500 )  # Predictive asymmetry causality test   predtest   =   TransferOperatorGridTest ( binning   =   RectangularBinning ( 5 ),   \u03b7s   =   - 5 : 5 )  test_pa   =   PredictiveAsymmetryTest ( predictive_test   =   predtest )  causality ( sys ,   setup ,   test_pa )   source", 
            "title": "Continuous systems"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/#discrete-systems", 
            "text": "#  CausalityToolsBase.causality     Method .  causality ( system :: DiscreteDynamicalSystem ,   setup :: DiscreteSystemSetup ,  \n     test :: CausalityTest )   Apply the causality  test  to the given discrete  system  using the provided  setup  parameters.  Example  # Use one of the built-in dynamical systems that ship with CausalityTools,   # `logistic2_unidir`, which is a system of two chaotic logistic maps `x` and `y` with   # coupling `x` to `y`, with `c_xy` controlling the coupling strength.  sys   =   logistic2_unidir ( c_xy   =   1.0 )  # We treat the first variable as the source and the second variable as the target.  # Iterate the system until we have time series with 200 observations.   setup   =   DiscreteSystemSetup ( source   =   1 ,   target   =   2 ,   n_pts   =   200 )  # Define some causality tests (with default values, you d want to tweak the   # parameters to your needs)  test_ccm   =   ConvergentCrossMappingTest ( timeseries_lengths   =   [ 45 ,   50 ],   n_reps   =   20 )  test_cm   =   CrossMappingTest ( n_reps   =   10 )  test_vf   =   VisitationFrequencyTest ( binning   =   RectangularBinning ( 5 ),   \u03b7s   =   1 : 5 )  test_tog   =   TransferOperatorGridTest ( binning   =   RectangularBinning ( 5 ),   \u03b7s   =   1 : 5 )  test_jdd   =   JointDistanceDistributionTest ()  test_jddt   =   JointDistanceDistributionTTest ()  predtest   =   VisitationFrequencyTest ( binning   =   RectangularBinning ( 5 ),   \u03b7s   =   - 5 : 5 )  test_pa   =   PredictiveAsymmetryTest ( predictive_test   =   predtest )  # Compute the causality statistics using the different tests  causality ( sys ,   setup ,   test_ccm )  causality ( sys ,   setup ,   test_cm )  causality ( sys ,   setup ,   test_vf )  causality ( sys ,   setup ,   test_tog )  causality ( sys ,   setup ,   test_jdd )  causality ( sys ,   setup ,   test_jddt )   source", 
            "title": "Discrete systems"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/#setting-up-the-analysis", 
            "text": "The setup for continuous and discrete systems is slightly different, so there are two separate  types where you can give instructions about step length, time series length, etc, to the solvers.", 
            "title": "Setting up the analysis"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/#continuous-system-setup", 
            "text": "#  CausalityTools.IntegrationDynamicalSystems.ContinuousSystemSetup     Type .  ContinuousSystemSetup ( resampling   =   NoResampling ,   dt :: Int   =   0.01 ,   Ttr :: Int   =   0 ,  \n     sample_step :: Int   =   1 ,   n_pts :: Int   =   100 , \n     diffeq   =   ( alg   =   SimpleDiffEq . SimpleATsit5 (),   abstol   =   1.0e-6 ,   reltol   =   1.0e-6 ), \n     source ,   target )   Setup for causality analysis of continuous dynamical systems.  Mandatory keyword arguments   source :  The variable(s) to use as  source  when calling  causality . Usually integer(s).  target :  The variable(s) to use as  target  when calling  causality . Usually integer(s).   Optional keywords   resampling : An instance of a resampling scheme. Defaults to  NoResampling() .  dt::Int : The time step when iterating the system. Defaults to 0.01.  Ttr::Int : The number of transient iterations before starting sampling. Defaults to 0.  sample_step::Int : The sampling step in the final orbit. If  sample_step    1, then the    orbit is generated until time  T*sample_step , after which each  sample_step th point is    drawn to generate the final orbit.  diffeq : Arguments propagated to init DifferentialEquations.jl. Defaults to    diffeq = (alg = SimpleDiffEq.SimpleATsit5(), abstol = 1.0e-6, reltol = 1.0e-6) .  n_pts::Int : The number of points in the orbit. Defaults to 100.   source", 
            "title": "Continuous system setup"
        }, 
        {
            "location": "/causalitytests/causality_from_dynamical_systems/#discrete-system-setup", 
            "text": "#  CausalityTools.IntegrationDynamicalSystems.DiscreteSystemSetup     Type .  DiscreteSystemSetup ( resampling   =   NoResampling ,   dt :: Int   =   1 ,   Ttr :: Int   =   0 ,  \n     diffeq   =   ( alg   =   SimpleDiffEq . SimpleATsit5 (),   abstol   =   1.0e-6 ,   reltol   =   1.0e-6 ), \n     n_pts   =   100 ,   source ,   target )   Setup for causality analysis of discrete dynamical systems.  Mandatory keyword arguments   source :  The variable(s) to use as  source  when calling  causality . Usually integer(s).  target :  The variable(s) to use as  target  when calling  causality . Usually integer(s).   Optional keywords   resampling : An instance of a resampling scheme. Defaults to  NoResampling() .  dt::Int : The time step when iterating the system.  Ttr::Int : The number of transient iterations before starting sampling.  diffeq : Arguments propagated to init DifferentialEquations.jl. Defaults to    diffeq = (alg = SimpleDiffEq.SimpleATsit5(), abstol = 1.0e-6, reltol = 1.0e-6) .  n_pts::Int : The number of points in the orbit.   source", 
            "title": "Discrete system setup"
        }, 
        {
            "location": "/causalitytests/causality_tests/", 
            "text": "Causality tests\n\n\ncausality\n accepts many different causality tests. The returned values will vary depending  on the type of test, but the syntax for performing a test is always the same.\n\n\nAll tests can be used in conjunction with a binning scheme with \nBinnedDataCausalityTest\n to perform the tests on data with uncertain index (e.g time) values. See also \ntutorials\n.\n\n\n\n\nDistance based causality tests\n\n\n\n\nCrossMappingTest\n\n\nConvergentCrossMappingTest\n\n\nJointDistanceDistributionTest\n\n\nJointDistanceDistributionTTest\n\n\n\n\n\n\nEntropy based causality tests\n\n\n\n\nVisitationFrequencyTest\n\n\nTransferOperatorGridTest\n\n\nApproximateSimplexIntersectionTest\n\n\nExactSimplexIntersectionTest\n\n\n\n\n\n\nPredictive asymmetry tests\n\n\n\n\nPredictiveAsymmetryTest", 
            "title": "List of tests"
        }, 
        {
            "location": "/causalitytests/causality_tests/#causality-tests", 
            "text": "causality  accepts many different causality tests. The returned values will vary depending  on the type of test, but the syntax for performing a test is always the same.  All tests can be used in conjunction with a binning scheme with  BinnedDataCausalityTest  to perform the tests on data with uncertain index (e.g time) values. See also  tutorials .", 
            "title": "Causality tests"
        }, 
        {
            "location": "/causalitytests/causality_tests/#distance-based-causality-tests", 
            "text": "CrossMappingTest  ConvergentCrossMappingTest  JointDistanceDistributionTest  JointDistanceDistributionTTest", 
            "title": "Distance based causality tests"
        }, 
        {
            "location": "/causalitytests/causality_tests/#entropy-based-causality-tests", 
            "text": "VisitationFrequencyTest  TransferOperatorGridTest  ApproximateSimplexIntersectionTest  ExactSimplexIntersectionTest", 
            "title": "Entropy based causality tests"
        }, 
        {
            "location": "/causalitytests/causality_tests/#predictive-asymmetry-tests", 
            "text": "PredictiveAsymmetryTest", 
            "title": "Predictive asymmetry tests"
        }, 
        {
            "location": "/causalitytests/JointDistanceDistributionTest/", 
            "text": "Joint distance distribution tests\n\n\n\n\nJointDistanceDistributionTest\n\n\n#\n\n\nCausalityTools.CausalityTests.JointDistanceDistributionTest\n \n \nType\n.\n\n\nJointDistanceDistributionTest\n(;\n \ndistance_metric\n \n=\n \nSqEuclidean\n(),\n \nB\n::\nInt\n \n=\n \n10\n,\n\n    \nD\n::\nInt\n \n=\n \n2\n,\n \n\u03c4\n::\nInt\n \n=\n \n1\n)\n\n\n\n\n\nThe parameters for a joint distance distribution [1] analysis.\n\n\nOptional keyword arguments\n\n\n\n\ndistance_metric\n: The distance metric used to compute distances. Has to be a instance of a    valid distance metric from \nDistances.jl\n. Defaults to \nSqEuclidean()\n.\n\n\nB::Int\n: The number of equidistant subintervals to divide the interval \n[0, 1]\n into   when comparing the normalised distances.\n\n\nD::Int\n: The dimension of the delay reconstructions.\n\n\n\u03c4::Int\n: The delay of the delay reconstructions.\n\n\n\n\nReferences\n\n\n[1]\u00a0Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by  the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear  Science 28.7 (2018): 075302.\n\n\nsource\n\n\n\n\nJointDistanceDistributionTTest\n\n\n#\n\n\nCausalityTools.CausalityTests.JointDistanceDistributionTTest\n \n \nType\n.\n\n\nJointDistanceDistributionTTest\n(;\n \ndistance_metric\n \n=\n \nSqEuclidean\n(),\n \nB\n::\nInt\n \n=\n \n10\n,\n\n    \nD\n::\nInt\n \n=\n \n2\n,\n \n\u03c4\n::\nInt\n \n=\n \n1\n,\n \n    \nhypothesis_test\n::\nOneSampleTTest\n \n=\n \nOneSampleTTest\n,\n\n    \n\u03bc0\n \n=\n \n0.0\n)\n\n\n\n\n\nThe parameters for a joint distance distribution [1] analysis.\n\n\nOptional keyword arguments\n\n\n\n\ndistance_metric\n: The distance metric used to compute distances. Has to be a instance of a\n\n\n\n\nvalid distance metric from \nDistances.jl\n. Defaults to \nSqEuclidean()\n.\n\n\n\n\nB::Int\n: The number of equidistant subintervals to divide the interval \n[0, 1]\n into\n\n\n\n\nwhen comparing the normalised distances. \n\n\n\n\nD::Int\n: The dimension of the delay reconstructions.\n\n\n\u03c4::Int\n: The delay of the delay reconstructions.\n\n\n\u03bc0\n: The hypothetical mean value of the joint distance distribution if there\n\n\n\n\nis no coupling between \nx\n and \ny\n (default is \n\u03bc0 = 0.0\n).\n\n\n\n\nhypothesis_test\n: A \nOneSampleTTest\n to test whether the joint distance distribution\n\n\n\n\nis skewed towards positive values.\n\n\nReferences\n\n\n[1]\u00a0Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by  the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear  Science 28.7 (2018): 075302.\n\n\nsource", 
            "title": "JointDistanceDistributionTest"
        }, 
        {
            "location": "/causalitytests/JointDistanceDistributionTest/#joint-distance-distribution-tests", 
            "text": "", 
            "title": "Joint distance distribution tests"
        }, 
        {
            "location": "/causalitytests/JointDistanceDistributionTest/#jointdistancedistributiontest", 
            "text": "#  CausalityTools.CausalityTests.JointDistanceDistributionTest     Type .  JointDistanceDistributionTest (;   distance_metric   =   SqEuclidean (),   B :: Int   =   10 , \n     D :: Int   =   2 ,   \u03c4 :: Int   =   1 )   The parameters for a joint distance distribution [1] analysis.  Optional keyword arguments   distance_metric : The distance metric used to compute distances. Has to be a instance of a    valid distance metric from  Distances.jl . Defaults to  SqEuclidean() .  B::Int : The number of equidistant subintervals to divide the interval  [0, 1]  into   when comparing the normalised distances.  D::Int : The dimension of the delay reconstructions.  \u03c4::Int : The delay of the delay reconstructions.   References  [1]\u00a0Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by  the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear  Science 28.7 (2018): 075302.  source", 
            "title": "JointDistanceDistributionTest"
        }, 
        {
            "location": "/causalitytests/JointDistanceDistributionTest/#jointdistancedistributionttest", 
            "text": "#  CausalityTools.CausalityTests.JointDistanceDistributionTTest     Type .  JointDistanceDistributionTTest (;   distance_metric   =   SqEuclidean (),   B :: Int   =   10 , \n     D :: Int   =   2 ,   \u03c4 :: Int   =   1 ,  \n     hypothesis_test :: OneSampleTTest   =   OneSampleTTest , \n     \u03bc0   =   0.0 )   The parameters for a joint distance distribution [1] analysis.  Optional keyword arguments   distance_metric : The distance metric used to compute distances. Has to be a instance of a   valid distance metric from  Distances.jl . Defaults to  SqEuclidean() .   B::Int : The number of equidistant subintervals to divide the interval  [0, 1]  into   when comparing the normalised distances.    D::Int : The dimension of the delay reconstructions.  \u03c4::Int : The delay of the delay reconstructions.  \u03bc0 : The hypothetical mean value of the joint distance distribution if there   is no coupling between  x  and  y  (default is  \u03bc0 = 0.0 ).   hypothesis_test : A  OneSampleTTest  to test whether the joint distance distribution   is skewed towards positive values.  References  [1]\u00a0Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by  the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear  Science 28.7 (2018): 075302.  source", 
            "title": "JointDistanceDistributionTTest"
        }, 
        {
            "location": "/causalitytests/CrossMappingTest/", 
            "text": "#\n\n\nCausalityTools.CausalityTests.CrossMappingTest\n \n \nType\n.\n\n\nCrossMappingTest\n(;\n \ndim\n::\nInt\n \n=\n \n3\n,\n \n\u03c4\n::\nInt\n \n=\n \n1\n,\n \nlibsize\n::\nInt\n \n=\n \n10\n,\n\n    \nreplace\n::\nBool\n \n=\n \nfalse\n,\n \nn_reps\n::\nInt\n \n=\n \n100\n,\n \nsurr_func\n::\nFunction\n \n=\n \nrandomshuffle\n,\n\n    \nwhich_is_surr\n::\nSymbol\n \n=\n \n:\nnone\n,\n \nexclusion_radius\n::\nInt\n \n=\n \n0\n,\n\n    \ntree_type\n \n=\n \nNearestNeighbors\n.\nKDTree\n,\n \ndistance_metric\n \n=\n \nDistances\n.\nEuclidean\n(),\n\n    \ncorrespondence_measure\n \n=\n \nStatsBase\n.\ncor\n,\n \n    \n\u03bd\n::\nInt\n \n=\n \n0\n)\n\n\n\n\n\nThe parameters for a cross mapping [1] test.\n\n\nOptional keyword arguments\n\n\n\n\ndim\n: The dimension of the state space reconstruction (delay embedding)   constructed from the \nresponse\n series. Default is \ndim = 3\n.\n\n\n\u03c4\n: The embedding lag for the delay embedding constructed from \nresponse\n.   Default is \n\u03c4 = 1\n.\n\n\n\u03bd\n: The prediction lag to use when predicting scalar values of \ndriver\n   fromthe delay embedding of \nresponse\n.   \n\u03bd \n 0\n are forward lags (causal; \ndriver\n's past influences \nresponse\n's future),   and \n\u03bd \n 0\n are backwards lags (non-causal; \ndriver\n's' future influences   \nresponse\n's past). Adjust the prediction lag if you   want to performed lagged ccm   \n(Ye et al., 2015)\n.   Default is \n\u03bd = 0\n, as in   \nSugihara et al. (2012)\n.   \nNote: The sign of the lag \n\u03bd\n is organized to conform with the conventions in   \nTransferEntropy.jl\n, and is opposite to the convention used in the   \nrEDM\n package   (\nYe et al., 2016\n).\n\n\nlibsize\n: Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are \nn_reps\n)?\n\n\nn_reps\n: The number of times we draw a library of \nlibsize\n points from the   delay embedding of \nresponse\n and try to predict \ndriver\n values. Equivalently,   how many times do we cross map for this value of \nlibsize\n?   Default is \nn_reps = 100\n.\n\n\nreplace\n: Sample delay embedding points with replacement? Default is \nreplace = true\n.\n\n\nexclusion_radius\n: How many temporal neighbors of the delay embedding   point \nresponse_embedding(t)\n to exclude when searching for neighbors to   determine weights for predicting the scalar point \ndriver(t + \u03bd)\n.   Default is \nexclusion_radius = 0\n.\n\n\nwhich_is_surr\n: Which data series should be replaced by a surrogate   realization of the type given by \nsurr_type\n? Must be one of the   following: \n:response\n, \n:driver\n, \n:none\n, \n:both\n.   Default is \n:none\n.\n\n\nsurr_func\n: A valid surrogate function from TimeseriesSurrogates.jl.\n\n\ntree_type\n: The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either   \nBruteTree\n, \nKDTree\n or \nBallTree\n.\n\n\ndistance_metric\n: An instance of a \nMetric\n from Distances.jl. \nBallTree\n and \nBruteTree\n work with any \nMetric\n.   \nKDTree\n only works with the axis aligned metrics \nEuclidean\n, \nChebyshev\n,   \nMinkowski\n and \nCityblock\n. Default is \nmetric = Euclidean()\n \n(note the instantiation of the metric)\n.\n\n\ncorrespondence_measure\n: The function that computes the correspondence   between actual values of \ndriver\n and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is \ncorrespondence_measure = StatsBase.cor\n, which returns values on \n[-1, 1]\n[-1, 1]\n.   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of \n1\n1\n means perfect prediction.   \nSugihara et al. (2012)\n   also proposes to use the root mean square deviation, for which a value of \n0\n0\n would   be perfect prediction.\n\n\n\n\nReferences\n\n\n\n\nSugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079.   \nhttp://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific   Reports 5 (2015): 14750. \nhttps://www.nature.com/articles/srep14750\n\n\nYe, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version   0.4 7 (2016). \nhttps://cran.r-project.org/web/packages/rEDM/index.html\n\n\n\n\nsource", 
            "title": "CrossMappingTest"
        }, 
        {
            "location": "/causalitytests/ConvergentCrossMappingTest/", 
            "text": "#\n\n\nCausalityTools.CausalityTests.ConvergentCrossMappingTest\n \n \nType\n.\n\n\nConvergentCrossMappingTest\n(\ntimeseries_lengths\n;\n \n    \ndim\n::\nInt\n \n=\n \n3\n,\n \n\u03c4\n::\nInt\n \n=\n \n1\n,\n \nlibsize\n::\nInt\n \n=\n \n10\n,\n\n    \nreplace\n::\nBool\n \n=\n \nfalse\n,\n \nn_reps\n::\nInt\n \n=\n \n100\n,\n \nsurr_func\n::\nFunction\n \n=\n \nrandomshuffle\n,\n\n    \nwhich_is_surr\n::\nSymbol\n \n=\n \n:\nnone\n,\n \nexclusion_radius\n::\nInt\n \n=\n \n0\n,\n\n    \ntree_type\n \n=\n \nNearestNeighbors\n.\nKDTree\n,\n \ndistance_metric\n \n=\n \nDistances\n.\nEuclidean\n(),\n\n    \ncorrespondence_measure\n \n=\n \nStatsBase\n.\ncor\n,\n \n    \n\u03bd\n::\nInt\n \n=\n \n0\n)\n\n\n\n\n\nThe parameters for a convergent cross mapping [1] test.\n\n\nMandatory keyword arguments\n\n\n\n\ntimeseries_lengths\n: The time series lengths over which to cross map and check convergence.\n\n\n\n\nOptional keyword arguments\n\n\n\n\ndim\n: The dimension of the state space reconstruction (delay embedding)   constructed from the \nresponse\n series. Default is \ndim = 3\n.\n\n\n\u03c4\n: The embedding lag for the delay embedding constructed from \nresponse\n.   Default is \n\u03c4 = 1\n.\n\n\n\u03bd\n: The prediction lag to use when predicting scalar values of \ndriver\n   fromthe delay embedding of \nresponse\n.   \n\u03bd \n 0\n are forward lags (causal; \ndriver\n's past influences \nresponse\n's future),   and \n\u03bd \n 0\n are backwards lags (non-causal; \ndriver\n's' future influences   \nresponse\n's past). Adjust the prediction lag if you   want to performed lagged ccm   \n(Ye et al., 2015)\n.   Default is \n\u03bd = 0\n, as in   \nSugihara et al. (2012)\n.   \nNote: The sign of the lag \n\u03bd\n is organized to conform with the conventions in   \nTransferEntropy.jl\n, and is opposite to the convention used in the   \nrEDM\n package   (\nYe et al., 2016\n).\n\n\nlibsize\n: Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are \nn_reps\n)?\n\n\nn_reps\n: The number of times we draw a library of \nlibsize\n points from the   delay embedding of \nresponse\n and try to predict \ndriver\n values. Equivalently,   how many times do we cross map for this value of \nlibsize\n?   Default is \nn_reps = 100\n.\n\n\nreplace\n: Sample delay embedding points with replacement? Default is \nreplace = true\n.\n\n\nexclusion_radius\n: How many temporal neighbors of the delay embedding   point \nresponse_embedding(t)\n to exclude when searching for neighbors to   determine weights for predicting the scalar point \ndriver(t + \u03bd)\n.   Default is \nexclusion_radius = 0\n.\n\n\nwhich_is_surr\n: Which data series should be replaced by a surrogate   realization of the type given by \nsurr_type\n? Must be one of the   following: \n:response\n, \n:driver\n, \n:none\n, \n:both\n.   Default is \n:none\n.\n\n\nsurr_func\n: A valid surrogate function from TimeseriesSurrogates.jl.\n\n\ntree_type\n: The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either   \nBruteTree\n, \nKDTree\n or \nBallTree\n.\n\n\ndistance_metric\n: An instance of a \nMetric\n from Distances.jl. \nBallTree\n and \nBruteTree\n work with any \nMetric\n.   \nKDTree\n only works with the axis aligned metrics \nEuclidean\n, \nChebyshev\n,   \nMinkowski\n and \nCityblock\n. Default is \nmetric = Euclidean()\n \n(note the instantiation of the metric)\n.\n\n\ncorrespondence_measure\n: The function that computes the correspondence   between actual values of \ndriver\n and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is \ncorrespondence_measure = StatsBase.cor\n, which returns values on \n[-1, 1]\n[-1, 1]\n.   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of \n1\n1\n means perfect prediction.   \nSugihara et al. (2012)\n   also proposes to use the root mean square deviation, for which a value of \n0\n0\n would   be perfect prediction.\n\n\nsummarise\n: Should cross map skills be summarised for each time series length? Default is    \nsummarise = false\n.\n\n\naverage_measure\n: Either \n:median\n or \n:mean\n. Default is \n:median\n\n\nuncertainty_measure\n: Either \n:quantile\n or \n:std\n.\n\n\nquantiles\n: Compute uncertainty over quantile(s) if \nuncertainty_measure\n is \n:quantile\n.    Default is \n[0.327, 0.673]\n, roughly corresponding to \n1s\n for normally distributed data.\n\n\n\n\nReferences\n\n\n\n\nSugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079.   \nhttp://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific   Reports 5 (2015): 14750. \nhttps://www.nature.com/articles/srep14750\n\n\nYe, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version   0.4 7 (2016). \nhttps://cran.r-project.org/web/packages/rEDM/index.html\n\n\n\n\nsource", 
            "title": "ConvergentCrossMappingTest"
        }, 
        {
            "location": "/causalitytests/TransferOperatorGridTest/", 
            "text": "#\n\n\nCausalityTools.CausalityTests.TransferOperatorGridTest\n \n \nType\n.\n\n\nTransferOperatorGridTest\n(\nk\n::\nInt\n \n=\n \n1\n,\n \nl\n::\nInt\n \n=\n \n1\n,\n \nm\n::\nInt\n \n=\n \n1\n,\n \nn\n::\nInt\n \n=\n \n1\n,\n \n    \n\u03c4\n::\nInt\n \n=\n \n1\n,\n \nb\n \n=\n \n2\n,\n \nestimator\n::\nTransferOperatorGrid\n \n=\n \nTransferOperatorGrid\n(),\n \n    \nbinning_summary_statistic\n::\nFunction\n \n=\n \nStatsBase\n.\nmean\n,\n\n    \nbinning\n::\nRectangularBinning\n,\n \n\u03b7s\n)\n\n\n\n\n\nThe parameters for a transfer entropy test using the \nTransferOperatorGrid\n estimator [1].\n\n\nMandatory keyword arguments\n\n\n\n\nbinning::RectangularBinning\n: An instance of a \nRectangularBinning\n that dictates    how the delay embedding is discretized.\n\n\n\u03b7s\n: The prediction lags (that gos into the \nT_{f}\nT_{f}\n component of the embedding).\n\n\n\n\nOptional keyword arguments\n\n\n\n\nk::Int\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl::Int\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm::Int\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\nn::Int\n: The dimension of the \nC_{pp}\nC_{pp}\n component of the embedding.\n\n\n\u03c4::Int\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\nestimator::TransferOperatorGrid\n: A \nTransferOperatorGrid\n estimator instance.\n\n\nbinning_summary_statistic::Function\n: A summary statistic to summarise the    transfer entropy values if multiple binnings are provided.\n\n\n\n\nEstimation of the invariant measure\n\n\nWith a \nTransferOperatorGridTest\n, the first step is to compute an approximation to the  \ntransfer operator\n over the partition elements of a rectangular \ndiscretization\n  of an appropriate \ndelay reconstruction\n of the time  series to be analysed. Transfer entropy is then computed from the \ninvariant  distribution\n arising from the transfer operator. \n\n\nAbout the delay reconstruction for transfer entropy analysis\n\n\nDenote the time series for the source process \nS\nS\n as \nS(t)\nS(t)\n, and the time series for  the target process \nT\nT\n as \nT(t)\nT(t)\n, and \nC_i(t)\nC_i(t)\n as the time series for any conditional  processes \nC_i\nC_i\n that also may influence \nT\nT\n. To compute (conditional) TE, we need a  generalised embedding [3, 4] incorporating all of these processes.\n\n\nFor convenience, define the state vectors\n\n\n\n\n\n\\begin{align}\nT_f^{(k)} \n= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1))\\}, \\label{eq:Tf} \\\\\nT_{pp}^{(l)} \n= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\\\\nS_{pp}^{(m)} \n= \\{(S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1}))\\},\\\\\nC_{pp}^{(n)} \n= \\{ (C_1(t), C_1(t-\\tau_1), \\ldots,  C_2(t), C_2(t-\\tau_1) \\},\n\\end{align}\n\n\n\n\n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1))\\}, \\label{eq:Tf} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\\\\nS_{pp}^{(m)} &= \\{(S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1}))\\},\\\\\nC_{pp}^{(n)} &= \\{ (C_1(t), C_1(t-\\tau_1), \\ldots,  C_2(t), C_2(t-\\tau_1) \\},\n\\end{align}\n\n\n\n\n\nwhere the state vectors \nT_f^{(k)}\nT_f^{(k)}\n contain \nk\nk\n future values of the target  variable, \nT_{pp}^{(l)}\nT_{pp}^{(l)}\n contain \nl\nl\n present and past values of the target  variable, \nS_{pp}^{(m)}\nS_{pp}^{(m)}\n contain \nm\nm\n present and past values of the source  variable, \nC_{pp}^{(n)}\nC_{pp}^{(n)}\n contain a total of \nn\nn\n present and past values of any  conditional variable(s).  Combining all variables, we have the generalised embedding \n\n\n\n\n\n\\begin{align}\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n\\end{align}\n\n\n\n\n\\begin{align}\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n\\end{align}\n\n\n\n\n\nwith a total embedding dimension of \nk + l + m + n\nk + l + m + n\n.  Here, \n\\tau\n\\tau\n indicates the \nembedding\n lag  (in the \nVisitationFrequencyTest\n, we set \n\\tau_1 = \\tau_2 = \\tau_3 = \\ldots\n\\tau_1 = \\tau_2 = \\tau_3 = \\ldots\n), and \n\\eta\n\\eta\n indicates the prediction lag (the lag of the influence the source  has on the target). \n\n\nHence, in the generalised embedding, only \nT_f\nT_f\n depends on the prediction lag \n\\eta\n\\eta\n,  which is to be determined by the analyst. For transfer entropy analysis, \n\\eta\n\\eta\n is chosen  to be some positive integer, while for  \npredictive asymmetry analysis\n,  symmetric negative and positive \n\\eta\n\\eta\ns are used for computing \n\\mathbb{A}\n\\mathbb{A}\n.  \n\n\nIn terms of this generalised embedding, transfer entropy from a source variable \nS\nS\n to a  target variable \nT\nT\n with conditioning on variable(s) \nC\nC\n is thus defined as \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T|C} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{2}{\\left( \\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T|C} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{2}{\\left( \\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nWithout conditioning, we have \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{2}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{2}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nLow-level estimator\n\n\nThis test uses the \nTransferOperatorGrid\n estimator on the following low-level method  under the hood. \n\n\n\n\ntransferentropy(::Any, ::TEVars, ::RectangularBinning, ::TransferEntropyEstimator)\n\n\n\n\nIn this estimator, the mapping between variables of the  \ngeneralised embedding\n and the marginals during  transfer entropy computation is controlled using a \nTEVars\n  instance. It is \nhighly\n recommended that you check the documentation for this  method, because it describes the transfer entropy estimation procedure in detail.\n\n\nNotes:\n\n\n\n\nUse \ncausality(source, target, params::TransferOperatorGridTest)\n for regular    transfer entropy analysis. This method uses only the \nk\n, \nl\n, \nm\n and ignores \nn\n    when constructing the delay reconstruction.\n\n\nUse \ncausality(source, target, cond, params::TransferOperatorGridTest)\n for conditional    transfer entropy analysis. This method uses the \nk\n, \nl\n, \nm\n \nand\n \nn\n when constructing    the delay reconstruction.\n\n\n\n\nExample\n\n\n# Prediction lags\n\n\n\u03b7s\n \n=\n \n1\n:\n10\n\n\nbinning\n \n=\n \nRectangularBinning\n(\n10\n)\n\n\n\n# Use defaults, binning and prediction lags are required. \n\n\n# Note that `binning` and `\u03b7s` are *mandatory* keyword arguments.\n\n\nTransferOperatorGridTest\n(\nbinning\n \n=\n \nbinning\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\n\n# The other keywords can also be adjusted\n\n\nTransferOperatorGridTest\n(\nk\n \n=\n \n1\n,\n \nl\n \n=\n \n2\n,\n \nbinning\n \n=\n \nbinning\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\n\n\n\nReferences\n\n\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.  \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.99.042212\n\n\n\n\nsource", 
            "title": "TransferOperatorGridTest"
        }, 
        {
            "location": "/causalitytests/VisitationFrequencyTest/", 
            "text": "#\n\n\nCausalityTools.CausalityTests.VisitationFrequencyTest\n \n \nType\n.\n\n\nVisitationFrequencyTest\n(\nk\n::\nInt\n \n=\n \n1\n,\n \nl\n::\nInt\n \n=\n \n1\n,\n \nm\n::\nInt\n \n=\n \n1\n,\n \nn\n::\nInt\n \n=\n \n1\n,\n \n    \n\u03c4\n::\nInt\n \n=\n \n1\n,\n \nb\n \n=\n \n2\n,\n \nestimator\n::\nVisitationFrequency\n \n=\n \nVisitationFrequency\n(),\n \n    \nbinning_summary_statistic\n::\nFunction\n \n=\n \nStatsBase\n.\nmean\n,\n\n    \nbinning\n::\nRectangularBinning\n,\n \n\u03b7s\n)\n\n\n\n\n\nThe parameters for a transfer entropy test using the \nVisitationFrequency\n estimator. This is the original transfer entropy estimator from [1], as implemented in [2].\n\n\nMandatory keyword arguments\n\n\n\n\nbinning::RectangularBinning\n: An instance of a \nRectangularBinning\n that dictates    how the delay embedding is discretized.\n\n\n\u03b7s\n: The prediction lags (that go into the \nT_{f}\nT_{f}\n component of the embedding).\n\n\n\n\nOptional keyword arguments\n\n\n\n\nk::Int\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl::Int\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm::Int\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\nn::Int\n: The dimension of the \nC_{pp}\nC_{pp}\n component of the embedding.\n\n\n\u03c4::Int\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\nestimator::VisitationFrequency\n: A \nVisitationFrequency\n estimator instance.\n\n\nbinning_summary_statistic::Function\n: A summary statistic to summarise the    transfer entropy values if multiple binnings are provided.\n\n\n\n\nEstimation of the invariant measure\n\n\nWith a \nVisitationFrequencyTest\n, the invariant distribution from which transfer  entropy is computed is estimated using simple counting (hence, visitation frequency).  Counting is done over the partition elements of a \ndiscretization\n  of an appropriate \ndelay reconstruction\n of the time  series to be analysed.\n\n\nAbout the delay reconstruction for transfer entropy analysis\n\n\nDenote the time series for the source process \nS\nS\n as \nS(t)\nS(t)\n, and the time series for  the target process \nT\nT\n as \nT(t)\nT(t)\n, and \nC_i(t)\nC_i(t)\n as the time series for any conditional  processes \nC_i\nC_i\n that also may influence \nT\nT\n. To compute (conditional) TE, we need a  generalised embedding [3, 4] incorporating all of these processes.\n\n\nFor convenience, define the state vectors\n\n\n\n\n\n\\begin{align}\nT_f^{(k)} \n= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1))\\}, \\label{eq:Tf} \\\\\nT_{pp}^{(l)} \n= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\\\\nS_{pp}^{(m)} \n= \\{(S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1}))\\},\\\\\nC_{pp}^{(n)} \n= \\{ (C_1(t), C_1(t-\\tau_1), \\ldots,  C_2(t), C_2(t-\\tau_1) \\},\n\\end{align}\n\n\n\n\n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1))\\}, \\label{eq:Tf} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\\\\nS_{pp}^{(m)} &= \\{(S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1}))\\},\\\\\nC_{pp}^{(n)} &= \\{ (C_1(t), C_1(t-\\tau_1), \\ldots,  C_2(t), C_2(t-\\tau_1) \\},\n\\end{align}\n\n\n\n\n\nwhere the state vectors \nT_f^{(k)}\nT_f^{(k)}\n contain \nk\nk\n future values of the target  variable, \nT_{pp}^{(l)}\nT_{pp}^{(l)}\n contain \nl\nl\n present and past values of the target  variable, \nS_{pp}^{(m)}\nS_{pp}^{(m)}\n contain \nm\nm\n present and past values of the source  variable, \nC_{pp}^{(n)}\nC_{pp}^{(n)}\n contain a total of \nn\nn\n present and past values of any  conditional variable(s).  Combining all variables, we have the generalised embedding \n\n\n\n\n\n\\begin{align}\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n\\end{align}\n\n\n\n\n\\begin{align}\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n\\end{align}\n\n\n\n\n\nwith a total embedding dimension of \nk + l + m + n\nk + l + m + n\n.  Here, \n\\tau\n\\tau\n indicates the \nembedding\n lag  (in the \nVisitationFrequencyTest\n, we set \n\\tau_1 = \\tau_2 = \\tau_3 = \\ldots\n\\tau_1 = \\tau_2 = \\tau_3 = \\ldots\n), and \n\\eta\n\\eta\n indicates the prediction lag (the lag of the influence the source  has on the target). \n\n\nHence, in the generalised embedding, only \nT_f\nT_f\n depends on the prediction lag \n\\eta\n\\eta\n,  which is to be determined by the analyst. For transfer entropy analysis, \n\\eta\n\\eta\n is chosen  to be some positive integer, while for  \npredictive asymmetry analysis\n,  symmetric negative and positive \n\\eta\n\\eta\ns are used for computing \n\\mathbb{A}\n\\mathbb{A}\n.  \n\n\nIn terms of this generalised embedding, transfer entropy from a source variable \nS\nS\n to a  target variable \nT\nT\n with conditioning on variable(s) \nC\nC\n is thus defined as \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T|C} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{2}{\\left( \\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T|C} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{2}{\\left( \\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nWithout conditioning, we have \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{2}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{2}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nLow-level estimator\n\n\nThis test uses the \nVisitationFrequency\n estimator on the following low-level method  under the hood. \n\n\n\n\ntransferentropy(::Any, ::TEVars, ::RectangularBinning, ::TransferEntropyEstimator)\n\n\n\n\nIn this estimator, the mapping between variables of the  \ngeneralised embedding\n and the marginals during  transfer entropy computation is controlled using a \nTEVars\n  instance. It is \nhighly\n recommended that you check the documentation for this  method, because it describes the transfer entropy estimation procedure in detail.\n\n\nNotes:\n\n\n\n\nUse \ncausality(source, target, params::VisitationFrequencyTest)\n for regular    transfer entropy analysis. This method uses only the \nk\n, \nl\n, \nm\n and ignores \nn\n    when constructing the delay reconstruction.\n\n\nUse \ncausality(source, target, cond, params::VisitationFrequencyTest)\n for conditional    transfer entropy analysis. This method uses the \nk\n, \nl\n, \nm\n \nand\n \nn\n when constructing    the delay reconstruction.\n\n\n\n\nExample\n\n\n# Prediction lags\n\n\n\u03b7s\n \n=\n \n1\n:\n10\n\n\nbinning\n \n=\n \nRectangularBinning\n(\n10\n)\n\n\n\n# Use defaults, binning and prediction lags are required. \n\n\n# Note that `binning` and `\u03b7s` are *mandatory* keyword arguments.\n\n\nVisitationFrequencyTest\n(\nbinning\n \n=\n \nbinning\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\n\n# The other keywords can also be adjusted\n\n\nVisitationFrequencyTest\n(\nk\n \n=\n \n1\n,\n \nl\n \n=\n \n2\n,\n \nbinning\n \n=\n \nbinning\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\n\n\n\nReferences\n\n\n\n\nSchreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.   \nhttps://journals.aps.org/prl/abstract/10.1103/PhysRevLett.85.461\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.  \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.99.042212\n\n\nSauer, Tim, James A. Yorke, and Martin Casdagli. \"Embedology.\" Journal of statistical Physics 65.3-4 (1991): 579-616.\n\n\nDeyle, Ethan R., and George Sugihara. \"Generalized theorems for nonlinear state space reconstruction.\" PLoS One 6.3 (2011): e18295.\n\n\n\n\nsource", 
            "title": "VisitationFrequencyTest"
        }, 
        {
            "location": "/causalitytests/ApproximateSimplexIntersectionTest/", 
            "text": "#\n\n\nCausalityTools.CausalityTests.ApproximateSimplexIntersectionTest\n \n \nType\n.\n\n\nApproximateSimplexIntersectionTest\n\n\n\n\n\nThe parameters for a transfer entropy test where the invariant measure is estimated using an approximation to the transfer operator over a triangulation of the delay reconstruction [1]. Essentially, this allows the generation of an unlimited amount of points from the transfer operator. Transfer entropy then can be estimated by superimposing a rectangular grid as usual over the points generated from the  transfer operator.\n\n\nNotes\n\n\n\n\nCompared to the rectangular estimators, the simplex intersection approach is much slower.   However, it might be beneficial for sparse time series[1].\n\n\nIf you're doing any sort of sensitivity analysis over binning schemes, then use the low-level   estimators to first construct the invariant measure over the triangulation. You can then   estimate transfer entropy \"for free\" from the points you generate from the triangulation.   Over these points you can superimpose any grid you want and quickly compute transfer entropy   using one of the rectangular estimators.\n\n\n\n\nMandatory keyword arguments\n\n\n\n\nbinning::RectangularBinning\n: An instance of a \nRectangularBinning\n that dictates   how the delay embedding is discretized.\n\n\n\u03b7s\n: The prediction lags (that gos into the \nT_{f}\nT_{f}\n component of the embedding).\n\n\n\n\nOptional keyword arguments\n\n\n\n\nk::Int\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl::Int\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm::Int\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\nn::Int\n: The dimension of the \nC_{pp}\nC_{pp}\n component of the embedding.\n\n\n\u03c4::Int\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\nestimator::Union{VisitationFrequency, TransferOperatorGrid}\n: A \nVisitationFrequency\n   or a \nTransferOperatorGrid\n estimator instance.\n\n\nn_pts::Int\n: The number of points to generate from the invariant distribution   over the triangulation. It is over these points transfer entropy is finally   generated using the provided rectangular \nestimator\n. Defaults to \nn_pts = 10000\n.\n\n\nbinning_summary_statistic::Function\n: A summary statistic to summarise the   transfer entropy values if multiple binnings are provided.\n\n\n\n\nExamples\n\n\nusing\n \nDynamicalSystems\n\n\n\n# Short time series of two coupled logistic maps (x drives y)\n\n\nsys\n \n=\n \nlogistic2_unidir\n(\nc_xy\n \n=\n \n0.5\n)\n\n\nnpts\n \n=\n \n75\n\n\nx\n,\n \ny\n \n=\n \ncolumns\n(\ntrajectory\n(\nsys\n,\n \nnpts\n,\n \nTtr\n \n=\n \n1000\n))\n\n\n\n# average over a few different binning schemes, use a few different\n\n\n# prediction lags and use the transfer operator grid estimator on the point\n\n\n# cloud generated from the invariant measure over the triangulation.\n\n\nbinnings\n \n=\n \n[\nRectangularBinning\n(\ni\n)\n \nfor\n \ni\n \n=\n \n3\n:\n5\n]\n\n\nestimator\n \n=\n \nTransferOperatorGrid\n()\n\n\n\u03b7s\n \n=\n \n1\n:\n3\n\n\n\n# Perform causality test, both from x to y and y to x\n\n\ntest\n \n=\n \nApproximateSimplexIntersectionTest\n(\nbinning\n \n=\n \nbinnings\n,\n \nestimator\n \n=\n \nestimator\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\nte_xtoy\n \n=\n \ncausality\n(\nx\n,\n \ny\n,\n \ntest\n)\n\n\nte_ytox\n \n=\n \ncausality\n(\ny\n,\n \nx\n,\n \ntest\n)\n\n\n\n\n\nReferences\n\n\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation  using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.  \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.99.042212\n\n\n\n\nsource", 
            "title": "ApproximateSimplexIntersectionTest"
        }, 
        {
            "location": "/causalitytests/ExactSimplexIntersectionTest/", 
            "text": "#\n\n\nCausalityTools.CausalityTests.ExactSimplexIntersectionTest\n \n \nType\n.\n\n\nExactSimplexIntersectionTest\n\n\n\n\n\nThe parameters for a transfer entropy test where the invariant measure is estimated using an approximation to the transfer operator over a triangulation of the delay reconstruction [1]. Essentially, this allows the generation of an unlimited amount of points from the transfer operator. Transfer entropy then can be estimated by superimposing a rectangular grid as usual over the points generated from the  transfer operator.\n\n\nNotes\n\n\n\n\nCompared to the rectangular estimators, the exact simplex intersection approach is much slower.   However, it might be beneficial for sparse time series[1].   A faster triangulation estimator is the \nApproximateSimplexIntersectionTest\n.\n\n\nIf you're doing any sort of sensitivity analysis over binning schemes, then use the low-level   estimators to first construct the invariant measure over the triangulation. You can then   estimate transfer entropy \"for free\" from the points you generate from the triangulation.   Over these points you can superimpose any grid you want and quickly compute transfer entropy   using one of the rectangular estimators.\n\n\n\n\nMandatory keyword arguments\n\n\n\n\nbinning::RectangularBinning\n: An instance of a \nRectangularBinning\n that dictates   how the delay embedding is discretized.\n\n\n\u03b7s\n: The prediction lags (that gos into the \nT_{f}\nT_{f}\n component of the embedding).\n\n\n\n\nOptional keyword arguments\n\n\n\n\nk::Int\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl::Int\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm::Int\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\nn::Int\n: The dimension of the \nC_{pp}\nC_{pp}\n component of the embedding.\n\n\n\u03c4::Int\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\nestimator::Union{VisitationFrequency, TransferOperatorGrid}\n: A \nVisitationFrequency\n   or a \nTransferOperatorGrid\n estimator instance.\n\n\nn_pts::Int\n: The number of points to generate from the invariant distribution   over the triangulation. It is over these points transfer entropy is finally   generated using the provided rectangular \nestimator\n. Defaults to \nn_pts = 10000\n.\n\n\nbinning_summary_statistic::Function\n: A summary statistic to summarise the   transfer entropy values if multiple binnings are provided.\n\n\n\n\nExamples\n\n\nusing\n \nDynamicalSystems\n\n\n\n# Short time series of two coupled logistic maps (x drives y)\n\n\nsys\n \n=\n \nlogistic2_unidir\n(\nc_xy\n \n=\n \n0.5\n)\n\n\nnpts\n \n=\n \n75\n\n\nx\n,\n \ny\n \n=\n \ncolumns\n(\ntrajectory\n(\nsys\n,\n \nnpts\n,\n \nTtr\n \n=\n \n1000\n))\n\n\n\n# average over a few different binning schemes, use a few different\n\n\n# prediction lags and use the transfer operator grid estimator on the point\n\n\n# cloud generated from the invariant measure over the triangulation.\n\n\nbinnings\n \n=\n \n[\nRectangularBinning\n(\ni\n)\n \nfor\n \ni\n \n=\n \n3\n:\n5\n]\n\n\nestimator\n \n=\n \nTransferOperatorGrid\n()\n\n\n\u03b7s\n \n=\n \n1\n:\n3\n\n\n\n# Perform causality test, both from x to y and y to x\n\n\ntest\n \n=\n \nExactSimplexIntersectionTest\n(\nbinning\n \n=\n \nbinnings\n,\n \nestimator\n \n=\n \nestimator\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\nte_xtoy\n \n=\n \ncausality\n(\nx\n,\n \ny\n,\n \ntest\n)\n\n\nte_ytox\n \n=\n \ncausality\n(\ny\n,\n \nx\n,\n \ntest\n)\n\n\n\n\n\nReferences\n\n\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation  using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.  \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.99.042212\n\n\n\n\nsource", 
            "title": "ExactSimplexIntersectionTest"
        }, 
        {
            "location": "/causalitytests/PredictiveAsymmetryTest/", 
            "text": "Predictive asymmetry\n\n\n#\n\n\nCausalityTools.CausalityTests.PredictiveAsymmetryTest\n \n \nType\n.\n\n\nPredictiveAsymmetryTest\n(\npredictive_test\n::\nCausalityTest\n)\n\n\n\n\n\nThe parameters for a predictive asymmetry causality test [1]. \n\n\nMandatory keywords\n\n\n\n\npredictive_test\n: An instance of a predictive causality test that explicitly    uses prediction lags (e.g. \nVisitationFrequencyTest\n or    \nTransferOperatorGridTest\n).\n\n\n\n\nAbout the prediction lags\n\n\nThe prediction lags in the predictive causality test must consist of \nn\n negative  integers and \nn\n positive integers that are symmetric around zero. \n\n\nIn other words, negative lags  must exactly match the positive lags but with opposite  sign. The zero lag can be included, but will be ignored, so it is possible to give  ranges too.\n\n\nExamples\n\n\ntest_visitfreq\n \n=\n \nVisitationFrequencyTest\n(\n\u03b7s\n \n=\n \n[\n-\n5\n,\n \n-\n4\n,\n \n-\n2\n,\n \n-\n1\n,\n \n0\n,\n \n1\n,\n \n2\n,\n \n4\n,\n \n5\n])\n\n\ntest_transferoperator\n \n=\n \nTransferOperatorGridTest\n(\n\u03b7s\n \n=\n \n-\n3\n:\n3\n)\n\n\n\n# Note that `predictive_test` is a *mandatory* keyword.\n\n\nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \ntest_visitfreq\n)\n\n\nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \ntest_transferoperator\n)\n\n\n\n\n\nReferences\n\n\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, Jo Brendryen, and Bjarte Hannisdal.   A simple test for causal asymmetry in complex systems. In prep.\n\n\n\n\nsource", 
            "title": "Predictive asymmetry test"
        }, 
        {
            "location": "/causalitytests/PredictiveAsymmetryTest/#predictive-asymmetry", 
            "text": "#  CausalityTools.CausalityTests.PredictiveAsymmetryTest     Type .  PredictiveAsymmetryTest ( predictive_test :: CausalityTest )   The parameters for a predictive asymmetry causality test [1].   Mandatory keywords   predictive_test : An instance of a predictive causality test that explicitly    uses prediction lags (e.g.  VisitationFrequencyTest  or     TransferOperatorGridTest ).   About the prediction lags  The prediction lags in the predictive causality test must consist of  n  negative  integers and  n  positive integers that are symmetric around zero.   In other words, negative lags  must exactly match the positive lags but with opposite  sign. The zero lag can be included, but will be ignored, so it is possible to give  ranges too.  Examples  test_visitfreq   =   VisitationFrequencyTest ( \u03b7s   =   [ - 5 ,   - 4 ,   - 2 ,   - 1 ,   0 ,   1 ,   2 ,   4 ,   5 ])  test_transferoperator   =   TransferOperatorGridTest ( \u03b7s   =   - 3 : 3 )  # Note that `predictive_test` is a *mandatory* keyword.  PredictiveAsymmetryTest ( predictive_test   =   test_visitfreq )  PredictiveAsymmetryTest ( predictive_test   =   test_transferoperator )   References   Diego, David, Kristian Agas\u00f8ster Haaga, Jo Brendryen, and Bjarte Hannisdal.   A simple test for causal asymmetry in complex systems. In prep.   source", 
            "title": "Predictive asymmetry"
        }, 
        {
            "location": "/causalitytests/CausalityTest/", 
            "text": "#\n\n\nCausalityToolsBase.CausalityTest\n \n \nType\n.\n\n\nCausalityTest\n\n\nAn abstract type that is the supertype of all causality test types  in the \nCausalityTools\n ecosystem. \n\n\nThe naming convention for abstract subtypes is \nSomeMethodTest\n. Examples of the type hierarchy of abstract test types could be:\n\n\n\n\nTransferEntropyTest \n: CausalityTest\n\n\nCrossMappingTest \n: CausalityTest\n\n\n\n\nSubtypes of those abstract types are named according to the specific algorithm. Examples of complete type hierachies for specific causality test types could be:\n\n\n\n\nVisitationFrequencyTest \n: TransferEntropyTest \n: CausalityTest\n.\n\n\nTransferOperatorGridTest \n: TransferEntropyTest \n: CausalityTest\n.\n\n\nSimpleCrossMapTest \n: CrossMappingTest \n: CausalityTest\n.\n\n\nConvergentCrossMapTest \n: CrossMappingTest \n: CausalityTest\n.", 
            "title": "CausalityTest"
        }, 
        {
            "location": "/causalitytests/DistanceBasedCausalityTest/", 
            "text": "Distance based causality tests\n\n\n#\n\n\nCausalityTools.CausalityTests.DistanceBasedCausalityTest\n \n \nType\n.\n\n\nDistanceBasedCausalityTest\n\n\n\n\n\nThe supertype of all abstract and composite types representing a causality  test based on some sort of distance computation.\n\n\nsource\n\n\n\n\nTests\n\n\n\n\nCrossMappingTest\n\n\nConvergentCrossMappingTest\n\n\nJointDistanceDistributionTest\n\n\nJointDistanceDistributionTTest", 
            "title": "Distance based tests"
        }, 
        {
            "location": "/causalitytests/DistanceBasedCausalityTest/#distance-based-causality-tests", 
            "text": "#  CausalityTools.CausalityTests.DistanceBasedCausalityTest     Type .  DistanceBasedCausalityTest   The supertype of all abstract and composite types representing a causality  test based on some sort of distance computation.  source", 
            "title": "Distance based causality tests"
        }, 
        {
            "location": "/causalitytests/DistanceBasedCausalityTest/#tests", 
            "text": "CrossMappingTest  ConvergentCrossMappingTest  JointDistanceDistributionTest  JointDistanceDistributionTTest", 
            "title": "Tests"
        }, 
        {
            "location": "/causalitytests/EntropyBasedCausalityTest/", 
            "text": "Entropy based causality tests\n\n\n#\n\n\nCausalityTools.CausalityTests.EntropyBasedCausalityTest\n \n \nType\n.\n\n\nEntropyBasedCausalityTest\n\n\n\n\n\nThe supertype of all abstract and composite types representing a causality  test based on some entropy based measure.\n\n\nsource\n\n\n\n\nTests\n\n\n\n\nTransferEntropyCausalityTest\n\n\nVisitationFrequencyTest\n\n\nTransferOperatorGridTest", 
            "title": "Entropy based tests"
        }, 
        {
            "location": "/causalitytests/EntropyBasedCausalityTest/#entropy-based-causality-tests", 
            "text": "#  CausalityTools.CausalityTests.EntropyBasedCausalityTest     Type .  EntropyBasedCausalityTest   The supertype of all abstract and composite types representing a causality  test based on some entropy based measure.  source", 
            "title": "Entropy based causality tests"
        }, 
        {
            "location": "/causalitytests/EntropyBasedCausalityTest/#tests", 
            "text": "TransferEntropyCausalityTest  VisitationFrequencyTest  TransferOperatorGridTest", 
            "title": "Tests"
        }, 
        {
            "location": "/causalitytests/TransferEntropyTest/", 
            "text": "Transfer entropy\n\n\n#\n\n\nCausalityTools.CausalityTests.TransferEntropyCausalityTest\n \n \nType\n.\n\n\nTransferEntropyCausalityTest\n\n\n\n\n\nThe supertype of all abstract and composite types representing a transfer  entropy causality test.\n\n\nsource\n\n\n\n\nTests\n\n\nThe following causality tests based on transfer entropy are implemented.\n\n\n\n\nTransferOperatorGridTest\n\n\nVisitationFrequencyTest\n\n\nApproximateSimplexIntersectionTest", 
            "title": "TransferEntropyTest"
        }, 
        {
            "location": "/causalitytests/TransferEntropyTest/#transfer-entropy", 
            "text": "#  CausalityTools.CausalityTests.TransferEntropyCausalityTest     Type .  TransferEntropyCausalityTest   The supertype of all abstract and composite types representing a transfer  entropy causality test.  source", 
            "title": "Transfer entropy"
        }, 
        {
            "location": "/causalitytests/TransferEntropyTest/#tests", 
            "text": "The following causality tests based on transfer entropy are implemented.   TransferOperatorGridTest  VisitationFrequencyTest  ApproximateSimplexIntersectionTest", 
            "title": "Tests"
        }, 
        {
            "location": "/causalitytoolsbase/discretization/", 
            "text": "Discretization\n\n\n\n\nRectangular partitions\n\n\n#\n\n\nCausalityToolsBase.RectangularBinning\n \n \nType\n.\n\n\nRectangularBinning\n(\n\u03f5\n)\n\n\n\n\n\nInstructions for creating a rectangular box partition using the binning scheme \n\u03f5\n. \n\n\nFields\n\n\n\n\n\u03f5\n: A valid binning scheme.\n\n\n\n\nValid binning schemes\n\n\nThe following \n\u03f5\n are valid.\n\n\nData ranges along each axis dictated by data ranges\n\n\n\n\nRectangularBinning(\u03f5::Int)\n divides each axis into \n\u03f5\n equal-length intervals,   extending the upper bound 1/100\nth\n of a bin size to ensure all points are covered.\n\n\nRectangularBinning(\u03f5::Float64)\n divides each axis into intervals of fixed size \n\u03f5\n.\n\n\nRectangularBinning(\u03f5::Vector{Int})\n divides the i-th axis into \n\u03f5\u1d62\n equal-length   intervals, extending the upper bound 1/100\nth\n of a bin size to ensure all points are   covered.\n\n\nRectangularBinning(\u03f5::Vector{Float64})\n divides the i-th axis into intervals of size   \n\u03f5[i]\n.\n\n\n\n\nIn these cases, the rectangular partition is constructed by locating the minima along each  coordinate axis, then constructing equal-length intervals until the data maxima are covered. \n\n\nCustom ranges along each axis\n\n\nRectangular binnings may also be specified on arbitrary min-max ranges. \n\n\n\n\nRectangularBinning(\u03f5::Tuple{Vector{Tuple{Float64,Float64}},Int64})\n creates intervals   along each axis from ranges indicated by a vector of \n(min, max)\n tuples, then divides   each axis into the same integer number of equal-length intervals.\n\n\n\n\nIt's probably easier to use the following constructors\n\n\n\n\nRectangularBinning(RectangularBinning(minmaxes::Vararg{\n:AbstractRange{T}, N}; n_intervals::Int = 10))\n takes a    vector of tuples indiciating the (min, max) along each axis and \nn_intervals\n that    indicates how many equal-length intervals those ranges should be split into.\n\n\nRectangularBinning(minmaxes::Vector{\n:AbstractRange{T}}, n_intervals::Int)\n does the    same, but the arguments are provided as ranges.\n\n\n\n\nExamples\n\n\nMinimal and maximal positions of the grid determined by the data points:\n\n\n\n\nRectangularBinning(10)\n: find the minima along each coordinate axis of the points, then    split the (extended) range into \n10\n equal-length intervals.\n\n\nRectangularBinning([10, 5])\n: find the minima along each coordinate axis of the points,    then split the (extended) range along the first coordinate axis into \n10\n equal-length    intervals and the range along the second coordinate axis into \n5\n equal-length intervals.\n\n\nRectangularBinning(0.5)\n: find the minima along each coordinate axis of the points, then    split the axis ranges into equal-length intervals of size \n0.5\n\n\nRectangularBinning([0.3, 0.1])\n: find the minima along each coordinate axis of the points, then    split the range along the first coordinate axis into equal-length intervals of size    \n0.3\n and the range along the second axis into equal-length intervals of size \n0.1\n.\n\n\n\n\nExplitly specifying data ranges (not guaranteed to cover data points): \n\n\n\n\nRectangularBinning(-5:5, 2:2, n_intervals = 5)\n: split the ranges \n-5:5\n and \n2:2\n into    \nn_intervals\n equal-length intervals.\n\n\n\n\n\n\nGenerating grid points from a rectangular binning\n\n\n#\n\n\nCausalityToolsBase.generate_gridpoints\n \n \nFunction\n.\n\n\ngenerate_gridpoints\n(\naxisminima\n,\n \nstepsizes\n,\n \nn_intervals_eachaxis\n,\n \n    \ngrid\n::\nGridType\n \n=\n \nOnGrid\n())\n\n\n\n\n\nReturn a set of points forming a grid over the hyperrectangular box  spanned by\n\n\n\n\n(axisminima, axisminima .+ (n_intervals_eachaxis .* stepsizes)\n if    \ngrid = OnGrid()\n, and\n\n\n(axisminima, axisminima .+ ((n_intervals_eachaxis .+ 1) .* stepsizes)\n if    \ngrid = OnCell()\n,\n\n\n\n\nwhere the minima along each coordinate axis (\naxisminima\n), the \nstepsizes\n along each axis, and the set of intervals (\nn_intervals_per_axis\n) indicating how many  equal-length intervals each axis should be divided into.\n\n\nIf \ngrid = OnGrid()\n, then the bin origins are taken as the grid points.  If \ngrid = OnCell()\n, then one additional interval is added and the grid is shifted half a bin outside the extrema along each axis, so that the grid points lie at the center of  the grid cells.\n\n\ngenerate_gridpoints(points, binning_scheme::RectangularBinning, \n    grid::GridType = OnGrid())\n\n\n\n\nReturn a set of points forming a rectangular grid covering a  hyperrectangular box specified  by the \nbinning_scheme\n and \ngrid\n type.  Provided a suitable binning scheme is given, this grid will provide a  covering of \npoints\n. See the documentation for \nRectangularBinning\n for  more details. \n\n\nArguments\n\n\n\n\npoints\n: A vector of points or a \nDataset\n instance.\n\n\nbinning_scheme\n: A \nRectangularBinning\n instance. See docs for \nRectangularBinning\n   for more details.\n\n\ngrid\n: A \nGridType\n instance. The grid follows the same convention    as in Interpolations.jl. Valid choices are \nOnGrid()\n (uses the bin    origins as the grid points), and \nOnCell()\n, which adds an additional    interval along each axis, shifts the grid half a bin outside the    extrema along each axis and retursn the centers of the resulting    grid cells.\n\n\n\n\nExamples\n\n\nFor example,\n\n\nusing\n \nCausalityToolsBase\n,\n \nDelayEmbeddings\n\n\n\npts\n \n=\n \nDataset\n([\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n100\n])\n\n\ngenerate_gridpoints\n(\npts\n,\n \nRectangularBinning\n(\n10\n),\n \nOnGrid\n()\n\n\n\n\n\ngenerates a rectangular grid covering the range of \npts\n constructed  by subdividing each coordinate axis into 10 equal-length intervals. Next,\n\n\nusing\n \nCausalityToolsBase\n,\n \nDelayEmbeddings\n\n\n\npts\n \n=\n \nDataset\n([\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n100\n])\n\n\ngenerate_gridpoints\n(\npts\n,\n \nRectangularBinning\n(\n10\n),\n \nOnCell\n()\n\n\n\n\n\nwill do the same, but adds another interval (11 in total), shifts the  entire hypercube so that the minima and maxima along each axis lie  half a bin outside the original extrema, then returns the centers of  the grid cells.\n\n\n\n\nProperties of a rectangular binning\n\n\n#\n\n\nCausalityToolsBase.get_minmaxes\n \n \nFunction\n.\n\n\nget_minmaxes\n(\npts\n)\n \n-\n \nTuple\n{\nVector\n{\nFloat\n},\n \nVector\n{\nFloat\n}}\n\n\n\n\n\nReturn a vector of tuples containing axis-wise (minimum, maximum) values.\n\n\n#\n\n\nCausalityToolsBase.get_minima\n \n \nFunction\n.\n\n\nget_minima\n(\npts\n)\n \n-\n \nSVector\n\n\n\n\n\nReturn the minima along each axis of the dataset \npts\n.\n\n\n#\n\n\nCausalityToolsBase.get_maxima\n \n \nFunction\n.\n\n\nget_maxima\n(\npts\n)\n \n-\n \nSVector\n\n\n\n\n\nReturn the maxima along each axis of the dataset \npts\n.\n\n\n#\n\n\nCausalityToolsBase.get_edgelengths\n \n \nFunction\n.\n\n\nget_edgelengths\n(\npts\n,\n \nbinning_scheme\n::\nRectangularBinning\n)\n \n-\n \nVector\n{\nFloat\n}\n\n\n\n\n\nReturn the box edge length along each axis resulting from discretizing \npts\n on a  rectangular grid specified by \nbinning_scheme\n.\n\n\nExample\n\n\nusing\n \nDynamicalSystems\n,\n \nCausalityToolsBase\n\n\npts\n \n=\n \nDataset\n([\nrand\n(\n5\n)\n \nfor\n \ni\n \n=\n \n1\n:\n1000\n])\n\n\n\nget_edgelengths\n(\npts\n,\n \nRectangularBinning\n(\n0.6\n))\n\n\nget_edgelengths\n(\npts\n,\n \nRectangularBinning\n([\n0.5\n,\n \n0.3\n,\n \n0.3\n,\n \n0.4\n,\n \n0.4\n]))\n\n\nget_edgelengths\n(\npts\n,\n \nRectangularBinning\n(\n8\n))\n\n\nget_edgelengths\n(\npts\n,\n \nRectangularBinning\n([\n10\n,\n \n8\n,\n \n5\n,\n \n4\n,\n \n22\n]))\n\n\n\n\n\n#\n\n\nCausalityToolsBase.get_minima_and_edgelengths\n \n \nFunction\n.\n\n\nget_minima_and_edgelengths\n(\npoints\n,\n \n    \nbinning_scheme\n::\nRectangularBinning\n)\n \n-\n \n(\nVector\n{\nFloat\n},\n \nVector\n{\nFloat\n})\n\n\n\n\n\nFind the minima along each axis of the embedding, and computes appropriate edge lengths given a rectangular \nbinning_scheme\n, which provide instructions on how to  grid the space. Assumes the input is a vector of points.\n\n\nSee documentation for \nRectangularBinning\n for details on the  binning scheme.\n\n\nExample\n\n\nusing\n \nDynamicalSystems\n,\n \nCausalityToolsBase\n\n\npts\n \n=\n \nDataset\n([\nrand\n(\n4\n)\n \nfor\n \ni\n \n=\n \n1\n:\n1000\n])\n\n\n\nget_minima_and_edgelengths\n(\npts\n,\n \nRectangularBinning\n(\n0.6\n))\n\n\nget_minima_and_edgelengths\n(\npts\n,\n \nRectangularBinning\n([\n0.5\n,\n \n0.3\n,\n \n0.4\n,\n \n0.4\n]))\n\n\nget_minima_and_edgelengths\n(\npts\n,\n \nRectangularBinning\n(\n10\n))\n\n\nget_minima_and_edgelengths\n(\npts\n,\n \nRectangularBinning\n([\n10\n,\n \n8\n,\n \n5\n,\n \n4\n]))\n\n\n\n\n\n\n\nTriangulated partitions\n\n\n#\n\n\nCausalityToolsBase.TriangulationBinning\n \n \nType\n.\n\n\nTriangulationBinning\n\n\n\n\n\nA type indicating that a triangulation partition in which a set of points is divided  into simplices should be used.", 
            "title": "Discretization"
        }, 
        {
            "location": "/causalitytoolsbase/discretization/#discretization", 
            "text": "", 
            "title": "Discretization"
        }, 
        {
            "location": "/causalitytoolsbase/discretization/#rectangular-partitions", 
            "text": "#  CausalityToolsBase.RectangularBinning     Type .  RectangularBinning ( \u03f5 )   Instructions for creating a rectangular box partition using the binning scheme  \u03f5 .   Fields   \u03f5 : A valid binning scheme.   Valid binning schemes  The following  \u03f5  are valid.  Data ranges along each axis dictated by data ranges   RectangularBinning(\u03f5::Int)  divides each axis into  \u03f5  equal-length intervals,   extending the upper bound 1/100 th  of a bin size to ensure all points are covered.  RectangularBinning(\u03f5::Float64)  divides each axis into intervals of fixed size  \u03f5 .  RectangularBinning(\u03f5::Vector{Int})  divides the i-th axis into  \u03f5\u1d62  equal-length   intervals, extending the upper bound 1/100 th  of a bin size to ensure all points are   covered.  RectangularBinning(\u03f5::Vector{Float64})  divides the i-th axis into intervals of size    \u03f5[i] .   In these cases, the rectangular partition is constructed by locating the minima along each  coordinate axis, then constructing equal-length intervals until the data maxima are covered.   Custom ranges along each axis  Rectangular binnings may also be specified on arbitrary min-max ranges.    RectangularBinning(\u03f5::Tuple{Vector{Tuple{Float64,Float64}},Int64})  creates intervals   along each axis from ranges indicated by a vector of  (min, max)  tuples, then divides   each axis into the same integer number of equal-length intervals.   It's probably easier to use the following constructors   RectangularBinning(RectangularBinning(minmaxes::Vararg{ :AbstractRange{T}, N}; n_intervals::Int = 10))  takes a    vector of tuples indiciating the (min, max) along each axis and  n_intervals  that    indicates how many equal-length intervals those ranges should be split into.  RectangularBinning(minmaxes::Vector{ :AbstractRange{T}}, n_intervals::Int)  does the    same, but the arguments are provided as ranges.   Examples  Minimal and maximal positions of the grid determined by the data points:   RectangularBinning(10) : find the minima along each coordinate axis of the points, then    split the (extended) range into  10  equal-length intervals.  RectangularBinning([10, 5]) : find the minima along each coordinate axis of the points,    then split the (extended) range along the first coordinate axis into  10  equal-length    intervals and the range along the second coordinate axis into  5  equal-length intervals.  RectangularBinning(0.5) : find the minima along each coordinate axis of the points, then    split the axis ranges into equal-length intervals of size  0.5  RectangularBinning([0.3, 0.1]) : find the minima along each coordinate axis of the points, then    split the range along the first coordinate axis into equal-length intervals of size     0.3  and the range along the second axis into equal-length intervals of size  0.1 .   Explitly specifying data ranges (not guaranteed to cover data points):    RectangularBinning(-5:5, 2:2, n_intervals = 5) : split the ranges  -5:5  and  2:2  into     n_intervals  equal-length intervals.", 
            "title": "Rectangular partitions"
        }, 
        {
            "location": "/causalitytoolsbase/discretization/#generating-grid-points-from-a-rectangular-binning", 
            "text": "#  CausalityToolsBase.generate_gridpoints     Function .  generate_gridpoints ( axisminima ,   stepsizes ,   n_intervals_eachaxis ,  \n     grid :: GridType   =   OnGrid ())   Return a set of points forming a grid over the hyperrectangular box  spanned by   (axisminima, axisminima .+ (n_intervals_eachaxis .* stepsizes)  if     grid = OnGrid() , and  (axisminima, axisminima .+ ((n_intervals_eachaxis .+ 1) .* stepsizes)  if     grid = OnCell() ,   where the minima along each coordinate axis ( axisminima ), the  stepsizes  along each axis, and the set of intervals ( n_intervals_per_axis ) indicating how many  equal-length intervals each axis should be divided into.  If  grid = OnGrid() , then the bin origins are taken as the grid points.  If  grid = OnCell() , then one additional interval is added and the grid is shifted half a bin outside the extrema along each axis, so that the grid points lie at the center of  the grid cells.  generate_gridpoints(points, binning_scheme::RectangularBinning, \n    grid::GridType = OnGrid())  Return a set of points forming a rectangular grid covering a  hyperrectangular box specified  by the  binning_scheme  and  grid  type.  Provided a suitable binning scheme is given, this grid will provide a  covering of  points . See the documentation for  RectangularBinning  for  more details.   Arguments   points : A vector of points or a  Dataset  instance.  binning_scheme : A  RectangularBinning  instance. See docs for  RectangularBinning    for more details.  grid : A  GridType  instance. The grid follows the same convention    as in Interpolations.jl. Valid choices are  OnGrid()  (uses the bin    origins as the grid points), and  OnCell() , which adds an additional    interval along each axis, shifts the grid half a bin outside the    extrema along each axis and retursn the centers of the resulting    grid cells.   Examples  For example,  using   CausalityToolsBase ,   DelayEmbeddings  pts   =   Dataset ([ rand ( 3 )   for   i   =   1 : 100 ])  generate_gridpoints ( pts ,   RectangularBinning ( 10 ),   OnGrid ()   generates a rectangular grid covering the range of  pts  constructed  by subdividing each coordinate axis into 10 equal-length intervals. Next,  using   CausalityToolsBase ,   DelayEmbeddings  pts   =   Dataset ([ rand ( 3 )   for   i   =   1 : 100 ])  generate_gridpoints ( pts ,   RectangularBinning ( 10 ),   OnCell ()   will do the same, but adds another interval (11 in total), shifts the  entire hypercube so that the minima and maxima along each axis lie  half a bin outside the original extrema, then returns the centers of  the grid cells.", 
            "title": "Generating grid points from a rectangular binning"
        }, 
        {
            "location": "/causalitytoolsbase/discretization/#properties-of-a-rectangular-binning", 
            "text": "#  CausalityToolsBase.get_minmaxes     Function .  get_minmaxes ( pts )   -   Tuple { Vector { Float },   Vector { Float }}   Return a vector of tuples containing axis-wise (minimum, maximum) values.  #  CausalityToolsBase.get_minima     Function .  get_minima ( pts )   -   SVector   Return the minima along each axis of the dataset  pts .  #  CausalityToolsBase.get_maxima     Function .  get_maxima ( pts )   -   SVector   Return the maxima along each axis of the dataset  pts .  #  CausalityToolsBase.get_edgelengths     Function .  get_edgelengths ( pts ,   binning_scheme :: RectangularBinning )   -   Vector { Float }   Return the box edge length along each axis resulting from discretizing  pts  on a  rectangular grid specified by  binning_scheme .  Example  using   DynamicalSystems ,   CausalityToolsBase  pts   =   Dataset ([ rand ( 5 )   for   i   =   1 : 1000 ])  get_edgelengths ( pts ,   RectangularBinning ( 0.6 ))  get_edgelengths ( pts ,   RectangularBinning ([ 0.5 ,   0.3 ,   0.3 ,   0.4 ,   0.4 ]))  get_edgelengths ( pts ,   RectangularBinning ( 8 ))  get_edgelengths ( pts ,   RectangularBinning ([ 10 ,   8 ,   5 ,   4 ,   22 ]))   #  CausalityToolsBase.get_minima_and_edgelengths     Function .  get_minima_and_edgelengths ( points ,  \n     binning_scheme :: RectangularBinning )   -   ( Vector { Float },   Vector { Float })   Find the minima along each axis of the embedding, and computes appropriate edge lengths given a rectangular  binning_scheme , which provide instructions on how to  grid the space. Assumes the input is a vector of points.  See documentation for  RectangularBinning  for details on the  binning scheme.  Example  using   DynamicalSystems ,   CausalityToolsBase  pts   =   Dataset ([ rand ( 4 )   for   i   =   1 : 1000 ])  get_minima_and_edgelengths ( pts ,   RectangularBinning ( 0.6 ))  get_minima_and_edgelengths ( pts ,   RectangularBinning ([ 0.5 ,   0.3 ,   0.4 ,   0.4 ]))  get_minima_and_edgelengths ( pts ,   RectangularBinning ( 10 ))  get_minima_and_edgelengths ( pts ,   RectangularBinning ([ 10 ,   8 ,   5 ,   4 ]))", 
            "title": "Properties of a rectangular binning"
        }, 
        {
            "location": "/causalitytoolsbase/discretization/#triangulated-partitions", 
            "text": "#  CausalityToolsBase.TriangulationBinning     Type .  TriangulationBinning   A type indicating that a triangulation partition in which a set of points is divided  into simplices should be used.", 
            "title": "Triangulated partitions"
        }, 
        {
            "location": "/causalitytoolsbase/delay_reconstructions/", 
            "text": "Custom delay reconstruction\n\n\nTo create delay embeddings from sequential data, use \ncustomembed\n. This function returns allows for more flexibility than \nembed\n in DynamicalSystems.jl\n1\n, but also returns a \nDataset\n, so these embedding methods may be used interchangeably.\n\n\n\n\nDocumentation\n\n\n#\n\n\nCausalityToolsBase.customembed\n \n \nFunction\n.\n\n\ncustomembed\n(\npts\n,\n \npositions\n::\nPositions\n,\n \nlags\n::\nLags\n)\n\n\n\n\n\nDo custom state space reconstructions with \ncustomembed(pts, positions::Positions, lags::Lags)\n.  This function acts almost as \nDynamicalSystems.reconstruct\n, but allows for more flexibility in  the ordering of dynamical variables and allows for negative lags. The \npositions\n variable  indicates which dynamical variables are mapped to which variables in the final  reconstruction, while \nlags\n indicates the lags for each of the embedding variables. \n\n\nNote: \ncustomembed\n expects an array of \nstate vectors\n, i.e. \npts[k]\n must refer to the  \nk\nth point of the dataset, not the \nk\nth dynamical variable/column.*. To embed a vector of  time series, load \nDynamicalSystems\n and wrap the time series in a \nDataset\n first, e.g. if  \nx = rand(100); y = rand(100)\n are two time series, then  \ncustomembed(Dataset(x, y), Positions(1, 2, 2), Lags(0, 0, 1)\n will create the embedding with  state vectors \n(x(t), y(t), y(t + 1))\n.\n\n\nPre-embedded points may be wrapped in a \nCustomReconstruction\n instance by simply calling  \ncustomembed(preembedded_pts)\n without any position/lag instructions.\n\n\nExamples\n\n\nExample 1\n\n\ncustomembed\n([\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n50\n],\n \nPositions\n(\n1\n,\n \n2\n,\n \n1\n,\n \n3\n),\n \nLags\n(\n0\n,\n \n0\n,\n \n1\n,\n \n-\n2\n)\n\n\n\n\n\ngives a 4-dimensional embedding with state vectors \n(x1(t), x2(t), x1(t + 1), x3(t - 2))\n. \n\n\nExample 2\n\n\nSay we want to construct an appropriate delay reconstruction for transfer entropy (TE) analysis\n\n\n\n\n\nE = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\}``),\n\n\n\n\nE = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\}``),\n\n\n\n\n\nso that we're computing the following TE\n\n\n\n\n\nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.\n\n\n\n\nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.\n\n\n\n\n\nWe'll use a prediction lag \n\\eta = 2\n\\eta = 2\n and use first minima of the lagged mutual  information function for the embedding delay \n\\tau\n\\tau\n.\n\n\nusing\n \nCausalityToolsBase\n,\n \nDynamicalSystems\n\n\n\nx\n,\n \ny\n \n=\n \nrand\n(\n100\n),\n \nrand\n(\n100\n)\n\n\nD\n \n=\n \nDataset\n(\nx\n,\n \ny\n)\n\n\nembedlag\n \n=\n \noptimal_delay\n(\ny\n)\n\n\nCustomReconstruction\n(\nD\n,\n \nPositions\n(\n2\n,\n \n2\n,\n \n2\n,\n \n1\n),\n \nLags\n(\n2\n,\n \n0\n,\n \nembedlag\n,\n \n0\n))\n\n\n\n\n\n#\n\n\nCausalityToolsBase.Positions\n \n \nType\n.\n\n\nPositions\n\n\n\n\n\nType specifying the positions the different dynamical variables appear in when  constructing a custom state space reconstruction. Used in combination with \nLags\n to specify how a \nCustomReconstruction\n should be constructed. Each  of the positions must refer to a dynamical variable (column) actually present in the  dataset.\n\n\nExamples\n\n\n\n\nPositions(1, 2, 1, 5)\n indicates a 4-dimensional state space reconstruction where the 1\nst\n    coordinate axis of the reconstruction should be formed from the first variable/column of    the input data, the 2\nnd\n coordinate axis of the reconstruction should be formed from the    2\nnd\n variable/column of the input data, the 3\nrd\n coordinate axis of the reconstruction should    be formed from the 1\nst\n variable/column of the input data, the 4\nth\n coordinate axis of the    reconstruction should be formed from the 5\nth\n variable/column of the input data.\n\n\nPositions(-1, 2)\n indicates a 2-dimensional reconstruction, but will not work, because    each position must refer to the index of a dynamical variable (column) of a dataset    (indexed from 1 and up, so negative values will not work).\n\n\n\n\n#\n\n\nCausalityToolsBase.Lags\n \n \nType\n.\n\n\nLags\n\n\n\n\n\nWrapper type for lags used when performing custom state space reconstructions. Used in combination with \nPositions\n to specify how a \nCustomReconstruction\n should be constructed.\n\n\nExamples\n\n\n\n\nLags(2, 0, -3, 1)\n indicates a 4-dimensional state space reconstruction where    the first variable has a positive lag of 2,    the second variable is not lagged,    the third variable has a lag of -3,    and the fourth variable has a positive lag of 1.\n\n\nLags(0, 0)\n indicates a 2-dimensional state space reconstruction where both    variables are not lagged.\n\n\n\n\n\n\n\n\n\n\n\n\nDatseris, George. \"DynamicalSystems. jl: A Julia software library for chaos and nonlinear dynamics.\" J. Open Source Software 3.23 (2018): 598.", 
            "title": "Delay reconstruction"
        }, 
        {
            "location": "/causalitytoolsbase/delay_reconstructions/#custom-delay-reconstruction", 
            "text": "To create delay embeddings from sequential data, use  customembed . This function returns allows for more flexibility than  embed  in DynamicalSystems.jl 1 , but also returns a  Dataset , so these embedding methods may be used interchangeably.", 
            "title": "Custom delay reconstruction"
        }, 
        {
            "location": "/causalitytoolsbase/delay_reconstructions/#documentation", 
            "text": "#  CausalityToolsBase.customembed     Function .  customembed ( pts ,   positions :: Positions ,   lags :: Lags )   Do custom state space reconstructions with  customembed(pts, positions::Positions, lags::Lags) .  This function acts almost as  DynamicalSystems.reconstruct , but allows for more flexibility in  the ordering of dynamical variables and allows for negative lags. The  positions  variable  indicates which dynamical variables are mapped to which variables in the final  reconstruction, while  lags  indicates the lags for each of the embedding variables.   Note:  customembed  expects an array of  state vectors , i.e.  pts[k]  must refer to the   k th point of the dataset, not the  k th dynamical variable/column.*. To embed a vector of  time series, load  DynamicalSystems  and wrap the time series in a  Dataset  first, e.g. if   x = rand(100); y = rand(100)  are two time series, then   customembed(Dataset(x, y), Positions(1, 2, 2), Lags(0, 0, 1)  will create the embedding with  state vectors  (x(t), y(t), y(t + 1)) .  Pre-embedded points may be wrapped in a  CustomReconstruction  instance by simply calling   customembed(preembedded_pts)  without any position/lag instructions.  Examples  Example 1  customembed ([ rand ( 3 )   for   i   =   1 : 50 ],   Positions ( 1 ,   2 ,   1 ,   3 ),   Lags ( 0 ,   0 ,   1 ,   - 2 )   gives a 4-dimensional embedding with state vectors  (x1(t), x2(t), x1(t + 1), x3(t - 2)) .   Example 2  Say we want to construct an appropriate delay reconstruction for transfer entropy (TE) analysis   \nE = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\}``),  \nE = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\}``),   so that we're computing the following TE   \nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.  \nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.   We'll use a prediction lag  \\eta = 2 \\eta = 2  and use first minima of the lagged mutual  information function for the embedding delay  \\tau \\tau .  using   CausalityToolsBase ,   DynamicalSystems  x ,   y   =   rand ( 100 ),   rand ( 100 )  D   =   Dataset ( x ,   y )  embedlag   =   optimal_delay ( y )  CustomReconstruction ( D ,   Positions ( 2 ,   2 ,   2 ,   1 ),   Lags ( 2 ,   0 ,   embedlag ,   0 ))   #  CausalityToolsBase.Positions     Type .  Positions   Type specifying the positions the different dynamical variables appear in when  constructing a custom state space reconstruction. Used in combination with  Lags  to specify how a  CustomReconstruction  should be constructed. Each  of the positions must refer to a dynamical variable (column) actually present in the  dataset.  Examples   Positions(1, 2, 1, 5)  indicates a 4-dimensional state space reconstruction where the 1 st     coordinate axis of the reconstruction should be formed from the first variable/column of    the input data, the 2 nd  coordinate axis of the reconstruction should be formed from the    2 nd  variable/column of the input data, the 3 rd  coordinate axis of the reconstruction should    be formed from the 1 st  variable/column of the input data, the 4 th  coordinate axis of the    reconstruction should be formed from the 5 th  variable/column of the input data.  Positions(-1, 2)  indicates a 2-dimensional reconstruction, but will not work, because    each position must refer to the index of a dynamical variable (column) of a dataset    (indexed from 1 and up, so negative values will not work).   #  CausalityToolsBase.Lags     Type .  Lags   Wrapper type for lags used when performing custom state space reconstructions. Used in combination with  Positions  to specify how a  CustomReconstruction  should be constructed.  Examples   Lags(2, 0, -3, 1)  indicates a 4-dimensional state space reconstruction where    the first variable has a positive lag of 2,    the second variable is not lagged,    the third variable has a lag of -3,    and the fourth variable has a positive lag of 1.  Lags(0, 0)  indicates a 2-dimensional state space reconstruction where both    variables are not lagged.       Datseris, George. \"DynamicalSystems. jl: A Julia software library for chaos and nonlinear dynamics.\" J. Open Source Software 3.23 (2018): 598.", 
            "title": "Documentation"
        }, 
        {
            "location": "/perronfrobenius/invariantmeasure/", 
            "text": "Invariant measure estimation\n\n\n\n\nRectangular partitions\n\n\n#\n\n\nPerronFrobenius.InvariantMeasures.invariantmeasure\n \n \nMethod\n.\n\n\ninvariantmeasure\n(\ndata\n,\n \nbinning_scheme\n::\nRectangularBinning\n;\n \nkwargs\n...\n)\n\n\n\n\n\nPartition \ndata\n according to the the given \nbinning_scheme\n and compute the invariant measure over the partition elements. \n\n\nExample\n\n\nAssume we have enough points that a rectangular partition yields a good estimate of the  invariant measure. Then the measure over the partition can be computed as follows.\n\n\npts\n \n=\n \n[\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n2000\n]\n\n\n\n# Use rectangular boxes constructed by subdividing each coordinate \n\n\n# axis into 10 subintervals of equal length.\n\n\nbinning_scheme\n \n=\n \nRectangularBinning\n(\n10\n)\n\n\ninvariantmeasure\n(\npts\n,\n \nbinning_scheme\n)\n\n\n\n\n\n\n\nTriangulated partitions\n\n\nSay we have a 3D delay reconstruction that we have partioned into simplices.\n\n\n\n\nThere are two methods that approximates invariant measures over that partition.\n\n\n\n\nExact simplex intersections\n\n\n#\n\n\nPerronFrobenius.InvariantMeasures.invariantmeasure\n \n \nMethod\n.\n\n\ninvariantmeasure\n(\npts\n,\n \n\u03f5\n::\nTriangulationBinning\n,\n \n    \nsimplex_intersection_type\n::\nExactIntersection\n)\n \n-\n \nTriangulationExactInvariantMeasure\n\n\n\n\n\nEstimate the invariant measure over the state space defined by \npts\n using a triangulation  of the phase space as the partition, using exact simplex intersections to compute transition probabilities between the states (simplices).\n\n\nExample\n\n\nAssume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.\n\n\npts\n \n=\n \n[\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n30\n]\n\n\ninvariantmeasure\n(\npts\n,\n \nTriangulationBinning\n(),\n \nExactIntersection\n())\n\n\n\n\n\n\n\nApproximate simplex intersections\n\n\n#\n\n\nPerronFrobenius.InvariantMeasures.invariantmeasure\n \n \nMethod\n.\n\n\ninvariantmeasure\n(\npts\n,\n \n\u03f5\n::\nTriangulationBinning\n,\n \n    \nsimplex_intersection_type\n::\nApproximateIntersection\n;\n\n    \nn\n::\nInt\n \n=\n \n100\n,\n \nsample_randomly\n::\nBool\n \n=\n \nfalse\n)\n \n-\n \nTriangulationApproxInvariantMeasure\n\n\n\n\n\nEstimate the invariant measure over the state space defined by \npts\n using a triangulation  of the phase space as the partition, using exact simplex intersections to compute transition probabilities between the states (simplices).\n\n\nExample\n\n\nAssume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.\n\n\npts\n \n=\n \n[\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n30\n]\n\n\ninvariantmeasure\n(\npts\n,\n \nTriangulationBinning\n(),\n \nApproximateIntersection\n())", 
            "title": "Invariant measure"
        }, 
        {
            "location": "/perronfrobenius/invariantmeasure/#invariant-measure-estimation", 
            "text": "", 
            "title": "Invariant measure estimation"
        }, 
        {
            "location": "/perronfrobenius/invariantmeasure/#rectangular-partitions", 
            "text": "#  PerronFrobenius.InvariantMeasures.invariantmeasure     Method .  invariantmeasure ( data ,   binning_scheme :: RectangularBinning ;   kwargs ... )   Partition  data  according to the the given  binning_scheme  and compute the invariant measure over the partition elements.   Example  Assume we have enough points that a rectangular partition yields a good estimate of the  invariant measure. Then the measure over the partition can be computed as follows.  pts   =   [ rand ( 3 )   for   i   =   1 : 2000 ]  # Use rectangular boxes constructed by subdividing each coordinate   # axis into 10 subintervals of equal length.  binning_scheme   =   RectangularBinning ( 10 )  invariantmeasure ( pts ,   binning_scheme )", 
            "title": "Rectangular partitions"
        }, 
        {
            "location": "/perronfrobenius/invariantmeasure/#triangulated-partitions", 
            "text": "Say we have a 3D delay reconstruction that we have partioned into simplices.   There are two methods that approximates invariant measures over that partition.", 
            "title": "Triangulated partitions"
        }, 
        {
            "location": "/perronfrobenius/invariantmeasure/#exact-simplex-intersections", 
            "text": "#  PerronFrobenius.InvariantMeasures.invariantmeasure     Method .  invariantmeasure ( pts ,   \u03f5 :: TriangulationBinning ,  \n     simplex_intersection_type :: ExactIntersection )   -   TriangulationExactInvariantMeasure   Estimate the invariant measure over the state space defined by  pts  using a triangulation  of the phase space as the partition, using exact simplex intersections to compute transition probabilities between the states (simplices).  Example  Assume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.  pts   =   [ rand ( 3 )   for   i   =   1 : 30 ]  invariantmeasure ( pts ,   TriangulationBinning (),   ExactIntersection ())", 
            "title": "Exact simplex intersections"
        }, 
        {
            "location": "/perronfrobenius/invariantmeasure/#approximate-simplex-intersections", 
            "text": "#  PerronFrobenius.InvariantMeasures.invariantmeasure     Method .  invariantmeasure ( pts ,   \u03f5 :: TriangulationBinning ,  \n     simplex_intersection_type :: ApproximateIntersection ; \n     n :: Int   =   100 ,   sample_randomly :: Bool   =   false )   -   TriangulationApproxInvariantMeasure   Estimate the invariant measure over the state space defined by  pts  using a triangulation  of the phase space as the partition, using exact simplex intersections to compute transition probabilities between the states (simplices).  Example  Assume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.  pts   =   [ rand ( 3 )   for   i   =   1 : 30 ]  invariantmeasure ( pts ,   TriangulationBinning (),   ApproximateIntersection ())", 
            "title": "Approximate simplex intersections"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/", 
            "text": "Transfer operator estimation\n\n\n\n\nRectangular partitions\n\n\n#\n\n\nPerronFrobenius.TransferOperators.Estimators.transferoperator\n \n \nMethod\n.\n\n\ntransferoperator\n(\npoints\n,\n \nbinning_scheme\n::\nRectangularBinning\n;\n \nkwargs\n...\n)\n\n\n\n\n\nDiscretize \npoints\n using the provided \nbinning_scheme\n and compute the transfer operator  over the partition elements. \n\n\nExample\n\n\nAssume we have enough points that a rectangular partition yields a good estimate of the  invariant measure. Then the transfer operator over the partition can be computed as follows.\n\n\npts\n \n=\n \n[\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n2000\n]\n\n\n\n# Use rectangular boxes constructed by subdividing each coordinate \n\n\n# axis into 10 subintervals of equal length.\n\n\ntransferoperator\n(\npts\n,\n \nRectangularBinning\n(\n10\n))\n\n\n\n\n\n\n\nTriangulated partitions\n\n\nSay we have a 3D delay reconstruction that we have partioned into simplices.\n\n\n\n\nThere are two methods that approximate the transfer operator over such a partition.\n\n\n\n\nExact simplex intersections\n\n\n#\n\n\nPerronFrobenius.TransferOperators.Estimators.transferoperator\n \n \nMethod\n.\n\n\ntransferoperator\n(\npts\n,\n \n\u03f5\n::\nTriangulationBinning\n,\n \n    \nsimplex_intersection_type\n::\nExactIntersection\n)\n \n-\n \nTransferOperatorTriangulationExact\n\n\n\n\n\nEstimate the invariant measure over the state space defined by \npts\n using a triangulation  of the phase space as the partition, using exact simplex intersections to compute transition probabilities between the states (simplices).\n\n\nExample\n\n\nAssume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.\n\n\npts\n \n=\n \n[\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n30\n]\n\n\ntransferoperator\n(\npts\n,\n \nTriangulationBinning\n(),\n \nExactIntersection\n())\n\n\n\n\n\n\n\nVisualising a transfer matrix computed using exact simplex intersections\n\n\nSay the delay embedding for which we constructed a triangulated partition above was constructed  from some time series. The transfer operator for the partition above is just a square matrix giving the probabilities of jumping between states of the partition (regardless of the dimension of the attractor over which it is approximated), so we can visualise it as a heatmap. Here, we show the probabilities of  jumping between states \nS_i\nS_i\n to all other states \nS_j\nS_j\n. \n\n\nEach state is simply the volume spanned by a simplex of the partition, and the transition probabilities are computed as the overlap of volumes when projecting the simplices one step ahead in time according to the time ordering of the original points of the delay reconstruction.\n\n\n\n\n\n\nApproximate simplex intersections\n\n\n#\n\n\nPerronFrobenius.TransferOperators.Estimators.transferoperator\n \n \nMethod\n.\n\n\ntransferoperator\n(\npts\n,\n \n\u03f5\n::\nTriangulationBinning\n,\n \n    \nsimplex_intersection_type\n::\nApproximateIntersection\n;\n \n    \nn\n::\nInt\n \n=\n \n200\n,\n \nsample_randomly\n::\nBool\n \n=\n \nfalse\n)\n \n-\n \nTransferOperatorTriangulationApprox\n\n\n\n\n\nEstimate the invariant measure over the state space defined by \npts\n using a triangulation  of the phase space as the partition, using approximate simplex intersections to compute transition probabilities between the states (simplices). \n\n\nn\n is the number of points that each simplex is  sampled with, and \nsample_randomly\n indicates whether points used be sampled randomly  within each simpled (\nsample_randomly = true\n) or by a regular simplex splitting routine  (\nsample_randomly = false\n, which is default).\n\n\nExample\n\n\nAssume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.\n\n\npts\n \n=\n \n[\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n30\n]\n\n\ntransferoperator\n(\npts\n,\n \nTriangulationBinning\n(),\n \nApproximateIntersection\n())\n\n\n\n\n\n\n\nVisualising a transfer matrix computed using approximate simplex intersections", 
            "title": "Transfer operator"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/#transfer-operator-estimation", 
            "text": "", 
            "title": "Transfer operator estimation"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/#rectangular-partitions", 
            "text": "#  PerronFrobenius.TransferOperators.Estimators.transferoperator     Method .  transferoperator ( points ,   binning_scheme :: RectangularBinning ;   kwargs ... )   Discretize  points  using the provided  binning_scheme  and compute the transfer operator  over the partition elements.   Example  Assume we have enough points that a rectangular partition yields a good estimate of the  invariant measure. Then the transfer operator over the partition can be computed as follows.  pts   =   [ rand ( 3 )   for   i   =   1 : 2000 ]  # Use rectangular boxes constructed by subdividing each coordinate   # axis into 10 subintervals of equal length.  transferoperator ( pts ,   RectangularBinning ( 10 ))", 
            "title": "Rectangular partitions"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/#triangulated-partitions", 
            "text": "Say we have a 3D delay reconstruction that we have partioned into simplices.   There are two methods that approximate the transfer operator over such a partition.", 
            "title": "Triangulated partitions"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/#exact-simplex-intersections", 
            "text": "#  PerronFrobenius.TransferOperators.Estimators.transferoperator     Method .  transferoperator ( pts ,   \u03f5 :: TriangulationBinning ,  \n     simplex_intersection_type :: ExactIntersection )   -   TransferOperatorTriangulationExact   Estimate the invariant measure over the state space defined by  pts  using a triangulation  of the phase space as the partition, using exact simplex intersections to compute transition probabilities between the states (simplices).  Example  Assume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.  pts   =   [ rand ( 3 )   for   i   =   1 : 30 ]  transferoperator ( pts ,   TriangulationBinning (),   ExactIntersection ())", 
            "title": "Exact simplex intersections"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/#visualising-a-transfer-matrix-computed-using-exact-simplex-intersections", 
            "text": "Say the delay embedding for which we constructed a triangulated partition above was constructed  from some time series. The transfer operator for the partition above is just a square matrix giving the probabilities of jumping between states of the partition (regardless of the dimension of the attractor over which it is approximated), so we can visualise it as a heatmap. Here, we show the probabilities of  jumping between states  S_i S_i  to all other states  S_j S_j .   Each state is simply the volume spanned by a simplex of the partition, and the transition probabilities are computed as the overlap of volumes when projecting the simplices one step ahead in time according to the time ordering of the original points of the delay reconstruction.", 
            "title": "Visualising a transfer matrix computed using exact simplex intersections"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/#approximate-simplex-intersections", 
            "text": "#  PerronFrobenius.TransferOperators.Estimators.transferoperator     Method .  transferoperator ( pts ,   \u03f5 :: TriangulationBinning ,  \n     simplex_intersection_type :: ApproximateIntersection ;  \n     n :: Int   =   200 ,   sample_randomly :: Bool   =   false )   -   TransferOperatorTriangulationApprox   Estimate the invariant measure over the state space defined by  pts  using a triangulation  of the phase space as the partition, using approximate simplex intersections to compute transition probabilities between the states (simplices).   n  is the number of points that each simplex is  sampled with, and  sample_randomly  indicates whether points used be sampled randomly  within each simpled ( sample_randomly = true ) or by a regular simplex splitting routine  ( sample_randomly = false , which is default).  Example  Assume we have sufficiently few points that a triangulation approach involving simplex  intersections is computationally feasible to compute the invariant measure. Then  a transfer operator can be computed as follows.  pts   =   [ rand ( 3 )   for   i   =   1 : 30 ]  transferoperator ( pts ,   TriangulationBinning (),   ApproximateIntersection ())", 
            "title": "Approximate simplex intersections"
        }, 
        {
            "location": "/perronfrobenius/transferoperator/#visualising-a-transfer-matrix-computed-using-approximate-simplex-intersections", 
            "text": "", 
            "title": "Visualising a transfer matrix computed using approximate simplex intersections"
        }, 
        {
            "location": "/transferentropy/convenience_functions_te/", 
            "text": "Convenience functions for TE estimation\n\n\n\n\nTE estimation between two data series\n\n\n#\n\n\nCausalityTools.te_reg\n \n \nFunction\n.\n\n\nte_reg\n(\nsource\n::\nAbstractArray\n{\n:\nReal\n,\n \n1\n},\n \n    \nresponse\n::\nAbstractArray\n{\n:\nReal\n,\n \n1\n},\n \n    \nk\n::\nInt\n,\n \nl\n::\nInt\n,\n \nm\n::\nInt\n;\n \n    \n\u03b7\n \n=\n \n1\n,\n \n\u03c4\n \n=\n \n1\n,\n \n    \nestimator\n \n=\n \nVisitationFrequency\n(),\n \n    \nn_subdivs\n \n=\n \n1\n,\n\n    \nb\n \n=\n \n2\n)\n\n\n\n\n\nTE estimation with default discretization scheme(s)\n\n\nCalculate transfer entropy from \nsource\n to \nresponse\n using the provided  \nestimator\n on a rectangular discretization of a \nk + l + m\n-dimensional  delay embedding of the input data, using an embedding delay of \n\u03c4\n across  all embedding components. \n\u03b7\n is the prediction lag. \n\n\nArguments\n\n\n\n\nsource\n: The source data series.\n\n\ntarget\n: The target data series.\n\n\nk\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\n\n\nKeyword arguments\n\n\n\n\n\u03c4\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\n\u03b7\n: The prediction lag. Default is \n\u03b7 = 1\n.\n\n\nestimator\n: The transfer entropy estimator to use. The default    is \nVisitationFrequency()\n.\n\n\nn_subdivs\n: The number of different partitions of varying coarseness   to compute TE over, as described below. Default is \nn_subdivs = 2\n.   (this way, TE is computed over two separate partitions). Unless \nn_subdivs = 0\n,   make sure that \nn_subdivs\n is the same across analyses if they    are to be compared. T TE\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\n\n\nMore about the embedding\n\n\nTo compute transfer entropy, we need an appropriate delay embedding  of \nsource\n (\nS\nS\n) and \ntarget\n (\nT\nT\n). For convenience, define \n\n\n\n\n\n\\begin{align}\nT_f^{(k)} \n= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} \n= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} \n= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}\n\n\n\n\n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}\n\n\n\n\n\nwhere \nT_f\nT_f\n denotes the \nk\n-dimensional set of vectors furnishing the future states of \nT\nT\n, \nT_{pp}\nT_{pp}\n denotes the \nl\n-dimensional set of vectors furnishing the past and present states of \nT\nT\n,  and \nS_{pp}\nS_{pp}\n denotes the \nm\n-dimensional set of vectors furnishing the past and present of \nS\nS\n.  \n\\eta\n\\eta\n is the prediction lag. This convenience function uses \n\\tau_1\n\\tau_1\n = \n\u03c4\n,  \n\\tau_2\n\\tau_2\n = \n2*\u03c4\n, \n\\tau_3\n\\tau_3\n = \n3*\u03c4\n, and so on.\n\n\nCombined, we get the generalised embedding \n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)})\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)})\n,  which is discretized as described below.\n\n\nMore about discretization\n\n\nTo compute TE, we coarse-grain the \nk+l+m\n-dimensional generalised embedding  \n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)})\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)})\n into hyperrectangular boxes. The magnitude of the TE may be biased by the particular choice of binning scheme, so we compute TE across a number of different box sizes, determined as follows.\n\n\nLet \nL\nL\n be the number of observations in \nS\nS\n (and \nT\nT\n). The coarsest box size is  determined by subdiving the i-th coordinate axis into  \nN = ceiling(L^\\frac{1}{k + l + m + 1})\nN = ceiling(L^\\frac{1}{k + l + m + 1})\n intervals of equal lengths,  resulting in a box size of \n|max(dim_{i}) - min(dim_{i})|/N\n|max(dim_{i}) - min(dim_{i})|/N\n. The next box size  is given by \n|max(dim_{i}) - min(dim_{i})|/(N+1)\n|max(dim_{i}) - min(dim_{i})|/(N+1)\n, then  \n|max(dim_{i}) - min(dim_{i})|/(N+2)\n|max(dim_{i}) - min(dim_{i})|/(N+2)\n, and so on, until the finest box  size which is given by \n|max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs})\n|max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs})\n. \n\n\nTransfer entropy computation\n\n\nTE is then computed as \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nusing the provided \nestimator\n (default = \nVisitationFrequency\n) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.\n\n\nsource\n\n\nte_reg(source::AbstractArray{\n:Real, 1}, \n    response::AbstractArray{\n:Real, 1}, \n    k::Int, l::Int, m::Int,\n    binning_scheme::Vector{RectangularBinning}; \n    \u03b7 = 1, \u03c4 = 1, \n    estimator = VisitationFrequency(), \n    b = 2)\n\n\n\n\nTE with user-provided discretization scheme(s)\n\n\nCalculate transfer entropy from \nsource\n to \nresponse\n using the provided  \nestimator\n on discretizations constructed by the provided \nbinning_scheme\n(s)  over a \nk + l + m\n-dimensional delay embedding of the input data,  using an embedding delay of \n\u03c4\n across all embedding components.  \n\u03b7\n is the prediction lag. \n\n\nArguments\n\n\n\n\nsource\n: The source data series.\n\n\ntarget\n: The target data series.\n\n\nk\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\nbinning_scheme\n: The binning scheme(s) used to construct the partitions   over which TE is computed. Must be either one or several instances of    \nRectangularBinning\ns (provided as a vector). TE is computed for each    of the resulting partitions.\n\n\n\n\nKeyword arguments\n\n\n\n\n\u03c4\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\n\u03b7\n: The prediction lag. Default is \n\u03b7 = 1\n.\n\n\nestimator\n: The transfer entropy estimator to use. The default    is \nVisitationFrequency()\n.\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\n\n\nMore about the embedding\n\n\nTo compute transfer entropy, we need an appropriate delay embedding  of \nsource\n (\nS\nS\n) and \ntarget\n (\nT\nT\n). For convenience, define \n\n\n\n\n\n\\begin{align}\nT_f^{(k)} \n= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} \n= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} \n= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}\n\n\n\n\n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}\n\n\n\n\n\nwhere \nT_f\nT_f\n denotes the \nk\n-dimensional set of vectors furnishing the future states of \nT\nT\n, \nT_{pp}\nT_{pp}\n denotes the \nl\n-dimensional set of vectors furnishing the past and present states of \nT\nT\n,  and \nS_{pp}\nS_{pp}\n denotes the \nm\n-dimensional set of vectors furnishing the past and present of \nS\nS\n.  \n\\eta\n\\eta\n is the prediction lag. This convenience function uses \n\\tau_1\n\\tau_1\n = \n\u03c4\n,  \n\\tau_2\n\\tau_2\n = \n2*\u03c4\n, \n\\tau_3\n\\tau_3\n = \n3*\u03c4\n, and so on.\n\n\nCombined, we get the generalised embedding \n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)})\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)})\n,  which is discretized as described below.\n\n\nMore about discretization\n\n\nThe discretization scheme must be either a single \nRectangularBinning\n instance, or a vector of  \nRectangularBinning\n instances. Run \n?RectangularBinning\n after loading \nCausalityTools\n for  details.\n\n\nTransfer entropy computation\n\n\nTE is then computed as \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nusing the provided \nestimator\n (default = \nVisitationFrequency\n) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.\n\n\nsource\n\n\n\n\nTE estimation between two data series conditioned on third series\n\n\n#\n\n\nCausalityTools.te_cond\n \n \nFunction\n.\n\n\nte_cond\n(\nsource\n::\nAbstractArray\n{\n:\nReal\n,\n \n1\n},\n \n    \nresponse\n::\nAbstractArray\n{\n:\nReal\n,\n \n1\n},\n\n    \ncond\n::\nAbstractArray\n{\n:\nReal\n,\n \n1\n},\n\n    \nk\n::\nInt\n,\n \nl\n::\nInt\n,\n \nm\n::\nInt\n,\n \nn\n::\nInt\n;\n \n    \n\u03b7\n \n=\n \n1\n,\n \n\u03c4\n \n=\n \n1\n,\n \n    \nestimator\n \n=\n \nVisitationFrequency\n(),\n \n    \nn_subdivs\n \n=\n \n1\n,\n\n    \nb\n \n=\n \n2\n)\n\n\n\n\n\nConditional TE with default discretization scheme(s)\n\n\nCalculate transfer entropy from \nsource\n to \nresponse\n conditioned on \ncond\n using the provided  \nestimator\n on a rectangular discretization of a \nk + l + m + n\n-dimensional  delay embedding of the input data, using an embedding delay of \n\u03c4\n across  all embedding components. \n\u03b7\n is the prediction lag. \n\n\nArguments\n\n\n\n\nsource\n: The source data series.\n\n\ntarget\n: The target data series.\n\n\ncond\n: The conditional data series.\n\n\nk\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\nn\n: The dimension of the \nC_{pp}\nC_{pp}\n component of the embedding.\n\n\n\n\nKeyword arguments\n\n\n\n\n\u03c4\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\n\u03b7\n: The prediction lag. Default is \n\u03b7 = 1\n.\n\n\nestimator\n: The transfer entropy estimator to use. The default    is \nVisitationFrequency()\n.\n\n\nn_subdivs\n: The number of different partitions of varying coarseness   to compute TE over, as described below. Default is \nn_subdivs = 2\n.   (this way, TE is computed over two separate partitions). Unless \nn_subdivs = 0\n,   make sure that \nn_subdivs\n is the same across analyses if they    are to be compared. T TE\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\n\n\nMore about the embedding\n\n\nTo compute transfer entropy, we need an appropriate delay embedding  of \nsource\n (\nS\nS\n), \ntarget\n (\nT\nT\n) and \ncond\n (\nC\nC\n). For convenience, define \n\n\n\n\n\n\\begin{align}\nT_f^{(k)} \n= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} \n= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} \n= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)} \n= \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}\n\n\n\n\n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)} &= \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}\n\n\n\n\n\nwhere \nT_f\nT_f\n denotes the \nk\n-dimensional set of vectors furnishing the future states of \nT\nT\n, \nT_{pp}\nT_{pp}\n denotes the \nl\n-dimensional set of vectors furnishing the past and present states of \nT\nT\n,  ,\nS_{pp}\nS_{pp}\n denotes the \nm\n-dimensional set of vectors furnishing the past and present of \nS\nS\n,  and \nC_{pp}\nC_{pp}\n denotes the \nn\n-dimensional set of vectors furnishing the past and present of \nC\nC\n. \n\\eta\n\\eta\n is the prediction lag. This convenience function uses \n\\tau_1\n\\tau_1\n = \n\u03c4\n,  \n\\tau_2\n\\tau_2\n = \n2*\u03c4\n, \n\\tau_3\n\\tau_3\n = \n3*\u03c4\n, and so on.\n\n\nCombined, we get the generalised embedding \n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n,  which is discretized as described below.\n\n\nMore about discretization\n\n\nTo compute TE, we coarse-grain the \nk+l+m+n\n-dimensional generalised embedding  \n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n into hyperrectangular boxes. The magnitude of the TE may be biased by the particular choice of binning scheme, so we compute TE across a number of different box sizes, determined as follows.\n\n\nLet \nL\nL\n be the number of observations in \nS\nS\n (and \nT\nT\n and \nC\nC\n). The coarsest box size is  determined by subdiving the i-th coordinate axis into  \nN = ceiling(L^\\frac{1}{k + l + m + n + 1})\nN = ceiling(L^\\frac{1}{k + l + m + n + 1})\n intervals of equal lengths,  resulting in a box size of \n|max(dim_{i}) - min(dim_{i})|/N\n|max(dim_{i}) - min(dim_{i})|/N\n. The next box size  is given by \n|max(dim_{i}) - min(dim_{i})|/(N+1)\n|max(dim_{i}) - min(dim_{i})|/(N+1)\n, then  \n|max(dim_{i}) - min(dim_{i})|/(N+2)\n|max(dim_{i}) - min(dim_{i})|/(N+2)\n, and so on, until the finest box  size which is given by \n|max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs})\n|max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs})\n. \n\n\nTransfer entropy computation\n\n\nTE is then computed as \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nusing the provided \nestimator\n (default = \nVisitationFrequency\n) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.\n\n\nsource\n\n\nte_cond(source::AbstractArray{\n:Real, 1}, \n    response::AbstractArray{\n:Real, 1},\n    cond::AbstractArray{\n:Real, 1},\n    k::Int, l::Int, m::Int, n::Int,\n    binning_scheme::Vector{RectangularBinning}; \n    \u03b7 = 1, \u03c4 = 1, \n    estimator = VisitationFrequency(), \n    b = 2)\n\n\n\n\nConditional TE with default discretization scheme(s\n\n\nCalculate transfer entropy from \nsource\n to \nresponse\n conditioned on \ncond\n using the provided  \nestimator\n on a rectangular discretization of a \nk + l + m + n\n-dimensional  delay embedding of the input data, using an embedding delay of \n\u03c4\n across  all embedding components. \n\u03b7\n is the prediction lag. \n\n\nArguments\n\n\n\n\nsource\n: The source data series.\n\n\ntarget\n: The target data series.\n\n\ncond\n: The conditional data series.\n\n\nk\n: The dimension of the \nT_{f}\nT_{f}\n component of the embedding.\n\n\nl\n: The dimension of the \nT_{pp}\nT_{pp}\n component of the embedding.\n\n\nm\n: The dimension of the \nS_{pp}\nS_{pp}\n component of the embedding.\n\n\nn\n: The dimension of the \nC_{pp}\nC_{pp}\n component of the embedding.\n\n\nbinning_scheme\n: The binning scheme(s) used to construct the partitions   over which TE is computed. Must be either one or several instances of    \nRectangularBinning\ns (provided as a vector). TE is computed for each    of the resulting partitions.\n\n\n\n\nKeyword arguments\n\n\n\n\n\u03c4\n: The embedding lag. Default is \n\u03c4 = 1\n.\n\n\n\u03b7\n: The prediction lag. Default is \n\u03b7 = 1\n.\n\n\nestimator\n: The transfer entropy estimator to use. The default    is \nVisitationFrequency()\n.\n\n\nb\n: Base of the logarithm. The default (\nb = 2\n) gives the TE in bits.\n\n\n\n\nMore about the embedding\n\n\nTo compute transfer entropy, we need an appropriate delay embedding  of \nsource\n (\nS\nS\n), \ntarget\n (\nT\nT\n) and \ncond\n (\nC\nC\n). For convenience, define \n\n\n\n\n\n\\begin{align}\nT_f^{(k)} \n= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} \n= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} \n= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)} \n= \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}\n\n\n\n\n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)} &= \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}\n\n\n\n\n\nwhere \nT_f\nT_f\n denotes the \nk\n-dimensional set of vectors furnishing the future states of \nT\nT\n, \nT_{pp}\nT_{pp}\n denotes the \nl\n-dimensional set of vectors furnishing the past and present states of \nT\nT\n,  ,\nS_{pp}\nS_{pp}\n denotes the \nm\n-dimensional set of vectors furnishing the past and present of \nS\nS\n,  and \nC_{pp}\nC_{pp}\n denotes the \nn\n-dimensional set of vectors furnishing the past and present of \nC\nC\n. \n\\eta\n\\eta\n is the prediction lag. This convenience function uses \n\\tau_1\n\\tau_1\n = \n\u03c4\n,  \n\\tau_2\n\\tau_2\n = \n2*\u03c4\n, \n\\tau_3\n\\tau_3\n = \n3*\u03c4\n, and so on.\n\n\nCombined, we get the generalised embedding \n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n\\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})\n,  which is discretized as described below.\n\n\nMore about discretization\n\n\nThe discretization scheme must be either a single \nRectangularBinning\n instance, or a vector of  \nRectangularBinning\n instances. Run \n?RectangularBinning\n after loading \nCausalityTools\n for  details.\n\n\nTransfer entropy computation\n\n\nTE is then computed as \n\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}\n\n\n\n\n\nusing the provided \nestimator\n (default = \nVisitationFrequency\n) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.\n\n\nsource", 
            "title": "Convenience functions"
        }, 
        {
            "location": "/transferentropy/convenience_functions_te/#convenience-functions-for-te-estimation", 
            "text": "", 
            "title": "Convenience functions for TE estimation"
        }, 
        {
            "location": "/transferentropy/convenience_functions_te/#te-estimation-between-two-data-series", 
            "text": "#  CausalityTools.te_reg     Function .  te_reg ( source :: AbstractArray { : Real ,   1 },  \n     response :: AbstractArray { : Real ,   1 },  \n     k :: Int ,   l :: Int ,   m :: Int ;  \n     \u03b7   =   1 ,   \u03c4   =   1 ,  \n     estimator   =   VisitationFrequency (),  \n     n_subdivs   =   1 , \n     b   =   2 )   TE estimation with default discretization scheme(s)  Calculate transfer entropy from  source  to  response  using the provided   estimator  on a rectangular discretization of a  k + l + m -dimensional  delay embedding of the input data, using an embedding delay of  \u03c4  across  all embedding components.  \u03b7  is the prediction lag.   Arguments   source : The source data series.  target : The target data series.  k : The dimension of the  T_{f} T_{f}  component of the embedding.  l : The dimension of the  T_{pp} T_{pp}  component of the embedding.  m : The dimension of the  S_{pp} S_{pp}  component of the embedding.   Keyword arguments   \u03c4 : The embedding lag. Default is  \u03c4 = 1 .  \u03b7 : The prediction lag. Default is  \u03b7 = 1 .  estimator : The transfer entropy estimator to use. The default    is  VisitationFrequency() .  n_subdivs : The number of different partitions of varying coarseness   to compute TE over, as described below. Default is  n_subdivs = 2 .   (this way, TE is computed over two separate partitions). Unless  n_subdivs = 0 ,   make sure that  n_subdivs  is the same across analyses if they    are to be compared. T TE  b : Base of the logarithm. The default ( b = 2 ) gives the TE in bits.   More about the embedding  To compute transfer entropy, we need an appropriate delay embedding  of  source  ( S S ) and  target  ( T T ). For convenience, define    \n\\begin{align}\nT_f^{(k)}  = \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)}  = \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)}  = \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}  \n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}   where  T_f T_f  denotes the  k -dimensional set of vectors furnishing the future states of  T T ,  T_{pp} T_{pp}  denotes the  l -dimensional set of vectors furnishing the past and present states of  T T ,  and  S_{pp} S_{pp}  denotes the  m -dimensional set of vectors furnishing the past and present of  S S .   \\eta \\eta  is the prediction lag. This convenience function uses  \\tau_1 \\tau_1  =  \u03c4 ,   \\tau_2 \\tau_2  =  2*\u03c4 ,  \\tau_3 \\tau_3  =  3*\u03c4 , and so on.  Combined, we get the generalised embedding  \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}) \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}) ,  which is discretized as described below.  More about discretization  To compute TE, we coarse-grain the  k+l+m -dimensional generalised embedding   \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}) \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)})  into hyperrectangular boxes. The magnitude of the TE may be biased by the particular choice of binning scheme, so we compute TE across a number of different box sizes, determined as follows.  Let  L L  be the number of observations in  S S  (and  T T ). The coarsest box size is  determined by subdiving the i-th coordinate axis into   N = ceiling(L^\\frac{1}{k + l + m + 1}) N = ceiling(L^\\frac{1}{k + l + m + 1})  intervals of equal lengths,  resulting in a box size of  |max(dim_{i}) - min(dim_{i})|/N |max(dim_{i}) - min(dim_{i})|/N . The next box size  is given by  |max(dim_{i}) - min(dim_{i})|/(N+1) |max(dim_{i}) - min(dim_{i})|/(N+1) , then   |max(dim_{i}) - min(dim_{i})|/(N+2) |max(dim_{i}) - min(dim_{i})|/(N+2) , and so on, until the finest box  size which is given by  |max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs}) |max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs}) .   Transfer entropy computation  TE is then computed as    \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}  \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}   using the provided  estimator  (default =  VisitationFrequency ) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.  source  te_reg(source::AbstractArray{ :Real, 1}, \n    response::AbstractArray{ :Real, 1}, \n    k::Int, l::Int, m::Int,\n    binning_scheme::Vector{RectangularBinning}; \n    \u03b7 = 1, \u03c4 = 1, \n    estimator = VisitationFrequency(), \n    b = 2)  TE with user-provided discretization scheme(s)  Calculate transfer entropy from  source  to  response  using the provided   estimator  on discretizations constructed by the provided  binning_scheme (s)  over a  k + l + m -dimensional delay embedding of the input data,  using an embedding delay of  \u03c4  across all embedding components.   \u03b7  is the prediction lag.   Arguments   source : The source data series.  target : The target data series.  k : The dimension of the  T_{f} T_{f}  component of the embedding.  l : The dimension of the  T_{pp} T_{pp}  component of the embedding.  m : The dimension of the  S_{pp} S_{pp}  component of the embedding.  binning_scheme : The binning scheme(s) used to construct the partitions   over which TE is computed. Must be either one or several instances of     RectangularBinning s (provided as a vector). TE is computed for each    of the resulting partitions.   Keyword arguments   \u03c4 : The embedding lag. Default is  \u03c4 = 1 .  \u03b7 : The prediction lag. Default is  \u03b7 = 1 .  estimator : The transfer entropy estimator to use. The default    is  VisitationFrequency() .  b : Base of the logarithm. The default ( b = 2 ) gives the TE in bits.   More about the embedding  To compute transfer entropy, we need an appropriate delay embedding  of  source  ( S S ) and  target  ( T T ). For convenience, define    \n\\begin{align}\nT_f^{(k)}  = \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)}  = \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)}  = \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}  \n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\}\n\\end{align}   where  T_f T_f  denotes the  k -dimensional set of vectors furnishing the future states of  T T ,  T_{pp} T_{pp}  denotes the  l -dimensional set of vectors furnishing the past and present states of  T T ,  and  S_{pp} S_{pp}  denotes the  m -dimensional set of vectors furnishing the past and present of  S S .   \\eta \\eta  is the prediction lag. This convenience function uses  \\tau_1 \\tau_1  =  \u03c4 ,   \\tau_2 \\tau_2  =  2*\u03c4 ,  \\tau_3 \\tau_3  =  3*\u03c4 , and so on.  Combined, we get the generalised embedding  \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}) \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}) ,  which is discretized as described below.  More about discretization  The discretization scheme must be either a single  RectangularBinning  instance, or a vector of   RectangularBinning  instances. Run  ?RectangularBinning  after loading  CausalityTools  for  details.  Transfer entropy computation  TE is then computed as    \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}  \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp})}{P(T_f | T_{pp})}\\right)}\n\\end{align}   using the provided  estimator  (default =  VisitationFrequency ) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.  source", 
            "title": "TE estimation between two data series"
        }, 
        {
            "location": "/transferentropy/convenience_functions_te/#te-estimation-between-two-data-series-conditioned-on-third-series", 
            "text": "#  CausalityTools.te_cond     Function .  te_cond ( source :: AbstractArray { : Real ,   1 },  \n     response :: AbstractArray { : Real ,   1 }, \n     cond :: AbstractArray { : Real ,   1 }, \n     k :: Int ,   l :: Int ,   m :: Int ,   n :: Int ;  \n     \u03b7   =   1 ,   \u03c4   =   1 ,  \n     estimator   =   VisitationFrequency (),  \n     n_subdivs   =   1 , \n     b   =   2 )   Conditional TE with default discretization scheme(s)  Calculate transfer entropy from  source  to  response  conditioned on  cond  using the provided   estimator  on a rectangular discretization of a  k + l + m + n -dimensional  delay embedding of the input data, using an embedding delay of  \u03c4  across  all embedding components.  \u03b7  is the prediction lag.   Arguments   source : The source data series.  target : The target data series.  cond : The conditional data series.  k : The dimension of the  T_{f} T_{f}  component of the embedding.  l : The dimension of the  T_{pp} T_{pp}  component of the embedding.  m : The dimension of the  S_{pp} S_{pp}  component of the embedding.  n : The dimension of the  C_{pp} C_{pp}  component of the embedding.   Keyword arguments   \u03c4 : The embedding lag. Default is  \u03c4 = 1 .  \u03b7 : The prediction lag. Default is  \u03b7 = 1 .  estimator : The transfer entropy estimator to use. The default    is  VisitationFrequency() .  n_subdivs : The number of different partitions of varying coarseness   to compute TE over, as described below. Default is  n_subdivs = 2 .   (this way, TE is computed over two separate partitions). Unless  n_subdivs = 0 ,   make sure that  n_subdivs  is the same across analyses if they    are to be compared. T TE  b : Base of the logarithm. The default ( b = 2 ) gives the TE in bits.   More about the embedding  To compute transfer entropy, we need an appropriate delay embedding  of  source  ( S S ),  target  ( T T ) and  cond  ( C C ). For convenience, define    \n\\begin{align}\nT_f^{(k)}  = \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)}  = \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)}  = \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)}  = \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}  \n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)} &= \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}   where  T_f T_f  denotes the  k -dimensional set of vectors furnishing the future states of  T T ,  T_{pp} T_{pp}  denotes the  l -dimensional set of vectors furnishing the past and present states of  T T ,  , S_{pp} S_{pp}  denotes the  m -dimensional set of vectors furnishing the past and present of  S S ,  and  C_{pp} C_{pp}  denotes the  n -dimensional set of vectors furnishing the past and present of  C C .  \\eta \\eta  is the prediction lag. This convenience function uses  \\tau_1 \\tau_1  =  \u03c4 ,   \\tau_2 \\tau_2  =  2*\u03c4 ,  \\tau_3 \\tau_3  =  3*\u03c4 , and so on.  Combined, we get the generalised embedding  \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)}) \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)}) ,  which is discretized as described below.  More about discretization  To compute TE, we coarse-grain the  k+l+m+n -dimensional generalised embedding   \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)}) \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)})  into hyperrectangular boxes. The magnitude of the TE may be biased by the particular choice of binning scheme, so we compute TE across a number of different box sizes, determined as follows.  Let  L L  be the number of observations in  S S  (and  T T  and  C C ). The coarsest box size is  determined by subdiving the i-th coordinate axis into   N = ceiling(L^\\frac{1}{k + l + m + n + 1}) N = ceiling(L^\\frac{1}{k + l + m + n + 1})  intervals of equal lengths,  resulting in a box size of  |max(dim_{i}) - min(dim_{i})|/N |max(dim_{i}) - min(dim_{i})|/N . The next box size  is given by  |max(dim_{i}) - min(dim_{i})|/(N+1) |max(dim_{i}) - min(dim_{i})|/(N+1) , then   |max(dim_{i}) - min(dim_{i})|/(N+2) |max(dim_{i}) - min(dim_{i})|/(N+2) , and so on, until the finest box  size which is given by  |max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs}) |max(dim_{i}) - min(dim_{i})|/(N+N_{subdivs}) .   Transfer entropy computation  TE is then computed as    \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}  \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}   using the provided  estimator  (default =  VisitationFrequency ) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.  source  te_cond(source::AbstractArray{ :Real, 1}, \n    response::AbstractArray{ :Real, 1},\n    cond::AbstractArray{ :Real, 1},\n    k::Int, l::Int, m::Int, n::Int,\n    binning_scheme::Vector{RectangularBinning}; \n    \u03b7 = 1, \u03c4 = 1, \n    estimator = VisitationFrequency(), \n    b = 2)  Conditional TE with default discretization scheme(s  Calculate transfer entropy from  source  to  response  conditioned on  cond  using the provided   estimator  on a rectangular discretization of a  k + l + m + n -dimensional  delay embedding of the input data, using an embedding delay of  \u03c4  across  all embedding components.  \u03b7  is the prediction lag.   Arguments   source : The source data series.  target : The target data series.  cond : The conditional data series.  k : The dimension of the  T_{f} T_{f}  component of the embedding.  l : The dimension of the  T_{pp} T_{pp}  component of the embedding.  m : The dimension of the  S_{pp} S_{pp}  component of the embedding.  n : The dimension of the  C_{pp} C_{pp}  component of the embedding.  binning_scheme : The binning scheme(s) used to construct the partitions   over which TE is computed. Must be either one or several instances of     RectangularBinning s (provided as a vector). TE is computed for each    of the resulting partitions.   Keyword arguments   \u03c4 : The embedding lag. Default is  \u03c4 = 1 .  \u03b7 : The prediction lag. Default is  \u03b7 = 1 .  estimator : The transfer entropy estimator to use. The default    is  VisitationFrequency() .  b : Base of the logarithm. The default ( b = 2 ) gives the TE in bits.   More about the embedding  To compute transfer entropy, we need an appropriate delay embedding  of  source  ( S S ),  target  ( T T ) and  cond  ( C C ). For convenience, define    \n\\begin{align}\nT_f^{(k)}  = \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)}  = \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)}  = \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)}  = \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}  \n\\begin{align}\nT_f^{(k)} &= \\{(T(t+\\eta_k), \\ldots, T(t+\\eta_2), T(t+\\eta_1)) \\} \\\\\nT_{pp}^{(l)} &= \\{ (T(t), T(t-\\tau_1), T(t-\\tau_2), \\ldots, T(t - \\tau_{l - 1})) \\} \\\\\nS_{pp}^{(m)} &= \\{ (S(t), S(t-\\tau_1), S(t-\\tau_2), \\ldots, S(t-\\tau_{m - 1})) \\} \\\\\nC_{pp}^{(n)} &= \\{ (C(t), C(t-\\tau_1), C(t-\\tau_2), \\ldots, C(t-\\tau_{n - 1})) \\}\n\\end{align}   where  T_f T_f  denotes the  k -dimensional set of vectors furnishing the future states of  T T ,  T_{pp} T_{pp}  denotes the  l -dimensional set of vectors furnishing the past and present states of  T T ,  , S_{pp} S_{pp}  denotes the  m -dimensional set of vectors furnishing the past and present of  S S ,  and  C_{pp} C_{pp}  denotes the  n -dimensional set of vectors furnishing the past and present of  C C .  \\eta \\eta  is the prediction lag. This convenience function uses  \\tau_1 \\tau_1  =  \u03c4 ,   \\tau_2 \\tau_2  =  2*\u03c4 ,  \\tau_3 \\tau_3  =  3*\u03c4 , and so on.  Combined, we get the generalised embedding  \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)}) \\mathbb{E} = (T_f^{(k)}, T_{pp}^{(l)}, S_{pp}^{(m)}, C_{pp}^{(n)}) ,  which is discretized as described below.  More about discretization  The discretization scheme must be either a single  RectangularBinning  instance, or a vector of   RectangularBinning  instances. Run  ?RectangularBinning  after loading  CausalityTools  for  details.  Transfer entropy computation  TE is then computed as    \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}  \n\\begin{align}\nTE_{S \\rightarrow T} = \\int_{\\mathbb{E}} P(T_f, T_{pp}, S_{pp}, C_{pp}) \\log_{b}{\\left(\\frac{P(T_f | T_{pp}, S_{pp}, C_{pp})}{P(T_f | T_{pp}, C_{pp})}\\right)}\n\\end{align}   using the provided  estimator  (default =  VisitationFrequency ) for  each of the discretizations. A vector of the TE estimates for each discretization  is returned.  source", 
            "title": "TE estimation between two data series conditioned on third series"
        }, 
        {
            "location": "/transferentropy/transferentropy_estimators/", 
            "text": "Low-level transfer entropy estimators\n\n\nFor complete control over the estimation procedure, the analyst must create a \ndelay  reconstruction\n from the input data, specify a \ndiscretization scheme\n which can be either \nrectangular\n or \ntriangulated\n, and \nmap variables of the delay reconstruction to the correct  marginals\n.\n\n\nThe package provides some convenience methods to compute TE directly from time series, see the \nwrappers\n for \nregular TE\n and \nconditional TE\n. However, be absolutely sure that you understand what they do before applying them to real problems.\n\n\n\n\nEstimators\n\n\nValid estimator types for rectangular partitions are \n\n\n\n\nVisitationFrequency\n. An implementation of the original TE estimator from Schreiber (2000)\n1\n\n\nTransferOperatorGrid\n. An implementation of the transfer operator grid estimator from Diego et al. (2019)\n2\n\n\n\n\n\n\nGeneral workflow for TE estimation\n\n\nThe general workflow for estimating transfer entropy over rectangular partitions is as follows.\n\n\n\n\nRectangular partitions\n\n\nTo estimate transfer entropy over rectangular partitions, you would use the following method, providing either a \nVisitationFrequency\n or \nTransferOperatorGrid\n instance to the \nestimator\n argument.\n\n\n#\n\n\nTransferEntropy.transferentropy\n \n \nMethod\n.\n\n\ntransferentropy\n(\npts\n,\n \nvars\n::\nTEVars\n,\n \n\u03f5\n::\nRectangularBinning\n,\n \n    \nestimator\n::\nTransferEntropyEstimator\n;\n \nb\n \n=\n \n2\n)\n\n\n\n\n\nTransfer entropy using a rectangular partition\n\n\nEstimate transfer entropy on a rectangular partition over  a set of points \npts\n, which represent a generalised embedding of  data series \nx\nx\n, \ny\ny\n and (potentially) \nz\nz\n of the form  outlined below. \n\n\nNote: \npts\n must be a vector of states, not a vector of  variables/(time series). Wrap your time series in a \nDataset\n first if the latter is the case.\n\n\nEstimators (and their acronyms)\n\n\n\n\nVisitation frequency estimator: \nVisitationFrequency\n; an instance must be provided.\n\n\nTransfer operator grid estimator: \nTransferOperatorGrid\n; an instance must be provided.\n\n\n\n\nRelationship between \npts\n and \nvars\n\n\npts\n should be an embedding of the form  \n(y(t + \\eta)^{k}, y(t)^{l}, x(t)^{m}, z(t)^{n}\n(y(t + \\eta)^{k}, y(t)^{l}, x(t)^{m}, z(t)^{n}\n.  Here, \ny(t + \\eta)^{k})\ny(t + \\eta)^{k})\n indicates that \nk\nk\n future states of \ny\n  should be included, \ny(t)^{l}\ny(t)^{l}\n indicates that \nl\nl\n present/past  states of \ny\ny\n should be included, \nx(t)^{m}\nx(t)^{m}\n indicates that \nm\nm\n  present/past states of \nx\nx\n should be included, and \nz(t)^{n}\nz(t)^{n}\n indicates  that \nn\nn\n present/past states of the variable we're conditioning on should  be included in the embedding vectors. Thus, the total dimension  of the embedding space will be \nk + l + m + n\nk + l + m + n\n. \n\n\nvars\n is a \nTEVars\n instance contain the instruction on which  variables of the embedding will be treated as part of which marginals during transfer entropy computation. Check the documentation of  \nTEVars\n for more details.\n\n\nExample\n\n\n1. Time series\n\n\nWe'll generate two 80-point long realizations, \nx\nx\n and \ny\ny\n, of two 1D  logistic maps starting at different initial conditions.\n\n\nsys1\n \n=\n \nDynamicalSystems\n.\nSystems\n.\nlogistic\n()\n\n\nsys2\n \n=\n \nDynamicalSystems\n.\nSystems\n.\nlogistic\n()\n\n\nx\n \n=\n \ntrajectory\n(\nsys1\n,\n \n80\n,\n \nTtr\n \n=\n \n1000\n);\n\n\ny\n \n=\n \ntrajectory\n(\nsys1\n,\n \n80\n,\n \nTtr\n \n=\n \n1000\n);\n\n\n\n# Wrap the time series in a dataset containing the states of the \n\n\n# composite system.\n\n\nD\n \n=\n \nDataset\n(\nx\n,\n \ny\n)\n\n\n\n\n\n2. Generalised embedding\n\n\nSay we want to compute transfer entropy from \nx\nx\n to \ny\ny\n, and that we  require a 4-dimensional embedding. We do an appropriate delay reconstruction of the data  (\nE = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\}\nE = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\}\n), so that  we're computing the following TE\n\n\n\n\n\nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.\n\n\n\n\nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.\n\n\n\n\n\nTo create the embedding, we'll use the \ncustomembed\n function (check its  documentation for a detailed explanation on how it works). \n\n\n# Embed the data, putting time series in the 2nd column (y) of `data` in the \n\n\n# first three embedding columns, lagging them with lags (\u03b7, 0, -\u03c4), and \n\n\n# putting the 1st column of `data` (x) in the last position of the embedding,\n\n\n# not lagging it.\n\n\n\u03c4\n \n=\n \noptimal_delay\n(\ny\n)\n \n# embedding lag\n\n\n\u03b7\n \n=\n \n2\n \n# prediction lag\n\n\n\npts\n \n=\n \ncustomembed\n(\nD\n,\n \nPositions\n(\n2\n,\n \n2\n,\n \n2\n,\n \n1\n),\n \nLags\n(\n\u03b7\n,\n \n0\n,\n \n-\n\u03c4\n,\n \n0\n))\n\n\n\n\n\n3. Instructions to the estimator\n\n\nNow, tell the estimator how to relative the dynamical variables of the  generalised embedding to the marginals in the transfer entropy computation.\n\n\nvars\n \n=\n \nTEVars\n(\nTf\n \n=\n \n[\n1\n],\n \nTpp\n \n=\n \n[\n2\n,\n \n3\n],\n \nSpp\n \n=\n \n[\n4\n])\n\n\n\n\n\n4. Rectangular grid specification\n\n\nWe'll compute transfer entropy using the visitation frequency estimator over  a rectangular partition where the box sizes are determined by  splitting each coordinate axis into \n6\n6\n equally spaced intervals each.\n\n\nbinning\n \n=\n \nRectangularBinning\n(\n6\n)\n\n\n\n\n\n5. Compute transfer entropy\n\n\n# Over a single rectangular grid\n\n\ntransferentropy\n(\npts\n,\n \nvars\n,\n \nbinning\n,\n \nVisitationFrequency\n())\n \n#, or\n\n\ntransferentropy\n(\npts\n,\n \nvars\n,\n \nbinning\n,\n \nTransferOperatorGrid\n())\n\n\n\n# Over multiple cubic grids with differing box sizes\n\n\n# logarithmically spaced from edge length 0.001 to 0.3\n\n\n\u03f5s\n \n=\n \n10\n \n.^\n \nrange\n(\nlog\n(\n10\n,\n \n0.001\n),\n \nlog10\n(\n0.3\n),\n \nlength\n \n=\n \n15\n)\n\n\nmap\n(\n\u03f5\n \n-\n \ntransferentropy\n(\npts\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nVisitationFrequency\n()))\n \n#, or\n\n\nmap\n(\n\u03f5\n \n-\n \ntransferentropy\n(\npts\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nTransferOperatorGrid\n()))\n\n\n\n\n\n\n\nTriangulated partitions\n\n\nEstimators for computing TE on triangulated partitions, whose invariant distribution is obtained  through the transfer operator, was also introduced in Diego et al. (2019)\n2\n.\n\n\n#\n\n\nTransferEntropy.transferentropy\n \n \nMethod\n.\n\n\ntransferentropy\n(\n\u03bc\n::\nAbstractTriangulationInvariantMeasure\n,\n \nvars\n::\nTEVars\n,\n\n    \nbinning_scheme\n::\nRectangularBinning\n;\n \n    \nestimator\n \n=\n \nVisitationFrequency\n(),\n \nn\n::\nInt\n \n=\n \n10000\n,\n \nb\n \n=\n \n2\n)\n\n\n\n\n\nTransfer entropy using a precomputed invariant measure over a triangulated partition\n\n\nEstimate transfer entropy from an invariant measure over a triangulation that has been precomputed either as \n\n\n\n\n\u03bc = invariantmeasure(pts, TriangulationBinning(), ApproximateIntersection())\n, or\n\n\n\u03bc = invariantmeasure(pts, TriangulationBinning(), ExactIntersection())\n\n\n\n\nwhere the first method uses approximate simplex intersections (faster) and  the second method uses exact simplex intersections (slow). \n\u03bc\n contains  all the information needed to compute transfer entropy. \n\n\nNote: \npts\n must be a vector of states, not a vector of  variables/(time series). Wrap your time series in a \nDataset\n first if the latter is the case.\n\n\nComputing transfer entropy (triangulation -\n rectangular partition)\n\n\nBecause we need to compute marginals, we need a rectangular grid. To do so, transfer entropy is computed by sampling the simplices of the  triangulation according to their measure with a total of approximately  \nn\n points. Introducing multiple points as representatives for the partition elements does not introduce any bias, because we in computing the  invariant measure, we use no more information than what is encoded in the  dynamics of the original data points. However, from the invariant measure, we can get a practically infinite amount of points to estimate transfer  entropy from.\n\n\nThen, transfer entropy is estimated using the visitation  frequency estimator on those points (see docs for \ntransferentropy_visitfreq\n  for more information), on a rectangular grid specified by \nbinning_scheme\n.\n\n\nCommon use case\n\n\nThis method is good to use if you want to explore the sensitivity  of transfer entropy to the bin size in the final rectangular grid,  when you have few observations in the time series. The invariant  measure, which encodes the dynamical information, is slow to compute over  the triangulation, but only needs to be computed once. After that, transfer entropy may be estimated at multiple scales very quickly.\n\n\nExample\n\n\n# Assume these points are an appropriate delay embedding {(x(t), y(t), y(t+1))} and \n\n\n# that we\nre measure transfer entropy from x -\n y. \n\n\npts\n \n=\n \ninvariantize\n([\nrand\n(\n3\n)\n \nfor\n \ni\n \n=\n \n1\n:\n30\n])\n\n\n\nv\n \n=\n \nTEVars\n(\nTf\n \n=\n \n[\n3\n],\n \nTpp\n \n=\n \n[\n2\n],\n \nSpp\n \n=\n \n[\n1\n])\n\n\n\n# Compute invariant measure over a triangulation using approximate \n\n\n# simplex intersections. This is relatively slow.\n\n\n\u03bc\n \n=\n \ninvariantmeasure\n(\npts\n,\n \nTriangulationBinning\n(),\n \nApproximateIntersection\n())\n\n\n\n# Compute transfer entropy from the invariant measure over multiple \n\n\n# bin sizes. This is fast, because the measure has been precomputed.\n\n\ntes\n \n=\n \nmap\n(\n\u03f5\n \n-\n \ntransferentropy\n(\n\u03bc\n,\n \nv\n,\n \nRectangularBinning\n(\n\u03f5\n)),\n \n2\n:\n50\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.\n\n\n\n\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.", 
            "title": "Low-level estimators"
        }, 
        {
            "location": "/transferentropy/transferentropy_estimators/#low-level-transfer-entropy-estimators", 
            "text": "For complete control over the estimation procedure, the analyst must create a  delay  reconstruction  from the input data, specify a  discretization scheme  which can be either  rectangular  or  triangulated , and  map variables of the delay reconstruction to the correct  marginals .  The package provides some convenience methods to compute TE directly from time series, see the  wrappers  for  regular TE  and  conditional TE . However, be absolutely sure that you understand what they do before applying them to real problems.", 
            "title": "Low-level transfer entropy estimators"
        }, 
        {
            "location": "/transferentropy/transferentropy_estimators/#estimators", 
            "text": "Valid estimator types for rectangular partitions are    VisitationFrequency . An implementation of the original TE estimator from Schreiber (2000) 1  TransferOperatorGrid . An implementation of the transfer operator grid estimator from Diego et al. (2019) 2", 
            "title": "Estimators"
        }, 
        {
            "location": "/transferentropy/transferentropy_estimators/#general-workflow-for-te-estimation", 
            "text": "The general workflow for estimating transfer entropy over rectangular partitions is as follows.", 
            "title": "General workflow for TE estimation"
        }, 
        {
            "location": "/transferentropy/transferentropy_estimators/#rectangular-partitions", 
            "text": "To estimate transfer entropy over rectangular partitions, you would use the following method, providing either a  VisitationFrequency  or  TransferOperatorGrid  instance to the  estimator  argument.  #  TransferEntropy.transferentropy     Method .  transferentropy ( pts ,   vars :: TEVars ,   \u03f5 :: RectangularBinning ,  \n     estimator :: TransferEntropyEstimator ;   b   =   2 )   Transfer entropy using a rectangular partition  Estimate transfer entropy on a rectangular partition over  a set of points  pts , which represent a generalised embedding of  data series  x x ,  y y  and (potentially)  z z  of the form  outlined below.   Note:  pts  must be a vector of states, not a vector of  variables/(time series). Wrap your time series in a  Dataset  first if the latter is the case.  Estimators (and their acronyms)   Visitation frequency estimator:  VisitationFrequency ; an instance must be provided.  Transfer operator grid estimator:  TransferOperatorGrid ; an instance must be provided.   Relationship between  pts  and  vars  pts  should be an embedding of the form   (y(t + \\eta)^{k}, y(t)^{l}, x(t)^{m}, z(t)^{n} (y(t + \\eta)^{k}, y(t)^{l}, x(t)^{m}, z(t)^{n} .  Here,  y(t + \\eta)^{k}) y(t + \\eta)^{k})  indicates that  k k  future states of  y   should be included,  y(t)^{l} y(t)^{l}  indicates that  l l  present/past  states of  y y  should be included,  x(t)^{m} x(t)^{m}  indicates that  m m   present/past states of  x x  should be included, and  z(t)^{n} z(t)^{n}  indicates  that  n n  present/past states of the variable we're conditioning on should  be included in the embedding vectors. Thus, the total dimension  of the embedding space will be  k + l + m + n k + l + m + n .   vars  is a  TEVars  instance contain the instruction on which  variables of the embedding will be treated as part of which marginals during transfer entropy computation. Check the documentation of   TEVars  for more details.  Example  1. Time series  We'll generate two 80-point long realizations,  x x  and  y y , of two 1D  logistic maps starting at different initial conditions.  sys1   =   DynamicalSystems . Systems . logistic ()  sys2   =   DynamicalSystems . Systems . logistic ()  x   =   trajectory ( sys1 ,   80 ,   Ttr   =   1000 );  y   =   trajectory ( sys1 ,   80 ,   Ttr   =   1000 );  # Wrap the time series in a dataset containing the states of the   # composite system.  D   =   Dataset ( x ,   y )   2. Generalised embedding  Say we want to compute transfer entropy from  x x  to  y y , and that we  require a 4-dimensional embedding. We do an appropriate delay reconstruction of the data  ( E = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\} E = \\{S_{pp}, T_{pp}, T_f \\}= \\{x_t, (y_t, y_{t-\\tau}), y_{t+\\eta} \\} ), so that  we're computing the following TE   \nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.  \nTE_{x \\to y} =  \\int_E P(x_t, y_{t-\\tau} y_t, y_{t + \\eta}) \\log{\\left( \\dfrac{P(y_{t + \\eta} | (y_t, y_{t - \\tau}, x_t)}{P(y_{t + \\eta} | y_t, y_{t-\\tau})} \\right)}.   To create the embedding, we'll use the  customembed  function (check its  documentation for a detailed explanation on how it works).   # Embed the data, putting time series in the 2nd column (y) of `data` in the   # first three embedding columns, lagging them with lags (\u03b7, 0, -\u03c4), and   # putting the 1st column of `data` (x) in the last position of the embedding,  # not lagging it.  \u03c4   =   optimal_delay ( y )   # embedding lag  \u03b7   =   2   # prediction lag  pts   =   customembed ( D ,   Positions ( 2 ,   2 ,   2 ,   1 ),   Lags ( \u03b7 ,   0 ,   - \u03c4 ,   0 ))   3. Instructions to the estimator  Now, tell the estimator how to relative the dynamical variables of the  generalised embedding to the marginals in the transfer entropy computation.  vars   =   TEVars ( Tf   =   [ 1 ],   Tpp   =   [ 2 ,   3 ],   Spp   =   [ 4 ])   4. Rectangular grid specification  We'll compute transfer entropy using the visitation frequency estimator over  a rectangular partition where the box sizes are determined by  splitting each coordinate axis into  6 6  equally spaced intervals each.  binning   =   RectangularBinning ( 6 )   5. Compute transfer entropy  # Over a single rectangular grid  transferentropy ( pts ,   vars ,   binning ,   VisitationFrequency ())   #, or  transferentropy ( pts ,   vars ,   binning ,   TransferOperatorGrid ())  # Over multiple cubic grids with differing box sizes  # logarithmically spaced from edge length 0.001 to 0.3  \u03f5s   =   10   .^   range ( log ( 10 ,   0.001 ),   log10 ( 0.3 ),   length   =   15 )  map ( \u03f5   -   transferentropy ( pts ,   vars ,   RectangularBinning ( \u03f5 ),   VisitationFrequency ()))   #, or  map ( \u03f5   -   transferentropy ( pts ,   vars ,   RectangularBinning ( \u03f5 ),   TransferOperatorGrid ()))", 
            "title": "Rectangular partitions"
        }, 
        {
            "location": "/transferentropy/transferentropy_estimators/#triangulated-partitions", 
            "text": "Estimators for computing TE on triangulated partitions, whose invariant distribution is obtained  through the transfer operator, was also introduced in Diego et al. (2019) 2 .  #  TransferEntropy.transferentropy     Method .  transferentropy ( \u03bc :: AbstractTriangulationInvariantMeasure ,   vars :: TEVars , \n     binning_scheme :: RectangularBinning ;  \n     estimator   =   VisitationFrequency (),   n :: Int   =   10000 ,   b   =   2 )   Transfer entropy using a precomputed invariant measure over a triangulated partition  Estimate transfer entropy from an invariant measure over a triangulation that has been precomputed either as    \u03bc = invariantmeasure(pts, TriangulationBinning(), ApproximateIntersection()) , or  \u03bc = invariantmeasure(pts, TriangulationBinning(), ExactIntersection())   where the first method uses approximate simplex intersections (faster) and  the second method uses exact simplex intersections (slow).  \u03bc  contains  all the information needed to compute transfer entropy.   Note:  pts  must be a vector of states, not a vector of  variables/(time series). Wrap your time series in a  Dataset  first if the latter is the case.  Computing transfer entropy (triangulation -  rectangular partition)  Because we need to compute marginals, we need a rectangular grid. To do so, transfer entropy is computed by sampling the simplices of the  triangulation according to their measure with a total of approximately   n  points. Introducing multiple points as representatives for the partition elements does not introduce any bias, because we in computing the  invariant measure, we use no more information than what is encoded in the  dynamics of the original data points. However, from the invariant measure, we can get a practically infinite amount of points to estimate transfer  entropy from.  Then, transfer entropy is estimated using the visitation  frequency estimator on those points (see docs for  transferentropy_visitfreq   for more information), on a rectangular grid specified by  binning_scheme .  Common use case  This method is good to use if you want to explore the sensitivity  of transfer entropy to the bin size in the final rectangular grid,  when you have few observations in the time series. The invariant  measure, which encodes the dynamical information, is slow to compute over  the triangulation, but only needs to be computed once. After that, transfer entropy may be estimated at multiple scales very quickly.  Example  # Assume these points are an appropriate delay embedding {(x(t), y(t), y(t+1))} and   # that we re measure transfer entropy from x -  y.   pts   =   invariantize ([ rand ( 3 )   for   i   =   1 : 30 ])  v   =   TEVars ( Tf   =   [ 3 ],   Tpp   =   [ 2 ],   Spp   =   [ 1 ])  # Compute invariant measure over a triangulation using approximate   # simplex intersections. This is relatively slow.  \u03bc   =   invariantmeasure ( pts ,   TriangulationBinning (),   ApproximateIntersection ())  # Compute transfer entropy from the invariant measure over multiple   # bin sizes. This is fast, because the measure has been precomputed.  tes   =   map ( \u03f5   -   transferentropy ( \u03bc ,   v ,   RectangularBinning ( \u03f5 )),   2 : 50 )       Schreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.    Diego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.", 
            "title": "Triangulated partitions"
        }, 
        {
            "location": "/transferentropy/TEVars/", 
            "text": "TEVars\n\n\nA \nTEVars\n instance maps variables of a \ncustom delay reconstruction\n to the correct marginals during \nTE computation\n. In practice, this translates to using the correct columns of the \nDataset\n furnishing the delay reconstruction when computing the different marginals  during transfer entropy computation.\n\n\n\n\nMap reconstruction variables to marginals using keywords\n\n\n#\n\n\nTransferEntropy.TEVars\n \n \nMethod\n.\n\n\nTEVars\n(;\nTf\n \n=\n \nInt\n[],\n \nTpp\n \n=\n \nInt\n[],\n \nSpp\n \n=\n \nInt\n[],\n \nCpp\n \n=\n \nInt\n[])\n\n\n\n\n\nWhich axes of the state space correspond to the future of the target (\nTf\n), the present/past of the target (\nTpp\n), the present/past of the source (\nSpp\n), and the present/past of any conditioned variables (\nCpp\n)? Indices correspond to variables of the embedding or colums of a \nDataset\n.\n\n\nThis information is used by the transfer entropy estimators to ensure the marginal distributions are computed correctly.\n\n\n\n\nInfer the mapping between reconstruction variables and marginals\n\n\nAlternatively, the mapping is inferred from the order of the inputs. Here, \ntarget_future\n is the \nT_{f}\nT_{f}\n component, \ntarget_presentpast\n is the \nT_{pp}\nT_{pp}\n component, and \nsource_presentpast\n is the  \nS_{pp}\nS_{pp}\n component of the delay reconstruction. For conditional analyses, the additional  component \nconditioned_presentpast\n (the \nC_{pp}\nC_{pp}\n component) is needed.\n\n\n#\n\n\nTransferEntropy.TEVars\n \n \nMethod\n.\n\n\nTEVars\n(\ntarget_future\n::\nVector\n{\nInt\n},\n\n        \ntarget_presentpast\n::\nVector\n{\nInt\n},\n\n        \nsource_presentpast\n::\nVector\n{\nInt\n})\n\n\n\n\n\nWhich axes of the state space correspond to the future of the target, the present/past of the target, and the present/past of the source? Indices correspond to variables of the embedding or colums of a \nDataset\n.\n\n\nThis information is used by the transfer entropy estimators to ensure the marginal distributions are computed correctly.\n\n\nThis three-argument constructor assumes there will be no conditional variables.\n\n\n#\n\n\nTransferEntropy.TEVars\n \n \nType\n.\n\n\nTEVars\n(\ntarget_future\n::\nVector\n{\nInt\n}\n\n    \ntarget_presentpast\n::\nVector\n{\nInt\n}\n\n    \nsource_presentpast\n::\nVector\n{\nInt\n}\n\n    \nconditioned_presentpast\n::\nVector\n{\nInt\n})\n\n\n\n\n\nWhich axes of the state space correspond to the future of the target, the present/past of the target, the present/past of the source, and the present/past of any conditioned variables? Indices correspond to variables of the embedding or colums of a \nDataset\n.\n\n\nThis information is used by the transfer entropy estimators to ensure the marginal distributions are computed correctly.", 
            "title": "Mapping delay reconstruction to marginals (TEVars)"
        }, 
        {
            "location": "/transferentropy/TEVars/#tevars", 
            "text": "A  TEVars  instance maps variables of a  custom delay reconstruction  to the correct marginals during  TE computation . In practice, this translates to using the correct columns of the  Dataset  furnishing the delay reconstruction when computing the different marginals  during transfer entropy computation.", 
            "title": "TEVars"
        }, 
        {
            "location": "/transferentropy/TEVars/#map-reconstruction-variables-to-marginals-using-keywords", 
            "text": "#  TransferEntropy.TEVars     Method .  TEVars (; Tf   =   Int [],   Tpp   =   Int [],   Spp   =   Int [],   Cpp   =   Int [])   Which axes of the state space correspond to the future of the target ( Tf ), the present/past of the target ( Tpp ), the present/past of the source ( Spp ), and the present/past of any conditioned variables ( Cpp )? Indices correspond to variables of the embedding or colums of a  Dataset .  This information is used by the transfer entropy estimators to ensure the marginal distributions are computed correctly.", 
            "title": "Map reconstruction variables to marginals using keywords"
        }, 
        {
            "location": "/transferentropy/TEVars/#infer-the-mapping-between-reconstruction-variables-and-marginals", 
            "text": "Alternatively, the mapping is inferred from the order of the inputs. Here,  target_future  is the  T_{f} T_{f}  component,  target_presentpast  is the  T_{pp} T_{pp}  component, and  source_presentpast  is the   S_{pp} S_{pp}  component of the delay reconstruction. For conditional analyses, the additional  component  conditioned_presentpast  (the  C_{pp} C_{pp}  component) is needed.  #  TransferEntropy.TEVars     Method .  TEVars ( target_future :: Vector { Int }, \n         target_presentpast :: Vector { Int }, \n         source_presentpast :: Vector { Int })   Which axes of the state space correspond to the future of the target, the present/past of the target, and the present/past of the source? Indices correspond to variables of the embedding or colums of a  Dataset .  This information is used by the transfer entropy estimators to ensure the marginal distributions are computed correctly.  This three-argument constructor assumes there will be no conditional variables.  #  TransferEntropy.TEVars     Type .  TEVars ( target_future :: Vector { Int } \n     target_presentpast :: Vector { Int } \n     source_presentpast :: Vector { Int } \n     conditioned_presentpast :: Vector { Int })   Which axes of the state space correspond to the future of the target, the present/past of the target, the present/past of the source, and the present/past of any conditioned variables? Indices correspond to variables of the embedding or colums of a  Dataset .  This information is used by the transfer entropy estimators to ensure the marginal distributions are computed correctly.", 
            "title": "Infer the mapping between reconstruction variables and marginals"
        }, 
        {
            "location": "/transferentropy/examples_TE_different_partitionings/", 
            "text": "Effect of dicretization scheme on transfer entropy estimates\n\n\n\n\nDifferent ways of partitioning\n\n\nThe \nTransferOperatorGrid\n and \nVisitationFrequency\n transfer entropy estimators both operate on partitions on the delay reconstructions. Below, we demonstrate the four different ways of discretizing the state space.\n\n\nFirst, let's create some example time series, embed them and organize the computation of marginal probabilities.\n\n\nx\n \n=\n \ncumsum\n(\nrand\n(\n300\n))\n\n\ny\n \n=\n \nsin\n.\n(\ncumsum\n(\nrand\n(\n300\n)))\n*\n0.3\n \n.+\n \nx\n\n\n\n\u03c4\n \n=\n \n1\n \n# embedding lag\n\n\n\u03bd\n \n=\n \n1\n \n# forward prediction lag\n\n\nE_xtoy\n \n=\n \ncustomembed\n(\nDataset\n(\nx\n,\n \ny\n),\n \nPositions\n([\n2\n,\n \n2\n,\n \n2\n,\n \n1\n]),\n \nLags\n([\n\u03bd\n,\n \n0\n,\n \n-\n\u03c4\n,\n \n0\n]))\n\n\nE_ytox\n \n=\n \ncustomembed\n(\nDataset\n(\ny\n,\n \nx\n),\n \nPositions\n([\n2\n,\n \n2\n,\n \n2\n,\n \n1\n]),\n \nLags\n([\n\u03bd\n,\n \n0\n,\n \n-\n\u03c4\n,\n \n0\n]))\n\n\n\n# Organize marginals\n\n\nTf\n \n=\n \n[\n1\n]\n     \n# target, future\n\n\nTpp\n \n=\n \n[\n2\n,\n \n3\n]\n \n# target, present and past\n\n\nSpp\n \n=\n \n[\n4\n]\n    \n# source, present (and past, if we wanted)\n\n\nv\n \n=\n \nTEVars\n(\nTf\n,\n \nTpp\n,\n \nSpp\n)\n\n\n\n\n\nTEVars([1], [2, 3], [4], Int64[])\n\n\n\n\n\n\nHyper-rectangles by subdivision of axes (\n\u03f5::Int\n)\n\n\nFirst, we use an integer number of subdivisions along each axis of the delay embedding when partitioning.\n\n\n\u03f5s\n \n=\n \n1\n:\n2\n:\n50\n \n# integer number of subdivisions along each axis of the embedding\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nvars\n \n=\n \nTEVars\n([\n1\n],\n \n[\n2\n,\n \n3\n],\n \n[\n4\n])\n\n\nestimator\n \n=\n \nVisitationFrequency\n()\n\n\n\nfor\n \n(\ni\n,\n \n\u03f5\n)\n \nin\n \nenumerate\n(\n\u03f5s\n)\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n\nend\n\n\n\np\n \n=\n \nplot\n(\n\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\np\n,\n \n\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\np\n,\n \n# subdivisions along each axis\n)\n\n\nylabel!\n(\np\n,\n \nTransfer entropy (bits)\n)\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n10\n\n\n\n\n\n\n20\n\n\n\n\n\n\n30\n\n\n\n\n\n\n40\n\n\n\n\n\n\n50\n\n\n\n\n\n\n0.000\n\n\n\n\n\n\n0.025\n\n\n\n\n\n\n0.050\n\n\n\n\n\n\n0.075\n\n\n\n\n\n\n0.100\n\n\n\n\n\n\n# subdivisions along each axis\n\n\n\n\n\n\nTransfer entropy (bits)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTE(x -\n y)\n\n\n\n\n\n\n\n\nTE(y -\n x)\n\n\n\n\n\n\n\n\n\nHyper-cubes of fixed size (\n\u03f5::Float\n)\n\n\nWe do precisely the same, but use fixed-width hyper-cube bins. The values of the logistic map take values on \n[0, 1]\n, so using bins width edge lengths \n0.1\n should give a covering corresponding to using \n10\n subdivisions along each axis of the delay embedding. We let \n\u03f5\n take values on \n[0.05, 0.5]\n.\n\n\n\u03f5s\n \n=\n \n0.02\n:\n0.02\n:\n0.5\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nvars\n \n=\n \nTEVars\n([\n1\n],\n \n[\n2\n,\n \n3\n],\n \n[\n4\n])\n\n\nestimator\n \n=\n \nVisitationFrequency\n()\n\n\n\nfor\n \n(\ni\n,\n \n\u03f5\n)\n \nin\n \nenumerate\n(\n\u03f5s\n)\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n\nend\n\n\n\nplot\n(\n\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\n\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\nHypercube edge length\n)\n\n\nylabel!\n(\nTransfer entropy (bits)\n)\n\n\nxflip!\n()\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.1\n\n\n\n\n\n\n0.2\n\n\n\n\n\n\n0.3\n\n\n\n\n\n\n0.4\n\n\n\n\n\n\n0.5\n\n\n\n\n\n\n0.00\n\n\n\n\n\n\n0.01\n\n\n\n\n\n\n0.02\n\n\n\n\n\n\n0.03\n\n\n\n\n\n\n0.04\n\n\n\n\n\n\n0.05\n\n\n\n\n\n\nHypercube edge length\n\n\n\n\n\n\nTransfer entropy (bits)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTE(x -\n y)\n\n\n\n\n\n\n\n\nTE(y -\n x)\n\n\n\n\n\n\n\n\n\nHyper-rectangles of fixed size (\n\u03f5::Vector{Float}\n)\n\n\nIt is also possible to use hyper-rectangles, by specifying the edge lengths along each coordinate axis of the delay embedding. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector of edge lengths\n\n\n# Define slightly different edge lengths along each axis\n\n\n\u03f5s_x1\n \n=\n \nLinRange\n(\n0.05\n,\n \n0.5\n,\n \n10\n)\n\n\n\u03f5s_x2\n \n=\n \nLinRange\n(\n0.02\n,\n \n0.4\n,\n \n10\n)\n\n\n\u03f5s_x3\n \n=\n \nLinRange\n(\n0.08\n,\n \n0.6\n,\n \n10\n)\n\n\n\u03f5s_x4\n \n=\n \nLinRange\n(\n0.10\n,\n \n0.3\n,\n \n10\n)\n\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s_x1\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s_x1\n))\n\n\nvars\n \n=\n \nTEVars\n([\n1\n],\n \n[\n2\n,\n \n3\n],\n \n[\n4\n])\n\n\nestimator\n \n=\n \nVisitationFrequency\n()\n\n\n\n\nmean_\u03f5s\n \n=\n \nzeros\n(\n10\n)\n\n\n\nfor\n \ni\n \n\u2208\n \n1\n:\n10\n\n    \n\u03f5\n \n=\n \n[\n\u03f5s_x1\n[\ni\n],\n \n\u03f5s_x2\n[\ni\n],\n \n\u03f5s_x3\n[\ni\n],\n \n\u03f5s_x4\n[\ni\n]]\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n\n    \n# Store average edge length (for plotting)\n\n    \nmean_\u03f5s\n[\ni\n]\n \n=\n \nmean\n(\n\u03f5\n)\n\n\nend\n\n\n\nplot\n(\nmean_\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\nmean_\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\nAverage hypercube edge length\n)\n\n\nylabel!\n(\nTransfer entropy (bits)\n)\n\n\nxflip!\n()\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.1\n\n\n\n\n\n\n0.2\n\n\n\n\n\n\n0.3\n\n\n\n\n\n\n0.4\n\n\n\n\n\n\n0.00\n\n\n\n\n\n\n0.01\n\n\n\n\n\n\n0.02\n\n\n\n\n\n\n0.03\n\n\n\n\n\n\n0.04\n\n\n\n\n\n\n0.05\n\n\n\n\n\n\nAverage hypercube edge length\n\n\n\n\n\n\nTransfer entropy (bits)\n\n\n\n\n\n\n\n\n\n\n\n\nTE(x -\n y)\n\n\n\n\n\n\n\n\nTE(y -\n x)\n\n\n\n\n\n\n\n\nHyper-rectangles by variable-width subdivision of axes (\n\u03f5::Vector{Int}\n)\n\n\nAnother way to construct hyper-rectangles is to subdivide each coordinate axis into segments of equal length. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector providing the number of subdivisions we want along each axis.\n\n\n# Define different number of subdivisions along each axis.\n\n\n\u03f5s\n \n=\n \n3\n:\n50\n\n\nmean_\u03f5s\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nvars\n \n=\n \nTEVars\n([\n1\n],\n \n[\n2\n,\n \n3\n],\n \n[\n4\n])\n\n\n\nfor\n \n(\ni\n,\n \n\u03f5\u1d62\n)\n \n\u2208\n \nenumerate\n(\n\u03f5s\n)\n\n    \n\u03f5\n \n=\n \n[\n\u03f5\u1d62\n \n-\n \n1\n,\n \n\u03f5\u1d62\n,\n \n\u03f5\u1d62\n,\n \n\u03f5\u1d62\n \n+\n \n1\n]\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nRectangularBinning\n(\n\u03f5\n),\n \nestimator\n)\n\n\n    \n# Store average number of subdivisions for plotting\n\n    \nmean_\u03f5s\n[\ni\n]\n \n=\n \nmean\n(\n\u03f5\n)\n\n\nend\n\n\n\nplot\n(\nmean_\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\nmean_\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\nAverage number of subdivisions along the embedding axes\n)\n\n\nylabel!\n(\nTransfer entropy (bits)\n)\n\n\n\n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n  \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n20\n\n\n\n\n\n\n30\n\n\n\n\n\n\n40\n\n\n\n\n\n\n50\n\n\n\n\n\n\n0.15\n\n\n\n\n\n\n0.20\n\n\n\n\n\n\n0.25\n\n\n\n\n\n\n0.30\n\n\n\n\n\n\n0.35\n\n\n\n\n\n\nAverage number of subdivisions along the embedding axes\n\n\n\n\n\n\nTransfer entropy (bits)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTE(x -\n y)\n\n\n\n\n\n\n\n\nTE(y -\n x)", 
            "title": "Effect of discretization scheme"
        }, 
        {
            "location": "/transferentropy/examples_TE_different_partitionings/#effect-of-dicretization-scheme-on-transfer-entropy-estimates", 
            "text": "", 
            "title": "Effect of dicretization scheme on transfer entropy estimates"
        }, 
        {
            "location": "/transferentropy/examples_TE_different_partitionings/#different-ways-of-partitioning", 
            "text": "The  TransferOperatorGrid  and  VisitationFrequency  transfer entropy estimators both operate on partitions on the delay reconstructions. Below, we demonstrate the four different ways of discretizing the state space.  First, let's create some example time series, embed them and organize the computation of marginal probabilities.  x   =   cumsum ( rand ( 300 ))  y   =   sin . ( cumsum ( rand ( 300 ))) * 0.3   .+   x  \u03c4   =   1   # embedding lag  \u03bd   =   1   # forward prediction lag  E_xtoy   =   customembed ( Dataset ( x ,   y ),   Positions ([ 2 ,   2 ,   2 ,   1 ]),   Lags ([ \u03bd ,   0 ,   - \u03c4 ,   0 ]))  E_ytox   =   customembed ( Dataset ( y ,   x ),   Positions ([ 2 ,   2 ,   2 ,   1 ]),   Lags ([ \u03bd ,   0 ,   - \u03c4 ,   0 ]))  # Organize marginals  Tf   =   [ 1 ]       # target, future  Tpp   =   [ 2 ,   3 ]   # target, present and past  Spp   =   [ 4 ]      # source, present (and past, if we wanted)  v   =   TEVars ( Tf ,   Tpp ,   Spp )   TEVars([1], [2, 3], [4], Int64[])", 
            "title": "Different ways of partitioning"
        }, 
        {
            "location": "/transferentropy/examples_TE_different_partitionings/#hyper-rectangles-by-subdivision-of-axes-int", 
            "text": "First, we use an integer number of subdivisions along each axis of the delay embedding when partitioning.  \u03f5s   =   1 : 2 : 50   # integer number of subdivisions along each axis of the embedding  te_estimates_xtoy   =   zeros ( length ( \u03f5s ))  te_estimates_ytox   =   zeros ( length ( \u03f5s ))  vars   =   TEVars ([ 1 ],   [ 2 ,   3 ],   [ 4 ])  estimator   =   VisitationFrequency ()  for   ( i ,   \u03f5 )   in   enumerate ( \u03f5s ) \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   RectangularBinning ( \u03f5 ),   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   RectangularBinning ( \u03f5 ),   estimator )  end  p   =   plot ( \u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( p ,   \u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( p ,   # subdivisions along each axis )  ylabel! ( p ,   Transfer entropy (bits) )     \n   \n     \n      \n   \n     \n      \n   \n     \n                              0    10    20    30    40    50    0.000    0.025    0.050    0.075    0.100    # subdivisions along each axis    Transfer entropy (bits)         TE(x -  y)     TE(y -  x)", 
            "title": "Hyper-rectangles by subdivision of axes (\u03f5::Int)"
        }, 
        {
            "location": "/transferentropy/examples_TE_different_partitionings/#hyper-cubes-of-fixed-size-float", 
            "text": "We do precisely the same, but use fixed-width hyper-cube bins. The values of the logistic map take values on  [0, 1] , so using bins width edge lengths  0.1  should give a covering corresponding to using  10  subdivisions along each axis of the delay embedding. We let  \u03f5  take values on  [0.05, 0.5] .  \u03f5s   =   0.02 : 0.02 : 0.5  te_estimates_xtoy   =   zeros ( length ( \u03f5s ))  te_estimates_ytox   =   zeros ( length ( \u03f5s ))  vars   =   TEVars ([ 1 ],   [ 2 ,   3 ],   [ 4 ])  estimator   =   VisitationFrequency ()  for   ( i ,   \u03f5 )   in   enumerate ( \u03f5s ) \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   RectangularBinning ( \u03f5 ),   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   RectangularBinning ( \u03f5 ),   estimator )  end  plot ( \u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( \u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( Hypercube edge length )  ylabel! ( Transfer entropy (bits) )  xflip! ()     \n   \n     \n      \n   \n     \n      \n   \n     \n                              0.1    0.2    0.3    0.4    0.5    0.00    0.01    0.02    0.03    0.04    0.05    Hypercube edge length    Transfer entropy (bits)         TE(x -  y)     TE(y -  x)", 
            "title": "Hyper-cubes of fixed size (\u03f5::Float)"
        }, 
        {
            "location": "/transferentropy/examples_TE_different_partitionings/#hyper-rectangles-of-fixed-size-vectorfloat", 
            "text": "It is also possible to use hyper-rectangles, by specifying the edge lengths along each coordinate axis of the delay embedding. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector of edge lengths  # Define slightly different edge lengths along each axis  \u03f5s_x1   =   LinRange ( 0.05 ,   0.5 ,   10 )  \u03f5s_x2   =   LinRange ( 0.02 ,   0.4 ,   10 )  \u03f5s_x3   =   LinRange ( 0.08 ,   0.6 ,   10 )  \u03f5s_x4   =   LinRange ( 0.10 ,   0.3 ,   10 )  te_estimates_xtoy   =   zeros ( length ( \u03f5s_x1 ))  te_estimates_ytox   =   zeros ( length ( \u03f5s_x1 ))  vars   =   TEVars ([ 1 ],   [ 2 ,   3 ],   [ 4 ])  estimator   =   VisitationFrequency ()  mean_\u03f5s   =   zeros ( 10 )  for   i   \u2208   1 : 10 \n     \u03f5   =   [ \u03f5s_x1 [ i ],   \u03f5s_x2 [ i ],   \u03f5s_x3 [ i ],   \u03f5s_x4 [ i ]] \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   RectangularBinning ( \u03f5 ),   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   RectangularBinning ( \u03f5 ),   estimator ) \n\n     # Store average edge length (for plotting) \n     mean_\u03f5s [ i ]   =   mean ( \u03f5 )  end  plot ( mean_\u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( mean_\u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( Average hypercube edge length )  ylabel! ( Transfer entropy (bits) )  xflip! ()     \n   \n     \n      \n   \n     \n      \n   \n     \n                            0.1    0.2    0.3    0.4    0.00    0.01    0.02    0.03    0.04    0.05    Average hypercube edge length    Transfer entropy (bits)       TE(x -  y)     TE(y -  x)", 
            "title": "Hyper-rectangles of fixed size (\u03f5::Vector{Float})"
        }, 
        {
            "location": "/transferentropy/examples_TE_different_partitionings/#hyper-rectangles-by-variable-width-subdivision-of-axes-vectorint", 
            "text": "Another way to construct hyper-rectangles is to subdivide each coordinate axis into segments of equal length. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector providing the number of subdivisions we want along each axis.  # Define different number of subdivisions along each axis.  \u03f5s   =   3 : 50  mean_\u03f5s   =   zeros ( length ( \u03f5s ))  te_estimates_xtoy   =   zeros ( length ( \u03f5s ))  te_estimates_ytox   =   zeros ( length ( \u03f5s ))  vars   =   TEVars ([ 1 ],   [ 2 ,   3 ],   [ 4 ])  for   ( i ,   \u03f5\u1d62 )   \u2208   enumerate ( \u03f5s ) \n     \u03f5   =   [ \u03f5\u1d62   -   1 ,   \u03f5\u1d62 ,   \u03f5\u1d62 ,   \u03f5\u1d62   +   1 ] \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   RectangularBinning ( \u03f5 ),   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   RectangularBinning ( \u03f5 ),   estimator ) \n\n     # Store average number of subdivisions for plotting \n     mean_\u03f5s [ i ]   =   mean ( \u03f5 )  end  plot ( mean_\u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( mean_\u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( Average number of subdivisions along the embedding axes )  ylabel! ( Transfer entropy (bits) )     \n   \n     \n      \n   \n     \n      \n   \n     \n                            10    20    30    40    50    0.15    0.20    0.25    0.30    0.35    Average number of subdivisions along the embedding axes    Transfer entropy (bits)         TE(x -  y)     TE(y -  x)", 
            "title": "Hyper-rectangles by variable-width subdivision of axes (\u03f5::Vector{Int})"
        }, 
        {
            "location": "/crossmappings/ccm/overview/", 
            "text": "Overview: convergent cross mapping (CCM)\n\n\nThe convergent cross mapping (CCM) algorithm \n1\n measures how well a \ndelay embedding\n of a putative response time series is able to predict a putative driver time series. The prediction skill is then taken as the unidirectional coupling strength from the driver to the response time series. One may also compute lagged CCM \n2\n by tuning the \n\u03bd\n parameter.\n\n\nThe following cross mapping estimators are implemented.\n\n\n\n\ncrossmap\n. Cross map for time series of fixed length. This algorithm can implicitly say something about convergence by adjusting the library size.\n\n\nconvergentcrossmap\n. Explicitly cross map over multiple time series lengths. This is the approach introduced in \n1\n.\n\n\n\n\n\n\nExample\n\n\nIn the animation below, we're cross mapping for increasing time series length for a \nundirectionally set of coupled Henon maps\n where \nx\n drives \ny\n. As expected from the underlying coupling, the cross mapping skill in the direction \nx \u2192 y\n converges at a higher value than in the opposite direction.\n\n\n\n\n\n\nRelated software\n\n\n\n\nCauseMap.jl\n also provides an implementation of the CCM algorithm, but this package has not been updated since 2015.\n\n\n\n\n\n\n\n\n\n\n\n\nSugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., \n Munch, S. (2012). Detecting causality in complex ecosystems. Science. \nhttps://doi.org/10.1126/science.1227079\n\n\n\n\n\n\nYe, H., Deyle, E. R., Gilarranz, L. J., \n Sugihara, G. (2015). Distinguishing time-delayed causal interactions using convergent cross mapping. Scientific Reports. \nhttps://doi.org/10.1038/srep14750", 
            "title": "Overview"
        }, 
        {
            "location": "/crossmappings/ccm/overview/#overview-convergent-cross-mapping-ccm", 
            "text": "The convergent cross mapping (CCM) algorithm  1  measures how well a  delay embedding  of a putative response time series is able to predict a putative driver time series. The prediction skill is then taken as the unidirectional coupling strength from the driver to the response time series. One may also compute lagged CCM  2  by tuning the  \u03bd  parameter.  The following cross mapping estimators are implemented.   crossmap . Cross map for time series of fixed length. This algorithm can implicitly say something about convergence by adjusting the library size.  convergentcrossmap . Explicitly cross map over multiple time series lengths. This is the approach introduced in  1 .", 
            "title": "Overview: convergent cross mapping (CCM)"
        }, 
        {
            "location": "/crossmappings/ccm/overview/#example", 
            "text": "In the animation below, we're cross mapping for increasing time series length for a  undirectionally set of coupled Henon maps  where  x  drives  y . As expected from the underlying coupling, the cross mapping skill in the direction  x \u2192 y  converges at a higher value than in the opposite direction.", 
            "title": "Example"
        }, 
        {
            "location": "/crossmappings/ccm/overview/#related-software", 
            "text": "CauseMap.jl  also provides an implementation of the CCM algorithm, but this package has not been updated since 2015.       Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M.,   Munch, S. (2012). Detecting causality in complex ecosystems. Science.  https://doi.org/10.1126/science.1227079    Ye, H., Deyle, E. R., Gilarranz, L. J.,   Sugihara, G. (2015). Distinguishing time-delayed causal interactions using convergent cross mapping. Scientific Reports.  https://doi.org/10.1038/srep14750", 
            "title": "Related software"
        }, 
        {
            "location": "/crossmappings/ccm/crossmapping/", 
            "text": "Cross mapping\n\n\nGiven two data series, the putative \ndriver\n and the putative \nresponse\n, the the \ncrossmap(driver, response; kwargs...)\n function computes how well a delay embedding of \nresponse\n predicts scalar values of \ndriver\n. This is the original cross mapping algorithm from Sugihara et al. \n1\n.\n\n\nTo perform lagged CCM analysis \n2\n as Ye et al., you can tune the \n\u03bd\n parameter.\n\n\n\n\nDocumentation\n\n\n#\n\n\nCrossMappings.crossmap\n \n \nFunction\n.\n\n\ncrossmap\n(\ndriver\n,\n \nresponse\n;\n\n    \ndim\n::\nInt\n \n=\n \n3\n,\n\n    \n\u03c4\n::\nInt\n \n=\n \n1\n,\n\n    \nlibsize\n::\nInt\n \n=\n \n10\n,\n\n    \nreplace\n::\nBool\n \n=\n \nfalse\n,\n\n    \nn_reps\n::\nInt\n \n=\n \n100\n,\n\n    \nsurr_func\n::\nFunction\n \n=\n \nrandomshuffle\n,\n\n    \nwhich_is_surr\n::\nSymbol\n \n=\n \n:\nnone\n,\n\n    \nexclusion_radius\n::\nInt\n \n=\n \n0\n,\n\n    \ntree_type\n \n=\n \nNearestNeighbors\n.\nKDTree\n,\n\n    \ndistance_metric\n \n=\n \nDistances\n.\nEuclidean\n(),\n\n    \ncorrespondence_measure\n \n=\n \nStatsBase\n.\ncor\n,\n\n    \n\u03bd\n::\nInt\n \n=\n \n0\n)\n\n\n\n\n\nAlgorithm\n\n\nCompute the cross mapping between a \ndriver\n series and a \nresponse\n series.\n\n\nArguments\n\n\n\n\ndriver\n: The data series representing the putative driver process.\n\n\nresponse\n: The data series representing the putative response process.\n\n\ndim\n: The dimension of the state space reconstruction (delay embedding)   constructed from the \nresponse\n series. Default is \ndim = 3\n.\n\n\n\u03c4\n: The embedding lag for the delay embedding constructed from \nresponse\n.   Default is \n\u03c4 = 1\n.\n\n\n\u03bd\n: The prediction lag to use when predicting scalar values of \ndriver\n   fromthe delay embedding of \nresponse\n.   \n\u03bd \n 0\n are forward lags (causal; \ndriver\n's past influences \nresponse\n's future),   and \n\u03bd \n 0\n are backwards lags (non-causal; \ndriver\n's' future influences   \nresponse\n's past). Adjust the prediction lag if you   want to performed lagged ccm   \n(Ye et al., 2015)\n.   Default is \n\u03bd = 0\n, as in   \nSugihara et al. (2012)\n.   \nNote: The sign of the lag \n\u03bd\n is organized to conform with the conventions in   \nTransferEntropy.jl\n, and is opposite to the convention used in the   \nrEDM\n package   (\nYe et al., 2016\n).\n\n\nlibsize\n: Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are \nn_reps\n)?\n\n\nn_reps\n: The number of times we draw a library of \nlibsize\n points from the   delay embedding of \nresponse\n and try to predict \ndriver\n values. Equivalently,   how many times do we cross map for this value of \nlibsize\n?   Default is \nn_reps = 100\n.\n\n\nreplace\n: Sample delay embedding points with replacement? Default is \nreplace = true\n.\n\n\nexclusion_radius\n: How many temporal neighbors of the delay embedding   point \nresponse_embedding(t)\n to exclude when searching for neighbors to   determine weights for predicting the scalar point \ndriver(t + \u03bd)\n.   Default is \nexclusion_radius = 0\n.\n\n\nwhich_is_surr\n: Which data series should be replaced by a surrogate   realization of the type given by \nsurr_type\n? Must be one of the   following: \n:response\n, \n:driver\n, \n:none\n, \n:both\n.   Default is \n:none\n.\n\n\nsurr_func\n: A valid surrogate function from TimeseriesSurrogates.jl.\n\n\ntree_type\n: The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either   \nBruteTree\n, \nKDTree\n or \nBallTree\n.\n\n\ndistance_metric\n: An instance of a \nMetric\n from Distances.jl. \nBallTree\n and \nBruteTree\n work with any \nMetric\n.   \nKDTree\n only works with the axis aligned metrics \nEuclidean\n, \nChebyshev\n,   \nMinkowski\n and \nCityblock\n. Default is \nmetric = Euclidean()\n \n(note the instantiation of the metric)\n.\n\n\ncorrespondence_measure\n: The function that computes the correspondence   between actual values of \ndriver\n and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is \ncorrespondence_measure = StatsBase.cor\n, which returns values on \n[-1, 1]\n[-1, 1]\n.   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of \n1\n1\n means perfect prediction.   \nSugihara et al. (2012)\n   also proposes to use the root mean square deviation, for which a value of \n0\n0\n would   be perfect prediction.\n\n\n\n\nReferences\n\n\nSugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079. \nhttp://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific Reports 5 (2015): 14750. \nhttps://www.nature.com/articles/srep14750\n\n\nYe, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version 0.4 7 (2016). \nhttps://cran.r-project.org/web/packages/rEDM/index.html\n\n\n\n\n\n\n\n\n\n\nSugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., \n Munch, S. (2012). Detecting causality in complex ecosystems. Science. \nhttps://doi.org/10.1126/science.1227079\n\n\n\n\n\n\nYe, H., Deyle, E. R., Gilarranz, L. J., \n Sugihara, G. (2015). Distinguishing time-delayed causal interactions using convergent cross mapping. Scientific Reports. \nhttps://doi.org/10.1038/srep14750", 
            "title": "Cross mapping"
        }, 
        {
            "location": "/crossmappings/ccm/crossmapping/#cross-mapping", 
            "text": "Given two data series, the putative  driver  and the putative  response , the the  crossmap(driver, response; kwargs...)  function computes how well a delay embedding of  response  predicts scalar values of  driver . This is the original cross mapping algorithm from Sugihara et al.  1 .  To perform lagged CCM analysis  2  as Ye et al., you can tune the  \u03bd  parameter.", 
            "title": "Cross mapping"
        }, 
        {
            "location": "/crossmappings/ccm/crossmapping/#documentation", 
            "text": "#  CrossMappings.crossmap     Function .  crossmap ( driver ,   response ; \n     dim :: Int   =   3 , \n     \u03c4 :: Int   =   1 , \n     libsize :: Int   =   10 , \n     replace :: Bool   =   false , \n     n_reps :: Int   =   100 , \n     surr_func :: Function   =   randomshuffle , \n     which_is_surr :: Symbol   =   : none , \n     exclusion_radius :: Int   =   0 , \n     tree_type   =   NearestNeighbors . KDTree , \n     distance_metric   =   Distances . Euclidean (), \n     correspondence_measure   =   StatsBase . cor , \n     \u03bd :: Int   =   0 )   Algorithm  Compute the cross mapping between a  driver  series and a  response  series.  Arguments   driver : The data series representing the putative driver process.  response : The data series representing the putative response process.  dim : The dimension of the state space reconstruction (delay embedding)   constructed from the  response  series. Default is  dim = 3 .  \u03c4 : The embedding lag for the delay embedding constructed from  response .   Default is  \u03c4 = 1 .  \u03bd : The prediction lag to use when predicting scalar values of  driver    fromthe delay embedding of  response .    \u03bd   0  are forward lags (causal;  driver 's past influences  response 's future),   and  \u03bd   0  are backwards lags (non-causal;  driver 's' future influences    response 's past). Adjust the prediction lag if you   want to performed lagged ccm    (Ye et al., 2015) .   Default is  \u03bd = 0 , as in    Sugihara et al. (2012) .    Note: The sign of the lag  \u03bd  is organized to conform with the conventions in    TransferEntropy.jl , and is opposite to the convention used in the    rEDM  package   ( Ye et al., 2016 ).  libsize : Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are  n_reps )?  n_reps : The number of times we draw a library of  libsize  points from the   delay embedding of  response  and try to predict  driver  values. Equivalently,   how many times do we cross map for this value of  libsize ?   Default is  n_reps = 100 .  replace : Sample delay embedding points with replacement? Default is  replace = true .  exclusion_radius : How many temporal neighbors of the delay embedding   point  response_embedding(t)  to exclude when searching for neighbors to   determine weights for predicting the scalar point  driver(t + \u03bd) .   Default is  exclusion_radius = 0 .  which_is_surr : Which data series should be replaced by a surrogate   realization of the type given by  surr_type ? Must be one of the   following:  :response ,  :driver ,  :none ,  :both .   Default is  :none .  surr_func : A valid surrogate function from TimeseriesSurrogates.jl.  tree_type : The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either    BruteTree ,  KDTree  or  BallTree .  distance_metric : An instance of a  Metric  from Distances.jl.  BallTree  and  BruteTree  work with any  Metric .    KDTree  only works with the axis aligned metrics  Euclidean ,  Chebyshev ,    Minkowski  and  Cityblock . Default is  metric = Euclidean()   (note the instantiation of the metric) .  correspondence_measure : The function that computes the correspondence   between actual values of  driver  and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is  correspondence_measure = StatsBase.cor , which returns values on  [-1, 1] [-1, 1] .   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of  1 1  means perfect prediction.    Sugihara et al. (2012)    also proposes to use the root mean square deviation, for which a value of  0 0  would   be perfect prediction.   References  Sugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079.  http://science.sciencemag.org/content/early/2012/09/19/science.1227079  Ye, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific Reports 5 (2015): 14750.  https://www.nature.com/articles/srep14750  Ye, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version 0.4 7 (2016).  https://cran.r-project.org/web/packages/rEDM/index.html      Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M.,   Munch, S. (2012). Detecting causality in complex ecosystems. Science.  https://doi.org/10.1126/science.1227079    Ye, H., Deyle, E. R., Gilarranz, L. J.,   Sugihara, G. (2015). Distinguishing time-delayed causal interactions using convergent cross mapping. Scientific Reports.  https://doi.org/10.1038/srep14750", 
            "title": "Documentation"
        }, 
        {
            "location": "/crossmappings/ccm/convergentcrossmapping/", 
            "text": "Cross map over multiple time series lengths\n\n\nTo perform a convergent cross map analysis as in \n1\n one can apply the \ncrossmap\n functions on time series of increasing length. The \nconvergentcrossmap(driver, response, timeserieslengths; kwargs...)\n function does so by applying \ncrossmap\n for each time series length in \ntimeserieslengths\n, where time windows always start at the first data point.\n\n\n\n\nDocumentation\n\n\n#\n\n\nCrossMappings.convergentcrossmap\n \n \nFunction\n.\n\n\nconvergentcrossmap\n(\ndriver\n,\n\n        \nresponse\n,\n\n        \ntimeseries_lengths\n;\n\n        \nsummarise\n::\nBool\n \n=\n \ntrue\n,\n\n        \naverage_measure\n::\nSymbol\n \n=\n \n:\nmedian\n,\n\n        \nuncertainty_measure\n::\nSymbol\n \n=\n \n:\nquantile\n,\n\n        \nquantiles\n \n=\n \n[\n0.327\n,\n \n0.673\n],\n\n        \nkwargs\n...\n)\n\n\n\n\n\nAlgorithm\n\n\nCompute the cross mapping between a \ndriver\n series and a \nresponse\n series over different \ntimeseries_lengths\n. If \nsummarise = true\n, then call \nccm_with_summary\n. If \nsummarise = false\n, then call \nccm\n (returns raw crossmap skills).\n\n\nArguments\n\n\n\n\ndriver\n: The data series representing the putative driver process.\n\n\nresponse\n: The data series representing the putative response process.\n\n\ntimeseries_lengths\n: Time series length(s) for which to compute the   cross mapping(s).\n\n\n\n\nSummary keyword arguments\n\n\n\n\nsummarise\n: Should cross map skills be summarised for each time series length?   Default is \nsummarise = true\n.\n\n\naverage_measure\n: Either \n:median\n or \n:mean\n. Default is \n:median\n.\n\n\nuncertainty_measure\n: Either \n:quantile\n or \n:std\n. Default is \n:quantile\n.\n\n\nquantiles\n: Compute uncertainty over quantile(s) if \nuncertainty_measure\n   is \n:quantile\n. Default is \n[0.327, 0.673]\n, roughly corresponding to 1s for   normally distributed data.\n\n\n\n\nKeyword arguments to \ncrossmap\n\n\n\n\ndim\n: The dimension of the state space reconstruction (delay embedding)   constructed from the \nresponse\n series. Default is \ndim = 3\n.\n\n\n\u03c4\n: The embedding lag for the delay embedding constructed from \nresponse\n.   Default is \n\u03c4 = 1\n.\n\n\n\u03bd\n: The prediction lag to use when predicting scalar values of \ndriver\n   fromthe delay embedding of \nresponse\n.   \n\u03bd \n 0\n are forward lags (causal; \ndriver\n's past influences \nresponse\n's future),   and \n\u03bd \n 0\n are backwards lags (non-causal; \ndriver\n's' future influences   \nresponse\n's past). Adjust the prediction lag if you   want to performed lagged ccm   \n(Ye et al., 2015)\n.   Default is \n\u03bd = 0\n, as in   \nSugihara et al. (2012)\n.   \nNote: The sign of the lag \n\u03bd\n is organized to conform with the conventions in   \nTransferEntropy.jl\n, and is opposite to the convention used in the   \nrEDM\n package   (\nYe et al., 2016\n).\n\n\nlibsize\n: Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are \nn_reps\n)?\n\n\nn_reps\n: The number of times we draw a library of \nlibsize\n points from the   delay embedding of \nresponse\n and try to predict \ndriver\n values. Equivalently,   how many times do we cross map for this value of \nlibsize\n?   Default is \nn_reps = 100\n.\n\n\nreplace\n: Sample delay embedding points with replacement? Default is \nreplace = true\n.\n\n\nexclusion_radius\n: How many temporal neighbors of the delay embedding   point \nresponse_embedding(t)\n to exclude when searching for neighbors to   determine weights for predicting the scalar point \ndriver(t + \u03bd)\n.   Default is \nexclusion_radius = 0\n.\n\n\nwhich_is_surr\n: Which data series should be replaced by a surrogate   realization of the type given by \nsurr_type\n? Must be one of the   following: \n:response\n, \n:driver\n, \n:none\n, \n:both\n.   Default is \n:none\n.\n\n\nsurr_func\n: A valid surrogate function from TimeseriesSurrogates.jl.\n\n\ntree_type\n: The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either   \nBruteTree\n, \nKDTree\n or \nBallTree\n.\n\n\ndistance_metric\n: An instance of a \nMetric\n from Distances.jl. \nBallTree\n and \nBruteTree\n work with any \nMetric\n.   \nKDTree\n only works with the axis aligned metrics \nEuclidean\n, \nChebyshev\n,   \nMinkowski\n and \nCityblock\n. Default is \nmetric = Euclidean()\n \n(note the instantiation of the metric)\n.\n\n\ncorrespondence_measure\n: The function that computes the correspondence   between actual values of \ndriver\n and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is \ncorrespondence_measure = StatsBase.cor\n, which returns values on \n[-1, 1]\n[-1, 1]\n.   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of \n1\n1\n means perfect prediction.   \nSugihara et al. (2012)\n   also proposes to use the root mean square deviation, for which a value of \n0\n0\n would   be perfect prediction.\n\n\n\n\nReferences\n\n\nSugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079. \nhttp://science.sciencemag.org/content/early/2012/09/19/science.1227079\n\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific Reports 5 (2015): 14750. \nhttps://www.nature.com/articles/srep14750\n\n\nYe, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version 0.4 7 (2016). \nhttps://cran.r-project.org/web/packages/rEDM/index.html\n\n\n\n\n\n\n\n\n\n\nSugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., \n Munch, S. (2012). Detecting causality in complex ecosystems. Science. \nhttps://doi.org/10.1126/science.1227079", 
            "title": "Converent cross mapping"
        }, 
        {
            "location": "/crossmappings/ccm/convergentcrossmapping/#cross-map-over-multiple-time-series-lengths", 
            "text": "To perform a convergent cross map analysis as in  1  one can apply the  crossmap  functions on time series of increasing length. The  convergentcrossmap(driver, response, timeserieslengths; kwargs...)  function does so by applying  crossmap  for each time series length in  timeserieslengths , where time windows always start at the first data point.", 
            "title": "Cross map over multiple time series lengths"
        }, 
        {
            "location": "/crossmappings/ccm/convergentcrossmapping/#documentation", 
            "text": "#  CrossMappings.convergentcrossmap     Function .  convergentcrossmap ( driver , \n         response , \n         timeseries_lengths ; \n         summarise :: Bool   =   true , \n         average_measure :: Symbol   =   : median , \n         uncertainty_measure :: Symbol   =   : quantile , \n         quantiles   =   [ 0.327 ,   0.673 ], \n         kwargs ... )   Algorithm  Compute the cross mapping between a  driver  series and a  response  series over different  timeseries_lengths . If  summarise = true , then call  ccm_with_summary . If  summarise = false , then call  ccm  (returns raw crossmap skills).  Arguments   driver : The data series representing the putative driver process.  response : The data series representing the putative response process.  timeseries_lengths : Time series length(s) for which to compute the   cross mapping(s).   Summary keyword arguments   summarise : Should cross map skills be summarised for each time series length?   Default is  summarise = true .  average_measure : Either  :median  or  :mean . Default is  :median .  uncertainty_measure : Either  :quantile  or  :std . Default is  :quantile .  quantiles : Compute uncertainty over quantile(s) if  uncertainty_measure    is  :quantile . Default is  [0.327, 0.673] , roughly corresponding to 1s for   normally distributed data.   Keyword arguments to  crossmap   dim : The dimension of the state space reconstruction (delay embedding)   constructed from the  response  series. Default is  dim = 3 .  \u03c4 : The embedding lag for the delay embedding constructed from  response .   Default is  \u03c4 = 1 .  \u03bd : The prediction lag to use when predicting scalar values of  driver    fromthe delay embedding of  response .    \u03bd   0  are forward lags (causal;  driver 's past influences  response 's future),   and  \u03bd   0  are backwards lags (non-causal;  driver 's' future influences    response 's past). Adjust the prediction lag if you   want to performed lagged ccm    (Ye et al., 2015) .   Default is  \u03bd = 0 , as in    Sugihara et al. (2012) .    Note: The sign of the lag  \u03bd  is organized to conform with the conventions in    TransferEntropy.jl , and is opposite to the convention used in the    rEDM  package   ( Ye et al., 2016 ).  libsize : Among how many delay embedding points should we sample time indices   and look for nearest neighbours at each cross mapping realization (of which there   are  n_reps )?  n_reps : The number of times we draw a library of  libsize  points from the   delay embedding of  response  and try to predict  driver  values. Equivalently,   how many times do we cross map for this value of  libsize ?   Default is  n_reps = 100 .  replace : Sample delay embedding points with replacement? Default is  replace = true .  exclusion_radius : How many temporal neighbors of the delay embedding   point  response_embedding(t)  to exclude when searching for neighbors to   determine weights for predicting the scalar point  driver(t + \u03bd) .   Default is  exclusion_radius = 0 .  which_is_surr : Which data series should be replaced by a surrogate   realization of the type given by  surr_type ? Must be one of the   following:  :response ,  :driver ,  :none ,  :both .   Default is  :none .  surr_func : A valid surrogate function from TimeseriesSurrogates.jl.  tree_type : The type of tree to build when looking for nearest neighbors.   Must be a tree type from NearestNeighbors.jl. For now, this is either    BruteTree ,  KDTree  or  BallTree .  distance_metric : An instance of a  Metric  from Distances.jl.  BallTree  and  BruteTree  work with any  Metric .    KDTree  only works with the axis aligned metrics  Euclidean ,  Chebyshev ,    Minkowski  and  Cityblock . Default is  metric = Euclidean()   (note the instantiation of the metric) .  correspondence_measure : The function that computes the correspondence   between actual values of  driver  and predicted values. Can be any   function returning a similarity measure between two vectors of values.   Default is  correspondence_measure = StatsBase.cor , which returns values on  [-1, 1] [-1, 1] .   In this case, any negative values are usually filtered out (interpreted as zero coupling) and   a value of  1 1  means perfect prediction.    Sugihara et al. (2012)    also proposes to use the root mean square deviation, for which a value of  0 0  would   be perfect prediction.   References  Sugihara, George, et al. \"Detecting causality in complex ecosystems.\" Science (2012): 1227079.  http://science.sciencemag.org/content/early/2012/09/19/science.1227079  Ye, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific Reports 5 (2015): 14750.  https://www.nature.com/articles/srep14750  Ye, H., et al. \"rEDM: Applications of empirical dynamic modeling from time series.\" R Package Version 0.4 7 (2016).  https://cran.r-project.org/web/packages/rEDM/index.html      Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M.,   Munch, S. (2012). Detecting causality in complex ecosystems. Science.  https://doi.org/10.1126/science.1227079", 
            "title": "Documentation"
        }, 
        {
            "location": "/algorithms/joint_distance_distribution/", 
            "text": "Distribution of joint distances\n\n\n#\n\n\nCausalityTools.joint_distance_distribution\n \n \nMethod\n.\n\n\njoint_distance_distribution\n(\nsource\n,\n \ntarget\n;\n\n    \ndistance_metric\n \n=\n \nSqEuclidean\n(),\n \n    \nB\n::\nInt\n \n=\n \n10\n,\n \n    \nD\n::\nInt\n \n=\n \n2\n,\n \n\u03c4\n::\nInt\n \n=\n \n1\n)\n \n-\n \nVector\n{\nFloat64\n}\n\n\n\n\n\nCompute the joint distance distribution [1] from \nsource\n to \ntarget\n using  the provided \ndistance_metric\n, with \nB\n controlling the number of subintervals,  \nD\n the embedding dimension and \n\u03c4\n the embedding lag.\n\n\nExample\n\n\nx\n,\n \ny\n \n=\n \nrand\n(\n1000\n),\n \nrand\n(\n1000\n)\n\n\n\njdd\n \n=\n \njoint_distance_distribution\n(\nx\n,\n \ny\n)\n\n\n\n\n\nKeyword arguments\n\n\n\n\ndistance_metric::Metric\n: An instance of a valid distance metric from \nDistances.jl\n.    Defaults to \nSqEuclidean()\n.\n\n\nB\n: The number of equidistant subintervals to divide the interval \n[0, 1]\n into   when comparing the normalised distances.\n\n\nD\n: Embedding dimension.\n\n\n\u03c4\n: Embedding delay.\n\n\n\n\nReferences\n\n\n[1] Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\nsource\n\n\n\n\nHypothesis test on the joint distance distribution\n\n\nFor the joint distance distribution to indicate a causal influence, it must be significantly  skewed towards positive values. \n\n\nProviding the \nOneSampleTTest\n type as the first  argument yields a one sample t-test on the joint distance distribution. From this test, you can extract p-values and obtain  confidence intervals like in \nHypothesisTests.jl\n as usual.\n\n\n#\n\n\nCausalityTools.joint_distance_distribution\n \n \nMethod\n.\n\n\njoint_distance_distribution\n(\ntest\n::\nOneSampleTTest\n,\n \nsource\n,\n \ntarget\n,\n \n    \ndistance_metric\n \n=\n \nSqEuclidean\n(),\n \n    \nB\n::\nInt\n \n=\n \n10\n,\n \n    \nD\n::\nInt\n \n=\n \n2\n,\n \n\u03c4\n::\nInt\n \n=\n \n1\n,\n \n    \n\u03bc0\n \n=\n \n0.0\n)\n \n-\n \nOneSampleTTest\n\n\n\n\n\nPerform a one sample t-test to check that the joint distance distribution [1]  computed from \nsource\n to \ntarget\n is biased towards positive values, using the null  hypothesis that the mean of the distribution is \n\u03bc0\n.\n\n\nThe interpretation of the t-test is that if we can reject the null, then the  joint distance distribution is biased towards positive values, and then there  exists an underlying coupling from \nsource\n to \ntarget\n. \n\n\nExample\n\n\nx\n,\n \ny\n \n=\n \nrand\n(\n1000\n),\n \nrand\n(\n1000\n)\n\n\n\njdd\n \n=\n \njoint_distance_distribution\n(\nOneSampleTTest\n,\n \nx\n,\n \ny\n)\n\n\n\n\n\nwhich gives \n\n\nOne\n \nsample\n \nt\n-\ntest\n\n\n-----------------\n\n\nPopulation\n \ndetails\n:\n\n    \nparameter\n \nof\n \ninterest\n:\n   \nMean\n\n    \nvalue\n \nunder\n \nh_0\n:\n         \n0.0\n\n    \npoint\n \nestimate\n:\n          \n0.06361857324022721\n\n    \n95\n%\n \nconfidence\n \ninterval\n:\n \n(\n0.0185\n,\n \n0.1087\n)\n\n\n\nTest\n \nsummary\n:\n\n    \noutcome\n \nwith\n \n95\n%\n \nconfidence\n:\n \nreject\n \nh_0\n\n    \ntwo\n-\nsided\n \np\n-\nvalue\n:\n           \n0.0082\n\n\n\nDetails\n:\n\n    \nnumber\n \nof\n \nobservations\n:\n   \n20\n\n    \nt\n-\nstatistic\n:\n              \n2.9517208721082873\n\n    \ndegrees\n \nof\n \nfreedom\n:\n       \n19\n\n    \nempirical\n \nstandard\n \nerror\n:\n \n0.0215530451545668\n\n\n\n\n\nThe lower bound of the confidence interval for the mean of the joint  distance distribution is \n0.0185\n at confidence level \n\u03b1 = 0.05\n. The  meaning that the test falsely detected causality from \nx\n to \ny\n between these two random time series. To get the confidence intervals at confidence level \n\u03b1\n, use \nconfinf(jdd, \u03b1)\n. If you just want the  p-value at 95% confidence, use \npvalue(jdd, tail = :left)\n\n\nKeyword arguments\n\n\n\n\ndistance_metric::Metric\n: An instance of a valid distance metric from \nDistances.jl\n.    Defaults to \nSqEuclidean()\n.\n\n\nB\n: The number of equidistant subintervals to divide the interval \n[0, 1]\n into   when comparing the normalised distances.\n\n\nD\n: Embedding dimension.\n\n\n\u03c4\n: Embedding delay.\n\n\n\u03bc0\n: The hypothetical mean value of the joint distance distribution if there    is no coupling between \nx\n and \ny\n (default is \n\u03bc0 = 0.0\n).\n\n\n\n\nReferences\n\n\n[1] Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\nsource", 
            "title": "Joint distance distribution"
        }, 
        {
            "location": "/algorithms/joint_distance_distribution/#distribution-of-joint-distances", 
            "text": "#  CausalityTools.joint_distance_distribution     Method .  joint_distance_distribution ( source ,   target ; \n     distance_metric   =   SqEuclidean (),  \n     B :: Int   =   10 ,  \n     D :: Int   =   2 ,   \u03c4 :: Int   =   1 )   -   Vector { Float64 }   Compute the joint distance distribution [1] from  source  to  target  using  the provided  distance_metric , with  B  controlling the number of subintervals,   D  the embedding dimension and  \u03c4  the embedding lag.  Example  x ,   y   =   rand ( 1000 ),   rand ( 1000 )  jdd   =   joint_distance_distribution ( x ,   y )   Keyword arguments   distance_metric::Metric : An instance of a valid distance metric from  Distances.jl .    Defaults to  SqEuclidean() .  B : The number of equidistant subintervals to divide the interval  [0, 1]  into   when comparing the normalised distances.  D : Embedding dimension.  \u03c4 : Embedding delay.   References  [1] Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.  source", 
            "title": "Distribution of joint distances"
        }, 
        {
            "location": "/algorithms/joint_distance_distribution/#hypothesis-test-on-the-joint-distance-distribution", 
            "text": "For the joint distance distribution to indicate a causal influence, it must be significantly  skewed towards positive values.   Providing the  OneSampleTTest  type as the first  argument yields a one sample t-test on the joint distance distribution. From this test, you can extract p-values and obtain  confidence intervals like in  HypothesisTests.jl  as usual.  #  CausalityTools.joint_distance_distribution     Method .  joint_distance_distribution ( test :: OneSampleTTest ,   source ,   target ,  \n     distance_metric   =   SqEuclidean (),  \n     B :: Int   =   10 ,  \n     D :: Int   =   2 ,   \u03c4 :: Int   =   1 ,  \n     \u03bc0   =   0.0 )   -   OneSampleTTest   Perform a one sample t-test to check that the joint distance distribution [1]  computed from  source  to  target  is biased towards positive values, using the null  hypothesis that the mean of the distribution is  \u03bc0 .  The interpretation of the t-test is that if we can reject the null, then the  joint distance distribution is biased towards positive values, and then there  exists an underlying coupling from  source  to  target .   Example  x ,   y   =   rand ( 1000 ),   rand ( 1000 )  jdd   =   joint_distance_distribution ( OneSampleTTest ,   x ,   y )   which gives   One   sample   t - test  -----------------  Population   details : \n     parameter   of   interest :     Mean \n     value   under   h_0 :           0.0 \n     point   estimate :            0.06361857324022721 \n     95 %   confidence   interval :   ( 0.0185 ,   0.1087 )  Test   summary : \n     outcome   with   95 %   confidence :   reject   h_0 \n     two - sided   p - value :             0.0082  Details : \n     number   of   observations :     20 \n     t - statistic :                2.9517208721082873 \n     degrees   of   freedom :         19 \n     empirical   standard   error :   0.0215530451545668   The lower bound of the confidence interval for the mean of the joint  distance distribution is  0.0185  at confidence level  \u03b1 = 0.05 . The  meaning that the test falsely detected causality from  x  to  y  between these two random time series. To get the confidence intervals at confidence level  \u03b1 , use  confinf(jdd, \u03b1) . If you just want the  p-value at 95% confidence, use  pvalue(jdd, tail = :left)  Keyword arguments   distance_metric::Metric : An instance of a valid distance metric from  Distances.jl .    Defaults to  SqEuclidean() .  B : The number of equidistant subintervals to divide the interval  [0, 1]  into   when comparing the normalised distances.  D : Embedding dimension.  \u03c4 : Embedding delay.  \u03bc0 : The hypothetical mean value of the joint distance distribution if there    is no coupling between  x  and  y  (default is  \u03bc0 = 0.0 ).   References  [1] Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.  source", 
            "title": "Hypothesis test on the joint distance distribution"
        }, 
        {
            "location": "/syntax_overview/", 
            "text": "Syntax overview\n\n\nThis is an overview over the low-level functionality in the CausalityTools.jl package and its subpackages.\n\n\n\n\nDiscretization\n\n\n\n\nRectangularBinning\n. Instructions for using rectangular partitions.\n\n\nTriangulationBinning\n. Instructions for using triangulated partitions.\n\n\n\n\n\n\nTransfer operator estimation\n\n\n\n\ntransferoperator(points, binning_scheme::RectangularBinning)\n\n\ntransferoperator(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ExactIntersection)\n\n\ntransferoperator(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ApproximateIntersection)\n\n\n\n\n\n\nInvariant measure estimation\n\n\n\n\ninvariantmeasure(points, binning_scheme::RectangularBinning)\n\n\ninvariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ExactIntersection)\n\n\ninvariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ApproximateIntersection)\n\n\n\n\n\n\nTransfer entropy estimation\n\n\nThere are two estimators that compute transfer entropy by rectangular partitions. \n\n\n\n\ntransferentropy(pts, vars::TEVars, \u03f5::RectangularBinning, estimator::VisitationFrequency; b = 2)\n\n\ntransferentropy(pts, vars::TEVars, \u03f5::RectangularBinning, estimator::TransferOperatorGrid; b = 2)\n\n\n\n\nTo compute transfer entropy over triangulated partitions, the invariant measure over the  triangulation must be precomputed, using either \n\n\n\n\ninvariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ExactIntersection)\n or\n\n\ninvariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ApproximateIntersection)\n).\n\n\n\n\nThen we can superimpose rectangular grid over the triangulation and compute the transfer entropy.\n\n\n\n\ntransferentropy(\u03bc::AbstractTriangulationInvariantMeasure, vars::TEVars, binning_scheme::RectangularBinning; estimator = VisitationFrequency(), n::Int = 10000, b = 2)\n.\n\n\n\n\n\n\nCross mappings\n\n\n\n\ncrossmap\n.\n\n\nconvergentcrossmap\n.\n\n\n\n\n\n\nJoint distance distribution\n\n\n\n\njoint_distance_distribution", 
            "title": "Syntax overview"
        }, 
        {
            "location": "/syntax_overview/#syntax-overview", 
            "text": "This is an overview over the low-level functionality in the CausalityTools.jl package and its subpackages.", 
            "title": "Syntax overview"
        }, 
        {
            "location": "/syntax_overview/#discretization", 
            "text": "RectangularBinning . Instructions for using rectangular partitions.  TriangulationBinning . Instructions for using triangulated partitions.", 
            "title": "Discretization"
        }, 
        {
            "location": "/syntax_overview/#transfer-operator-estimation", 
            "text": "transferoperator(points, binning_scheme::RectangularBinning)  transferoperator(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ExactIntersection)  transferoperator(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ApproximateIntersection)", 
            "title": "Transfer operator estimation"
        }, 
        {
            "location": "/syntax_overview/#invariant-measure-estimation", 
            "text": "invariantmeasure(points, binning_scheme::RectangularBinning)  invariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ExactIntersection)  invariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ApproximateIntersection)", 
            "title": "Invariant measure estimation"
        }, 
        {
            "location": "/syntax_overview/#transfer-entropy-estimation", 
            "text": "There are two estimators that compute transfer entropy by rectangular partitions.    transferentropy(pts, vars::TEVars, \u03f5::RectangularBinning, estimator::VisitationFrequency; b = 2)  transferentropy(pts, vars::TEVars, \u03f5::RectangularBinning, estimator::TransferOperatorGrid; b = 2)   To compute transfer entropy over triangulated partitions, the invariant measure over the  triangulation must be precomputed, using either    invariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ExactIntersection)  or  invariantmeasure(pts, \u03f5::TriangulationBinning, simplex_intersection_type::ApproximateIntersection) ).   Then we can superimpose rectangular grid over the triangulation and compute the transfer entropy.   transferentropy(\u03bc::AbstractTriangulationInvariantMeasure, vars::TEVars, binning_scheme::RectangularBinning; estimator = VisitationFrequency(), n::Int = 10000, b = 2) .", 
            "title": "Transfer entropy estimation"
        }, 
        {
            "location": "/syntax_overview/#cross-mappings", 
            "text": "crossmap .  convergentcrossmap .", 
            "title": "Cross mappings"
        }, 
        {
            "location": "/syntax_overview/#joint-distance-distribution", 
            "text": "joint_distance_distribution", 
            "title": "Joint distance distribution"
        }, 
        {
            "location": "/example_systems/example_systems_overview/", 
            "text": "Dynamical systems library\n\n\n\n\nDiscrete systems\n\n\nThe following discrete systems are implemented:\n\n\n\n\nar1_unidir\n\n\nnonlinear3d\n\n\nlogistic2_unidir\n\n\nlogistic3\n\n\nlogistic4\n\n\nhenon2\n\n\nanishchenko1\n\n\n\n\n\n\nContinuous systems\n\n\nThe following continuous systems are implemented:\n\n\n\n\nlorenz_lorenz_bidir\n\n\nlorenz_lorenz_lorenz_bidir_forced\n\n\nlorenz_lorenz_lorenz_transitive\n\n\nrossler_rossler_bidir\n\n\nrossler_rossler_rossler_bidir_forced\n\n\nmediated_link\n\n\nrossler_lorenz\n\n\nchuacircuit_nscroll_sine", 
            "title": "Overview"
        }, 
        {
            "location": "/example_systems/example_systems_overview/#dynamical-systems-library", 
            "text": "", 
            "title": "Dynamical systems library"
        }, 
        {
            "location": "/example_systems/example_systems_overview/#discrete-systems", 
            "text": "The following discrete systems are implemented:   ar1_unidir  nonlinear3d  logistic2_unidir  logistic3  logistic4  henon2  anishchenko1", 
            "title": "Discrete systems"
        }, 
        {
            "location": "/example_systems/example_systems_overview/#continuous-systems", 
            "text": "The following continuous systems are implemented:   lorenz_lorenz_bidir  lorenz_lorenz_lorenz_bidir_forced  lorenz_lorenz_lorenz_transitive  rossler_rossler_bidir  rossler_rossler_rossler_bidir_forced  mediated_link  rossler_lorenz  chuacircuit_nscroll_sine", 
            "title": "Continuous systems"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/", 
            "text": "Discrete coupled dynamical systems\n\n\n\n\nAutoregressive order one 2D system\n\n\n#\n\n\nCausalityTools.Systems.ar1_unidir\n \n \nMethod\n.\n\n\nar1\n(\nu\u1d62\n,\n \na\u2081\n \n=\n \n0.90693\n,\n \nb\u2081\n \n=\n \n0.40693\n,\n \nc_xy\n \n=\n \n0.5\n,\n \n    \n\u03c3\n \n=\n \n0.40662\n)\n \n-\n \nDiscreteDynamicalSystem\n\n\n\n\n\nA bivariate, order one autoregressive model, where \nx \\to y\nx \\to y\n [1].\n\n\nEquations of motion\n\n\n\n\n\n\\begin{aligned}\ndx \n= a_1 x + \\xi_{1} \\\\\ndy \n= b_1 y - c_{xy} x + \\xi_{2},\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\ndx &= a_1 x + \\xi_{1} \\\\\ndy &= b_1 y - c_{xy} x + \\xi_{2},\n\\end{aligned}\n\n\n\n\n\nwhere \n\\xi_{1}\n\\xi_{1}\n and \n\\xi_{2}\n\\xi_{2}\n are drawn from normal distributions  with zero mean and standard deviation \n\u03c3\n at each iteration.\n\n\nReferences\n\n\n\n\nPalu\u0161, M., Krakovsk\u00e1, A., Jakub\u00edk, J., \n Chvostekov\u00e1, M. (2018). Causality,  dynamical systems and the arrow of time. Chaos: An Interdisciplinary Journal of  Nonlinear Science, 28(7), 075307. \nhttp://doi.org/10.1063/1.5019944\n\n\n\n\nsource\n\n\n\n\nNonlinear 3D system with nonlinear coupling\n\n\n#\n\n\nCausalityTools.Systems.nonlinear3d\n \n \nFunction\n.\n\n\nnonlinear3d\n(;\nu\u1d62\n \n=\n \nrand\n(\n3\n),\n \n    \n\u03c3\u2081\n \n=\n \n1.0\n,\n \n\u03c3\u2082\n \n=\n \n1.0\n,\n \n\u03c3\u2083\n \n=\n \n1.0\n,\n \n    \na\u2081\n \n=\n \n3.4\n,\n \na\u2082\n \n=\n \n3.4\n,\n \na\u2083\n \n=\n \n3.4\n,\n \n    \nb\u2081\n \n=\n \n0.4\n,\n \nb\u2082\n \n=\n \n0.4\n,\n \nb\u2083\n \n=\n \n0.4\n,\n \n    \nc\u2081\u2082\n \n=\n \n0.5\n,\n \nc\u2082\u2083\n \n=\n \n0.3\n,\n \nc\u2081\u2083\n \n=\n \n0.5\n)\n \n-\n \nDiscreteDynamicalSystem\n\n\n\n\n\nA 3d nonlinear system with nonlinear couplings \nx_1 \\to x_2\nx_1 \\to x_2\n,  \nx_2 \\to x_3\nx_2 \\to x_3\n and \nx_1 \\to x_3\nx_1 \\to x_3\n. Modified from [1]. \n\n\nEquations of motion\n\n\nThe equations of motion are\n\n\n\n\n\n\\begin{aligned}\nx_1(t+1) \n= a_1 x_1 (1-x_1(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{1}(t) \\\\\nx_2(t+1) \n= a_1 x_2 (1-x_2(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{2}(t) + b x_1 x_2 \\\\\nx_3(t+1) \n= a_3 x_3 (1-x_3(t))^2  e^{-x_3(t)^2} + 0.4 \\xi_{3}(t) + c x_{2}(t) + d x_{1}(t)^2.\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\nx_1(t+1) &= a_1 x_1 (1-x_1(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{1}(t) \\\\\nx_2(t+1) &= a_1 x_2 (1-x_2(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{2}(t) + b x_1 x_2 \\\\\nx_3(t+1) &= a_3 x_3 (1-x_3(t))^2  e^{-x_3(t)^2} + 0.4 \\xi_{3}(t) + c x_{2}(t) + d x_{1}(t)^2.\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nGour\u00e9vitch, B., Le Bouquin-Jeann\u00e8s, R., \n Faucon, G. (2006). Linear and nonlinear   causality between signals: methods, examples and neurophysiological   applications. Biological Cybernetics, 95(4), 349\u2013369.\n\n\n\n\nsource\n\n\n\n\nUnidirectionally coupled 2D logistic maps\n\n\n#\n\n\nCausalityTools.Systems.logistic2_unidir\n \n \nMethod\n.\n\n\nlogistic2\n(;\nu\u2080\n \n=\n \nrand\n(\n2\n),\n \nc_xy\n \n=\n \n0.1\n,\n \n\u03c3\n \n=\n \n0.05\n,\n\n    \nr\u2081\n \n=\n \n3.78\n,\n \nr\u2082\n \n=\n \n3.66\n)\n \n-\n \nDiscreteDynamicalSystem\n\n\n\n\n\nInitialise a system consisting of two coupled logistic maps where X unidirectionally influences Y. By default, the parameters \nr\u2081\n and \nr\u2082\n are set to values yielding chaotic behaviour.\n\n\nEquations of motion\n\n\nThe equations of motion are\n\n\n\n\n\n\\begin{aligned}\ndx \n= r_1 x(1 - x) \\\\\ndy \n= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}\n\n\n\n\n\nwith\n\n\n\n\n\n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c_{xy}(x \\xi )}{2}}{1 + \\frac{c_{xy}}{2}(1+ \\sigma )}\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c_{xy}(x \\xi )}{2}}{1 + \\frac{c_{xy}}{2}(1+ \\sigma )}\n\\end{aligned}\n\n\n\n\n\nThe parameter \nc_xy\n controls how strong the dynamical forcing is. If \n\u03c3 \n 0\n, dynamical noise masking the influence of  \nx\n on \ny\n equivalent to \n\\sigma \\cdot \\xi\n\\sigma \\cdot \\xi\n is added at each iteration. Here,\n\\xi\n\\xi\n is a draw from a flat distribution on \n[0, 1]\n[0, 1]\n. Thus, setting \n\u03c3 = 0.05\n is equivalent to add dynamical noise corresponding to a maximum of \n5 \\%\n5 \\%\n of the possible range of values of the logistic map.\n\n\nReferences\n\n\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\nsource\n\n\n\n\nBidirectionally coupled 2D logistic maps\n\n\n#\n\n\nCausalityTools.Systems.logistic2_bidir\n \n \nMethod\n.\n\n\nlogistic2_bidir\n(;\nu\u2080\n \n=\n \nrand\n(\n2\n),\n \nc_xy\n \n=\n \n0.1\n,\n \nc_yx\n \n=\n \n0.1\n,\n \n    \nr\u2081\n \n=\n \n3.78\n,\n \nr\u2082\n \n=\n \n3.66\n,\n \n\u03c3_xy\n \n=\n \n0.05\n,\n \n\u03c3_yx\n \n=\n \n0.05\n)\n\n\n\n\n\nA bidirectional logistic model for the chaotic population dynamics of two interacting  species [1].\n\n\nEquations of motion\n\n\nThe equations of motion are \n\n\n\n\n\n\\begin{align}\nx(t+1) \n= r_1 f_{yx}^{t}(1 - f_{yx}^{t}) \\\\\ny(t+1) \n= r_2 f_{xy}^{t}(1 - f_{xy}^{t}) \\\\\nf_{xy}^t \n= \\dfrac{y(t) + c_{xy}(x(t) + \\sigma_{xy} \\xi_{xy}^t )}{1 + c_{xy} (1 + \\sigma_{xy} )} \\\\ \nf_{yx}^t \n= \\dfrac{x(t) + c_{yx}(y(t) + \\sigma_{yx} \\xi_{yx}^t )}{1 + c_{yx} (1 + \\sigma_{yx} )},\n\\end{align}\n\n\n\n\n\\begin{align}\nx(t+1) &= r_1 f_{yx}^{t}(1 - f_{yx}^{t}) \\\\\ny(t+1) &= r_2 f_{xy}^{t}(1 - f_{xy}^{t}) \\\\\nf_{xy}^t &= \\dfrac{y(t) + c_{xy}(x(t) + \\sigma_{xy} \\xi_{xy}^t )}{1 + c_{xy} (1 + \\sigma_{xy} )} \\\\ \nf_{yx}^t &= \\dfrac{x(t) + c_{yx}(y(t) + \\sigma_{yx} \\xi_{yx}^t )}{1 + c_{yx} (1 + \\sigma_{yx} )},\n\\end{align}\n\n\n\n\n\nwhere the coupling strength \nc_{xy}\nc_{xy}\n controls how strongly species \nx\nx\n influences species  \ny\ny\n, and vice versa for \nc_{yx}\nc_{yx}\n. To simulate time-varying influence of unobserved  processes, we use the dynamical noise terms \n\\xi_{xy}^t\n\\xi_{xy}^t\n and \n\\xi_{yx}^t\n\\xi_{yx}^t\n, drawn from a  uniform distribution with support on \n[0, 1]\n[0, 1]\n. If \n\\sigma_{xy} \n 0\n\\sigma_{xy} > 0\n, then the influence  of \nx\nx\n on \ny\ny\n is masked by dynamical noise equivalent to \n\\sigma_{xy} \\xi_{xy}^{t}\n\\sigma_{xy} \\xi_{xy}^{t}\n at  the \nt\nt\n-th iteration of the map, and vice versa for \n\\sigma_{yx}\n\\sigma_{yx}\n.\n\n\nReferences\n\n\n\n\nDiego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\nsource\n\n\n\n\nForcing of two independent logistic maps from common logistic map driver\n\n\n#\n\n\nCausalityTools.Systems.logistic3\n \n \nMethod\n.\n\n\nlogistic3\n(;\nu\u2080\n \n=\n \nrand\n(\n3\n),\n \nr\n \n=\n \n4\n,\n\n    \n\u03c3x\n \n=\n \n0.05\n,\n \n\u03c3y\n \n=\n \n0.05\n,\n \n\u03c3z\n \n=\n \n0.05\n)\n \n-\n \nDiscreteDynamicalSystem\n\n\n\n\n\nInitialise a dynamical system consisting of three coupled logistic map representing the response of two independent dynamical variables to the forcing from a common driver. The dynamical influence goes in the directions \nZ \\to X\nZ \\to X\n and \nZ \\to Y\nZ \\to Y\n.\n\n\nEquations of motion\n\n\nThe equations of motion are\n\n\n\n\n\n\\begin{aligned}\nx(t+1) = (x(t)(r - r_1 x(t) - z(t) + \u03c3_x \u03b7_x)) \\mod 1 \\\\\ny(t+1) = (y(t)(r - r_2 y(t) - z(t) + \u03c3_y \u03b7_y)) \\mod 1 \\\\\nz(t+1) = (z(t)(r - r_3 z(t) + \u03c3_z \u03b7_z)) \\mod 1\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\nx(t+1) = (x(t)(r - r_1 x(t) - z(t) + \u03c3_x \u03b7_x)) \\mod 1 \\\\\ny(t+1) = (y(t)(r - r_2 y(t) - z(t) + \u03c3_y \u03b7_y)) \\mod 1 \\\\\nz(t+1) = (z(t)(r - r_3 z(t) + \u03c3_z \u03b7_z)) \\mod 1\n\\end{aligned}\n\n\n\n\n\nDynamical noise may be added to each of the dynamical variables by tuning the parameters \n\u03c3z\n, \n\u03c3x\n and \n\u03c3z\n. Default values for the parameters \nr\u2081\n, \nr\u2082\n and \nr\u2083\n are set such that the system exhibits chaotic behaviour, with \nr\u2081 = r\u2082 = r\u2083 = 4\n.\n\n\nReferences\n\n\n\n\nRunge, Jakob. Causal network reconstruction from time series: From theoretical   assumptions to practical estimation, Chaos 28, 075310 (2018);   doi: 10.1063/1.5025050\n\n\n\n\nsource\n\n\n\n\nUnidirectional, transitive chain of logistic maps\n\n\n#\n\n\nCausalityTools.Systems.logistic4\n \n \nMethod\n.\n\n\nlogistic4\n(;\nu\u2080\n \n=\n \nrand\n(\n4\n),\n \nr\u2081\n \n=\n \n3.9\n,\n \nr\u2082\n \n=\n \n3.6\n,\n \nr\u2083\n \n=\n \n3.6\n,\n \nr\u2084\n \n=\n \n3.8\n,\n\n    \nc\u2081\u2082\n \n=\n \n0.4\n,\n \nc\u2082\u2083\n \n=\n \n0.4\n,\n \nc\u2083\u2084\n \n=\n \n0.35\n)\n \n-\n \nDiscreteDynamicalSystem\n\n\n\n\n\nInitialise a system of a transitive chain of four unidirectionally coupled logistic maps, where \ny_1 \\to y_2 \\to y_3 \\to y_4\ny_1 \\to y_2 \\to y_3 \\to y_4\n [1]. Default  parameters are as in [1].\n\n\nNote: With the default parameters which are as in [1], for some initial conditions,  this system wanders off to \n\\pm \\infty\n\\pm \\infty\n for some of the variables. Make sure that  you have a good realisation before using the orbit for anything.*\n\n\nEquations of motion\n\n\n\n\n\n\\begin{aligned}\ny_1(t+1) \n= y_1(t)(r_1 - r_1 y_1) \\\\\ny_2(t+1) \n= y_2(t)(r_2 - c_{12} y_1 - r_2 y_2) \\\\\ny_3(t+1) \n= y_3(t)(r_3 - c_{23} y_2 - r_3 y_3) \\\\\ny_4(t+1) \n= y_4(t)(r_4 - c_{34} y_3 - r_4 y_4)\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\ny_1(t+1) &= y_1(t)(r_1 - r_1 y_1) \\\\\ny_2(t+1) &= y_2(t)(r_2 - c_{12} y_1 - r_2 y_2) \\\\\ny_3(t+1) &= y_3(t)(r_3 - c_{23} y_2 - r_3 y_3) \\\\\ny_4(t+1) &= y_4(t)(r_4 - c_{34} y_3 - r_4 y_4)\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nYe, Hao, et al. \"Distinguishing time-delayed causal interactions using  convergent cross mapping.\" Scientific reports 5 (2015): 14750\n\n\n\n\nsource\n\n\n\n\nTwo unidirectionally coupled Henon maps\n\n\n#\n\n\nCausalityTools.Systems.henon2\n \n \nMethod\n.\n\n\nhenon2\n(;\nu\u2080\n \n=\n \nrand\n(\n4\n),\n \nc_xy\n \n=\n \n2.0\n)\n \n-\n \nDiscreteDynamicalSystem\n\n\n\n\n\nInitialize a 2D Henon system consisting of two identical Henon maps with unidirectional forcing \nX \\to Y\nX \\to Y\n [1].\n\n\nEquations of motion\n\n\nThe equations of motion are \n\n\n\n\n\n\\begin{aligned}\nx_1(t+1) \n= 1.4 - x_1^2(t) + 0.3x_2(t) \\\\\nx_2(t+1) \n= x_1(t) \\\\\ny_1(t+1) \n= 1.4 - [c_{xy} x_1(t) y_1(t) + (1-c_{xy}) y_1^2(t)] + 0.3 y_2(t) \\\\\ny_2(t+1) \n= y_1(t)\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\nx_1(t+1) &= 1.4 - x_1^2(t) + 0.3x_2(t) \\\\\nx_2(t+1) &= x_1(t) \\\\\ny_1(t+1) &= 1.4 - [c_{xy} x_1(t) y_1(t) + (1-c_{xy}) y_1^2(t)] + 0.3 y_2(t) \\\\\ny_2(t+1) &= y_1(t)\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nKrakovsk\u00e1, A., Jakub\u00edk, J., Chvostekov\u00e1, M., Coufal, D., Jajcay, N., \n Palu\u0161, M. (2018).   Comparison of six methods for the detection of causality in a bivariate time series.   Physical Review E, 97(4), 042207.\n\n\n\n\nsource\n\n\n\n\nStrange, nonchaotic attractors\n\n\n#\n\n\nCausalityTools.Systems.anishchenko1\n \n \nMethod\n.\n\n\nanishchenko1\n(;\nu\u2080\n \n=\n \nrand\n(\n2\n),\n \n\u03b1\n \n=\n3.277\n,\n \ns\n=\n0.1\n,\n \n\u03c9\n=\n0.5\n*\n(\nsqrt\n(\n5\n)\n-\n1\n))\n\n\n\n\n\nInitialise the system defined by eq. 13 in [1], which can give strange,  nonchaotic attractors.\n\n\nEquations of motion\n\n\nThe equations of motion are \n\n\n\n\n\n\\begin{aligned}\ndx \n= \\alpha (1-s \\cos (2 \\pi \\phi )) \\cdot x(1-x) \\\\\nd\u03d5 \n= (\\phi + \\omega ) \\mod{1}\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\ndx &= \\alpha (1-s \\cos (2 \\pi \\phi )) \\cdot x(1-x) \\\\\nd\u03d5 &= (\\phi + \\omega ) \\mod{1}\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nAnishchenko, Vadim S., and Galina I. Strelkova. \"Irregular attractors.\"  Discrete dynamics in Nature and Society 2.1 (1998): 53-72.\n\n\n\n\nsource", 
            "title": "Discrete systems"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#discrete-coupled-dynamical-systems", 
            "text": "", 
            "title": "Discrete coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#autoregressive-order-one-2d-system", 
            "text": "#  CausalityTools.Systems.ar1_unidir     Method .  ar1 ( u\u1d62 ,   a\u2081   =   0.90693 ,   b\u2081   =   0.40693 ,   c_xy   =   0.5 ,  \n     \u03c3   =   0.40662 )   -   DiscreteDynamicalSystem   A bivariate, order one autoregressive model, where  x \\to y x \\to y  [1].  Equations of motion   \n\\begin{aligned}\ndx  = a_1 x + \\xi_{1} \\\\\ndy  = b_1 y - c_{xy} x + \\xi_{2},\n\\end{aligned}  \n\\begin{aligned}\ndx &= a_1 x + \\xi_{1} \\\\\ndy &= b_1 y - c_{xy} x + \\xi_{2},\n\\end{aligned}   where  \\xi_{1} \\xi_{1}  and  \\xi_{2} \\xi_{2}  are drawn from normal distributions  with zero mean and standard deviation  \u03c3  at each iteration.  References   Palu\u0161, M., Krakovsk\u00e1, A., Jakub\u00edk, J.,   Chvostekov\u00e1, M. (2018). Causality,  dynamical systems and the arrow of time. Chaos: An Interdisciplinary Journal of  Nonlinear Science, 28(7), 075307.  http://doi.org/10.1063/1.5019944   source", 
            "title": "Autoregressive order one 2D system"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#nonlinear-3d-system-with-nonlinear-coupling", 
            "text": "#  CausalityTools.Systems.nonlinear3d     Function .  nonlinear3d (; u\u1d62   =   rand ( 3 ),  \n     \u03c3\u2081   =   1.0 ,   \u03c3\u2082   =   1.0 ,   \u03c3\u2083   =   1.0 ,  \n     a\u2081   =   3.4 ,   a\u2082   =   3.4 ,   a\u2083   =   3.4 ,  \n     b\u2081   =   0.4 ,   b\u2082   =   0.4 ,   b\u2083   =   0.4 ,  \n     c\u2081\u2082   =   0.5 ,   c\u2082\u2083   =   0.3 ,   c\u2081\u2083   =   0.5 )   -   DiscreteDynamicalSystem   A 3d nonlinear system with nonlinear couplings  x_1 \\to x_2 x_1 \\to x_2 ,   x_2 \\to x_3 x_2 \\to x_3  and  x_1 \\to x_3 x_1 \\to x_3 . Modified from [1].   Equations of motion  The equations of motion are   \n\\begin{aligned}\nx_1(t+1)  = a_1 x_1 (1-x_1(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{1}(t) \\\\\nx_2(t+1)  = a_1 x_2 (1-x_2(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{2}(t) + b x_1 x_2 \\\\\nx_3(t+1)  = a_3 x_3 (1-x_3(t))^2  e^{-x_3(t)^2} + 0.4 \\xi_{3}(t) + c x_{2}(t) + d x_{1}(t)^2.\n\\end{aligned}  \n\\begin{aligned}\nx_1(t+1) &= a_1 x_1 (1-x_1(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{1}(t) \\\\\nx_2(t+1) &= a_1 x_2 (1-x_2(t))^2  e^{-x_2(t)^2} + 0.4 \\xi_{2}(t) + b x_1 x_2 \\\\\nx_3(t+1) &= a_3 x_3 (1-x_3(t))^2  e^{-x_3(t)^2} + 0.4 \\xi_{3}(t) + c x_{2}(t) + d x_{1}(t)^2.\n\\end{aligned}   References   Gour\u00e9vitch, B., Le Bouquin-Jeann\u00e8s, R.,   Faucon, G. (2006). Linear and nonlinear   causality between signals: methods, examples and neurophysiological   applications. Biological Cybernetics, 95(4), 349\u2013369.   source", 
            "title": "Nonlinear 3D system with nonlinear coupling"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#unidirectionally-coupled-2d-logistic-maps", 
            "text": "#  CausalityTools.Systems.logistic2_unidir     Method .  logistic2 (; u\u2080   =   rand ( 2 ),   c_xy   =   0.1 ,   \u03c3   =   0.05 , \n     r\u2081   =   3.78 ,   r\u2082   =   3.66 )   -   DiscreteDynamicalSystem   Initialise a system consisting of two coupled logistic maps where X unidirectionally influences Y. By default, the parameters  r\u2081  and  r\u2082  are set to values yielding chaotic behaviour.  Equations of motion  The equations of motion are   \n\\begin{aligned}\ndx  = r_1 x(1 - x) \\\\\ndy  = r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}  \n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}   with   \n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c_{xy}(x \\xi )}{2}}{1 + \\frac{c_{xy}}{2}(1+ \\sigma )}\n\\end{aligned}  \n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c_{xy}(x \\xi )}{2}}{1 + \\frac{c_{xy}}{2}(1+ \\sigma )}\n\\end{aligned}   The parameter  c_xy  controls how strong the dynamical forcing is. If  \u03c3   0 , dynamical noise masking the influence of   x  on  y  equivalent to  \\sigma \\cdot \\xi \\sigma \\cdot \\xi  is added at each iteration. Here, \\xi \\xi  is a draw from a flat distribution on  [0, 1] [0, 1] . Thus, setting  \u03c3 = 0.05  is equivalent to add dynamical noise corresponding to a maximum of  5 \\% 5 \\%  of the possible range of values of the logistic map.  References   Diego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.   source", 
            "title": "Unidirectionally coupled 2D logistic maps"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#bidirectionally-coupled-2d-logistic-maps", 
            "text": "#  CausalityTools.Systems.logistic2_bidir     Method .  logistic2_bidir (; u\u2080   =   rand ( 2 ),   c_xy   =   0.1 ,   c_yx   =   0.1 ,  \n     r\u2081   =   3.78 ,   r\u2082   =   3.66 ,   \u03c3_xy   =   0.05 ,   \u03c3_yx   =   0.05 )   A bidirectional logistic model for the chaotic population dynamics of two interacting  species [1].  Equations of motion  The equations of motion are    \n\\begin{align}\nx(t+1)  = r_1 f_{yx}^{t}(1 - f_{yx}^{t}) \\\\\ny(t+1)  = r_2 f_{xy}^{t}(1 - f_{xy}^{t}) \\\\\nf_{xy}^t  = \\dfrac{y(t) + c_{xy}(x(t) + \\sigma_{xy} \\xi_{xy}^t )}{1 + c_{xy} (1 + \\sigma_{xy} )} \\\\ \nf_{yx}^t  = \\dfrac{x(t) + c_{yx}(y(t) + \\sigma_{yx} \\xi_{yx}^t )}{1 + c_{yx} (1 + \\sigma_{yx} )},\n\\end{align}  \n\\begin{align}\nx(t+1) &= r_1 f_{yx}^{t}(1 - f_{yx}^{t}) \\\\\ny(t+1) &= r_2 f_{xy}^{t}(1 - f_{xy}^{t}) \\\\\nf_{xy}^t &= \\dfrac{y(t) + c_{xy}(x(t) + \\sigma_{xy} \\xi_{xy}^t )}{1 + c_{xy} (1 + \\sigma_{xy} )} \\\\ \nf_{yx}^t &= \\dfrac{x(t) + c_{yx}(y(t) + \\sigma_{yx} \\xi_{yx}^t )}{1 + c_{yx} (1 + \\sigma_{yx} )},\n\\end{align}   where the coupling strength  c_{xy} c_{xy}  controls how strongly species  x x  influences species   y y , and vice versa for  c_{yx} c_{yx} . To simulate time-varying influence of unobserved  processes, we use the dynamical noise terms  \\xi_{xy}^t \\xi_{xy}^t  and  \\xi_{yx}^t \\xi_{yx}^t , drawn from a  uniform distribution with support on  [0, 1] [0, 1] . If  \\sigma_{xy}   0 \\sigma_{xy} > 0 , then the influence  of  x x  on  y y  is masked by dynamical noise equivalent to  \\sigma_{xy} \\xi_{xy}^{t} \\sigma_{xy} \\xi_{xy}^{t}  at  the  t t -th iteration of the map, and vice versa for  \\sigma_{yx} \\sigma_{yx} .  References   Diego, David, Kristian Agas\u00f8ster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation   using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.   source", 
            "title": "Bidirectionally coupled 2D logistic maps"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#forcing-of-two-independent-logistic-maps-from-common-logistic-map-driver", 
            "text": "#  CausalityTools.Systems.logistic3     Method .  logistic3 (; u\u2080   =   rand ( 3 ),   r   =   4 , \n     \u03c3x   =   0.05 ,   \u03c3y   =   0.05 ,   \u03c3z   =   0.05 )   -   DiscreteDynamicalSystem   Initialise a dynamical system consisting of three coupled logistic map representing the response of two independent dynamical variables to the forcing from a common driver. The dynamical influence goes in the directions  Z \\to X Z \\to X  and  Z \\to Y Z \\to Y .  Equations of motion  The equations of motion are   \n\\begin{aligned}\nx(t+1) = (x(t)(r - r_1 x(t) - z(t) + \u03c3_x \u03b7_x)) \\mod 1 \\\\\ny(t+1) = (y(t)(r - r_2 y(t) - z(t) + \u03c3_y \u03b7_y)) \\mod 1 \\\\\nz(t+1) = (z(t)(r - r_3 z(t) + \u03c3_z \u03b7_z)) \\mod 1\n\\end{aligned}  \n\\begin{aligned}\nx(t+1) = (x(t)(r - r_1 x(t) - z(t) + \u03c3_x \u03b7_x)) \\mod 1 \\\\\ny(t+1) = (y(t)(r - r_2 y(t) - z(t) + \u03c3_y \u03b7_y)) \\mod 1 \\\\\nz(t+1) = (z(t)(r - r_3 z(t) + \u03c3_z \u03b7_z)) \\mod 1\n\\end{aligned}   Dynamical noise may be added to each of the dynamical variables by tuning the parameters  \u03c3z ,  \u03c3x  and  \u03c3z . Default values for the parameters  r\u2081 ,  r\u2082  and  r\u2083  are set such that the system exhibits chaotic behaviour, with  r\u2081 = r\u2082 = r\u2083 = 4 .  References   Runge, Jakob. Causal network reconstruction from time series: From theoretical   assumptions to practical estimation, Chaos 28, 075310 (2018);   doi: 10.1063/1.5025050   source", 
            "title": "Forcing of two independent logistic maps from common logistic map driver"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#unidirectional-transitive-chain-of-logistic-maps", 
            "text": "#  CausalityTools.Systems.logistic4     Method .  logistic4 (; u\u2080   =   rand ( 4 ),   r\u2081   =   3.9 ,   r\u2082   =   3.6 ,   r\u2083   =   3.6 ,   r\u2084   =   3.8 , \n     c\u2081\u2082   =   0.4 ,   c\u2082\u2083   =   0.4 ,   c\u2083\u2084   =   0.35 )   -   DiscreteDynamicalSystem   Initialise a system of a transitive chain of four unidirectionally coupled logistic maps, where  y_1 \\to y_2 \\to y_3 \\to y_4 y_1 \\to y_2 \\to y_3 \\to y_4  [1]. Default  parameters are as in [1].  Note: With the default parameters which are as in [1], for some initial conditions,  this system wanders off to  \\pm \\infty \\pm \\infty  for some of the variables. Make sure that  you have a good realisation before using the orbit for anything.*  Equations of motion   \n\\begin{aligned}\ny_1(t+1)  = y_1(t)(r_1 - r_1 y_1) \\\\\ny_2(t+1)  = y_2(t)(r_2 - c_{12} y_1 - r_2 y_2) \\\\\ny_3(t+1)  = y_3(t)(r_3 - c_{23} y_2 - r_3 y_3) \\\\\ny_4(t+1)  = y_4(t)(r_4 - c_{34} y_3 - r_4 y_4)\n\\end{aligned}  \n\\begin{aligned}\ny_1(t+1) &= y_1(t)(r_1 - r_1 y_1) \\\\\ny_2(t+1) &= y_2(t)(r_2 - c_{12} y_1 - r_2 y_2) \\\\\ny_3(t+1) &= y_3(t)(r_3 - c_{23} y_2 - r_3 y_3) \\\\\ny_4(t+1) &= y_4(t)(r_4 - c_{34} y_3 - r_4 y_4)\n\\end{aligned}   References   Ye, Hao, et al. \"Distinguishing time-delayed causal interactions using  convergent cross mapping.\" Scientific reports 5 (2015): 14750   source", 
            "title": "Unidirectional, transitive chain of logistic maps"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#two-unidirectionally-coupled-henon-maps", 
            "text": "#  CausalityTools.Systems.henon2     Method .  henon2 (; u\u2080   =   rand ( 4 ),   c_xy   =   2.0 )   -   DiscreteDynamicalSystem   Initialize a 2D Henon system consisting of two identical Henon maps with unidirectional forcing  X \\to Y X \\to Y  [1].  Equations of motion  The equations of motion are    \n\\begin{aligned}\nx_1(t+1)  = 1.4 - x_1^2(t) + 0.3x_2(t) \\\\\nx_2(t+1)  = x_1(t) \\\\\ny_1(t+1)  = 1.4 - [c_{xy} x_1(t) y_1(t) + (1-c_{xy}) y_1^2(t)] + 0.3 y_2(t) \\\\\ny_2(t+1)  = y_1(t)\n\\end{aligned}  \n\\begin{aligned}\nx_1(t+1) &= 1.4 - x_1^2(t) + 0.3x_2(t) \\\\\nx_2(t+1) &= x_1(t) \\\\\ny_1(t+1) &= 1.4 - [c_{xy} x_1(t) y_1(t) + (1-c_{xy}) y_1^2(t)] + 0.3 y_2(t) \\\\\ny_2(t+1) &= y_1(t)\n\\end{aligned}   References   Krakovsk\u00e1, A., Jakub\u00edk, J., Chvostekov\u00e1, M., Coufal, D., Jajcay, N.,   Palu\u0161, M. (2018).   Comparison of six methods for the detection of causality in a bivariate time series.   Physical Review E, 97(4), 042207.   source", 
            "title": "Two unidirectionally coupled Henon maps"
        }, 
        {
            "location": "/example_systems/example_systems_discrete/#strange-nonchaotic-attractors", 
            "text": "#  CausalityTools.Systems.anishchenko1     Method .  anishchenko1 (; u\u2080   =   rand ( 2 ),   \u03b1   = 3.277 ,   s = 0.1 ,   \u03c9 = 0.5 * ( sqrt ( 5 ) - 1 ))   Initialise the system defined by eq. 13 in [1], which can give strange,  nonchaotic attractors.  Equations of motion  The equations of motion are    \n\\begin{aligned}\ndx  = \\alpha (1-s \\cos (2 \\pi \\phi )) \\cdot x(1-x) \\\\\nd\u03d5  = (\\phi + \\omega ) \\mod{1}\n\\end{aligned}  \n\\begin{aligned}\ndx &= \\alpha (1-s \\cos (2 \\pi \\phi )) \\cdot x(1-x) \\\\\nd\u03d5 &= (\\phi + \\omega ) \\mod{1}\n\\end{aligned}   References   Anishchenko, Vadim S., and Galina I. Strelkova. \"Irregular attractors.\"  Discrete dynamics in Nature and Society 2.1 (1998): 53-72.   source", 
            "title": "Strange, nonchaotic attractors"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/", 
            "text": "Continuous coupled dynamical systems\n\n\n\n\nMediated link\n\n\n#\n\n\nCausalityTools.Systems.mediated_link\n \n \nMethod\n.\n\n\nmediated_link\n(;\nu\u2080\n \n=\n \nrand\n(\n9\n),\n \n\u03c9x\n \n=\n \n1\n,\n \n\u03c9y\n \n=\n \n1.015\n,\n \n\u03c9z\n \n=\n \n0.985\n,\n\n    \nk\n \n=\n \n0.15\n,\n \nl\n \n=\n \n0.2\n,\n \nm\n \n=\n \n10.0\n,\n \n    \nc\n \n=\n \n0.06\n)\n \n-\n \nContinuousDynamicalSystem\n\n\n\n\n\nInitialise a three-subsystem dynamical system where \nX\n and \nY\n are driven by \nZ\n. At the default value of the coupling constant \nc = 0.06\n, the responses \nX\n and \nY\n are already synchronized to the driver \nZ\n.\n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\ndx_1 \n= -\\omega_x x_2 - x_3 + c*(z_1 - x_1) \\\\\ndx_2 \n= \\omega_x x_1 + k*x_2  \\\\\ndx_3 \n= l + x_3(x_1 - m)  \\\\\ndy_1 \n= -\\omega_y y_2 - y_3 + c*(z_1 - y_1)  \\\\\ndy_2 \n= \\omega_y y_1 + k*y_2  \\\\\ndy_3 \n= l + y_3(y_1 - m)  \\\\\ndz_1 \n= -\\omega_z z_2 - z_3  \\\\\ndz_2 \n= \\omega_z z_1 + k*z_2  \\\\\ndz_3 \n= l + z_3(z_1 - m)\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\ndx_1 &= -\\omega_x x_2 - x_3 + c*(z_1 - x_1) \\\\\ndx_2 &= \\omega_x x_1 + k*x_2  \\\\\ndx_3 &= l + x_3(x_1 - m)  \\\\\ndy_1 &= -\\omega_y y_2 - y_3 + c*(z_1 - y_1)  \\\\\ndy_2 &= \\omega_y y_1 + k*y_2  \\\\\ndy_3 &= l + y_3(y_1 - m)  \\\\\ndz_1 &= -\\omega_z z_2 - z_3  \\\\\ndz_2 &= \\omega_z z_1 + k*z_2  \\\\\ndz_3 &= l + z_3(z_1 - m)\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nKrakovsk\u00e1, Anna, et al. \"Comparison of six methods for the detection of   causality in a bivariate time series.\" Physical Review E 97.4 (2018): 042207\n\n\n\n\nsource\n\n\n\n\nTwo bidirectionally coupled 3D Lorenz systems\n\n\n#\n\n\nCausalityTools.Systems.lorenz_lorenz_bidir\n \n \nMethod\n.\n\n\nlorenz_lorenz_bidir\n(;\n \nu0\n \n=\n \nrand\n(\n6\n),\n \n    \nc_xy\n \n=\n \n0.2\n,\n \nc_yx\n \n=\n \n0.2\n,\n \n    \na\u2081\n \n=\n \n10\n,\n \na\u2082\n \n=\n \n28\n,\n \na\u2083\n \n=\n \n8\n/\n3\n,\n \n    \nb\u2081\n \n=\n \n10\n,\n \nb\u2082\n \n=\n \n28\n,\n \nb\u2083\n \n=\n \n9\n/\n3\n)\n \n-\n \nContinuousDynamicalSystem\n\n\n\n\n\nInitialise a bidirectionally coupled Lorenz-Lorenz system, where each  subsystem is a 3D Lorenz system [1]. Default values for the parameters  \na\u2081\n, \na\u2082\n, \na\u2083\n, \nb\u2081\n, \nb\u2082\n, \nb\u2083\n are as in [1].\n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} \n= -a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2} \n= -x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3} \n= x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1} \n= -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) \\\\\n\\dot{y_2} \n= -y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3} \n= y_1 y_2 - b_3 y_3\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} &= -a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2} &= -x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3} &= x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1} &= -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) \\\\\n\\dot{y_2} &= -y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3} &= y_1 y_2 - b_3 y_3\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nAmig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\nsource\n\n\n\n\nTwo bidirectionally coupled 3D Lorenz systems forced by another 3D Lorenz system\n\n\n#\n\n\nCausalityTools.Systems.lorenz_lorenz_lorenz_bidir_forced\n \n \nMethod\n.\n\n\nlorenz_lorenz_lorenz_bidir_forced\n(;\n \nu0\n \n=\n \nrand\n(\n9\n),\n \n    \nc_xy\n \n=\n \n0.1\n,\n \nc_yx\n \n=\n \n0.1\n,\n\n    \nc_zx\n \n=\n \n0.05\n,\n \nc_zy\n \n=\n \n0.05\n,\n \n    \na\u2081\n \n=\n \n10\n,\n \na\u2082\n \n=\n \n28\n,\n \na\u2083\n \n=\n \n8\n/\n3\n,\n\n    \nb\u2081\n \n=\n \n10\n,\n \nb\u2082\n \n=\n \n28\n,\n \nb\u2083\n \n=\n \n8\n/\n3\n,\n\n    \nc\u2081\n \n=\n \n10\n,\n \nc\u2082\n \n=\n \n28\n,\n \nc\u2083\n \n=\n \n8\n/\n3\n)\n\n\n\n\n\nInitialise a system consisting of two bidirectionally coupled 3D Lorenz  systems forced by an external 3D Lorenz system, giving a 9D system.\n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} \n= - a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2} \n= - x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3} \n= x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1} \n= -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{y_2} \n= - y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3} \n= y_1 y_2 - b_3 y_3 \\\\\n\\dot{z_1} \n= - c_1 (z_1 - z_2) \\\\\n\\dot{z_2} \n= - z_1 z_3 + c_2 z_1 - z_2 \\\\\n\\dot{z_3} \n= z_1 z_2 - c_3 z_3 \n\\end{aligned}\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} &= - a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2} &= - x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3} &= x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1} &= -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{y_2} &= - y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3} &= y_1 y_2 - b_3 y_3 \\\\\n\\dot{z_1} &= - c_1 (z_1 - z_2) \\\\\n\\dot{z_2} &= - z_1 z_3 + c_2 z_1 - z_2 \\\\\n\\dot{z_3} &= z_1 z_2 - c_3 z_3 \n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nAmig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\nsource\n\n\n\n\nThree transitively connected 3D Lorenz systems\n\n\n#\n\n\nCausalityTools.Systems.lorenz_lorenz_lorenz_transitive\n \n \nMethod\n.\n\n\nlorenz_lorenz_lorenz_transitive\n(;\nu\u1d62\n=\nrand\n(\n9\n),\n\n            \n\u03c3\u2081\n \n=\n \n10.0\n,\n \n\u03c3\u2082\n \n=\n \n10.0\n,\n \n\u03c3\u2083\n \n=\n \n10.0\n,\n\n            \n\u03c1\u2081\n \n=\n \n28.0\n,\n \n\u03c1\u2082\n \n=\n \n28.0\n,\n \n\u03c1\u2083\n \n=\n \n28.0\n,\n\n            \n\u03b2\u2081\n \n=\n \n8\n/\n3\n,\n  \n\u03b2\u2082\n \n=\n \n8\n/\n3\n,\n  \n\u03b2\u2083\n \n=\n \n8.3\n,\n\n            \nc\u2081\u2082\n \n=\n \n1.0\n,\n \nc\u2082\u2083\n \n=\n \n1.0\n)\n \n-\n \nContinuousDynamicalSystem\n\n\n\n\n\nInitalise a dynamical system consisting of three coupled Lorenz attractors with a transitive causality chain where X\u2081 \u2192 X\u2082 and X\u2082 \u2192 X\u2083. In total, the three 3D-subsystems create a 9-dimensional dynamical system.\n\n\nThe strength of the forcing X\u2081 \u2192 X\u2082 is controlled by the parameter \nc\u2081\n, and the forcing from X\u2082 \u2192 X\u2083 by \nc\u2082\n. The remaining parameters are the usual parameters for the Lorenz system, where the subscript \ni\n refers to the subsystem X\u1d62. \n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} \n= \\sigma_1(y_1 - x_1) \\\\\n\\dot{y_1} \n= \\rho_1 x_1 - y_1 - x_1 z_1 \\\\\n\\dot{z_1} \n= x_1 y_1 - \\beta_1 z_1 \\\\\n\\dot{x_2} \n=  \\sigma_2 (y_2 - x_2) + c_{12}(x_1 - x_2) \\\\\n\\dot{y_2} \n= \\rho_2 x_2 - y_2 - x_2 z_2 \\\\\n\\dot{z_2} \n= x_2 y_2 - \\beta_2 z_2 \\\\\n\\dot{x_3} \n= \\sigma_3 (y_3 - x_3) + c_{23} (x_2 - x_3) \\\\\n\\dot{y_3} \n= \\rho_3 x_3 - y_3 - x_3 z_3 \\\\\n\\dot{z_3} \n= x_3 y_3 - \\beta_3 z_3\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} &= \\sigma_1(y_1 - x_1) \\\\\n\\dot{y_1} &= \\rho_1 x_1 - y_1 - x_1 z_1 \\\\\n\\dot{z_1} &= x_1 y_1 - \\beta_1 z_1 \\\\\n\\dot{x_2} &=  \\sigma_2 (y_2 - x_2) + c_{12}(x_1 - x_2) \\\\\n\\dot{y_2} &= \\rho_2 x_2 - y_2 - x_2 z_2 \\\\\n\\dot{z_2} &= x_2 y_2 - \\beta_2 z_2 \\\\\n\\dot{x_3} &= \\sigma_3 (y_3 - x_3) + c_{23} (x_2 - x_3) \\\\\n\\dot{y_3} &= \\rho_3 x_3 - y_3 - x_3 z_3 \\\\\n\\dot{z_3} &= x_3 y_3 - \\beta_3 z_3\n\\end{aligned}\n\n\n\n\n\nUsage in literature\n\n\nThis system was studied by Papana et al. (2013) for coupling strengths  \nc_{12} = 0, 1, 3, 5\nc_{12} = 0, 1, 3, 5\n and \nc_{23} = 0, 1, 3, 5\nc_{23} = 0, 1, 3, 5\n.\n\n\nReferences\n\n\n\n\nPapana et al., Simulation Study of Direct Causality Measures in Multivariate   Time Series. Entropy 2013, 15(7), 2635-2661; doi:10.3390/e15072635\n\n\n\n\nsource\n\n\n\n\nTwo bidirectionally coupled 3D R\u00f6ssler systems\n\n\n#\n\n\nCausalityTools.Systems.rossler_rossler_bidir\n \n \nMethod\n.\n\n\nrossler_rossler_bidir\n(;\n \nu0\n \n=\n \nrand\n(\n6\n),\n \n    \n\u03c9\u2081\n \n=\n \n1.015\n,\n \n\u03c9\u2082\n \n=\n \n0.985\n,\n \n    \nc_xy\n \n=\n \n0.1\n,\n \nc_yx\n \n=\n \n0.1\n,\n \n    \na\u2081\n \n=\n \n0.15\n,\n \na\u2082\n \n=\n \n0.2\n,\n \na\u2083\n \n=\n \n10\n,\n\n    \nb\u2081\n \n=\n \n0.15\n,\n \nb\u2082\n \n=\n \n0.2\n,\n \nb\u2083\n \n=\n \n10\n)\n\n\n\n\n\nInitialise a system of two bidirectionally coupled 3D R\u00f6ssler systems.  This system has been modified from [1] to allow other parameterisations,  but default parameters are as in [1].\n\n\nThe \nX\nX\n and \nY\nY\n subsystems are mostly synchronized for  \nc_xy \n 0.1\n or \nc_yx \n 0.1\n.\n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} \n= -\\omega_1(x_2 + x_3) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2} \n= \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3} \n= a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1} \n= -\\omega_2 (y_2 + y_3) + c_{xy}(x_1 - y_1) \\\\\n\\dot{y_2} \n= \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{y_3} \n= b_2 + y_3 (y_1 - b_3)\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} &= -\\omega_1(x_2 + x_3) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2} &= \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3} &= a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1} &= -\\omega_2 (y_2 + y_3) + c_{xy}(x_1 - y_1) \\\\\n\\dot{y_2} &= \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{y_3} &= b_2 + y_3 (y_1 - b_3)\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nAmig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\nsource\n\n\n\n\nTwo bidirectionally coupled 3D R\u00f6ssler systems forced by another 3D R\u00f6ssler system\n\n\n#\n\n\nCausalityTools.Systems.rossler_rossler_rossler_bidir_forced\n \n \nMethod\n.\n\n\nrossler_rossler_rossler_bidir_forced\n(;\n \nu0\n \n=\n \nrand\n(\n9\n),\n \n    \n\u03c9\u2081\n \n=\n \n1.015\n,\n \n\u03c9\u2082\n \n=\n \n0.985\n,\n \n\u03c9\u2083\n \n=\n \n0.95\n,\n\n    \nc_xy\n \n=\n \n0.1\n,\n \nc_yx\n \n=\n \n0.1\n,\n\n    \nc_zx\n \n=\n \n0.05\n,\n \nc_zy\n \n=\n \n0.05\n,\n \n    \na\u2081\n \n=\n \n0.15\n,\n \na\u2082\n \n=\n \n0.2\n,\n \na\u2083\n \n=\n \n10\n,\n\n    \nb\u2081\n \n=\n \n0.15\n,\n \nb\u2082\n \n=\n \n0.2\n,\n \nb\u2083\n \n=\n \n10\n,\n\n    \nc\u2081\n \n=\n \n0.15\n,\n \nc\u2082\n \n=\n \n0.2\n,\n \nc\u2083\n \n=\n \n10\n)\n\n\n\n\n\nEquations of motion for a system consisting of three coupled 3D R\u00f6ssler systems  (\nX\nX\n, \nY\nY\n, \nZ\nZ\n), giving a 9D system [1]. The external system  \nZ\nZ\n influences both \nX\nX\n and \nY\nY\n (controlled by \nc_zx\n and \nc_zy\n).  Simultaneously, the subsystems  \nX\nX\n and \nY\nY\n bidirectionally  influences each other (controlled by \nc_xy\n and \nc_yx\n).\n\n\nThe \nX\nX\n and \nY\nY\n subsystems are mostly synchronized for \nc_xy \n 0.1\n or  \nc_yx \n 0.1\n.\n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} \n= -\\omega_1 (x_2 + x_3) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2} \n= \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3} \n= a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1} \n= -\\omega_1 (y_2 + y_3) + c_{xy}(x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{x_2} \n= \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{x_3} \n= b_2 + x_3 (y_1 - b_3) \\\\\n\\dot{y_1} \n= -\\omega_2 (z_2  + z_3) \\\\\n\\dot{x_2} \n= \\omega_2 z_1 + c_1 z_2 \\\\\n\\dot{x_3} \n= c_2 + z_3 (z_1 - c_3)\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\n\\dot{x_1} &= -\\omega_1 (x_2 + x_3) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2} &= \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3} &= a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1} &= -\\omega_1 (y_2 + y_3) + c_{xy}(x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{x_2} &= \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{x_3} &= b_2 + x_3 (y_1 - b_3) \\\\\n\\dot{y_1} &= -\\omega_2 (z_2  + z_3) \\\\\n\\dot{x_2} &= \\omega_2 z_1 + c_1 z_2 \\\\\n\\dot{x_3} &= c_2 + z_3 (z_1 - c_3)\n\\end{aligned}\n\n\n\n\n\nReferences\n\n\n\n\nAmig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\nsource\n\n\n\n\nUnidirectonal forcing from a 3D R\u00f6ssler system to a 3D Lorenz system\n\n\n#\n\n\nCausalityTools.Systems.rossler_lorenz\n \n \nMethod\n.\n\n\nrossler_lorenz\n(;\nu\u2080\n \n=\n \nrand\n(\n6\n),\n \na\u2081\n \n=\n \n-\n6\n,\n \na\u2082\n \n=\n \n6\n,\n \na\u2083\n \n=\n \n2.0\n,\n \n    \nb\u2081\n \n=\n \n10\n,\n \nb\u2082\n \n=\n \n28\n,\n \nb\u2083\n \n=\n \n8\n/\n3\n,\n \nc_xy\n \n=\n \n1\n)\n \n-\n \nContinuousDynamicalSystem\n\n\n\n\n\nInitialise a R\u00f6ssler-Lorenz system consisting of two independent 3D subsystems: one R\u00f6ssler system and one Lorenz system. They are coupled such that the second component (\nx\u2082\n) of the R\u00f6ssler system unidirectionally forces the second component (\ny\u2082\n) of the Lorenz system. \n\n\nThe parameter \nc_xy\n controls the coupling strength. The implementation here also  allows for tuning the parameters of each subsystem by introducing the constants  \na\u2081\n, \na\u2082\n, \na\u2083\n, \nb\u2081\n, \nb\u2082\n, \nb\u2083\n. Default values for these parameters are  as in [1].\n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\n\\dot x_1 \n= a_1(x_2 + x_3) \\\\\n\\dot x_2 \n= a_2(x_1 + 0.2x_2) \\\\\n\\dot x_3 \n= a_2(0.2 + x_3(x_1 - a_3)) \\\\\n\\dot y_1 \n= b_1(y_2 - y_1) \\\\\n\\dot y_2 \n= y_1(b_2 - y_3) - y_2 +c_{xy}(x_2)^2 \\\\\n\\dot y_3 \n= y_1 y_2 - b_3y_3\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\n\\dot x_1 &= a_1(x_2 + x_3) \\\\\n\\dot x_2 &= a_2(x_1 + 0.2x_2) \\\\\n\\dot x_3 &= a_2(0.2 + x_3(x_1 - a_3)) \\\\\n\\dot y_1 &= b_1(y_2 - y_1) \\\\\n\\dot y_2 &= y_1(b_2 - y_3) - y_2 +c_{xy}(x_2)^2 \\\\\n\\dot y_3 &= y_1 y_2 - b_3y_3\n\\end{aligned}\n\n\n\n\n\nwith the coupling constant \nc_{xy} \\geq 0\nc_{xy} \\geq 0\n.\n\n\nReferences\n\n\n\n\nKrakovsk\u00e1, Anna, et al. \"Comparison of six methods for the detection of causality in a   bivariate time series.\" Physical Review E 97.4 (2018):042207.   \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.97.042207\n\n\n\n\nsource\n\n\n\n\nN-scroll Chua attractors\n\n\n#\n\n\nCausalityTools.Systems.chuacircuit_nscroll_sine\n \n \nMethod\n.\n\n\nchuacircuit_nscroll_sine\n(;\nu\u2080\n \n=\n \n[\n0.0\n,\n \n0.0\n,\n \n0.28695\n],\n\n    \n\u03b1\n \n=\n \n10.814\n,\n \n\u03b2\n \n=\n \n14\n,\n \n\u03b3\n \n=\n \n0\n,\n \na\n \n=\n \n1.3\n,\n \nb\n \n=\n \n0.11\n,\n \nc\n \n=\n \n2\n,\n\n    \n\u03c3x\n \n=\n \n0.0\n,\n \n\u03c3y\n \n=\n \n0.0\n,\n \n\u03c3z\n \n=\n \n0.0\n)\n\n\n\n\n\nInitialise an adjusted Chua system giving rise to n-scroll attractors [1].\n\n\nEquations of motion\n\n\nThe dynamics is generated by the following vector field\n\n\n\n\n\n\\begin{aligned}\n\\dot{x} = \\alpha (y - fx) + \\eta x \\\\\n\\dot{y} = x - y + z + \\eta y \\\\\n\\dot{z} = -\\beta y - \\gamma z + \\eta z \n\\end{aligned}\n\n\n\n\n\\begin{aligned}\n\\dot{x} = \\alpha (y - fx) + \\eta x \\\\\n\\dot{y} = x - y + z + \\eta y \\\\\n\\dot{z} = -\\beta y - \\gamma z + \\eta z \n\\end{aligned}\n\n\n\n\n\nwhere \n\\eta x\n\\eta x\n, \n\\eta z\n\\eta z\n, and \n\\eta z\n\\eta z\n are drawn independently from  normal distributions with zero mean and standard deviations \n\u03c3x\n, \n\u03c3y\n  and \n\u03c3z\n at each iteration.\n\n\nfx\nfx\n is given by the following conditions: \n\n\nn\n::\nInt\n \n=\n \nc\n \n+\n \n1\n\n\n\nif\n \nx\n \n=\n \n2\n*\na\n*\nc\n\n    \nfx\n \n=\n \n(\nb\n*\npi\n/\n2\n*\na\n)\n*\n(\nx\n \n-\n \n2\n*\na\n*\nc\n)\n\n\nelseif\n \n-\n2\n*\na\n*\nc\n \n \nx\n \n \n2\n*\na\n*\nc\n\n    \nd\n \n=\n \nifelse\n(\nisodd\n(\nn\n),\n \npi\n,\n \n0\n)\n\n    \nfx\n \n=\n \n-\nb\n*\nsin\n((\npi\n*\nx\n/\n2\n*\na\n)\n \n+\n \nd\n)\n\n\nelseif\n \nx\n \n=\n \n-\n2\n*\na\n*\nc\n\n    \nfx\n \n=\n \n(\nb\n*\npi\n/\n2\n*\na\n)\n*\n(\nx\n \n+\n \n2\n*\na\n*\nc\n)\n\n\nend\n\n\n\n\n\nReferences\n\n\n\n\nTang, Wallace KS, et al. \"Generation of n-scroll attractors via   sine function.\" IEEE Transactions on Circuits and Systems I:   Fundamental Theory and Applications 48.11 (2001): 1369-1372.\n\n\n\n\nsource", 
            "title": "Continuous systems"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#continuous-coupled-dynamical-systems", 
            "text": "", 
            "title": "Continuous coupled dynamical systems"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#mediated-link", 
            "text": "#  CausalityTools.Systems.mediated_link     Method .  mediated_link (; u\u2080   =   rand ( 9 ),   \u03c9x   =   1 ,   \u03c9y   =   1.015 ,   \u03c9z   =   0.985 , \n     k   =   0.15 ,   l   =   0.2 ,   m   =   10.0 ,  \n     c   =   0.06 )   -   ContinuousDynamicalSystem   Initialise a three-subsystem dynamical system where  X  and  Y  are driven by  Z . At the default value of the coupling constant  c = 0.06 , the responses  X  and  Y  are already synchronized to the driver  Z .  Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\ndx_1  = -\\omega_x x_2 - x_3 + c*(z_1 - x_1) \\\\\ndx_2  = \\omega_x x_1 + k*x_2  \\\\\ndx_3  = l + x_3(x_1 - m)  \\\\\ndy_1  = -\\omega_y y_2 - y_3 + c*(z_1 - y_1)  \\\\\ndy_2  = \\omega_y y_1 + k*y_2  \\\\\ndy_3  = l + y_3(y_1 - m)  \\\\\ndz_1  = -\\omega_z z_2 - z_3  \\\\\ndz_2  = \\omega_z z_1 + k*z_2  \\\\\ndz_3  = l + z_3(z_1 - m)\n\\end{aligned}  \n\\begin{aligned}\ndx_1 &= -\\omega_x x_2 - x_3 + c*(z_1 - x_1) \\\\\ndx_2 &= \\omega_x x_1 + k*x_2  \\\\\ndx_3 &= l + x_3(x_1 - m)  \\\\\ndy_1 &= -\\omega_y y_2 - y_3 + c*(z_1 - y_1)  \\\\\ndy_2 &= \\omega_y y_1 + k*y_2  \\\\\ndy_3 &= l + y_3(y_1 - m)  \\\\\ndz_1 &= -\\omega_z z_2 - z_3  \\\\\ndz_2 &= \\omega_z z_1 + k*z_2  \\\\\ndz_3 &= l + z_3(z_1 - m)\n\\end{aligned}   References   Krakovsk\u00e1, Anna, et al. \"Comparison of six methods for the detection of   causality in a bivariate time series.\" Physical Review E 97.4 (2018): 042207   source", 
            "title": "Mediated link"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#two-bidirectionally-coupled-3d-lorenz-systems", 
            "text": "#  CausalityTools.Systems.lorenz_lorenz_bidir     Method .  lorenz_lorenz_bidir (;   u0   =   rand ( 6 ),  \n     c_xy   =   0.2 ,   c_yx   =   0.2 ,  \n     a\u2081   =   10 ,   a\u2082   =   28 ,   a\u2083   =   8 / 3 ,  \n     b\u2081   =   10 ,   b\u2082   =   28 ,   b\u2083   =   9 / 3 )   -   ContinuousDynamicalSystem   Initialise a bidirectionally coupled Lorenz-Lorenz system, where each  subsystem is a 3D Lorenz system [1]. Default values for the parameters   a\u2081 ,  a\u2082 ,  a\u2083 ,  b\u2081 ,  b\u2082 ,  b\u2083  are as in [1].  Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\n\\dot{x_1}  = -a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2}  = -x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3}  = x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1}  = -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) \\\\\n\\dot{y_2}  = -y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3}  = y_1 y_2 - b_3 y_3\n\\end{aligned}  \n\\begin{aligned}\n\\dot{x_1} &= -a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2} &= -x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3} &= x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1} &= -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) \\\\\n\\dot{y_2} &= -y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3} &= y_1 y_2 - b_3 y_3\n\\end{aligned}   References   Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.   source", 
            "title": "Two bidirectionally coupled 3D Lorenz systems"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#two-bidirectionally-coupled-3d-lorenz-systems-forced-by-another-3d-lorenz-system", 
            "text": "#  CausalityTools.Systems.lorenz_lorenz_lorenz_bidir_forced     Method .  lorenz_lorenz_lorenz_bidir_forced (;   u0   =   rand ( 9 ),  \n     c_xy   =   0.1 ,   c_yx   =   0.1 , \n     c_zx   =   0.05 ,   c_zy   =   0.05 ,  \n     a\u2081   =   10 ,   a\u2082   =   28 ,   a\u2083   =   8 / 3 , \n     b\u2081   =   10 ,   b\u2082   =   28 ,   b\u2083   =   8 / 3 , \n     c\u2081   =   10 ,   c\u2082   =   28 ,   c\u2083   =   8 / 3 )   Initialise a system consisting of two bidirectionally coupled 3D Lorenz  systems forced by an external 3D Lorenz system, giving a 9D system.  Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\n\\dot{x_1}  = - a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2}  = - x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3}  = x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1}  = -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{y_2}  = - y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3}  = y_1 y_2 - b_3 y_3 \\\\\n\\dot{z_1}  = - c_1 (z_1 - z_2) \\\\\n\\dot{z_2}  = - z_1 z_3 + c_2 z_1 - z_2 \\\\\n\\dot{z_3}  = z_1 z_2 - c_3 z_3 \n\\end{aligned}  \n\\begin{aligned}\n\\dot{x_1} &= - a_1 (x_1 - x_2) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2} &= - x_1 x_3 + a_2 x_1 - x_2 \\\\\n\\dot{x_3} &= x_1 x_2 - a_3 x_3 \\\\\n\\dot{y_1} &= -b_1 (y_1 - y_2) + c_{xy} (x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{y_2} &= - y_1 y_3 + b_2 y_1 - y_2 \\\\\n\\dot{y_3} &= y_1 y_2 - b_3 y_3 \\\\\n\\dot{z_1} &= - c_1 (z_1 - z_2) \\\\\n\\dot{z_2} &= - z_1 z_3 + c_2 z_1 - z_2 \\\\\n\\dot{z_3} &= z_1 z_2 - c_3 z_3 \n\\end{aligned}   References   Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.   source", 
            "title": "Two bidirectionally coupled 3D Lorenz systems forced by another 3D Lorenz system"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#three-transitively-connected-3d-lorenz-systems", 
            "text": "#  CausalityTools.Systems.lorenz_lorenz_lorenz_transitive     Method .  lorenz_lorenz_lorenz_transitive (; u\u1d62 = rand ( 9 ), \n             \u03c3\u2081   =   10.0 ,   \u03c3\u2082   =   10.0 ,   \u03c3\u2083   =   10.0 , \n             \u03c1\u2081   =   28.0 ,   \u03c1\u2082   =   28.0 ,   \u03c1\u2083   =   28.0 , \n             \u03b2\u2081   =   8 / 3 ,    \u03b2\u2082   =   8 / 3 ,    \u03b2\u2083   =   8.3 , \n             c\u2081\u2082   =   1.0 ,   c\u2082\u2083   =   1.0 )   -   ContinuousDynamicalSystem   Initalise a dynamical system consisting of three coupled Lorenz attractors with a transitive causality chain where X\u2081 \u2192 X\u2082 and X\u2082 \u2192 X\u2083. In total, the three 3D-subsystems create a 9-dimensional dynamical system.  The strength of the forcing X\u2081 \u2192 X\u2082 is controlled by the parameter  c\u2081 , and the forcing from X\u2082 \u2192 X\u2083 by  c\u2082 . The remaining parameters are the usual parameters for the Lorenz system, where the subscript  i  refers to the subsystem X\u1d62.   Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\n\\dot{x_1}  = \\sigma_1(y_1 - x_1) \\\\\n\\dot{y_1}  = \\rho_1 x_1 - y_1 - x_1 z_1 \\\\\n\\dot{z_1}  = x_1 y_1 - \\beta_1 z_1 \\\\\n\\dot{x_2}  =  \\sigma_2 (y_2 - x_2) + c_{12}(x_1 - x_2) \\\\\n\\dot{y_2}  = \\rho_2 x_2 - y_2 - x_2 z_2 \\\\\n\\dot{z_2}  = x_2 y_2 - \\beta_2 z_2 \\\\\n\\dot{x_3}  = \\sigma_3 (y_3 - x_3) + c_{23} (x_2 - x_3) \\\\\n\\dot{y_3}  = \\rho_3 x_3 - y_3 - x_3 z_3 \\\\\n\\dot{z_3}  = x_3 y_3 - \\beta_3 z_3\n\\end{aligned}  \n\\begin{aligned}\n\\dot{x_1} &= \\sigma_1(y_1 - x_1) \\\\\n\\dot{y_1} &= \\rho_1 x_1 - y_1 - x_1 z_1 \\\\\n\\dot{z_1} &= x_1 y_1 - \\beta_1 z_1 \\\\\n\\dot{x_2} &=  \\sigma_2 (y_2 - x_2) + c_{12}(x_1 - x_2) \\\\\n\\dot{y_2} &= \\rho_2 x_2 - y_2 - x_2 z_2 \\\\\n\\dot{z_2} &= x_2 y_2 - \\beta_2 z_2 \\\\\n\\dot{x_3} &= \\sigma_3 (y_3 - x_3) + c_{23} (x_2 - x_3) \\\\\n\\dot{y_3} &= \\rho_3 x_3 - y_3 - x_3 z_3 \\\\\n\\dot{z_3} &= x_3 y_3 - \\beta_3 z_3\n\\end{aligned}   Usage in literature  This system was studied by Papana et al. (2013) for coupling strengths   c_{12} = 0, 1, 3, 5 c_{12} = 0, 1, 3, 5  and  c_{23} = 0, 1, 3, 5 c_{23} = 0, 1, 3, 5 .  References   Papana et al., Simulation Study of Direct Causality Measures in Multivariate   Time Series. Entropy 2013, 15(7), 2635-2661; doi:10.3390/e15072635   source", 
            "title": "Three transitively connected 3D Lorenz systems"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#two-bidirectionally-coupled-3d-rossler-systems", 
            "text": "#  CausalityTools.Systems.rossler_rossler_bidir     Method .  rossler_rossler_bidir (;   u0   =   rand ( 6 ),  \n     \u03c9\u2081   =   1.015 ,   \u03c9\u2082   =   0.985 ,  \n     c_xy   =   0.1 ,   c_yx   =   0.1 ,  \n     a\u2081   =   0.15 ,   a\u2082   =   0.2 ,   a\u2083   =   10 , \n     b\u2081   =   0.15 ,   b\u2082   =   0.2 ,   b\u2083   =   10 )   Initialise a system of two bidirectionally coupled 3D R\u00f6ssler systems.  This system has been modified from [1] to allow other parameterisations,  but default parameters are as in [1].  The  X X  and  Y Y  subsystems are mostly synchronized for   c_xy   0.1  or  c_yx   0.1 .  Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\n\\dot{x_1}  = -\\omega_1(x_2 + x_3) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2}  = \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3}  = a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1}  = -\\omega_2 (y_2 + y_3) + c_{xy}(x_1 - y_1) \\\\\n\\dot{y_2}  = \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{y_3}  = b_2 + y_3 (y_1 - b_3)\n\\end{aligned}  \n\\begin{aligned}\n\\dot{x_1} &= -\\omega_1(x_2 + x_3) + c_{yx}(y_1 - x_1) \\\\\n\\dot{x_2} &= \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3} &= a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1} &= -\\omega_2 (y_2 + y_3) + c_{xy}(x_1 - y_1) \\\\\n\\dot{y_2} &= \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{y_3} &= b_2 + y_3 (y_1 - b_3)\n\\end{aligned}   References   Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.   source", 
            "title": "Two bidirectionally coupled 3D R\u00f6ssler systems"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#two-bidirectionally-coupled-3d-rossler-systems-forced-by-another-3d-rossler-system", 
            "text": "#  CausalityTools.Systems.rossler_rossler_rossler_bidir_forced     Method .  rossler_rossler_rossler_bidir_forced (;   u0   =   rand ( 9 ),  \n     \u03c9\u2081   =   1.015 ,   \u03c9\u2082   =   0.985 ,   \u03c9\u2083   =   0.95 , \n     c_xy   =   0.1 ,   c_yx   =   0.1 , \n     c_zx   =   0.05 ,   c_zy   =   0.05 ,  \n     a\u2081   =   0.15 ,   a\u2082   =   0.2 ,   a\u2083   =   10 , \n     b\u2081   =   0.15 ,   b\u2082   =   0.2 ,   b\u2083   =   10 , \n     c\u2081   =   0.15 ,   c\u2082   =   0.2 ,   c\u2083   =   10 )   Equations of motion for a system consisting of three coupled 3D R\u00f6ssler systems  ( X X ,  Y Y ,  Z Z ), giving a 9D system [1]. The external system   Z Z  influences both  X X  and  Y Y  (controlled by  c_zx  and  c_zy ).  Simultaneously, the subsystems   X X  and  Y Y  bidirectionally  influences each other (controlled by  c_xy  and  c_yx ).  The  X X  and  Y Y  subsystems are mostly synchronized for  c_xy   0.1  or   c_yx   0.1 .  Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\n\\dot{x_1}  = -\\omega_1 (x_2 + x_3) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2}  = \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3}  = a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1}  = -\\omega_1 (y_2 + y_3) + c_{xy}(x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{x_2}  = \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{x_3}  = b_2 + x_3 (y_1 - b_3) \\\\\n\\dot{y_1}  = -\\omega_2 (z_2  + z_3) \\\\\n\\dot{x_2}  = \\omega_2 z_1 + c_1 z_2 \\\\\n\\dot{x_3}  = c_2 + z_3 (z_1 - c_3)\n\\end{aligned}  \n\\begin{aligned}\n\\dot{x_1} &= -\\omega_1 (x_2 + x_3) + c_{yx}(y_1 - x_1) + c_{zx}(z_1 - x_1) \\\\\n\\dot{x_2} &= \\omega_1 x_1 + a_1 x_2 \\\\\n\\dot{x_3} &= a_2 + x_3 (x_1 - a_3) \\\\\n\\dot{y_1} &= -\\omega_1 (y_2 + y_3) + c_{xy}(x_1 - y_1) + c_{zy}(z_1 - y_1) \\\\\n\\dot{x_2} &= \\omega_2 y_1 + b_1 y_2 \\\\\n\\dot{x_3} &= b_2 + x_3 (y_1 - b_3) \\\\\n\\dot{y_1} &= -\\omega_2 (z_2  + z_3) \\\\\n\\dot{x_2} &= \\omega_2 z_1 + c_1 z_2 \\\\\n\\dot{x_3} &= c_2 + z_3 (z_1 - c_3)\n\\end{aligned}   References   Amig\u00f3, Jos\u00e9 M., and Yoshito Hirata. \"Detecting directional couplings from   multivariate flows by the joint distance distribution.\" Chaos: An   Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.   source", 
            "title": "Two bidirectionally coupled 3D R\u00f6ssler systems forced by another 3D R\u00f6ssler system"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#unidirectonal-forcing-from-a-3d-rossler-system-to-a-3d-lorenz-system", 
            "text": "#  CausalityTools.Systems.rossler_lorenz     Method .  rossler_lorenz (; u\u2080   =   rand ( 6 ),   a\u2081   =   - 6 ,   a\u2082   =   6 ,   a\u2083   =   2.0 ,  \n     b\u2081   =   10 ,   b\u2082   =   28 ,   b\u2083   =   8 / 3 ,   c_xy   =   1 )   -   ContinuousDynamicalSystem   Initialise a R\u00f6ssler-Lorenz system consisting of two independent 3D subsystems: one R\u00f6ssler system and one Lorenz system. They are coupled such that the second component ( x\u2082 ) of the R\u00f6ssler system unidirectionally forces the second component ( y\u2082 ) of the Lorenz system.   The parameter  c_xy  controls the coupling strength. The implementation here also  allows for tuning the parameters of each subsystem by introducing the constants   a\u2081 ,  a\u2082 ,  a\u2083 ,  b\u2081 ,  b\u2082 ,  b\u2083 . Default values for these parameters are  as in [1].  Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\n\\dot x_1  = a_1(x_2 + x_3) \\\\\n\\dot x_2  = a_2(x_1 + 0.2x_2) \\\\\n\\dot x_3  = a_2(0.2 + x_3(x_1 - a_3)) \\\\\n\\dot y_1  = b_1(y_2 - y_1) \\\\\n\\dot y_2  = y_1(b_2 - y_3) - y_2 +c_{xy}(x_2)^2 \\\\\n\\dot y_3  = y_1 y_2 - b_3y_3\n\\end{aligned}  \n\\begin{aligned}\n\\dot x_1 &= a_1(x_2 + x_3) \\\\\n\\dot x_2 &= a_2(x_1 + 0.2x_2) \\\\\n\\dot x_3 &= a_2(0.2 + x_3(x_1 - a_3)) \\\\\n\\dot y_1 &= b_1(y_2 - y_1) \\\\\n\\dot y_2 &= y_1(b_2 - y_3) - y_2 +c_{xy}(x_2)^2 \\\\\n\\dot y_3 &= y_1 y_2 - b_3y_3\n\\end{aligned}   with the coupling constant  c_{xy} \\geq 0 c_{xy} \\geq 0 .  References   Krakovsk\u00e1, Anna, et al. \"Comparison of six methods for the detection of causality in a   bivariate time series.\" Physical Review E 97.4 (2018):042207.    https://journals.aps.org/pre/abstract/10.1103/PhysRevE.97.042207   source", 
            "title": "Unidirectonal forcing from a 3D R\u00f6ssler system to a 3D Lorenz system"
        }, 
        {
            "location": "/example_systems/example_systems_continuous/#n-scroll-chua-attractors", 
            "text": "#  CausalityTools.Systems.chuacircuit_nscroll_sine     Method .  chuacircuit_nscroll_sine (; u\u2080   =   [ 0.0 ,   0.0 ,   0.28695 ], \n     \u03b1   =   10.814 ,   \u03b2   =   14 ,   \u03b3   =   0 ,   a   =   1.3 ,   b   =   0.11 ,   c   =   2 , \n     \u03c3x   =   0.0 ,   \u03c3y   =   0.0 ,   \u03c3z   =   0.0 )   Initialise an adjusted Chua system giving rise to n-scroll attractors [1].  Equations of motion  The dynamics is generated by the following vector field   \n\\begin{aligned}\n\\dot{x} = \\alpha (y - fx) + \\eta x \\\\\n\\dot{y} = x - y + z + \\eta y \\\\\n\\dot{z} = -\\beta y - \\gamma z + \\eta z \n\\end{aligned}  \n\\begin{aligned}\n\\dot{x} = \\alpha (y - fx) + \\eta x \\\\\n\\dot{y} = x - y + z + \\eta y \\\\\n\\dot{z} = -\\beta y - \\gamma z + \\eta z \n\\end{aligned}   where  \\eta x \\eta x ,  \\eta z \\eta z , and  \\eta z \\eta z  are drawn independently from  normal distributions with zero mean and standard deviations  \u03c3x ,  \u03c3y   and  \u03c3z  at each iteration.  fx fx  is given by the following conditions:   n :: Int   =   c   +   1  if   x   =   2 * a * c \n     fx   =   ( b * pi / 2 * a ) * ( x   -   2 * a * c )  elseif   - 2 * a * c     x     2 * a * c \n     d   =   ifelse ( isodd ( n ),   pi ,   0 ) \n     fx   =   - b * sin (( pi * x / 2 * a )   +   d )  elseif   x   =   - 2 * a * c \n     fx   =   ( b * pi / 2 * a ) * ( x   +   2 * a * c )  end   References   Tang, Wallace KS, et al. \"Generation of n-scroll attractors via   sine function.\" IEEE Transactions on Circuits and Systems I:   Fundamental Theory and Applications 48.11 (2001): 1369-1372.   source", 
            "title": "N-scroll Chua attractors"
        }, 
        {
            "location": "/example_systems/noise/", 
            "text": "#\n\n\nCausalityTools.Systems.noise_uu\n \n \nFunction\n.\n\n\nnoise_uu\n(\nn\n::\nInt\n,\n \nlo\n \n=\n \n-\n \n1\n,\n \nhi\n \n=\n \n1\n)\n\n\n\n\n\nGenerate a signal consisting of \nn\n steps of uncorrelated uniform noise from  a uniform distribution on \n[lo, hi]\n.\n\n\nsource\n\n\n#\n\n\nCausalityTools.Systems.noise_ug\n \n \nFunction\n.\n\n\nnoise_ug\n(\nn\n::\nInt\n;\n \n\u03bc\n \n=\n \n0\n,\n \n\u03c3\n \n=\n \n1\n)\n\n\n\n\n\nGenerate a signal consisting of \nn\n steps of uncorrelated Gaussian noise from a normal distribution with mean \n\u03bc\n and standard deviation \n\u03c3\n.\n\n\nsource\n\n\n#\n\n\nCausalityTools.Systems.noise_brownian\n \n \nFunction\n.\n\n\nnoise_brownian\n(\nn\n::\nInt\n;\n \nlo\n \n=\n \n-\n \n1\n,\n \nhi\n \n=\n \n1\n)\n\n\nnoise_brownian\n(\nd\n::\nDistribution\n,\n \nn\n::\nInt\n)\n\n\n\n\n\nGenerate a signal consisting of \nn\n steps of Brownian noise, generated as the zero-mean and unit standard deviation normalised cumulative sum of noise generated from a uniform distribution on \n[lo, hi]\n. Optionally, a distribution \nd\n from which to sample can be provided.\n\n\nExamples\n\n\n# Based on uncorrelated uniform noise\n\n\nnoise_brownian\n(\n100\n)\n\n\nnoise_brownian\n(\n100\n,\n \nlo\n \n=\n \n-\n2\n,\n \nhi\n \n=\n \n2\n)\n\n\nnoise_brownian\n(\nUniform\n(\n-\n3\n,\n \n3\n),\n \n100\n)\n\n\n\n# Based on uncorrelated Gaussian noise\n\n\n\u03bc\n,\n \n\u03c3\n \n=\n \n0\n,\n \n2\n\n\nnoise_brownian\n(\nNormal\n(\n\u03bc\n,\n \n\u03c3\n),\n \n100\n)\n\n\n\n\n\nsource", 
            "title": "Noise processes"
        }, 
        {
            "location": "/surrogates/surrogates_overview/", 
            "text": "Surrogate method overview\n\n\nThe method of surrogate data is commonly used in the analysis of dynamical systems. For an overview of surrogate methods, see the recent review by \n1\n.\n\n\n\n\nWhat is a surrogate realization?\n\n\nA surrogate realization of a dataset is a dataset that is formed either by  shuffling the values of the original dataset in a particular way. There are  two main ways of creating the data: constrained realizations and typical  realizations \n4\n.\n\n\n\n\nConstrained realizations\n\n\nConstrained surrogate realizations are formed by shuffling the values of the input data series in a way that retains some property of the original data.\n\n\nFor example, random shuffle surrogates (\nrandomshuffle\n) retain the histogram of the data, while AAFT surrogates (\naaft\n) aim to preserve the periodogram of the original data series.  \n\n\n\n\nTypical realizations\n\n\nTypical surrogate realizations are generated by first fitting a model to the input data, then generating new data from that model.\n\n\nFor example, random phase Fourier surrogates (\nrandomphases\n) retain the  amplitudes of the original data, but shuffles the phases.\n\n\n\n\nImplemented algorithms\n\n\nThe following surrogate methods are implemented. Function documentation and basic examples are available from the menu. For more details and demonstrations, visit the \nTimeseriesSurrogates.jl documentation\n.\n\n\n\n\n\n\n\n\nAlgorithm\n\n\nFunction\n\n\nType\n\n\nReference\n\n\n\n\n\n\n\n\n\n\nRandomly shuffling the values of the dataset\n\n\nrandomshuffle\n\n\nConstrained\n\n\n3\n\n\n\n\n\n\nFourier transform phase surrogates\n\n\nrandomphases\n\n\nTypical\n\n\n\n\n\n\n\n\nFourier transform amplitude surrogates\n\n\nrandomamplitudes\n\n\nTypical\n\n\n\n\n\n\n\n\nAmplitude-adjusted Fourier transform surrogates (AAFT)\n\n\naaft\n\n\nConstrained\n\n\n3\n\n\n\n\n\n\nIterated amplitude-adjusted Fourier transform surrogates (iAAFT)\n\n\niaaft\n\n\nConstrained\n\n\n2\n, \n4\n\n\n\n\n\n\n\n\n\n\nValid inputs\n\n\nRandom shuffle surrogates may be generated from the following inputs:\n\n\n\n\nAbstractArray{T, 1}\n instances (scalar-valued data series).\n\n\nAbstractArray{Number, 2}\n instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.\n\n\nDataset\n instances from \nDynamicalSystems.jl\n, for which surrogates are generated column-wise.\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\n\n\n\n\nLancaster, G., Iatsenko, D., Pidde, A., Ticcinelli, V., \n Stefanovska, A. (2018). Surrogate data     for hypothesis testing of physical systems. Physics Reports.  \nhttps://doi.org/10.1016/j.physrep.2018.06.001\n\n\n\n\n\n\nSchreiber, T., \n Schmitz, A. (1996). Improved surrogate data for nonlinearity tests.  Physical Review Letters, 77(4), 635. \nhttps://journals.aps.org/prl/abstract/10.1103/PhysRevLett.77.635\n\n\n\n\n\n\nTheiler, J., Eubank, S., Longtin, A., Galdrikian, B., \n Doyne Farmer, J. (1992).  Testing for nonlinearity in time series: the method of surrogate data. Physica D: Nonlinear Phenomena. \nhttps://doi.org/10.1016/0167-2789(92)90102-S\n\n\n\n\n\n\nTheiler, J., \n Prichard, D. (1996). Constrained-realization Monte-Carlo method for  hypothesis testing. Physica D: Nonlinear Phenomena, 94(4), 221\u2013235.  \nhttps://www.sciencedirect.com/science/article/pii/0167278996000504", 
            "title": "Overview"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#surrogate-method-overview", 
            "text": "The method of surrogate data is commonly used in the analysis of dynamical systems. For an overview of surrogate methods, see the recent review by  1 .", 
            "title": "Surrogate method overview"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#what-is-a-surrogate-realization", 
            "text": "A surrogate realization of a dataset is a dataset that is formed either by  shuffling the values of the original dataset in a particular way. There are  two main ways of creating the data: constrained realizations and typical  realizations  4 .", 
            "title": "What is a surrogate realization?"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#constrained-realizations", 
            "text": "Constrained surrogate realizations are formed by shuffling the values of the input data series in a way that retains some property of the original data.  For example, random shuffle surrogates ( randomshuffle ) retain the histogram of the data, while AAFT surrogates ( aaft ) aim to preserve the periodogram of the original data series.", 
            "title": "Constrained realizations"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#typical-realizations", 
            "text": "Typical surrogate realizations are generated by first fitting a model to the input data, then generating new data from that model.  For example, random phase Fourier surrogates ( randomphases ) retain the  amplitudes of the original data, but shuffles the phases.", 
            "title": "Typical realizations"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#implemented-algorithms", 
            "text": "The following surrogate methods are implemented. Function documentation and basic examples are available from the menu. For more details and demonstrations, visit the  TimeseriesSurrogates.jl documentation .     Algorithm  Function  Type  Reference      Randomly shuffling the values of the dataset  randomshuffle  Constrained  3    Fourier transform phase surrogates  randomphases  Typical     Fourier transform amplitude surrogates  randomamplitudes  Typical     Amplitude-adjusted Fourier transform surrogates (AAFT)  aaft  Constrained  3    Iterated amplitude-adjusted Fourier transform surrogates (iAAFT)  iaaft  Constrained  2 ,  4", 
            "title": "Implemented algorithms"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#valid-inputs", 
            "text": "Random shuffle surrogates may be generated from the following inputs:   AbstractArray{T, 1}  instances (scalar-valued data series).  AbstractArray{Number, 2}  instances (multivarate scalar-valued data series), for which surrogates are generated column-wise.  Dataset  instances from  DynamicalSystems.jl , for which surrogates are generated column-wise.", 
            "title": "Valid inputs"
        }, 
        {
            "location": "/surrogates/surrogates_overview/#references", 
            "text": "Lancaster, G., Iatsenko, D., Pidde, A., Ticcinelli, V.,   Stefanovska, A. (2018). Surrogate data     for hypothesis testing of physical systems. Physics Reports.   https://doi.org/10.1016/j.physrep.2018.06.001    Schreiber, T.,   Schmitz, A. (1996). Improved surrogate data for nonlinearity tests.  Physical Review Letters, 77(4), 635.  https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.77.635    Theiler, J., Eubank, S., Longtin, A., Galdrikian, B.,   Doyne Farmer, J. (1992).  Testing for nonlinearity in time series: the method of surrogate data. Physica D: Nonlinear Phenomena.  https://doi.org/10.1016/0167-2789(92)90102-S    Theiler, J.,   Prichard, D. (1996). Constrained-realization Monte-Carlo method for  hypothesis testing. Physica D: Nonlinear Phenomena, 94(4), 221\u2013235.   https://www.sciencedirect.com/science/article/pii/0167278996000504", 
            "title": "References"
        }, 
        {
            "location": "/surrogates/iaaft_docs/", 
            "text": "Surrogate methods\n\n\n\n\nIterated amplitude-adjusted Fourier transform (iAAFT) surrogates\n\n\n#\n\n\nTimeseriesSurrogates.iaaft\n \n \nFunction\n.\n\n\niaaft\n(\na\n::\nAbstractArray\n{\nNumber\n,\n \n2\n};\n \ncols\n \n=\n \n1\n:\nsize\n(\nd\n,\n \n2\n))\n\n\n\n\n\nIterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\niaaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\niaaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nIterated amplitude-adjusted Fourier transform (IAAFT) surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\n\n\nExamples\n\n\n\n\nExample 1\n\n\nnpts\n \n=\n \n200\n\n\nts\n \n=\n \nsin\n.\n(\ndiff\n(\nrand\n(\nnpts\n \n+\n \n1\n)))\n*\n0.5\n \n.+\n \ncos\n.\n(\nLinRange\n(\n0\n,\n \n14\n*\npi\n,\n \nnpts\n))\n\n\np1\n \n=\n \nplot\n(\nts\n,\n \nlabel\n \n=\n \nts\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\np2\n \n=\n \nplot\n(\niaaft\n(\nts\n),\n \nlabel\n \n=\n \niaaft(ts)\n,\n \nxlabel\n \n=\n \nTime step\n)\n\n\nplot\n(\np1\n,\n \np2\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n))\n\n\nylabel!\n(\nValue\n);\n\n\n\n\n\n\n\n\n\nExample 2\n\n\nThis gif shows iAAFT surrogate realizations for an cyclostationary AR2 (NSAR2) process (\nnsar2\n) from [1].\n\n\n\n\n\n\nExample 3\n\n\nThis gif shows iAAFT surrogate realizations for an AR1 process.\n\n\n\n\n\n\nReferences\n\n\n\n\nLucio et al., Phys. Rev. E \n85\n, 056202 (2012), after J. Timmer, Phys. Rev. E \n58\n, 5153   (1998). \nhttps://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202", 
            "title": "IAAFT"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#iterated-amplitude-adjusted-fourier-transform-iaaft-surrogates", 
            "text": "#  TimeseriesSurrogates.iaaft     Function .  iaaft ( a :: AbstractArray { Number ,   2 };   cols   =   1 : size ( d ,   2 ))   Iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  iaaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))  Column-wise iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  iaaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Iterated amplitude-adjusted Fourier transform (IAAFT) surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Iterated amplitude-adjusted Fourier transform (iAAFT) surrogates"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#example-1", 
            "text": "npts   =   200  ts   =   sin . ( diff ( rand ( npts   +   1 ))) * 0.5   .+   cos . ( LinRange ( 0 ,   14 * pi ,   npts ))  p1   =   plot ( ts ,   label   =   ts ,   lc   =   : black )  p2   =   plot ( iaaft ( ts ),   label   =   iaaft(ts) ,   xlabel   =   Time step )  plot ( p1 ,   p2 ,   layout   =   ( 2 ,   1 ))  ylabel! ( Value );", 
            "title": "Example 1"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#example-2", 
            "text": "This gif shows iAAFT surrogate realizations for an cyclostationary AR2 (NSAR2) process ( nsar2 ) from [1].", 
            "title": "Example 2"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#example-3", 
            "text": "This gif shows iAAFT surrogate realizations for an AR1 process.", 
            "title": "Example 3"
        }, 
        {
            "location": "/surrogates/iaaft_docs/#references", 
            "text": "Lucio et al., Phys. Rev. E  85 , 056202 (2012), after J. Timmer, Phys. Rev. E  58 , 5153   (1998).  https://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.056202", 
            "title": "References"
        }, 
        {
            "location": "/surrogates/aaft_docs/", 
            "text": "Surrogate methods\n\n\n\n\nAmplitude-adjusted Fourier transform (AAFT) surrogates\n\n\n#\n\n\nTimeseriesSurrogates.aaft\n \n \nFunction\n.\n\n\naaft\n(\na\n::\nAbstractArray\n{\nNumber\n,\n \n2\n};\n \ncols\n \n=\n \n1\n:\nsize\n(\nd\n,\n \n2\n))\n\n\n\n\n\nColumn-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\naaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\naaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise amplitude-adjusted Fourier transform (AAFT) surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\n\n\nExample\n\n\nnpts\n \n=\n \n200\n\n\nts\n \n=\n \nsin\n.\n(\ndiff\n(\nrand\n(\nnpts\n \n+\n \n1\n)))\n*\n0.5\n \n.+\n \ncos\n.\n(\nLinRange\n(\n0\n,\n \n14\n*\npi\n,\n \nnpts\n))\n\n\np1\n \n=\n \nplot\n(\nts\n,\n \nlabel\n \n=\n \nts\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\np2\n \n=\n \nplot\n(\naaft\n(\nts\n),\n \nlabel\n \n=\n \naaft(ts)\n,\n \nxlabel\n \n=\n \nTime step\n)\n\n\nplot\n(\np1\n,\n \np2\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n))\n\n\nylabel!\n(\nValue\n);", 
            "title": "AAFT"
        }, 
        {
            "location": "/surrogates/aaft_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/aaft_docs/#amplitude-adjusted-fourier-transform-aaft-surrogates", 
            "text": "#  TimeseriesSurrogates.aaft     Function .  aaft ( a :: AbstractArray { Number ,   2 };   cols   =   1 : size ( d ,   2 ))   Column-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  aaft(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))  Column-wise amplitude-adjusted Fourier transform (AAFT) surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  aaft(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise amplitude-adjusted Fourier transform (AAFT) surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Amplitude-adjusted Fourier transform (AAFT) surrogates"
        }, 
        {
            "location": "/surrogates/aaft_docs/#example", 
            "text": "npts   =   200  ts   =   sin . ( diff ( rand ( npts   +   1 ))) * 0.5   .+   cos . ( LinRange ( 0 ,   14 * pi ,   npts ))  p1   =   plot ( ts ,   label   =   ts ,   lc   =   : black )  p2   =   plot ( aaft ( ts ),   label   =   aaft(ts) ,   xlabel   =   Time step )  plot ( p1 ,   p2 ,   layout   =   ( 2 ,   1 ))  ylabel! ( Value );", 
            "title": "Example"
        }, 
        {
            "location": "/surrogates/randomphases_docs/", 
            "text": "Surrogate methods\n\n\n\n\nRandom phases Fourier surrogates\n\n\n#\n\n\nTimeseriesSurrogates.randomphases\n \n \nFunction\n.\n\n\nrandomphases\n(\na\n::\nAbstractArray\n{\nNumber\n,\n \n2\n},\n \ncols\n \n=\n \n1\n:\nsize\n(\nd\n,\n \n2\n))\n\n\n\n\n\nColumn-wise random phases Fourier surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomphases(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise random phases Fourier surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomphases(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random phases Fourier surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\n\n\nExample\n\n\nGenerating random phase surrogates is done as follows.\n\n\n# Generate a dynamical system, create an orbit and extract a time series from\n\n\n# the second component.\n\n\ns\n \n=\n \nCausalityTools\n.\nSystems\n.\nlogistic3\n()\n\n\norbit\n \n=\n \ntrajectory\n(\ns\n,\n \n150\n)\n\n\nx\n \n=\n \norbit\n[\n:\n,\n \n2\n]\n\n\n\n\n# Plot the time series along with its random phase surrogate\n\n\ntimesteps\n \n=\n \n1\n:\nsize\n(\norbit\n,\n \n1\n)\n\n\np1\n \n=\n \nplot\n(\ntimesteps\n,\n \nx\n,\n \nlabel\n \n=\n \nx\n)\n\n\np2\n \n=\n \nplot\n(\ntimesteps\n,\n \nrandomphases\n(\nx\n),\n \nlabel\n \n=\n \nrandomphases(x)\n,\n\n    \nxlabel\n \n=\n \nTime step\n)\n\n\nplot\n(\np1\n,\n \np2\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n))\n\n\nylabel!\n(\nValue\n)", 
            "title": "Fourier surrogates, random phases"
        }, 
        {
            "location": "/surrogates/randomphases_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomphases_docs/#random-phases-fourier-surrogates", 
            "text": "#  TimeseriesSurrogates.randomphases     Function .  randomphases ( a :: AbstractArray { Number ,   2 },   cols   =   1 : size ( d ,   2 ))   Column-wise random phases Fourier surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  randomphases(E::Embeddings.AbstractEmbedding; cols = 1:size(E.points, 1))  Column-wise random phases Fourier surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  randomphases(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise random phases Fourier surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Random phases Fourier surrogates"
        }, 
        {
            "location": "/surrogates/randomphases_docs/#example", 
            "text": "Generating random phase surrogates is done as follows.  # Generate a dynamical system, create an orbit and extract a time series from  # the second component.  s   =   CausalityTools . Systems . logistic3 ()  orbit   =   trajectory ( s ,   150 )  x   =   orbit [ : ,   2 ]  # Plot the time series along with its random phase surrogate  timesteps   =   1 : size ( orbit ,   1 )  p1   =   plot ( timesteps ,   x ,   label   =   x )  p2   =   plot ( timesteps ,   randomphases ( x ),   label   =   randomphases(x) , \n     xlabel   =   Time step )  plot ( p1 ,   p2 ,   layout   =   ( 2 ,   1 ))  ylabel! ( Value )", 
            "title": "Example"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/", 
            "text": "Surrogate methods\n\n\n\n\nRandom amplitude Fourier surrogates\n\n\n#\n\n\nTimeseriesSurrogates.randomamplitudes\n \n \nFunction\n.\n\n\nrandomamplitudes\n(\na\n::\nAbstractArray\n{\nNumber\n,\n \n2\n};\n \ncols\n \n=\n \n1\n:\nsize\n(\nd\n,\n \n2\n))\n\n\n\n\n\nColumn-wise random amplitude Fourier surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomamplitudes(E::Embeddings.AbstractEmbedding;\n                    cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise random amplitude Fourier surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomamplitudes(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random amplitude Fourier surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\n\n\nExample\n\n\n# Generate a dynamical system, create an orbit and extract a time series from\n\n\n# the first component.\n\n\ns\n \n=\n \nCausalityTools\n.\nSystems\n.\nlogistic3\n()\n\n\norbit\n \n=\n \ntrajectory\n(\ns\n,\n \n150\n)\n\n\nx\n \n=\n \norbit\n[\n:\n,\n \n1\n]\n\n\n\n# Compare original time series and random amplitude surrogate\n\n\np1\n \n=\n \nplot\n(\nx\n,\n \nlabel\n \n=\n \nx\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\np2\n \n=\n \nplot\n(\nrandomamplitudes\n(\nx\n),\n \nlabel\n \n=\n \nrandomamplitudes(x)\n,\n\n            \nxlabel\n \n=\n \nTime step\n)\n\n\nplot\n(\np1\n,\n \np2\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n))\n\n\nylabel!\n(\nValue\n)", 
            "title": "Fourier surrogates, random amplitudes"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/#random-amplitude-fourier-surrogates", 
            "text": "#  TimeseriesSurrogates.randomamplitudes     Function .  randomamplitudes ( a :: AbstractArray { Number ,   2 };   cols   =   1 : size ( d ,   2 ))   Column-wise random amplitude Fourier surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  randomamplitudes(E::Embeddings.AbstractEmbedding;\n                    cols = 1:size(E.points, 1))  Column-wise random amplitude Fourier surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  randomamplitudes(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise random amplitude Fourier surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Random amplitude Fourier surrogates"
        }, 
        {
            "location": "/surrogates/randomamplitudes_docs/#example", 
            "text": "# Generate a dynamical system, create an orbit and extract a time series from  # the first component.  s   =   CausalityTools . Systems . logistic3 ()  orbit   =   trajectory ( s ,   150 )  x   =   orbit [ : ,   1 ]  # Compare original time series and random amplitude surrogate  p1   =   plot ( x ,   label   =   x ,   lc   =   : black )  p2   =   plot ( randomamplitudes ( x ),   label   =   randomamplitudes(x) , \n             xlabel   =   Time step )  plot ( p1 ,   p2 ,   layout   =   ( 2 ,   1 ))  ylabel! ( Value )", 
            "title": "Example"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/", 
            "text": "Surrogate methods\n\n\n\n\nRandom shuffle surrogates\n\n\n#\n\n\nTimeseriesSurrogates.randomshuffle\n \n \nFunction\n.\n\n\nrandomshuffle\n(\na\n::\nAbstractArray\n{\nNumber\n,\n \n2\n};\n \ncols\n \n=\n \n1\n:\nsize\n(\nd\n,\n \n2\n))\n\n\n\n\n\nColumn-wise random shuffle surrogate of an array, where each column is a scalar-valued time series. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomshuffle(E::Embeddings.AbstractEmbedding;\n                cols = 1:size(E.points, 1))\n\n\n\n\nColumn-wise random shuffle surrogate of an embedding. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\nrandomshuffle(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))\n\n\n\n\nColumn-wise random shuffle surrogate of a Dataset. \ncols\n controls which variables of the embedding are shuffled.\n\n\nsource\n\n\n\n\nExample\n\n\nnpts\n \n=\n \n100\n\n\nts\n \n=\n \nsin\n.\n(\ndiff\n(\nrand\n(\nnpts\n \n+\n \n1\n)))\n*\n0.5\n \n.+\n \ncos\n.\n(\nLinRange\n(\n0\n,\n \n8\n*\npi\n,\n \nnpts\n))\n\n\np1\n \n=\n \nplot\n(\nts\n,\n \nlabel\n \n=\n \nts\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\np2\n \n=\n \nplot\n(\nrandomshuffle\n(\nts\n),\n \nlabel\n \n=\n \nrandomshuffle(ts)\n,\n \nxlabel\n \n=\n \nTime step\n)\n\n\nplot\n(\np1\n,\n \np2\n,\n \nlayout\n \n=\n \n(\n2\n,\n \n1\n))\n\n\nylabel!\n(\nValue\n);", 
            "title": "Random shuffle"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/#surrogate-methods", 
            "text": "", 
            "title": "Surrogate methods"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/#random-shuffle-surrogates", 
            "text": "#  TimeseriesSurrogates.randomshuffle     Function .  randomshuffle ( a :: AbstractArray { Number ,   2 };   cols   =   1 : size ( d ,   2 ))   Column-wise random shuffle surrogate of an array, where each column is a scalar-valued time series.  cols  controls which variables of the embedding are shuffled.  source  randomshuffle(E::Embeddings.AbstractEmbedding;\n                cols = 1:size(E.points, 1))  Column-wise random shuffle surrogate of an embedding.  cols  controls which variables of the embedding are shuffled.  source  randomshuffle(d::DynamicalSystemsBase.Dataset; cols = 1:size(d, 2))  Column-wise random shuffle surrogate of a Dataset.  cols  controls which variables of the embedding are shuffled.  source", 
            "title": "Random shuffle surrogates"
        }, 
        {
            "location": "/surrogates/randomshuffle_docs/#example", 
            "text": "npts   =   100  ts   =   sin . ( diff ( rand ( npts   +   1 ))) * 0.5   .+   cos . ( LinRange ( 0 ,   8 * pi ,   npts ))  p1   =   plot ( ts ,   label   =   ts ,   lc   =   : black )  p2   =   plot ( randomshuffle ( ts ),   label   =   randomshuffle(ts) ,   xlabel   =   Time step )  plot ( p1 ,   p2 ,   layout   =   ( 2 ,   1 ))  ylabel! ( Value );", 
            "title": "Example"
        }, 
        {
            "location": "/uncertaindata/BinnedDataCausalityTest/", 
            "text": "#\n\n\nCausalityTools.CausalityTests.BinnedDataCausalityTest\n \n \nType\n.\n\n\nBinnedDataCausalityTest\n(\ntest\n::\nCT\n,\n \nbinning\n::\nAbstractBinnedUncertainValueResampling\n,\n \nn_realizations\n::\nInt\n)\n\n\nBinnedDataCausalityTest\n(\ntest\n::\nCT\n,\n \nbinning\n::\nAbstractBinnedSummarisedResampling\n)\n\n\n\n\n\nA causality test where the data are binned before applying the test. If \nbinning\n is some  binning scheme that returns an uncertain value for each  bin (e.g. \nBinnedResampling\n or \nBinnedWeightedResampling\n), then the \ntest\n is  applied \nn_realizations\n times. If \nbinning\n returns a single value for each bin  (e.g. \nBinnedMeanResampling\n or \nBinnedMeanWeightedResampling\n, the \ntest\n is applied only once.\n\n\nFields\n\n\n\n\ntest::CausalityTest\n. An instance of a causality test, e.g. \nVisitationFrequencyTest\n,    \nPredictiveAsymmetryTest\n, \nCrossMappingTest\n or \nJointDistanceDistributionTest\n.\n\n\nbinning::AbstractBinnedResampling\n. An instance of a resampling scheme    indicating the type of binning, e.g \nBinnedResampling\n, \nBinnedWeightedResampling\n,    \nBinnedMeanResampling\n or \nBinnedMeanWeightedResampling\n.\n\n\nn_realizations::Int\n: The number of independent draws of the binned data set over   which to compute the causality \ntest\n. Only taken into consideration of \nbinning\n is    a scheme resulting in each bin being represented by an uncertain value. If bins are    summarized, then the \ntest\n is always applied only once.\n\n\n\n\nExamples\n\n\nBinned convergent cross mapping test\n\n\nLet's say we want to bin the data by drawing n_draws = 5000 realiastions of  each uncertain data point, then assign the draws to the correct bins, and finally get a kernel density estimate to the distribution of values in each bin.\n\n\ngrid\n \n=\n \n0\n:\n10\n:\n1000\n \n# left bin edges\n\n\nn_draw\n \n=\n \n5000\n \n# sample each point 5000 times and distribute among bins\n\n\nbinning\n \n=\n \nBinnedResampling\n(\ngrid\n,\n \n5000\n)\n\n\n\nccm_test\n \n=\n \nConvergentCrossMappingTest\n(\ntimeseries_lengths\n \n=\n \n15\n:\n5\n:\n100\n \n|\n \ncollect\n,\n \n    \nn_reps\n \n=\n \n100\n,\n \nlibsize\n \n=\n \n100\n,\n \n\u03c4\n \n=\n \n1\n,\n \nreplace\n \n=\n \ntrue\n)\n\n\n\ntest\n \n=\n \nBinnedDataCausalityTest\n(\nccm_test\n,\n \nbinning\n,\n \nn_realizations\n)\n\n\n\n\n\nBinned transfer entropy test\n\n\nLet's say we want to bin the data by drawing n_draws = 7500 realiastions of  each uncertain data point, then assign the draws to the correct bins, and finally get a kernel density estimate to the distribution of values in each bin.\n\n\ngrid\n \n=\n \n0\n:\n5\n:\n1000\n \n# left bin edges\n\n\nn_draws\n \n=\n \n7500\n\n\nbinning\n \n=\n \nBinnedResampling\n(\ngrid\n,\n \nn_draws\n)\n\n\n\n# A transfer entropy causality test using the visitation frequency estimator\n\n\nstate_space_binning\n \n=\n \nRectangularBinning\n(\n5\n)\n\n\nte_test\n \n=\n \nVisitationFrequencyTest\n(\nbinning\n \n=\n \nstate_space_binning\n,\n \n\u03b7s\n \n=\n \n1\n)\n\n\n\ntest\n \n=\n \nBinnedDataCausalityTest\n(\nte_test\n,\n \nbinning\n,\n \nn_realizations\n)\n\n\n\n\n\nsource", 
            "title": "BinnedDataCausalityTest"
        }, 
        {
            "location": "/tutorials/list_of_tutorials/", 
            "text": "Tutorial notebooks\n\n\nThe following tutorials are also available as \nJuputer notebooks\n.\n\n\n\n\nCausality from uncertain data\n\n\n\n\nPredictiveAsymmetryTest\n with naive resampling\n\n\n\n\n\n\nCausality from uncertain data, binning\n\n\nMany causality tests require data that are equally spaced in time. If your time series have timing uncertainties, they can be brought on a common, regularly spaced time grid  by using a \nbinned resampling scheme\n. Then, define an instance of a \nBinnedDataCausalityTest\n with the desired binning scheme and \ncausality test\n of your choice, and apply the test over one or more draws of the binned dataset. Below are some examples:\n\n\n\n\nPredictiveAsymmetryTest\n with \nBinnedResampling\n [\nnotebook\n]\n\n\nPredictiveAsymmetryTest\n with \nBinnedMeanResampling\n [\nnotebook\n]", 
            "title": "List of tutorials"
        }, 
        {
            "location": "/tutorials/list_of_tutorials/#tutorial-notebooks", 
            "text": "The following tutorials are also available as  Juputer notebooks .", 
            "title": "Tutorial notebooks"
        }, 
        {
            "location": "/tutorials/list_of_tutorials/#causality-from-uncertain-data", 
            "text": "PredictiveAsymmetryTest  with naive resampling", 
            "title": "Causality from uncertain data"
        }, 
        {
            "location": "/tutorials/list_of_tutorials/#causality-from-uncertain-data-binning", 
            "text": "Many causality tests require data that are equally spaced in time. If your time series have timing uncertainties, they can be brought on a common, regularly spaced time grid  by using a  binned resampling scheme . Then, define an instance of a  BinnedDataCausalityTest  with the desired binning scheme and  causality test  of your choice, and apply the test over one or more draws of the binned dataset. Below are some examples:   PredictiveAsymmetryTest  with  BinnedResampling  [ notebook ]  PredictiveAsymmetryTest  with  BinnedMeanResampling  [ notebook ]", 
            "title": "Causality from uncertain data, binning"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedResampling/", 
            "text": "PredictiveAsymmetryTest\n with \nBinnedResampling\n\n\n\n\nExample data\n\n\nFirst, we'll generate some example data from an AR1 system. We'll use the \nar1_unidir1\n system that ships with \nCausalityTools\n. \n\n\nUncertainties are  added using the \nexample_uncertain_indexvalue_datasets\n function from  \nUncertainData.jl\n. It accepts as inputs a \nDiscreteDynamicalSystems\n instance, the number of time steps to iterate the system, a 2-tuple of variables  to use (by their index) and, optionally, a time step. \n\n\nusing\n \nCausalityTools\n,\n \nUncertainData\n,\n \nPlots\n\n\nsys\n \n=\n \nar1_unidir\n(\nu\u1d62\n \n=\n \n[\n0.1\n,\n \n0.1\n],\n \nc_xy\n \n=\n \n0.41\n)\n\n\nvars\n \n=\n \n(\n1\n,\n \n2\n)\n \n# ar1_unidir has only two variables, X and Y, we want both\n\n\nn_steps\n \n=\n \n100\n \n# the number of points in the time series\n\n\ntstep\n \n=\n \n10\n \n# the mean of each time value is stepped by `tstep`\n\n\n\nX\n,\n \nY\n \n=\n \nexample_uncertain_indexvalue_datasets\n(\nsys\n,\n \nn_steps\n,\n \nvars\n,\n \ntstep\n \n=\n \n10\n,\n\n    \nd_xind\n \n=\n \nUniform\n(\n7.5\n,\n \n15.5\n),\n\n    \nd_yind\n \n=\n \nUniform\n(\n5.5\n,\n \n15.5\n),\n\n    \nd_xval\n \n=\n \nUniform\n(\n0.1\n,\n \n0.5\n));\n\n\n\n\n\nNow we have a time series with normally distributed time indices with means  ranging from 1 to 1001 in steps of 10. Let's say we want to bin the data  with equally-sized bins with size 25.\n\n\n```@example PredictiveAsymmetryTest_BinnedResampling\nqs = [0.1, 0.9] # quantiles to display\n\n\npX = plot(X, mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black), qs, qs, ylabel = \"X\")\nplot!(pX, mean.(X.indices), mean.(X.values), c = :black, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dash, \u03b1 = 0.5, lw = 0.5)\npY = plot(Y, mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red), qs, qs, ylabel = \"Y\")\nplot!(pY, mean.(Y.indices), mean.(Y.values), c = :red, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dash, \u03b1 = 0.5, lw = 0.5)\n\n\nplot(pX, pY, layout = (2, 1), xlabel = \"Time step\", ylims = (-2.5, 2.5), legend = false)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedResampling_x_and_y.svg\"); nothing # hide\n\n![](figs/PredictiveAsymmetryTest_BinnedResampling_x_and_y.svg)\n\n\n\na id=\nHow-do-the-data-look-when-binned?-1\n/a\n\n\n## How do the data look when binned?\n\n\nLet\ns use a slightly finer binning and investigate what the binned data look like.\n\n\nThe time uncertainty in each bin is assumed uniform, while the value uncertainty  is represented as a kernel density estimate to the distribution of points in each bin.\n\n\n```@example PredictiveAsymmetryTest_BinnedResampling\ngrid = 0:5:1000\nn_draws = 5000 # resample each point 5000 times and distribute among bins\nbinned_resampling = BinnedResampling(grid, n_draws)\n\nX_binned = resample(X, binned_resampling)\nY_binned = resample(Y, binned_resampling)\n\npX = plot(X_binned, qs, qs, ylabel = \nX\n,\n    mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black))\nvline!(binned_resampling.left_bin_edges, ls = :dash, \u03b1 = 0.5, lw = 0.5)\n\npY = plot(Y_binned, qs, qs, ylabel = \nY\n,\n    mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red))\nvline!(binned_resampling.left_bin_edges, ls = :dash, \u03b1 = 0.5, lw = 0.5)\n\nplot(pX, pY, layout = (2, 1), xlabel = \nTime step\n, legend = false)\nsavefig(\nfigs/PredictiveAsymmetryTest_BinnedResampling_x_and_y_binned.svg\n); nothing # hide\n\n\n\n\n\n\n\nDefining a PredictiveAsymmetryTest\n\n\nA \nPredictiveAsymmetryTest\n takes as input a causality test that uses lagged  prediction. Here, we'll use a transfer entropy test, using a visitation  frequency estimator to perform the computations . We'll predict 5 time steps  forwards and backwards in time (\n\u03b7s = -5:5\n).\n\n\nTransfer entropy is computing over a discretization of a state space  reconstruction of the time series involves. Here, we'll use a  3-dimensional reconstruction for the transfer entropy computations.\n\n\nTo do this computation, we need to specify how to discretize the reconstructed  state space. To keeo it simple, we'll divide the state-space into hyperrectangular  regions defined by diving each coordinate axis of the state space into equally-spaced  intervals. We'll determine the number of intervals to split each axis into by a  trade-off between the dimensionality of the system (ensuring enough points in each bin on average) and the number of points in the system. The approach below  roughly follows Krakovska et al. (2018).\n\n\nk\n,\n \nl\n,\n \nm\n \n=\n \n1\n,\n \n1\n,\n \n1\n \n# embedding parameters, total dimension is k + l + m\n\n\nn_subdivisions\n \n=\n \nfloor\n(\nInt\n,\n \nlength\n(\ngrid\n)\n^\n(\n1\n/\n(\nk\n \n+\n \nl\n \n+\n \nm\n \n+\n \n1\n)))\n\n\nstate_space_binning\n \n=\n \nRectangularBinning\n(\nn_subdivisions\n);\n\n\n\n# Configure a visitation frequency estimator transfer entropy test\n\n\n# using base-2 logarithms\n\n\n\u03b7s\n \n=\n \n-\n5\n:\n5\n \n# we\nre predicting five steps forwards and backwards in time\n\n\nte_test\n \n=\n \nVisitationFrequencyTest\n(\nk\n \n=\n \nk\n,\n \nl\n \n=\n \nl\n,\n \nm\n \n=\n \nm\n,\n\n    \nbinning\n \n=\n \nstate_space_binning\n,\n \nb\n \n=\n \n2\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\n\npa_test\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \nte_test\n)\n\n\n\n# Now we can combine the binning from above with the predictive asymmetry\n\n\n# test we just defined. We\nll apply the causality test to 50 independent\n\n\n# draws of the binned dataset.\n\n\nn_realizations\n \n=\n \n50\n\n\ntest\n \n=\n \nBinnedDataCausalityTest\n(\npa_test\n,\n \nbinned_resampling\n,\n \nn_realizations\n)\n\n\n\n\n\nFinally, we can compute the predictive asymmetry in both directions for the bin means.\n\n\ntes_xy\n \n=\n \ncausality\n(\nX\n,\n \nY\n,\n \ntest\n)\n\n\ntes_yx\n \n=\n \ncausality\n(\nY\n,\n \nX\n,\n \ntest\n)\n\n\n\n\n\ntes_xy\n and \ntes_yx\n are now both length-\n50\n vectors, where each element is a length-\n5\n vector containing the predictive asymmetries for prediction lags \n1:5\n. Let's summarise the data for each prediction lag and plot the results.\n\n\n```@example PredictiveAsymmetryTest_BinnedResampling\n\n\nGather results in a matrix and compute means and standard deviations\n\n\nfor the predictive asymmetries at each prediction lag\n\n\nM_xy = hcat(tes_xy...,)\nM_yx = hcat(tes_yx...,)\n\n\nmeans_xy = mean(M_xy, dims = 2)[:, 1]\nmeans_yx = mean(M_yx, dims = 2)[:, 1]\nstdevs_xy = std(M_yx, dims = 2)[:, 1]\nstdevs_yx = std(M_yx, dims = 2)[:, 1]\n\n\nPlot the predictive asymmetry as a function of prediction lag\n\n\nplot(xlabel = \"Prediction lag (eta)\", ylabel = \"Predictive asymmetry (bits)\")\nplot!(\u03b7s[\u03b7s .\n 0], means_xy, ribbon = stdevs_xy, label = \"x -\n y (binned)\", c = :black)\nplot!(\u03b7s[\u03b7s .\n 0], means_yx, ribbon = stdevs_yx, label = \"y -\n x (binned)\", c = :red)\nhline!([0], lw = 2, ls = :dot, \u03b1 = 0.5, label = \"\", c = :grey)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedResampling_lag_vs_A.svg\"); nothing # hide\n```", 
            "title": "BinnedResampling"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedResampling/#predictiveasymmetrytest-with-binnedresampling", 
            "text": "", 
            "title": "PredictiveAsymmetryTest with BinnedResampling"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedResampling/#example-data", 
            "text": "First, we'll generate some example data from an AR1 system. We'll use the  ar1_unidir1  system that ships with  CausalityTools .   Uncertainties are  added using the  example_uncertain_indexvalue_datasets  function from   UncertainData.jl . It accepts as inputs a  DiscreteDynamicalSystems  instance, the number of time steps to iterate the system, a 2-tuple of variables  to use (by their index) and, optionally, a time step.   using   CausalityTools ,   UncertainData ,   Plots  sys   =   ar1_unidir ( u\u1d62   =   [ 0.1 ,   0.1 ],   c_xy   =   0.41 )  vars   =   ( 1 ,   2 )   # ar1_unidir has only two variables, X and Y, we want both  n_steps   =   100   # the number of points in the time series  tstep   =   10   # the mean of each time value is stepped by `tstep`  X ,   Y   =   example_uncertain_indexvalue_datasets ( sys ,   n_steps ,   vars ,   tstep   =   10 , \n     d_xind   =   Uniform ( 7.5 ,   15.5 ), \n     d_yind   =   Uniform ( 5.5 ,   15.5 ), \n     d_xval   =   Uniform ( 0.1 ,   0.5 ));   Now we have a time series with normally distributed time indices with means  ranging from 1 to 1001 in steps of 10. Let's say we want to bin the data  with equally-sized bins with size 25.  ```@example PredictiveAsymmetryTest_BinnedResampling\nqs = [0.1, 0.9] # quantiles to display  pX = plot(X, mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black), qs, qs, ylabel = \"X\")\nplot!(pX, mean.(X.indices), mean.(X.values), c = :black, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dash, \u03b1 = 0.5, lw = 0.5)\npY = plot(Y, mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red), qs, qs, ylabel = \"Y\")\nplot!(pY, mean.(Y.indices), mean.(Y.values), c = :red, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dash, \u03b1 = 0.5, lw = 0.5)  plot(pX, pY, layout = (2, 1), xlabel = \"Time step\", ylims = (-2.5, 2.5), legend = false)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedResampling_x_and_y.svg\"); nothing # hide ![](figs/PredictiveAsymmetryTest_BinnedResampling_x_and_y.svg) a id= How-do-the-data-look-when-binned?-1 /a \n\n## How do the data look when binned?\n\n\nLet s use a slightly finer binning and investigate what the binned data look like.\n\n\nThe time uncertainty in each bin is assumed uniform, while the value uncertainty  is represented as a kernel density estimate to the distribution of points in each bin.\n\n\n```@example PredictiveAsymmetryTest_BinnedResampling\ngrid = 0:5:1000\nn_draws = 5000 # resample each point 5000 times and distribute among bins\nbinned_resampling = BinnedResampling(grid, n_draws)\n\nX_binned = resample(X, binned_resampling)\nY_binned = resample(Y, binned_resampling)\n\npX = plot(X_binned, qs, qs, ylabel =  X ,\n    mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black))\nvline!(binned_resampling.left_bin_edges, ls = :dash, \u03b1 = 0.5, lw = 0.5)\n\npY = plot(Y_binned, qs, qs, ylabel =  Y ,\n    mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red))\nvline!(binned_resampling.left_bin_edges, ls = :dash, \u03b1 = 0.5, lw = 0.5)\n\nplot(pX, pY, layout = (2, 1), xlabel =  Time step , legend = false)\nsavefig( figs/PredictiveAsymmetryTest_BinnedResampling_x_and_y_binned.svg ); nothing # hide", 
            "title": "Example data"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedResampling/#defining-a-predictiveasymmetrytest", 
            "text": "A  PredictiveAsymmetryTest  takes as input a causality test that uses lagged  prediction. Here, we'll use a transfer entropy test, using a visitation  frequency estimator to perform the computations . We'll predict 5 time steps  forwards and backwards in time ( \u03b7s = -5:5 ).  Transfer entropy is computing over a discretization of a state space  reconstruction of the time series involves. Here, we'll use a  3-dimensional reconstruction for the transfer entropy computations.  To do this computation, we need to specify how to discretize the reconstructed  state space. To keeo it simple, we'll divide the state-space into hyperrectangular  regions defined by diving each coordinate axis of the state space into equally-spaced  intervals. We'll determine the number of intervals to split each axis into by a  trade-off between the dimensionality of the system (ensuring enough points in each bin on average) and the number of points in the system. The approach below  roughly follows Krakovska et al. (2018).  k ,   l ,   m   =   1 ,   1 ,   1   # embedding parameters, total dimension is k + l + m  n_subdivisions   =   floor ( Int ,   length ( grid ) ^ ( 1 / ( k   +   l   +   m   +   1 )))  state_space_binning   =   RectangularBinning ( n_subdivisions );  # Configure a visitation frequency estimator transfer entropy test  # using base-2 logarithms  \u03b7s   =   - 5 : 5   # we re predicting five steps forwards and backwards in time  te_test   =   VisitationFrequencyTest ( k   =   k ,   l   =   l ,   m   =   m , \n     binning   =   state_space_binning ,   b   =   2 ,   \u03b7s   =   \u03b7s )  pa_test   =   PredictiveAsymmetryTest ( predictive_test   =   te_test )  # Now we can combine the binning from above with the predictive asymmetry  # test we just defined. We ll apply the causality test to 50 independent  # draws of the binned dataset.  n_realizations   =   50  test   =   BinnedDataCausalityTest ( pa_test ,   binned_resampling ,   n_realizations )   Finally, we can compute the predictive asymmetry in both directions for the bin means.  tes_xy   =   causality ( X ,   Y ,   test )  tes_yx   =   causality ( Y ,   X ,   test )   tes_xy  and  tes_yx  are now both length- 50  vectors, where each element is a length- 5  vector containing the predictive asymmetries for prediction lags  1:5 . Let's summarise the data for each prediction lag and plot the results.  ```@example PredictiveAsymmetryTest_BinnedResampling", 
            "title": "Defining a PredictiveAsymmetryTest"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedResampling/#gather-results-in-a-matrix-and-compute-means-and-standard-deviations", 
            "text": "", 
            "title": "Gather results in a matrix and compute means and standard deviations"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedResampling/#for-the-predictive-asymmetries-at-each-prediction-lag", 
            "text": "M_xy = hcat(tes_xy...,)\nM_yx = hcat(tes_yx...,)  means_xy = mean(M_xy, dims = 2)[:, 1]\nmeans_yx = mean(M_yx, dims = 2)[:, 1]\nstdevs_xy = std(M_yx, dims = 2)[:, 1]\nstdevs_yx = std(M_yx, dims = 2)[:, 1]", 
            "title": "for the predictive asymmetries at each prediction lag"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedResampling/#plot-the-predictive-asymmetry-as-a-function-of-prediction-lag", 
            "text": "plot(xlabel = \"Prediction lag (eta)\", ylabel = \"Predictive asymmetry (bits)\")\nplot!(\u03b7s[\u03b7s .  0], means_xy, ribbon = stdevs_xy, label = \"x -  y (binned)\", c = :black)\nplot!(\u03b7s[\u03b7s .  0], means_yx, ribbon = stdevs_yx, label = \"y -  x (binned)\", c = :red)\nhline!([0], lw = 2, ls = :dot, \u03b1 = 0.5, label = \"\", c = :grey)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedResampling_lag_vs_A.svg\"); nothing # hide\n```", 
            "title": "Plot the predictive asymmetry as a function of prediction lag"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedMeanResampling/", 
            "text": "PredictiveAsymmetryTest\n with \nBinnedMeanResampling\n\n\n\n\nExample data\n\n\nFirst, we'll generate some example data from an AR1 system. We'll use the \nar1_unidir1\n system that ships with \nCausalityTools\n. \n\n\nUncertainties are  added using the \nexample_uncertain_indexvalue_datasets\n function from  \nUncertainData.jl\n. It accepts as inputs a \nDiscreteDynamicalSystems\n instance, the number of time steps to iterate the system, a 2-tuple of variables  to use (by their index) and, optionally, a time step. \n\n\nusing\n \nCausalityTools\n,\n \nUncertainData\n,\n \nPlots\n,\n \nDistributions\n\n\nsys\n \n=\n \nar1_unidir\n(\nu\u1d62\n \n=\n \n[\n0.1\n,\n \n0.1\n],\n \nc_xy\n \n=\n \n0.41\n)\n\n\nvars\n \n=\n \n(\n1\n,\n \n2\n)\n \n# ar1_unidir has only two variables, X and Y\n\n\nn_steps\n \n=\n \n100\n\n\ntstep\n \n=\n \n10\n \n# the mean of each time value is stepped by `tstep`\n\n\n\nX\n,\n \nY\n \n=\n \nexample_uncertain_indexvalue_datasets\n(\nsys\n,\n \nn_steps\n,\n \nvars\n,\n \ntstep\n \n=\n \n10\n,\n\n    \nd_xind\n \n=\n \nUniform\n(\n7.5\n,\n \n15.5\n),\n\n    \nd_yind\n \n=\n \nUniform\n(\n5.5\n,\n \n15.5\n),\n\n    \nd_xval\n \n=\n \nUniform\n(\n0.1\n,\n \n0.5\n));\n\n\n\n\n\nNow we have a time series with normally distributed time indices with means  ranging from 1 to 1001 in steps of 10. Let's say we want to bin the data  with equally-sized bins with size 25.\n\n\n```@example PredictiveAsymmetryTest_BinnedMeanResampling\nqs = [0.1, 0.9] # quantiles to display\n\n\npX = plot(X, mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black), qs, qs, ylabel = \"X\")\nplot!(pX, mean.(X.indices), mean.(X.values), c = :black, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dot, \u03b1 = 0.5, lw = 0.5)\npY = plot(Y, mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red), qs, qs, ylabel = \"Y\")\nplot!(pY, mean.(Y.indices), mean.(Y.values), c = :red, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dot, \u03b1 = 0.5, lw = 0.5)\nplot(pX, pY, layout = (2, 1), xlabel = \"Time step\", ylims = (-2.5, 2.5), legend = false)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedMeanResampling_x_and_y.svg\"); nothing # hide\n\n![](figs/PredictiveAsymmetryTest_BinnedMeanResampling_x_and_y.svg)\n\n\n\na id=\nHow-do-the-data-look-when-binned?-1\n/a\n\n\n## How do the data look when binned?\n\n\nLet\ns use a slightly finer binning and investigate what the binned data look like.\n\n\nThe time uncertainty in each bin is assumed uniform, while the value uncertainty  is represented as a kernel density estimate to the distribution of points in each bin.\n\n\n```@example PredictiveAsymmetryTest_BinnedMeanResampling\ngrid = 0:5:1000\nn_draws = 10000 # resample each point 10000 times and distribute among bins\nbinned_resampling = BinnedMeanResampling(grid, n_draws)\n\n# Compute bin means\nX_bin_means = resample(X, binned_resampling)\nY_bin_means = resample(Y, binned_resampling)\n\nqs = [0.1, 0.9]\npX = plot(grid[1:end-1] .+ step(grid)/2, X_bin_means, ylabel = \nX\n,\n    mc = :blue, ms = 2, lw = 2, marker = stroke(0.0, :blue))\nplot!(X, mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black), qs, qs, ylabel = \nX\n)\n\npY = plot(grid[1:end-1] .+ step(grid)/2, Y_bin_means, ylabel = \nY\n,\n    mc = :red, ms = 1, lw = 2, marker = stroke(0.0, :red))\nplot!(Y, mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red), qs, qs, ylabel = \nY\n)\n\nplot(pX, pY, layout = (2, 1), xlabel = \nTime step\n, legend = false)\nsavefig(\nfigs/PredictiveAsymmetryTest_BinnedMeanResampling_x_and_y_binned.svg\n); nothing # hide\n\n\n\n\n\n\n\nDefining a PredictiveAsymmetryTest\n\n\nA \nPredictiveAsymmetryTest\n takes as input a causality test that uses lagged  prediction. Here, we'll use a transfer entropy test, using a visitation  frequency estimator to perform the computations . We'll predict 5 time steps  forwards and backwards in time (\n\u03b7s = -5:5\n).\n\n\nTransfer entropy is computing over a discretization of a state space  reconstruction of the time series involves. Here, we'll use a  3-dimensional reconstruction for the transfer entropy computations.\n\n\nTo do this computation, we need to specify how to discretize the reconstructed  state space. To keeo it simple, we'll divide the state-space into hyperrectangular  regions defined by diving each coordinate axis of the state space into equally-spaced  intervals. We'll determine the number of intervals to split each axis into by a  trade-off between the dimensionality of the system (ensuring enough points in each bin on average) and the number of points in the system. The approach below  roughly follows Krakovska et al. (2018).\n\n\nk\n,\n \nl\n,\n \nm\n \n=\n \n1\n,\n \n1\n,\n \n1\n \n# embedding parameters, total dimension is k + l + m\n\n\nn_subdivisions\n \n=\n \nfloor\n(\nInt\n,\n \nlength\n(\ngrid\n)\n^\n(\n1\n/\n(\nk\n \n+\n \nl\n \n+\n \nm\n \n+\n \n1\n)))\n\n\nstate_space_binning\n \n=\n \nRectangularBinning\n(\nn_subdivisions\n);\n\n\n\n# Configure a visitation frequency estimator transfer entropy test\n\n\n# using base-2 logarithms\n\n\n\u03b7s\n \n=\n \n-\n5\n:\n5\n \n# we\nre predicting five steps forwards and backwards in time\n\n\nte_test\n \n=\n \nVisitationFrequencyTest\n(\nk\n \n=\n \nk\n,\n \nl\n \n=\n \nl\n,\n \nm\n \n=\n \nm\n,\n\n    \nbinning\n \n=\n \nstate_space_binning\n,\n \nb\n \n=\n \n2\n,\n \n\u03b7s\n \n=\n \n\u03b7s\n)\n\n\n\npa_test\n \n=\n \nPredictiveAsymmetryTest\n(\npredictive_test\n \n=\n \nte_test\n)\n\n\n\n# Now we can combine the binning from above with the predictive asymmetry\n\n\n# test we just defined. Since we\nre using a BinnedMeanResampling, we\n\n\n# only have one realisation of the datasets - the bin means - to compute\n\n\n# the causality test for, so we don\nt specify the number of realizations.\n\n\ntest\n \n=\n \nBinnedDataCausalityTest\n(\npa_test\n,\n \nbinned_resampling\n);\n\n\n\n\n\nFinally, we can compute the predictive asymmetry in both directions for the bin means.\n\n\ntes_xy\n \n=\n \ncausality\n(\nX\n,\n \nY\n,\n \ntest\n)\n\n\ntes_yx\n \n=\n \ncausality\n(\nY\n,\n \nX\n,\n \ntest\n)\n\n\n\n\n\ntes_xy\n and \ntes_yx\n are now both length-\n5\n vectors containing the predictive asymmetries for prediction lags \n1:5\n. Let's summarise the data for each prediction lag and plot the results.\n\n\n```@example PredictiveAsymmetryTest_BinnedMeanResampling\n\n\nPlot the predictive asymmetry as a function of prediction lag\n\n\nplot(xlabel = \"Prediction lag (eta)\", ylabel = \"Predictive asymmetry (bits)\")\nplot!(\u03b7s[\u03b7s .\n 0], tes_xy, label = \"x -\n y (binned)\", c = :black)\nplot!(\u03b7s[\u03b7s .\n 0], tes_yx, label = \"y -\n x (binned)\", c = :red)\nhline!([0], lw = 2, ls = :dot, \u03b1 = 0.5, label = \"\", c = :grey)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedMeanResampling_lag_vs_A.svg\"); nothing # hide\n```", 
            "title": "BinnedMeanResampling"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedMeanResampling/#predictiveasymmetrytest-with-binnedmeanresampling", 
            "text": "", 
            "title": "PredictiveAsymmetryTest with BinnedMeanResampling"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedMeanResampling/#example-data", 
            "text": "First, we'll generate some example data from an AR1 system. We'll use the  ar1_unidir1  system that ships with  CausalityTools .   Uncertainties are  added using the  example_uncertain_indexvalue_datasets  function from   UncertainData.jl . It accepts as inputs a  DiscreteDynamicalSystems  instance, the number of time steps to iterate the system, a 2-tuple of variables  to use (by their index) and, optionally, a time step.   using   CausalityTools ,   UncertainData ,   Plots ,   Distributions  sys   =   ar1_unidir ( u\u1d62   =   [ 0.1 ,   0.1 ],   c_xy   =   0.41 )  vars   =   ( 1 ,   2 )   # ar1_unidir has only two variables, X and Y  n_steps   =   100  tstep   =   10   # the mean of each time value is stepped by `tstep`  X ,   Y   =   example_uncertain_indexvalue_datasets ( sys ,   n_steps ,   vars ,   tstep   =   10 , \n     d_xind   =   Uniform ( 7.5 ,   15.5 ), \n     d_yind   =   Uniform ( 5.5 ,   15.5 ), \n     d_xval   =   Uniform ( 0.1 ,   0.5 ));   Now we have a time series with normally distributed time indices with means  ranging from 1 to 1001 in steps of 10. Let's say we want to bin the data  with equally-sized bins with size 25.  ```@example PredictiveAsymmetryTest_BinnedMeanResampling\nqs = [0.1, 0.9] # quantiles to display  pX = plot(X, mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black), qs, qs, ylabel = \"X\")\nplot!(pX, mean.(X.indices), mean.(X.values), c = :black, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dot, \u03b1 = 0.5, lw = 0.5)\npY = plot(Y, mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red), qs, qs, ylabel = \"Y\")\nplot!(pY, mean.(Y.indices), mean.(Y.values), c = :red, lw = 1, \u03b1 = 0.2, label = \"\")\nvline!(0:25:1000, ls = :dot, \u03b1 = 0.5, lw = 0.5)\nplot(pX, pY, layout = (2, 1), xlabel = \"Time step\", ylims = (-2.5, 2.5), legend = false)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedMeanResampling_x_and_y.svg\"); nothing # hide ![](figs/PredictiveAsymmetryTest_BinnedMeanResampling_x_and_y.svg) a id= How-do-the-data-look-when-binned?-1 /a \n\n## How do the data look when binned?\n\n\nLet s use a slightly finer binning and investigate what the binned data look like.\n\n\nThe time uncertainty in each bin is assumed uniform, while the value uncertainty  is represented as a kernel density estimate to the distribution of points in each bin.\n\n\n```@example PredictiveAsymmetryTest_BinnedMeanResampling\ngrid = 0:5:1000\nn_draws = 10000 # resample each point 10000 times and distribute among bins\nbinned_resampling = BinnedMeanResampling(grid, n_draws)\n\n# Compute bin means\nX_bin_means = resample(X, binned_resampling)\nY_bin_means = resample(Y, binned_resampling)\n\nqs = [0.1, 0.9]\npX = plot(grid[1:end-1] .+ step(grid)/2, X_bin_means, ylabel =  X ,\n    mc = :blue, ms = 2, lw = 2, marker = stroke(0.0, :blue))\nplot!(X, mc = :black, ms = 2, lw = 0.5, marker = stroke(0.0, :black), qs, qs, ylabel =  X )\n\npY = plot(grid[1:end-1] .+ step(grid)/2, Y_bin_means, ylabel =  Y ,\n    mc = :red, ms = 1, lw = 2, marker = stroke(0.0, :red))\nplot!(Y, mc = :red, ms = 2, lw = 0.5, marker = stroke(0.0, :red), qs, qs, ylabel =  Y )\n\nplot(pX, pY, layout = (2, 1), xlabel =  Time step , legend = false)\nsavefig( figs/PredictiveAsymmetryTest_BinnedMeanResampling_x_and_y_binned.svg ); nothing # hide", 
            "title": "Example data"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedMeanResampling/#defining-a-predictiveasymmetrytest", 
            "text": "A  PredictiveAsymmetryTest  takes as input a causality test that uses lagged  prediction. Here, we'll use a transfer entropy test, using a visitation  frequency estimator to perform the computations . We'll predict 5 time steps  forwards and backwards in time ( \u03b7s = -5:5 ).  Transfer entropy is computing over a discretization of a state space  reconstruction of the time series involves. Here, we'll use a  3-dimensional reconstruction for the transfer entropy computations.  To do this computation, we need to specify how to discretize the reconstructed  state space. To keeo it simple, we'll divide the state-space into hyperrectangular  regions defined by diving each coordinate axis of the state space into equally-spaced  intervals. We'll determine the number of intervals to split each axis into by a  trade-off between the dimensionality of the system (ensuring enough points in each bin on average) and the number of points in the system. The approach below  roughly follows Krakovska et al. (2018).  k ,   l ,   m   =   1 ,   1 ,   1   # embedding parameters, total dimension is k + l + m  n_subdivisions   =   floor ( Int ,   length ( grid ) ^ ( 1 / ( k   +   l   +   m   +   1 )))  state_space_binning   =   RectangularBinning ( n_subdivisions );  # Configure a visitation frequency estimator transfer entropy test  # using base-2 logarithms  \u03b7s   =   - 5 : 5   # we re predicting five steps forwards and backwards in time  te_test   =   VisitationFrequencyTest ( k   =   k ,   l   =   l ,   m   =   m , \n     binning   =   state_space_binning ,   b   =   2 ,   \u03b7s   =   \u03b7s )  pa_test   =   PredictiveAsymmetryTest ( predictive_test   =   te_test )  # Now we can combine the binning from above with the predictive asymmetry  # test we just defined. Since we re using a BinnedMeanResampling, we  # only have one realisation of the datasets - the bin means - to compute  # the causality test for, so we don t specify the number of realizations.  test   =   BinnedDataCausalityTest ( pa_test ,   binned_resampling );   Finally, we can compute the predictive asymmetry in both directions for the bin means.  tes_xy   =   causality ( X ,   Y ,   test )  tes_yx   =   causality ( Y ,   X ,   test )   tes_xy  and  tes_yx  are now both length- 5  vectors containing the predictive asymmetries for prediction lags  1:5 . Let's summarise the data for each prediction lag and plot the results.  ```@example PredictiveAsymmetryTest_BinnedMeanResampling", 
            "title": "Defining a PredictiveAsymmetryTest"
        }, 
        {
            "location": "/tutorials/causality/binned_uncertain_data/tutorial_BinnedDataCausalityTest_PredictiveAsymmetryTest_BinnedMeanResampling/#plot-the-predictive-asymmetry-as-a-function-of-prediction-lag", 
            "text": "plot(xlabel = \"Prediction lag (eta)\", ylabel = \"Predictive asymmetry (bits)\")\nplot!(\u03b7s[\u03b7s .  0], tes_xy, label = \"x -  y (binned)\", c = :black)\nplot!(\u03b7s[\u03b7s .  0], tes_yx, label = \"y -  x (binned)\", c = :red)\nhline!([0], lw = 2, ls = :dot, \u03b1 = 0.5, label = \"\", c = :grey)\nsavefig(\"figs/PredictiveAsymmetryTest_BinnedMeanResampling_lag_vs_A.svg\"); nothing # hide\n```", 
            "title": "Plot the predictive asymmetry as a function of prediction lag"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/", 
            "text": "Example: measuring transfer entropy between two unidirectionally coupled logistic maps\n\n\nHere, we present an example of how one can measure the influence between variables of a dynamical system using transfer entropy (TE).\n\n\n\n\nDefining a system\n\n\nFor this example, we'll consider a unidirectionally coupled system consisting of two logistic maps, given by the vector field\n\n\n\n\n\n\\begin{aligned}\ndx \n= r_1 x(1 - x) \\\\\ndy \n= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}\n\n\n\n\n\nwith\n\n\n\n\n\n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}\n\n\n\n\n\nThe parameter \nc\n controls how strong the dynamical forcing is. If \n\u03c3 \n 0\n, dynamical noise masking the influence of  \nx\nx\n on \ny\ny\n, equivalent to \n\\sigma \\cdot \\xi\n\\sigma \\cdot \\xi\n, is added at each iteration. Here, \n\\xi\n\\xi\n is a draw from a flat distribution on \n[0, 1]\n. Thus, setting \n\u03c3 = 0.05\n\u03c3 = 0.05\n is equivalent to add dynamical noise corresponding to a maximum of \n5 \\%\n5 \\%\n of the possible range of values of the logistic map.\n\n\n\n\nRepresent the system as a DiscreteDynamicalSystem\n\n\nWe first define the equations of motion.\n\n\nfunction\n \neom_logistic2\n(\ndx\n,\n \nx\n,\n \np\n,\n \nn\n)\n\n    \nc\n,\n \nr\u2081\n,\n \nr\u2082\n,\n \n\u03c3\n \n=\n \n(\np\n...\n,)\n\n    \n\u03be\n \n=\n \nrand\n()\n \n# random number from flat distribution on [0, 1]\n\n    \nx\n,\n \ny\n \n=\n \nx\n[\n1\n],\n \nx\n[\n2\n]\n\n    \nf_xy\n \n=\n \n(\ny\n \n+\n  \n(\nc\n*\n(\nx\n \n+\n \n\u03c3\n*\n\u03be\n)\n/\n2\n)\n \n)\n \n/\n \n(\n1\n \n+\n \n(\nc\n/\n2\n)\n*\n(\n1\n+\n\u03c3\n))\n\n\n    \ndx\n[\n1\n]\n \n=\n \nr\u2081\n \n*\n \nx\n \n*\n \n(\n1\n \n-\n \nx\n)\n\n    \ndx\n[\n2\n]\n \n=\n \nr\u2082\n \n*\n \n(\nf_xy\n)\n \n*\n \n(\n1\n \n-\n \nf_xy\n)\n\n    \nreturn\n\n\nend\n\n\n\n\n\nTo make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters \nr\u2081\n and \nr\u2082\n, coupling strength \nc\n, initial condition \nu\u2080\n and dynamical noise level \n\u03c3\n. Selecting parameter values on \n[3.6, 4.0]\n yield mostly chaotic realizations of the maps, so we set the default to some random values on this interval.\n\n\nfunction\n \nlogistic2\n(;\nu\u2080\n \n=\n \nrand\n(\n2\n),\n \nc\n \n=\n \n0.0\n,\n \nr\u2081\n \n=\n \n3.66\n,\n \nr\u2082\n \n=\n \n3.77\n,\n \n\u03c3\n \n=\n \n0.05\n)\n\n    \np\n \n=\n \n[\nc\n,\n \nr\u2081\n,\n \nr\u2082\n,\n \n\u03c3\n]\n\n    \nDiscreteDynamicalSystem\n(\neom_logistic2\n,\n \nu\u2080\n,\n \np\n)\n\n\nend\n\n\n\n\n\nBy tuning the coupling strength \nc\n, we may control the strength of the influence \nx\nx\n has on \ny\ny\n. Depending on the particular values of \nr\u2081\n and \nr\u2082\n, the subsystems become synchronized at different values of \nc\n. Choosing \nc \u2208 [0, 2]\n usually still gives some independence between the subsystems.\n\n\nAn example realization of the system when there is no coupling is:\n\n\ns\n \n=\n \nlogistic2\n(\nc\n \n=\n \n0.0\n)\n\n\norbit\n \n=\n \ntrajectory\n(\ns\n,\n \n100\n)\n\n\nx\n,\n \ny\n \n=\n \norbit\n[\n:\n,\n \n1\n],\n \norbit\n[\n:\n,\n \n2\n]\n\n\nplot\n(\nx\n,\n \nlabel\n \n=\n \nx\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\ny\n,\n \nlabel\n \n=\n \ny\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\nTime step\n);\n \nylabel!\n(\nValue\n)\n\n\n\n\n\n\n\n\n\nDelay embedding for transfer entropy\n\n\nThe minimum embedding dimension for this system is 4 (try to figure this out yourself using the machinery in DynamicalSystems.jl!).\n\n\nWe want to measure the information flow \nx \\rightarrow y\nx \\rightarrow y\n. To do this, we express the transfer entropy as a conditional mutual information. For that, we need an embedding consisting of the following set of vectors\n\n\n\n\n\n\\begin{aligned}\nE = \\{ (y(t + \\nu), y(t), x(t), y(t - \\tau) ) \\},\n\\end{aligned}\n\n\n\n\n\\begin{aligned}\nE = \\{ (y(t + \\nu), y(t), x(t), y(t - \\tau) ) \\},\n\\end{aligned}\n\n\n\n\n\nwhere \n\\nu\n\\nu\n is the forward prediction lag and \n\\tau\n\\tau\n is the embedding lag. If a higher dimension was needed, we would add more lagged instances of the target variable \ny\ny\n.\n\n\n\n\nConstruct the embedding\n\n\nTo construct the embedding, we use the \ncustomembed\n function as follows.\n\n\n\u03c4\n \n=\n \n1\n \n# embedding lag\n\n\n\u03bd\n \n=\n \n1\n \n# forward prediction lag\n\n\nE\n \n=\n \ncustomembed\n(\nDataset\n(\nx\n,\n \ny\n),\n \nPositions\n([\n2\n,\n \n2\n,\n \n2\n,\n \n1\n]),\n \nLags\n([\n\u03bd\n,\n \n0\n,\n \n-\n\u03c4\n,\n \n0\n]))\n\n\n\n\n\nThis means that \ny\n appears in the 1\nst\n, 2\nnd\n and 3\nrd\n columns of the embedding, with lags 1, 0 and -1, respectively. The 4\nth\n column is occupied by \nx\n, which is not lagged.\n\n\n\n\nKeeping track of embedding information using TEVars\n\n\nKeeping track of how the embedding is organized is done using a \nTEVars\n instance.\n\n\nIt takes requires as inputs the column indices corresponding to 1) the future of the target variable, 2) the present and past of the target, and 3) the present and past of the source variable, in that order. Let's define this for our system:\n\n\nTf\n \n=\n \n[\n1\n]\n     \n# target, future\n\n\nTpp\n \n=\n \n[\n2\n,\n \n3\n]\n \n# target, present and past\n\n\nSpp\n \n=\n \n[\n4\n]\n    \n# source, present (and past, if we wanted)\n\n\nv\n \n=\n \nTEVars\n(\nTf\n,\n \nTpp\n,\n \nSpp\n)\n\n\n\n\n\nThe last field is an empty array because we are not doing any conditioning on other variables.\n\n\n\n\nTE estimator\n\n\nWe will use the transfer operator grid TE estimator, found in the \nTransferOperatorGrid\n transfer entropy estimator. This estimator takes as input an embedding, given as a set of points,  a \nRectangularBinning\n instance, and a \nTEVars\n instance. \n\n\nWe will compute TE over a range of bin sizes, for a slightly longer time series  than we plotted before, with \nc = 0.7\n.\n\n\n\n\nEmbedding\n\n\nLet's create a realization of the system, embed it and create a \nTEVars\n instance. We'll use these throughout the examples below.\n\n\n# Orbit of the system\n\n\ns\n \n=\n \nlogistic2\n(\nc\n \n=\n \n0.7\n)\n\n\norbit\n \n=\n \ntrajectory\n(\ns\n,\n \n500\n)\n\n\nx\n,\n \ny\n \n=\n \norbit\n[\n:\n,\n \n1\n],\n \norbit\n[\n:\n,\n \n2\n]\n\n\n\n# Embedding\n\n\n\u03c4\n \n=\n \n1\n \n# embedding lag\n\n\n\u03bd\n \n=\n \n1\n \n# forward prediction lag\n\n\nE_xtoy\n \n=\n \ncustomembed\n(\nDataset\n(\nx\n,\n \ny\n),\n \nPositions\n([\n2\n,\n \n2\n,\n \n2\n,\n \n1\n]),\n \nLags\n([\n\u03bd\n,\n \n0\n,\n \n-\n\u03c4\n,\n \n0\n]))\n\n\nE_ytox\n \n=\n \ncustomembed\n(\nDataset\n(\ny\n,\n \nx\n),\n \nPositions\n([\n2\n,\n \n2\n,\n \n2\n,\n \n1\n]),\n \nLags\n([\n\u03bd\n,\n \n0\n,\n \n-\n\u03c4\n,\n \n0\n]))\n\n\n\n# Which variables go where?\n\n\nTf\n \n=\n \n[\n1\n]\n     \n# target, future\n\n\nTpp\n \n=\n \n[\n2\n,\n \n3\n]\n \n# target, present and past\n\n\nSpp\n \n=\n \n[\n4\n]\n    \n# source, present (and past, if we wanted)\n\n\nvars\n \n=\n \nTEVars\n(\nTf\n,\n \nTpp\n,\n \nSpp\n)\n\n\n\n\n\n\n\nDifferent ways of partitioning\n\n\nThe \nTransferOperatorGrid\n and \nVisitationFrequency\n transfer entropy estimators both operate on partitions on the state space.\n\n\nThere are four different ways of partitioning the state space using a  \nRectangularBinning\n. The partition scheme is controlled by \n\u03f5\n,  and the following \n\u03f5\n will work (check \nRectangularBinning\n documentation  for more binning alternatives):\n\n\n\n\n\u03f5::Int\n divide each axis into \n\u03f5\n intervals of the same size.\n\n\n\u03f5::Float\n divide each axis into intervals of size \n\u03f5\n.\n\n\n\u03f5::Vector{Int}\n divide the i-th axis into \n\u03f5\u1d62\n intervals of the same size.\n\n\n\u03f5::Vector{Float64}\n divide the i-th axis into intervals of size \n\u03f5\u1d62\n.\n\n\n\n\nBelow, we demonstrate how TE may be computed using the four different ways of discretizing the state space.\n\n\n\n\nHyper-rectangles by subdivision of axes\n\n\nFirst, we use an integer number of subdivisions along each axis of the delay embedding when partitioning (\n\u03f5::Int\n).\n\n\n\u03f5s\n \n=\n \n1\n:\n2\n:\n50\n \n# integer number of subdivisions along each axis of the embedding\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\n\nestimator\n \n=\n \nVisitationFrequency\n()\n\n\n\nfor\n \n(\ni\n,\n \n\u03f5\n)\n \nin\n \nenumerate\n(\n\u03f5s\n)\n\n    \nbinning\n \n=\n \nRectangularBinning\n(\n\u03f5\n)\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n\nend\n\n\n\nplot\n(\n\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\n\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\n# subdivisions along each axis\n)\n\n\nylabel!\n(\nTransfer entropy (bits)\n)\n\n\n\n\n\n\n\nAs expected, there is much higher information flow from \nx\n to \ny\n (where there is an underlying coupling) than from \ny\n to \nx\n, where there is no underlying coupling.\n\n\n\n\nHyper-cubes of fixed size\n\n\nWe do precisely the same, but use fixed-width hyper-cube bins (\n\u03f5::Float\n). The values of the logistic map take values on \n[0, 1]\n, so using bins width edge lengths \n0.1\n should give a covering corresponding to using \n10\n subdivisions along each axis of the delay embedding. We let \n\u03f5\n take values on \n[0.05, 0.5]\n\n\n\u03f5s\n \n=\n \n0.05\n:\n0.05\n:\n0.5\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\n\nestimator\n \n=\n \nVisitationFrequency\n()\n\n\n\nfor\n \n(\ni\n,\n \n\u03f5\n)\n \nin\n \nenumerate\n(\n\u03f5s\n)\n\n    \nbinning\n \n=\n \nRectangularBinning\n(\n\u03f5\n)\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n\nend\n\n\n\nplot\n(\n\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\n\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\nHypercube edge length\n)\n\n\nylabel!\n(\nTransfer entropy (bits)\n)\n\n\nxflip!\n()\n\n\n\n\n\n\n\n\n\nHyper-rectangles of fixed size\n\n\nIt is also possible to use hyper-rectangles  (\n\u03f5::Vector{Float}\n), by specifying the edge lengths along each coordinate axis of the delay embedding. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector of edge lengths.\n\n\n# Define slightly different edge lengths along each axis\n\n\n\u03f5s_x1\n \n=\n \nLinRange\n(\n0.05\n,\n \n0.5\n,\n \n10\n)\n\n\n\u03f5s_x2\n \n=\n \nLinRange\n(\n0.02\n,\n \n0.4\n,\n \n10\n)\n\n\n\u03f5s_x3\n \n=\n \nLinRange\n(\n0.08\n,\n \n0.6\n,\n \n10\n)\n\n\n\u03f5s_x4\n \n=\n \nLinRange\n(\n0.10\n,\n \n0.3\n,\n \n10\n)\n\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nmean_\u03f5s\n \n=\n \nzeros\n(\n10\n)\n\n\n\nestimator\n \n=\n \nVisitationFrequency\n()\n\n\n\nfor\n \ni\n \n\u2208\n \n1\n:\n10\n\n    \n\u03f5\n \n=\n \n[\n\u03f5s_x1\n[\ni\n],\n \n\u03f5s_x2\n[\ni\n],\n \n\u03f5s_x3\n[\ni\n],\n \n\u03f5s_x4\n[\ni\n]]\n\n    \nbinning\n \n=\n \nRectangularBinning\n(\n\u03f5\n)\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n\n    \n# Store average edge length (for plotting)\n\n    \nmean_\u03f5s\n[\ni\n]\n \n=\n \nmean\n(\n\u03f5\n)\n\n\nend\n\n\n\nplot\n(\nmean_\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\nmean_\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\nAverage hypercube edge length\n)\n\n\nylabel!\n(\nTransfer entropy (bits)\n)\n\n\nxflip!\n()\n\n\n\n\n\n\n\n\n\nHyper-rectangles by variable-width subdivision of axes\n\n\nAnother way to construct hyper-rectangles is to subdivide each coordinate axis into segments of equal length (\n\u03f5::Vector{Int}\n). In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector providing the number of subdivisions we want along each axis.\n\n\n# Define different number of subdivisions along each axis.\n\n\n\u03f5s\n \n=\n \n3\n:\n50\n\n\nmean_\u03f5s\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\n\nte_estimates_xtoy\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nte_estimates_ytox\n \n=\n \nzeros\n(\nlength\n(\n\u03f5s\n))\n\n\nestimator\n \n=\n \nVisitationFrequency\n()\n\n\n\nfor\n \n(\ni\n,\n \n\u03f5\u1d62\n)\n \n\u2208\n \nenumerate\n(\n\u03f5s\n)\n\n    \n\u03f5\n \n=\n \n[\n\u03f5\u1d62\n \n-\n \n1\n,\n \n\u03f5\u1d62\n,\n \n\u03f5\u1d62\n,\n \n\u03f5\u1d62\n \n+\n \n1\n]\n\n    \nbinning\n \n=\n \nRectangularBinning\n(\n\u03f5\n)\n\n    \nte_estimates_xtoy\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_xtoy\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n    \nte_estimates_ytox\n[\ni\n]\n \n=\n \ntransferentropy\n(\nE_ytox\n,\n \nvars\n,\n \nbinning\n,\n \nestimator\n)\n\n\n    \n# Store average number of subdivisions for plotting\n\n    \nmean_\u03f5s\n[\ni\n]\n \n=\n \nmean\n(\n\u03f5\n)\n\n\nend\n\n\n\nplot\n(\nmean_\u03f5s\n,\n \nte_estimates_xtoy\n,\n \nlabel\n \n=\n \nTE(x -\n y)\n,\n \nlc\n \n=\n \n:\nblack\n)\n\n\nplot!\n(\nmean_\u03f5s\n,\n \nte_estimates_ytox\n,\n \nlabel\n \n=\n \nTE(y -\n x)\n,\n \nlc\n \n=\n \n:\nred\n)\n\n\nxlabel!\n(\nAverage number of subdivisions along the embedding axes\n)\n\n\nylabel!\n(\nTransfer entropy (bits)\n)\n\n\n\n\n\n\n\n\n\nConclusion\n\n\nThe value of the TE depends on the system under consideration, and on the way one chooses to discretize the state space reconstruction.\n\n\nFor this example, TE is consistently larger for the expected direction \nTE(x -\n y)\n than in the opposite direction \nTE(y -\n x)\n, where we expect no information flow.", 
            "title": "Transfer entropy"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#example-measuring-transfer-entropy-between-two-unidirectionally-coupled-logistic-maps", 
            "text": "Here, we present an example of how one can measure the influence between variables of a dynamical system using transfer entropy (TE).", 
            "title": "Example: measuring transfer entropy between two unidirectionally coupled logistic maps"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#defining-a-system", 
            "text": "For this example, we'll consider a unidirectionally coupled system consisting of two logistic maps, given by the vector field   \n\\begin{aligned}\ndx  = r_1 x(1 - x) \\\\\ndy  = r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}  \n\\begin{aligned}\ndx &= r_1 x(1 - x) \\\\\ndy &= r_2 f(x,y)(1 - f(x,y)),\n\\end{aligned}   with   \n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}  \n\\begin{aligned}\nf(x,y) = \\dfrac{y + \\frac{c(x \\xi )}{2}}{1 + \\frac{c}{2}(1+ \\sigma )}\n\\end{aligned}   The parameter  c  controls how strong the dynamical forcing is. If  \u03c3   0 , dynamical noise masking the influence of   x x  on  y y , equivalent to  \\sigma \\cdot \\xi \\sigma \\cdot \\xi , is added at each iteration. Here,  \\xi \\xi  is a draw from a flat distribution on  [0, 1] . Thus, setting  \u03c3 = 0.05 \u03c3 = 0.05  is equivalent to add dynamical noise corresponding to a maximum of  5 \\% 5 \\%  of the possible range of values of the logistic map.", 
            "title": "Defining a system"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#represent-the-system-as-a-discretedynamicalsystem", 
            "text": "We first define the equations of motion.  function   eom_logistic2 ( dx ,   x ,   p ,   n ) \n     c ,   r\u2081 ,   r\u2082 ,   \u03c3   =   ( p ... ,) \n     \u03be   =   rand ()   # random number from flat distribution on [0, 1] \n     x ,   y   =   x [ 1 ],   x [ 2 ] \n     f_xy   =   ( y   +    ( c * ( x   +   \u03c3 * \u03be ) / 2 )   )   /   ( 1   +   ( c / 2 ) * ( 1 + \u03c3 )) \n\n     dx [ 1 ]   =   r\u2081   *   x   *   ( 1   -   x ) \n     dx [ 2 ]   =   r\u2082   *   ( f_xy )   *   ( 1   -   f_xy ) \n     return  end   To make things easier to use, we create function that generates a DiscreteDynamicalSystem instance for any set of parameters  r\u2081  and  r\u2082 , coupling strength  c , initial condition  u\u2080  and dynamical noise level  \u03c3 . Selecting parameter values on  [3.6, 4.0]  yield mostly chaotic realizations of the maps, so we set the default to some random values on this interval.  function   logistic2 (; u\u2080   =   rand ( 2 ),   c   =   0.0 ,   r\u2081   =   3.66 ,   r\u2082   =   3.77 ,   \u03c3   =   0.05 ) \n     p   =   [ c ,   r\u2081 ,   r\u2082 ,   \u03c3 ] \n     DiscreteDynamicalSystem ( eom_logistic2 ,   u\u2080 ,   p )  end   By tuning the coupling strength  c , we may control the strength of the influence  x x  has on  y y . Depending on the particular values of  r\u2081  and  r\u2082 , the subsystems become synchronized at different values of  c . Choosing  c \u2208 [0, 2]  usually still gives some independence between the subsystems.  An example realization of the system when there is no coupling is:  s   =   logistic2 ( c   =   0.0 )  orbit   =   trajectory ( s ,   100 )  x ,   y   =   orbit [ : ,   1 ],   orbit [ : ,   2 ]  plot ( x ,   label   =   x ,   lc   =   : black )  plot! ( y ,   label   =   y ,   lc   =   : red )  xlabel! ( Time step );   ylabel! ( Value )", 
            "title": "Represent the system as a DiscreteDynamicalSystem"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#delay-embedding-for-transfer-entropy", 
            "text": "The minimum embedding dimension for this system is 4 (try to figure this out yourself using the machinery in DynamicalSystems.jl!).  We want to measure the information flow  x \\rightarrow y x \\rightarrow y . To do this, we express the transfer entropy as a conditional mutual information. For that, we need an embedding consisting of the following set of vectors   \n\\begin{aligned}\nE = \\{ (y(t + \\nu), y(t), x(t), y(t - \\tau) ) \\},\n\\end{aligned}  \n\\begin{aligned}\nE = \\{ (y(t + \\nu), y(t), x(t), y(t - \\tau) ) \\},\n\\end{aligned}   where  \\nu \\nu  is the forward prediction lag and  \\tau \\tau  is the embedding lag. If a higher dimension was needed, we would add more lagged instances of the target variable  y y .", 
            "title": "Delay embedding for transfer entropy"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#construct-the-embedding", 
            "text": "To construct the embedding, we use the  customembed  function as follows.  \u03c4   =   1   # embedding lag  \u03bd   =   1   # forward prediction lag  E   =   customembed ( Dataset ( x ,   y ),   Positions ([ 2 ,   2 ,   2 ,   1 ]),   Lags ([ \u03bd ,   0 ,   - \u03c4 ,   0 ]))   This means that  y  appears in the 1 st , 2 nd  and 3 rd  columns of the embedding, with lags 1, 0 and -1, respectively. The 4 th  column is occupied by  x , which is not lagged.", 
            "title": "Construct the embedding"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#keeping-track-of-embedding-information-using-tevars", 
            "text": "Keeping track of how the embedding is organized is done using a  TEVars  instance.  It takes requires as inputs the column indices corresponding to 1) the future of the target variable, 2) the present and past of the target, and 3) the present and past of the source variable, in that order. Let's define this for our system:  Tf   =   [ 1 ]       # target, future  Tpp   =   [ 2 ,   3 ]   # target, present and past  Spp   =   [ 4 ]      # source, present (and past, if we wanted)  v   =   TEVars ( Tf ,   Tpp ,   Spp )   The last field is an empty array because we are not doing any conditioning on other variables.", 
            "title": "Keeping track of embedding information using TEVars"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#te-estimator", 
            "text": "We will use the transfer operator grid TE estimator, found in the  TransferOperatorGrid  transfer entropy estimator. This estimator takes as input an embedding, given as a set of points,  a  RectangularBinning  instance, and a  TEVars  instance.   We will compute TE over a range of bin sizes, for a slightly longer time series  than we plotted before, with  c = 0.7 .", 
            "title": "TE estimator"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#embedding", 
            "text": "Let's create a realization of the system, embed it and create a  TEVars  instance. We'll use these throughout the examples below.  # Orbit of the system  s   =   logistic2 ( c   =   0.7 )  orbit   =   trajectory ( s ,   500 )  x ,   y   =   orbit [ : ,   1 ],   orbit [ : ,   2 ]  # Embedding  \u03c4   =   1   # embedding lag  \u03bd   =   1   # forward prediction lag  E_xtoy   =   customembed ( Dataset ( x ,   y ),   Positions ([ 2 ,   2 ,   2 ,   1 ]),   Lags ([ \u03bd ,   0 ,   - \u03c4 ,   0 ]))  E_ytox   =   customembed ( Dataset ( y ,   x ),   Positions ([ 2 ,   2 ,   2 ,   1 ]),   Lags ([ \u03bd ,   0 ,   - \u03c4 ,   0 ]))  # Which variables go where?  Tf   =   [ 1 ]       # target, future  Tpp   =   [ 2 ,   3 ]   # target, present and past  Spp   =   [ 4 ]      # source, present (and past, if we wanted)  vars   =   TEVars ( Tf ,   Tpp ,   Spp )", 
            "title": "Embedding"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#different-ways-of-partitioning", 
            "text": "The  TransferOperatorGrid  and  VisitationFrequency  transfer entropy estimators both operate on partitions on the state space.  There are four different ways of partitioning the state space using a   RectangularBinning . The partition scheme is controlled by  \u03f5 ,  and the following  \u03f5  will work (check  RectangularBinning  documentation  for more binning alternatives):   \u03f5::Int  divide each axis into  \u03f5  intervals of the same size.  \u03f5::Float  divide each axis into intervals of size  \u03f5 .  \u03f5::Vector{Int}  divide the i-th axis into  \u03f5\u1d62  intervals of the same size.  \u03f5::Vector{Float64}  divide the i-th axis into intervals of size  \u03f5\u1d62 .   Below, we demonstrate how TE may be computed using the four different ways of discretizing the state space.", 
            "title": "Different ways of partitioning"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#hyper-rectangles-by-subdivision-of-axes", 
            "text": "First, we use an integer number of subdivisions along each axis of the delay embedding when partitioning ( \u03f5::Int ).  \u03f5s   =   1 : 2 : 50   # integer number of subdivisions along each axis of the embedding  te_estimates_xtoy   =   zeros ( length ( \u03f5s ))  te_estimates_ytox   =   zeros ( length ( \u03f5s ))  estimator   =   VisitationFrequency ()  for   ( i ,   \u03f5 )   in   enumerate ( \u03f5s ) \n     binning   =   RectangularBinning ( \u03f5 ) \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   binning ,   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   binning ,   estimator )  end  plot ( \u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( \u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( # subdivisions along each axis )  ylabel! ( Transfer entropy (bits) )    As expected, there is much higher information flow from  x  to  y  (where there is an underlying coupling) than from  y  to  x , where there is no underlying coupling.", 
            "title": "Hyper-rectangles by subdivision of axes"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#hyper-cubes-of-fixed-size", 
            "text": "We do precisely the same, but use fixed-width hyper-cube bins ( \u03f5::Float ). The values of the logistic map take values on  [0, 1] , so using bins width edge lengths  0.1  should give a covering corresponding to using  10  subdivisions along each axis of the delay embedding. We let  \u03f5  take values on  [0.05, 0.5]  \u03f5s   =   0.05 : 0.05 : 0.5  te_estimates_xtoy   =   zeros ( length ( \u03f5s ))  te_estimates_ytox   =   zeros ( length ( \u03f5s ))  estimator   =   VisitationFrequency ()  for   ( i ,   \u03f5 )   in   enumerate ( \u03f5s ) \n     binning   =   RectangularBinning ( \u03f5 ) \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   binning ,   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   binning ,   estimator )  end  plot ( \u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( \u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( Hypercube edge length )  ylabel! ( Transfer entropy (bits) )  xflip! ()", 
            "title": "Hyper-cubes of fixed size"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#hyper-rectangles-of-fixed-size", 
            "text": "It is also possible to use hyper-rectangles  ( \u03f5::Vector{Float} ), by specifying the edge lengths along each coordinate axis of the delay embedding. In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector of edge lengths.  # Define slightly different edge lengths along each axis  \u03f5s_x1   =   LinRange ( 0.05 ,   0.5 ,   10 )  \u03f5s_x2   =   LinRange ( 0.02 ,   0.4 ,   10 )  \u03f5s_x3   =   LinRange ( 0.08 ,   0.6 ,   10 )  \u03f5s_x4   =   LinRange ( 0.10 ,   0.3 ,   10 )  te_estimates_xtoy   =   zeros ( length ( \u03f5s ))  te_estimates_ytox   =   zeros ( length ( \u03f5s ))  mean_\u03f5s   =   zeros ( 10 )  estimator   =   VisitationFrequency ()  for   i   \u2208   1 : 10 \n     \u03f5   =   [ \u03f5s_x1 [ i ],   \u03f5s_x2 [ i ],   \u03f5s_x3 [ i ],   \u03f5s_x4 [ i ]] \n     binning   =   RectangularBinning ( \u03f5 ) \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   binning ,   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   binning ,   estimator ) \n\n     # Store average edge length (for plotting) \n     mean_\u03f5s [ i ]   =   mean ( \u03f5 )  end  plot ( mean_\u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( mean_\u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( Average hypercube edge length )  ylabel! ( Transfer entropy (bits) )  xflip! ()", 
            "title": "Hyper-rectangles of fixed size"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#hyper-rectangles-by-variable-width-subdivision-of-axes", 
            "text": "Another way to construct hyper-rectangles is to subdivide each coordinate axis into segments of equal length ( \u03f5::Vector{Int} ). In our case, we use a four-dimensional, embedding, so we must provide a 4-element vector providing the number of subdivisions we want along each axis.  # Define different number of subdivisions along each axis.  \u03f5s   =   3 : 50  mean_\u03f5s   =   zeros ( length ( \u03f5s ))  te_estimates_xtoy   =   zeros ( length ( \u03f5s ))  te_estimates_ytox   =   zeros ( length ( \u03f5s ))  estimator   =   VisitationFrequency ()  for   ( i ,   \u03f5\u1d62 )   \u2208   enumerate ( \u03f5s ) \n     \u03f5   =   [ \u03f5\u1d62   -   1 ,   \u03f5\u1d62 ,   \u03f5\u1d62 ,   \u03f5\u1d62   +   1 ] \n     binning   =   RectangularBinning ( \u03f5 ) \n     te_estimates_xtoy [ i ]   =   transferentropy ( E_xtoy ,   vars ,   binning ,   estimator ) \n     te_estimates_ytox [ i ]   =   transferentropy ( E_ytox ,   vars ,   binning ,   estimator ) \n\n     # Store average number of subdivisions for plotting \n     mean_\u03f5s [ i ]   =   mean ( \u03f5 )  end  plot ( mean_\u03f5s ,   te_estimates_xtoy ,   label   =   TE(x -  y) ,   lc   =   : black )  plot! ( mean_\u03f5s ,   te_estimates_ytox ,   label   =   TE(y -  x) ,   lc   =   : red )  xlabel! ( Average number of subdivisions along the embedding axes )  ylabel! ( Transfer entropy (bits) )", 
            "title": "Hyper-rectangles by variable-width subdivision of axes"
        }, 
        {
            "location": "/worked_examples/worked_example_transferentropy/#conclusion", 
            "text": "The value of the TE depends on the system under consideration, and on the way one chooses to discretize the state space reconstruction.  For this example, TE is consistently larger for the expected direction  TE(x -  y)  than in the opposite direction  TE(y -  x) , where we expect no information flow.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/CHANGELOG/", 
            "text": "Changelog for CausalityTools.jl\n\n\n\n\nRelease v0.4.1\n\n\n\n\nNew functionality\n\n\n\n\nAdded \nConstrainedTest\n.\n\n\n\n\n\n\nImprovements\n\n\n\n\nBetter documentation for uncertain data causality tests.\n\n\n\n\n\n\nRelease v0.4.0\n\n\n\n\nBug fixes\n\n\n\n\nInstalling the package using \nPkg.add(\"CausalityTools\")\n from the central repository caused an  installation error related to \nSpecialFunctions.jl\n. Building \nSpecialFunctions.jl\n is now added to the build step of \nCausalityTools\n.\n\n\n\n\n\n\nNew functionality\n\n\n\n\nAdded \nExactSimplexIntersectionTest\n causality test.\n\n\n\n\n\n\nRelease v0.3.0\n\n\n\n\nNew functionality\n\n\nThe workflow for estimating causality from time series and dynamical systems has been completely overhauled. New features:\n\n\n\n\nCommon interface to causality testing using the \ncausality\n function and its methods.\n\n\nIt handles any of the following causality tests:\n\n\n\n\nCrossMappingTest\n\n\nConvergentCrossMappingTest\n\n\nJointDistanceDistributionTest\n\n\nJointDistanceDistributionTTest\n\n\nVisitationFrequencyTest\n\n\nTransferOperatorGridTest\n\n\nApproximateSimplexIntersectionTest\n\n\n\n\n\n\nOther features\n\n\n\n\nIntegration with \nUncertainData.jl\n, allowing easy resampling of   uncertain data to obtain uncertainty estimates on causality statistics. This is done   by accepting uncertain data as inputs to \ncausality\n.\n\n\nIntegration with \nDynamicalSystems.jl\n. \ncausality\n accepts   \nDiscreteDynamicalSystem\ns or \nContinuousDynamicalSystem\ns as inputs.\n\n\nSyntax overhaul for \nlow-level estimators\n.\n\n\nUpdated library of \nexample systems\n.", 
            "title": "Changelog"
        }, 
        {
            "location": "/CHANGELOG/#changelog-for-causalitytoolsjl", 
            "text": "", 
            "title": "Changelog for CausalityTools.jl"
        }, 
        {
            "location": "/CHANGELOG/#release-v041", 
            "text": "", 
            "title": "Release v0.4.1"
        }, 
        {
            "location": "/CHANGELOG/#new-functionality", 
            "text": "Added  ConstrainedTest .", 
            "title": "New functionality"
        }, 
        {
            "location": "/CHANGELOG/#improvements", 
            "text": "Better documentation for uncertain data causality tests.", 
            "title": "Improvements"
        }, 
        {
            "location": "/CHANGELOG/#release-v040", 
            "text": "", 
            "title": "Release v0.4.0"
        }, 
        {
            "location": "/CHANGELOG/#bug-fixes", 
            "text": "Installing the package using  Pkg.add(\"CausalityTools\")  from the central repository caused an  installation error related to  SpecialFunctions.jl . Building  SpecialFunctions.jl  is now added to the build step of  CausalityTools .", 
            "title": "Bug fixes"
        }, 
        {
            "location": "/CHANGELOG/#new-functionality_1", 
            "text": "Added  ExactSimplexIntersectionTest  causality test.", 
            "title": "New functionality"
        }, 
        {
            "location": "/CHANGELOG/#release-v030", 
            "text": "", 
            "title": "Release v0.3.0"
        }, 
        {
            "location": "/CHANGELOG/#new-functionality_2", 
            "text": "The workflow for estimating causality from time series and dynamical systems has been completely overhauled. New features:", 
            "title": "New functionality"
        }, 
        {
            "location": "/CHANGELOG/#common-interface-to-causality-testing-using-the-causality-function-and-its-methods", 
            "text": "It handles any of the following causality tests:   CrossMappingTest  ConvergentCrossMappingTest  JointDistanceDistributionTest  JointDistanceDistributionTTest  VisitationFrequencyTest  TransferOperatorGridTest  ApproximateSimplexIntersectionTest", 
            "title": "Common interface to causality testing using the causality function and its methods."
        }, 
        {
            "location": "/CHANGELOG/#other-features", 
            "text": "Integration with  UncertainData.jl , allowing easy resampling of   uncertain data to obtain uncertainty estimates on causality statistics. This is done   by accepting uncertain data as inputs to  causality .  Integration with  DynamicalSystems.jl .  causality  accepts    DiscreteDynamicalSystem s or  ContinuousDynamicalSystem s as inputs.  Syntax overhaul for  low-level estimators .  Updated library of  example systems .", 
            "title": "Other features"
        }
    ]
}