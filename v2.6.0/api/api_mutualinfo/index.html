<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Mutual information API · CausalityTools.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="CausalityTools.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="CausalityTools.jl logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Overview</a></li><li><a class="tocitem" href="../../measures/">Association measures</a></li><li><a class="tocitem" href="../../independence/">Independence testing</a></li><li><a class="tocitem" href="../../causal_graphs/">Causal graphs</a></li><li><a class="tocitem" href="../">APIs and estimators</a></li><li><a class="tocitem" href="../../examples/">Examples</a></li><li><a class="tocitem" href="../../coupled_systems/">Predefined systems</a></li><li><a class="tocitem" href="../../experimental/">Experimental</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Mutual information API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Mutual information API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/master/docs/src/api/api_mutualinfo.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="api_mutualinfo"><a class="docs-heading-anchor" href="#api_mutualinfo">Mutual information API</a><a id="api_mutualinfo-1"></a><a class="docs-heading-anchor-permalink" href="#api_mutualinfo" title="Permalink"></a></h1><p>The mutual information (MI) API is defined by</p><ul><li><a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a>, and its subtypes.</li><li><a href="#CausalityTools.mutualinfo"><code>mutualinfo</code></a>,</li><li><a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>, and its subtypes.</li></ul><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.mutualinfo" href="#CausalityTools.mutualinfo"><code>CausalityTools.mutualinfo</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mutualinfo([measure::MutualInformation], m::ContingencyMatrix) → mi::Real</code></pre><p>Estimate the mutual information between <code>x</code> and <code>y</code>, the variables corresponding to the columns and rows of the 2-dimensional contingency matrix <code>m</code>, respectively.</p><p>Estimates the discrete version of the given <a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a> <code>measure</code> from its direct definition (double-sum), using the probabilities from a pre-computed <a href="../api_contingency_table/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>. If <code>measure</code> is not given, then the default is <code>MIShannon()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L65-L75">source</a></section><section><div><pre><code class="nohighlight hljs">mutualinfo([measure::MutualInformation], est::ProbabilitiesEstimator, x, y) → mi::Real ∈ [0, a]</code></pre><p>Estimate the mutual information between <code>x</code> and <code>y</code> using the discrete version of the given <code>measure</code>, using the given <a href="../api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator"><code>ProbabilitiesEstimator</code></a> <code>est</code> (which must accept multivariate data and have an implementation for <a href="../api_contingency_table/#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>). See examples <a href="../../examples/examples_mi/#example_mi_ProbabilitiesEstimator">here</a>. If <code>measure</code> is not given, then the default is <code>MIShannon()</code>.</p><p><strong>Estimators</strong></p><p>The mutual information is computed as sum of three entropy terms, without any bias correction. The exception is when using <a href="../api_probabilities/#CausalityTools.Contingency"><code>Contingency</code></a>; then the mutual information is computed using a <a href="../api_contingency_table/#CausalityTools.ContingencyMatrix"><code>ContingencyMatrix</code></a>.</p><p>Joint and marginal probabilities are computed by jointly discretizing <code>x</code> and <code>y</code> using the approach given by <code>est</code> (using <a href="../api_contingency_table/#CausalityTools.marginal_encodings"><code>marginal_encodings</code></a>), and obtaining marginal distributions from the joint distribution.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="../../measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></th><th style="text-align: center"><a href="../../measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></th><th style="text-align: center"><a href="../../measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></th><th style="text-align: center"><a href="../../measures/#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></th><th style="text-align: center"><a href="../../measures/#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></th></tr><tr><td style="text-align: right"><a href="../api_probabilities/#CausalityTools.Contingency"><code>Contingency</code></a></td><td style="text-align: right">Contingency table</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="../api_probabilities/#ComplexityMeasures.CountOccurrences"><code>CountOccurrences</code></a></td><td style="text-align: right">Frequencies</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right"><a href="../api_probabilities/#ComplexityMeasures.ValueHistogram"><code>ValueHistogram</code></a></td><td style="text-align: right">Binning (histogram)</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right"><a href="../api_probabilities/#ComplexityMeasures.SymbolicPermutation"><code>SymbolicPermutation</code></a></td><td style="text-align: right">Ordinal patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr><tr><td style="text-align: right"><a href="../api_probabilities/#ComplexityMeasures.Dispersion"><code>Dispersion</code></a></td><td style="text-align: right">Dispersion patterns</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✓</td><td style="text-align: center">✖</td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L78-L104">source</a></section><section><div><pre><code class="nohighlight hljs">mutualinfo([measure::MutualInformation], est::DifferentialEntropyEstimator, x, y)</code></pre><p>Estimate the mutual information <code>measure</code> between <code>x</code> and <code>y</code> by a sum of three entropy terms, without any bias correction, using any <a href="../api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a> compatible with multivariate data. See examples <a href="../../examples/examples_mi/#example_mi_DifferentialEntropyEstimator">here</a>. If <code>measure</code> is not given, then the default is <code>MIShannon()</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><a href="../api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>s have their own <code>base</code> field which is not used here. Instead, this method creates a copy of <code>est</code> internally, where <code>est.base</code> is replaced by <code>measure.e.base</code>. Therefore, use <code>measure</code> to control the &quot;unit&quot; of the mutual information.</p></div></div><p><strong>Estimators</strong></p><p>Some <a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a> measures can be computed using a <a href="../api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator"><code>DifferentialEntropyEstimator</code></a>, provided it supports multivariate input data. These estimators compute mutual information as a sum of of entropy terms (with different dimensions), without any bias correction.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: right">Principle</th><th style="text-align: center"><a href="../../measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></th><th style="text-align: center"><a href="../../measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></th><th style="text-align: center"><a href="../../measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></th><th style="text-align: center"><a href="../../measures/#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></th><th style="text-align: center"><a href="api/@ref"><code>MIRenyiSurbu</code></a></th></tr><tr><td style="text-align: right"><a href="../api_entropies/#ComplexityMeasures.Kraskov"><code>Kraskov</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../api_entropies/#ComplexityMeasures.Zhu"><code>Zhu</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../api_entropies/#ComplexityMeasures.ZhuSingh"><code>ZhuSingh</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../api_entropies/#ComplexityMeasures.Gao"><code>Gao</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../api_entropies/#ComplexityMeasures.Goria"><code>Goria</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="../api_entropies/#ComplexityMeasures.Lord"><code>Lord</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr><tr><td style="text-align: right"><a href="api/@ref"><code>LeonenkoProzantoSavani</code></a></td><td style="text-align: right">Nearest neighbors</td><td style="text-align: center">✓</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td><td style="text-align: center">x</td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L113-L143">source</a></section><section><div><pre><code class="nohighlight hljs">mutualinfo([measure::MutualInformation], est::MutualInformationEstimator, x, y)</code></pre><p>Estimate the mutual information <code>measure</code> between <code>x</code> and <code>y</code> using the dedicated <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a> <code>est</code>. See examples <a href="../../examples/examples_mi/#example_mi_MutualInformationEstimator">here</a>. If <code>measure</code> is not given, then the default is <code>MIShannon()</code>.</p><p><strong>Estimators</strong></p><p>Dedicated <a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s are either discrete, continuous, or a mixture of both. Typically, these estimators apply bias correction.</p><table><tr><th style="text-align: right">Estimator</th><th style="text-align: center">Type</th><th style="text-align: center"><a href="../../measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></th></tr><tr><td style="text-align: right"><a href="api/@ref"><code>GaussanMI</code></a></td><td style="text-align: center">Parametric</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="api/@ref"><code>KSG1</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="api/@ref"><code>KSG2</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></td><td style="text-align: center">Mixed</td><td style="text-align: center">✓</td></tr><tr><td style="text-align: right"><a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></td><td style="text-align: center">Continuous</td><td style="text-align: center">✓</td></tr></table></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L151-L171">source</a></section></article><h2 id="Definitions"><a class="docs-heading-anchor" href="#Definitions">Definitions</a><a id="Definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Definitions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MutualInformation" href="#CausalityTools.MutualInformation"><code>CausalityTools.MutualInformation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MutualInformation &lt;: AssociationMeasure</code></pre><p>The supertype of all mutual information measures. Concrete subtypes are</p><ul><li><a href="../../measures/#CausalityTools.MIShannon"><code>MIShannon</code></a></li><li><a href="../../measures/#CausalityTools.MITsallisFuruichi"><code>MITsallisFuruichi</code></a></li><li><a href="../../measures/#CausalityTools.MITsallisMartin"><code>MITsallisMartin</code></a></li><li><a href="../../measures/#CausalityTools.MIRenyiJizba"><code>MIRenyiJizba</code></a></li><li><a href="../../measures/#CausalityTools.MIRenyiSarbu"><code>MIRenyiSarbu</code></a></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L6-L16">source</a></section></article><h2 id="[MutualInformationEstimator](@ref)s"><a class="docs-heading-anchor" href="#[MutualInformationEstimator](@ref)s"><a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s</a><a id="[MutualInformationEstimator](@ref)s-1"></a><a class="docs-heading-anchor-permalink" href="#[MutualInformationEstimator](@ref)s" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.MutualInformationEstimator" href="#CausalityTools.MutualInformationEstimator"><code>CausalityTools.MutualInformationEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MutualInformationEstimator</code></pre><p>The supertype of all dedicated mutual information estimators.</p><p><a href="#CausalityTools.MutualInformationEstimator"><code>MutualInformationEstimator</code></a>s can be either mixed, discrete or a combination of both. Each estimator uses a specialized technique to approximate relevant densities/integrals and/or probabilities, and is typically tailored to a specific type of <a href="#CausalityTools.MutualInformation"><code>MutualInformation</code></a> (mostly <a href="../../measures/#CausalityTools.MIShannon"><code>MIShannon</code></a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/mutualinfo.jl#L19-L28">source</a></section></article><h2 id="[GaussianMI](@ref)-(parametric)"><a class="docs-heading-anchor" href="#[GaussianMI](@ref)-(parametric)"><a href="#CausalityTools.GaussianMI"><code>GaussianMI</code></a> (parametric)</a><a id="[GaussianMI](@ref)-(parametric)-1"></a><a class="docs-heading-anchor-permalink" href="#[GaussianMI](@ref)-(parametric)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.GaussianMI" href="#CausalityTools.GaussianMI"><code>CausalityTools.GaussianMI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaussianMI &lt;: MutualInformationEstimator
GaussianMI(; normalize::Bool = false)</code></pre><p><code>GaussianMI</code> is a parametric estimator for Shannon mutual information.</p><p><strong>Description</strong></p><p>Given <span>$d_x$</span>-dimensional and <span>$d_y$</span>-dimensional input data <code>X</code> and <code>Y</code>, <code>GaussianMI</code> first constructs the <span>$d_x + d_y$</span>-dimensional joint <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> <code>XY</code>. If <code>normalize == true</code>, then we follow the approach in Vejmelka &amp; Palus (2008)<sup class="footnote-reference"><a id="citeref-Vejmelka2008" href="#footnote-Vejmelka2008">[Vejmelka2008]</a></sup> and transform each column in <code>XY</code> to have zero mean and unit standard deviation. If <code>normalize == false</code>, then the algorithm proceeds without normalization.</p><p>Next, the <code>C_{XY}</code>, the correlation matrix for the (normalized) joint data <code>XY</code> is computed. The mutual information estimate</p><p><code>GaussianMI</code> assumes the input variables are distributed according to normal distributions with zero means and unit standard deviations. Therefore, given <span>$d_x$</span>-dimensional and <span>$d_y$</span>-dimensional input data <code>X</code> and <code>Y</code>, <code>GaussianMI</code> first constructs the joint <a href="../../#StateSpaceSets.StateSpaceSet"><code>StateSpaceSet</code></a> <code>XY</code>, then transforms each column in <code>XY</code> to have zero mean and unit standard deviation, and finally computes the <code>\Sigma</code>, the correlation matrix for <code>XY</code>.</p><p>The mutual information estimated (for <code>normalize == false</code>) is then estimated as</p><p class="math-container">\[\hat{I}^S_{Gaussian}(X; Y) = \dfrac{1}{2}
\dfrac{ \det(\Sigma_X) \det(\Sigma_Y)) }{\det(\Sigma))},\]</p><p>where we <span>$\Sigma_X$</span> and <span>$\Sigma_Y$</span> appear in <span>$\Sigma$</span> as</p><p class="math-container">\[\Sigma = \begin{matrix}
\Sigma_{X} &amp; \Sigma^{&#39;}\\
\Sigma^{&#39;} &amp; \Sigma_{Y}
\end{matrix}.\]</p><p>If <code>normalize == true</code>, then the mutual information is estimated as</p><p class="math-container">\[\hat{I}^S_{Gaussian}(X; Y) = -\dfrac{1}{2} \sum{i = 1}^{d_x + d_y} \sigma_i,\]</p><p>where <span>$\sigma_i$</span> are the eigenvalues for <span>$\Sigma$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/estimators/GaussianMI.jl#L6-L58">source</a></section></article><h3 id="[KraskovStögbauerGrassberger1](@ref)"><a class="docs-heading-anchor" href="#[KraskovStögbauerGrassberger1](@ref)"><a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a></a><a id="[KraskovStögbauerGrassberger1](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[KraskovStögbauerGrassberger1](@ref)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.KraskovStögbauerGrassberger1" href="#CausalityTools.KraskovStögbauerGrassberger1"><code>CausalityTools.KraskovStögbauerGrassberger1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger1</code> mutual information estimator (you can use <code>KSG1</code> for short) is the <span>$I^{(1)}$</span> <code>k</code>-th nearest neighbor estimator from Kraskov et al. (2004)<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Description</strong></p><p>Let the joint StateSpaceSet <span>$X := \{\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m \}$</span> be defined by the concatenation of the marginal StateSpaceSets <span>$\{ \bf{X}_k \}_{k=1}^m$</span>, where each <span>$\bf{X}_k$</span> is potentially multivariate. Let <span>$\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N$</span> be the points in the joint space <span>$X$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/estimators/KSG1.jl#L9-L39">source</a></section></article><h3 id="[KraskovStögbauerGrassberger2](@ref)"><a class="docs-heading-anchor" href="#[KraskovStögbauerGrassberger2](@ref)"><a href="#CausalityTools.KraskovStögbauerGrassberger2"><code>KraskovStögbauerGrassberger2</code></a></a><a id="[KraskovStögbauerGrassberger2](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[KraskovStögbauerGrassberger2](@ref)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.KraskovStögbauerGrassberger2" href="#CausalityTools.KraskovStögbauerGrassberger2"><code>CausalityTools.KraskovStögbauerGrassberger2</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">KSG2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2 &lt;: MutualInformationEstimator
KraskovStögbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())</code></pre><p>The <code>KraskovStögbauerGrassberger2</code> mutual information estimator (you can use <code>KSG2</code> for short) is the <span>$I^{(2)}$</span> <code>k</code>-th nearest neighbor estimator from Kraskov et al. (2004)<sup class="footnote-reference"><a id="citeref-Kraskov2004" href="#footnote-Kraskov2004">[Kraskov2004]</a></sup>.</p><p><strong>Keyword arguments</strong></p><ul><li><strong><code>k::Int</code></strong>: The number of nearest neighbors to consider. Only information about the   <code>k</code>-th nearest neighbor is actually used.</li><li><strong><code>metric_marginals</code></strong>: The distance metric for the marginals for the marginals can be   any metric from <code>Distances.jl</code>. It defaults to <code>metric_marginals = Chebyshev()</code>, which   is the same as in Kraskov et al. (2004).</li><li><strong><code>w::Int</code></strong>: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to <code>0</code>, meaning that only the   point itself is excluded.</li></ul><p><strong>Description</strong></p><p>Let the joint StateSpaceSet <span>$X := \{\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m \}$</span> be defined by the concatenation of the marginal StateSpaceSets <span>$\{ \bf{X}_k \}_{k=1}^m$</span>, where each <span>$\bf{X}_k$</span> is potentially multivariate. Let <span>$\bf{x}_1, \bf{x}_2, \ldots, \bf{x}_N$</span> be the points in the joint space <span>$X$</span>.</p><p>The <code>KraskovStögbauerGrassberger2</code> estimator first locates, for each <span>$\bf{x}_i \in X$</span>, the point <span>$\bf{n}_i \in X$</span>, the <code>k</code>-th nearest neighbor to <span>$\bf{x}_i$</span>, according to the maximum norm (<code>Chebyshev</code> metric). Let <span>$\epsilon_i$</span> be the distance <span>$d(\bf{x}_i, \bf{n}_i)$</span>.</p><p>Consider <span>$x_i^m \in \bf{X}_m$</span>, the <span>$i$</span>-th point in the marginal space <span>$\bf{X}_m$</span>. For each <span>$\bf{x}_i^m$</span>, we determine <span>$\theta_i^m$</span> := the number of points <span>$\bf{x}_k^m \in \bf{X}_m$</span> that are a distance less than <span>$\epsilon_i$</span> away from <span>$\bf{x}_i^m$</span>. That is, we use the distance from a query point <span>$\bf{x}_i \in X$</span> (in the <em>joint</em> space) to count neighbors of <span>$x_i^m \in \bf{X}_m$</span> (in the marginal space).</p><p>Mutual information between the variables <span>$\bf{X}_1, \bf{X_2}, \ldots, \bf{X}_m$</span> is then estimated as</p><p class="math-container">\[\hat{I}_{KSG2}(\bf{X}) =
    \psi{(k)} -
    \dfrac{m - 1}{k} +
    (m - 1)\psi{(N)} -
    \dfrac{1}{N} \sum_{i = 1}^N \sum_{j = 1}^m \psi{(\theta_i^j + 1)}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/estimators/KSG2.jl#L8-L60">source</a></section></article><h3 id="[GaoKannanOhViswanath](@ref)"><a class="docs-heading-anchor" href="#[GaoKannanOhViswanath](@ref)"><a href="#CausalityTools.GaoKannanOhViswanath"><code>GaoKannanOhViswanath</code></a></a><a id="[GaoKannanOhViswanath](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[GaoKannanOhViswanath](@ref)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.GaoKannanOhViswanath" href="#CausalityTools.GaoKannanOhViswanath"><code>CausalityTools.GaoKannanOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoKannanOhViswanath &lt;: MutualInformationEstimator
GaoKannanOhViswanath(; k = 1, w = 0)</code></pre><p>The <code>GaoKannanOhViswanath</code> (Shannon) estimator is designed for estimating mutual information between variables that may be either discrete, continuous or a mixture of both (Gao et al., 2017).</p><div class="admonition is-info"><header class="admonition-header">Explicitly convert your discrete data to floats</header><div class="admonition-body"><p>Even though the <code>GaoKannanOhViswanath</code> estimator is designed to handle discrete data, our implementation demands that all input data are <code>StateSpaceSet</code>s whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to <a href="#CausalityTools.mutualinfo"><code>mutualinfo</code></a>.</p></div></div><p><strong>Description</strong></p><p>The estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using <code>k</code>-nearest neighbor distances from empirical samples.</p><p>The estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator <em>&quot;strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.&quot;</em></p><div class="admonition is-category-warn"><header class="admonition-header">Implementation note</header><div class="admonition-body"><p>In Gao et al., (2017), they claim (roughly speaking) that the estimator reduces to the <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> estimator for continuous-valued data. However, <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> uses the digamma function, while <code>GaoKannanOhViswanath</code> uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.</p><p>Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the <code>KSG1</code> approach. The <code>KSG1</code> estimator uses <code>k</code>-th neighbor distances in the <em>joint</em> space, while the <code>GaoKannanOhViswanath</code> algorithm selects the maximum <code>k</code>-th nearest distances among the two marginal spaces, which are in general not the same as the <code>k</code>-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in <code>GaoKannanOhViswanath</code>. We have modified it in a way that mimics <a href="#CausalityTools.KraskovStögbauerGrassberger1"><code>KraskovStögbauerGrassberger1</code></a> for continous data. Note that because of using the <code>log</code> function instead of <code>digamma</code>, there will be slight differences between the methods. See the source code for more details.</p></div></div><p>See also: <a href="#CausalityTools.mutualinfo"><code>mutualinfo</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/estimators/GaoKannanOhViswanath.jl#L5-L56">source</a></section></article><h3 id="[GaoOhViswanath](@ref)"><a class="docs-heading-anchor" href="#[GaoOhViswanath](@ref)"><a href="#CausalityTools.GaoOhViswanath"><code>GaoOhViswanath</code></a></a><a id="[GaoOhViswanath](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[GaoOhViswanath](@ref)" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CausalityTools.GaoOhViswanath" href="#CausalityTools.GaoOhViswanath"><code>CausalityTools.GaoOhViswanath</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GaoOhViswanath &lt;: MutualInformationEstimator</code></pre><p>The <code>GaoOhViswanath</code> mutual information estimator, also called the bias-improved-KSG estimator, or BI-KSG, by Gao et al. (2018)<sup class="footnote-reference"><a id="citeref-Gao2018" href="#footnote-Gao2018">[Gao2018]</a></sup>, is given by</p><p class="math-container">\[\begin{align*}
\hat{H}_{GAO}(X, Y)
&amp;= \hat{H}_{KSG}(X) + \hat{H}_{KSG}(Y) - \hat{H}_{KZL}(X, Y) \\
&amp;= \psi{(k)} +
    \log{(N)} +
    \log{
        \left(
            \dfrac{c_{d_{x}, 2} c_{d_{y}, 2}}{c_{d_{x} + d_{y}, 2}}
        \right)
    } - \\
    &amp; \dfrac{1}{N} \sum_{i=1}^N \left( \log{(n_{x, i, 2})} + \log{(n_{y, i, 2})} \right)
\end{align*},\]</p><p>where <span>$c_{d, 2} = \dfrac{\pi^{\frac{d}{2}}}{\Gamma{(\dfrac{d}{2} + 1)}}$</span> is the volume of a <span>$d$</span>-dimensional unit <span>$\mathcal{l}_2$</span>-ball.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaDynamics/CausalityTools.jl/blob/3eb9c4ca9c0e7388e935e24ff90da6de5f07dec0/src/methods/infomeasures/mutualinfo/estimators/GaoOhViswanath.jl#L4-L31">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Vejmelka2008"><a class="tag is-link" href="#citeref-Vejmelka2008">Vejmelka2008</a>Vejmelka, M., &amp; Paluš, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E, 77(2), 026214.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-Kraskov2004"><a class="tag is-link" href="#citeref-Kraskov2004">Kraskov2004</a>Kraskov, A., Stögbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.</li><li class="footnote" id="footnote-GaoKannanOhViswanath2017"><a class="tag is-link" href="#citeref-GaoKannanOhViswanath2017">GaoKannanOhViswanath2017</a>Gao, W., Kannan, S., Oh, S., &amp; Viswanath, P. (2017). Estimating mutual information for discrete-continuous mixtures. Advances in neural information processing systems, 30.</li><li class="footnote" id="footnote-Gao2018"><a class="tag is-link" href="#citeref-Gao2018">Gao2018</a>Gao, W., Oh, S., &amp; Viswanath, P. (2018). Demystifying fixed k-nearest neighbor information estimators. IEEE Transactions on Information Theory, 64(8), 5629-5661.</li></ul></section></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Saturday 8 April 2023 08:25">Saturday 8 April 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
