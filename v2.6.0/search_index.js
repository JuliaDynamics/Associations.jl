var documenterSearchIndex = {"docs":
[{"location":"examples/examples_predictive_asymmetry/#examples_predictive_asymmetry","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"","category":"section"},{"location":"examples/examples_predictive_asymmetry/#examples_pa_asymmetry_dist","page":"Predictive asymmetry","title":"Computing the asymmetry distribution","text":"","category":"section"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"The following example demonstrates how to compute the asymmetry distribution from time series input. We'll use timeseries from a chain of unidirectionally coupled logistic maps that are coupled X to Y to Z to W.","category":"page"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"These examples compute the asymmetry distribution directly. Use the PA measure with PATest for formal independence testing.","category":"page"},{"location":"examples/examples_predictive_asymmetry/#Pairwise-analysis","page":"Predictive asymmetry","title":"Pairwise analysis","text":"","category":"section"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"When considering only two variables V_1 and V_2, we expect the distribution DeltaA_X to Y to be skewed towards positive values if V_1 to V2.","category":"page"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"Parameters are tuned by providing an instance of the PA measure, which quantifies directional influence. We'll use the FPVP estimator, and compute the asymmetry distribution over prediction lags ηT = 1:10. In real applications, it is important to ensure proper embeddings for the source (and conditional, if relevant) variables. We will optimize embedding parameters using the \"traditional\" approach from DelayEmbeddings.jl.","category":"page"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"using CausalityTools\nusing DelayEmbeddings\nusing Random\nrng = MersenneTwister(1234)\n\nsys = system(Logistic4Chain(xi = [0.1, 0.2, 0.3, 0.4]; rng))\nx, y, z, w = columns(trajectory(sys, 1000))\nτx = estimate_delay(x, \"mi_min\")\nτy = estimate_delay(y, \"mi_min\")\nest = FPVP(; k = 3, w = 5)\nΔA_xy = asymmetry(PA(ηT = 1:10, τS = τx), est, x, y)\nΔA_yx = asymmetry(PA(ηT = 1:10, τS = τy), est, y, x)\nΔA_xy, ΔA_yx","category":"page"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"As expected, since there is coupling X to Y, Delta A_X to Y is skewed towards positive values, while Delta A_Y to X is skewed towards negative values because there is no coupling Y to X.","category":"page"},{"location":"examples/examples_predictive_asymmetry/#Conditional-analysis","page":"Predictive asymmetry","title":"Conditional analysis","text":"","category":"section"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"What happens if we computeDelta A_X to Z? We'd maybe expect there to be  some information transfer X to Z, even though ther are not directly linked, because information is transferred through Y.","category":"page"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"ΔA_xz = asymmetry(PA(ηT = 1:10, τS = estimate_delay(x, \"mi_min\")), est, x, z)","category":"page"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"As expected, the distribution is still skewed towards positive values. To determine whether the information flow between x and z is mediated by y, we can compute the conditional distribution Delta A_X to Z  Y. If these values are still positively skewed, we conclude that Y is not a mediating variable. If conditioning on Y causes Delta A_X to Z  Y to not be skewed towards positive values any more, then we conclude that Y is a mediating variable and that X and Z are linked X to Y to Z.","category":"page"},{"location":"examples/examples_predictive_asymmetry/","page":"Predictive asymmetry","title":"Predictive asymmetry","text":"In these examples, the same time series are formally tested for independence using a PATest.","category":"page"},{"location":"api/api_mutualinfo/#api_mutualinfo","page":"Mutual information API","title":"Mutual information API","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"The mutual information (MI) API is defined by","category":"page"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"MutualInformation, and its subtypes.\nmutualinfo,\nMutualInformationEstimator, and its subtypes.","category":"page"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"mutualinfo","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.mutualinfo","page":"Mutual information API","title":"CausalityTools.mutualinfo","text":"mutualinfo([measure::MutualInformation], m::ContingencyMatrix) → mi::Real\n\nEstimate the mutual information between x and y, the variables corresponding to the columns and rows of the 2-dimensional contingency matrix m, respectively.\n\nEstimates the discrete version of the given MutualInformation measure from its direct definition (double-sum), using the probabilities from a pre-computed ContingencyMatrix. If measure is not given, then the default is MIShannon().\n\n\n\n\n\nmutualinfo([measure::MutualInformation], est::ProbabilitiesEstimator, x, y) → mi::Real ∈ [0, a]\n\nEstimate the mutual information between x and y using the discrete version of the given measure, using the given ProbabilitiesEstimator est (which must accept multivariate data and have an implementation for marginal_encodings). See examples here. If measure is not given, then the default is MIShannon().\n\nEstimators\n\nThe mutual information is computed as sum of three entropy terms, without any bias correction. The exception is when using Contingency; then the mutual information is computed using a ContingencyMatrix.\n\nJoint and marginal probabilities are computed by jointly discretizing x and y using the approach given by est (using marginal_encodings), and obtaining marginal distributions from the joint distribution.\n\nEstimator Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiJizba MIRenyiSarbu\nContingency Contingency table ✓ ✓ ✓ ✓ ✓\nCountOccurrences Frequencies ✓ ✓ ✓ ✓ ✖\nValueHistogram Binning (histogram) ✓ ✓ ✓ ✓ ✖\nSymbolicPermutation Ordinal patterns ✓ ✓ ✓ ✓ ✖\nDispersion Dispersion patterns ✓ ✓ ✓ ✓ ✖\n\n\n\n\n\nmutualinfo([measure::MutualInformation], est::DifferentialEntropyEstimator, x, y)\n\nEstimate the mutual information measure between x and y by a sum of three entropy terms, without any bias correction, using any DifferentialEntropyEstimator compatible with multivariate data. See examples here. If measure is not given, then the default is MIShannon().\n\nnote: Note\nDifferentialEntropyEstimators have their own base field which is not used here. Instead, this method creates a copy of est internally, where est.base is replaced by measure.e.base. Therefore, use measure to control the \"unit\" of the mutual information.\n\nEstimators\n\nSome MutualInformation measures can be computed using a DifferentialEntropyEstimator, provided it supports multivariate input data. These estimators compute mutual information as a sum of of entropy terms (with different dimensions), without any bias correction.\n\nEstimator Principle MIShannon MITsallisFuruichi MITsallisMartin MIRenyiJizba MIRenyiSurbu\nKraskov Nearest neighbors ✓ x x x x\nZhu Nearest neighbors ✓ x x x x\nZhuSingh Nearest neighbors ✓ x x x x\nGao Nearest neighbors ✓ x x x x\nGoria Nearest neighbors ✓ x x x x\nLord Nearest neighbors ✓ x x x x\nLeonenkoProzantoSavani Nearest neighbors ✓ x x x x\n\n\n\n\n\nmutualinfo([measure::MutualInformation], est::MutualInformationEstimator, x, y)\n\nEstimate the mutual information measure between x and y using the dedicated MutualInformationEstimator est. See examples here. If measure is not given, then the default is MIShannon().\n\nEstimators\n\nDedicated MutualInformationEstimators are either discrete, continuous, or a mixture of both. Typically, these estimators apply bias correction.\n\nEstimator Type MIShannon\nGaussanMI Parametric ✓\nKSG1 Continuous ✓\nKSG2 Continuous ✓\nGaoKannanOhViswanath Mixed ✓\nGaoOhViswanath Continuous ✓\n\n\n\n\n\n","category":"function"},{"location":"api/api_mutualinfo/#Definitions","page":"Mutual information API","title":"Definitions","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"MutualInformation","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.MutualInformation","page":"Mutual information API","title":"CausalityTools.MutualInformation","text":"MutualInformation <: AssociationMeasure\n\nThe supertype of all mutual information measures. Concrete subtypes are\n\nMIShannon\nMITsallisFuruichi\nMITsallisMartin\nMIRenyiJizba\nMIRenyiSarbu\n\n\n\n\n\n","category":"type"},{"location":"api/api_mutualinfo/#[MutualInformationEstimator](@ref)s","page":"Mutual information API","title":"MutualInformationEstimators","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"MutualInformationEstimator","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.MutualInformationEstimator","page":"Mutual information API","title":"CausalityTools.MutualInformationEstimator","text":"MutualInformationEstimator\n\nThe supertype of all dedicated mutual information estimators.\n\nMutualInformationEstimators can be either mixed, discrete or a combination of both. Each estimator uses a specialized technique to approximate relevant densities/integrals and/or probabilities, and is typically tailored to a specific type of MutualInformation (mostly MIShannon).\n\n\n\n\n\n","category":"type"},{"location":"api/api_mutualinfo/#[GaussianMI](@ref)-(parametric)","page":"Mutual information API","title":"GaussianMI (parametric)","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"GaussianMI","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.GaussianMI","page":"Mutual information API","title":"CausalityTools.GaussianMI","text":"GaussianMI <: MutualInformationEstimator\nGaussianMI(; normalize::Bool = false)\n\nGaussianMI is a parametric estimator for Shannon mutual information.\n\nDescription\n\nGiven d_x-dimensional and d_y-dimensional input data X and Y, GaussianMI first constructs the d_x + d_y-dimensional joint StateSpaceSet XY. If normalize == true, then we follow the approach in Vejmelka & Palus (2008)[Vejmelka2008] and transform each column in XY to have zero mean and unit standard deviation. If normalize == false, then the algorithm proceeds without normalization.\n\nNext, the C_{XY}, the correlation matrix for the (normalized) joint data XY is computed. The mutual information estimate\n\nGaussianMI assumes the input variables are distributed according to normal distributions with zero means and unit standard deviations. Therefore, given d_x-dimensional and d_y-dimensional input data X and Y, GaussianMI first constructs the joint StateSpaceSet XY, then transforms each column in XY to have zero mean and unit standard deviation, and finally computes the \\Sigma, the correlation matrix for XY.\n\nThe mutual information estimated (for normalize == false) is then estimated as\n\nhatI^S_Gaussian(X Y) = dfrac12\ndfrac det(Sigma_X) det(Sigma_Y)) det(Sigma))\n\nwhere we Sigma_X and Sigma_Y appear in Sigma as\n\nSigma = beginmatrix\nSigma_X  Sigma^\nSigma^  Sigma_Y\nendmatrix\n\nIf normalize == true, then the mutual information is estimated as\n\nhatI^S_Gaussian(X Y) = -dfrac12 sumi = 1^d_x + d_y sigma_i\n\nwhere sigma_i are the eigenvalues for Sigma.\n\n[Vejmelka2008]: Vejmelka, M., & Paluš, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E, 77(2), 026214.\n\n\n\n\n\n","category":"type"},{"location":"api/api_mutualinfo/#[KraskovStögbauerGrassberger1](@ref)","page":"Mutual information API","title":"KraskovStögbauerGrassberger1","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"KraskovStögbauerGrassberger1","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.KraskovStögbauerGrassberger1","page":"Mutual information API","title":"CausalityTools.KraskovStögbauerGrassberger1","text":"KSG1 <: MutualInformationEstimator\nKraskovStögbauerGrassberger1 <: MutualInformationEstimator\nKraskovStögbauerGrassberger1(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovStögbauerGrassberger1 mutual information estimator (you can use KSG1 for short) is the I^(1) k-th nearest neighbor estimator from Kraskov et al. (2004)[Kraskov2004].\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nDescription\n\nLet the joint StateSpaceSet X = bfX_1 bfX_2 ldots bfX_m  be defined by the concatenation of the marginal StateSpaceSets  bfX_k _k=1^m, where each bfX_k is potentially multivariate. Let bfx_1 bfx_2 ldots bfx_N be the points in the joint space X.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"api/api_mutualinfo/#[KraskovStögbauerGrassberger2](@ref)","page":"Mutual information API","title":"KraskovStögbauerGrassberger2","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"KraskovStögbauerGrassberger2","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.KraskovStögbauerGrassberger2","page":"Mutual information API","title":"CausalityTools.KraskovStögbauerGrassberger2","text":"KSG2 <: MutualInformationEstimator\nKraskovStögbauerGrassberger2 <: MutualInformationEstimator\nKraskovStögbauerGrassberger2(; k::Int = 1, w = 0, metric_marginals = Chebyshev())\n\nThe KraskovStögbauerGrassberger2 mutual information estimator (you can use KSG2 for short) is the I^(2) k-th nearest neighbor estimator from Kraskov et al. (2004)[Kraskov2004].\n\nKeyword arguments\n\nk::Int: The number of nearest neighbors to consider. Only information about the   k-th nearest neighbor is actually used.\nmetric_marginals: The distance metric for the marginals for the marginals can be   any metric from Distances.jl. It defaults to metric_marginals = Chebyshev(), which   is the same as in Kraskov et al. (2004).\nw::Int: The Theiler window, which determines if temporal neighbors are excluded   during neighbor searches in the joint space. Defaults to 0, meaning that only the   point itself is excluded.\n\nDescription\n\nLet the joint StateSpaceSet X = bfX_1 bfX_2 ldots bfX_m  be defined by the concatenation of the marginal StateSpaceSets  bfX_k _k=1^m, where each bfX_k is potentially multivariate. Let bfx_1 bfx_2 ldots bfx_N be the points in the joint space X.\n\nThe KraskovStögbauerGrassberger2 estimator first locates, for each bfx_i in X, the point bfn_i in X, the k-th nearest neighbor to bfx_i, according to the maximum norm (Chebyshev metric). Let epsilon_i be the distance d(bfx_i bfn_i).\n\nConsider x_i^m in bfX_m, the i-th point in the marginal space bfX_m. For each bfx_i^m, we determine theta_i^m := the number of points bfx_k^m in bfX_m that are a distance less than epsilon_i away from bfx_i^m. That is, we use the distance from a query point bfx_i in X (in the joint space) to count neighbors of x_i^m in bfX_m (in the marginal space).\n\nMutual information between the variables bfX_1 bfX_2 ldots bfX_m is then estimated as\n\nhatI_KSG2(bfX) =\n    psi(k) -\n    dfracm - 1k +\n    (m - 1)psi(N) -\n    dfrac1N sum_i = 1^N sum_j = 1^m psi(theta_i^j + 1)\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"api/api_mutualinfo/#[GaoKannanOhViswanath](@ref)","page":"Mutual information API","title":"GaoKannanOhViswanath","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"GaoKannanOhViswanath","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.GaoKannanOhViswanath","page":"Mutual information API","title":"CausalityTools.GaoKannanOhViswanath","text":"GaoKannanOhViswanath <: MutualInformationEstimator\nGaoKannanOhViswanath(; k = 1, w = 0)\n\nThe GaoKannanOhViswanath (Shannon) estimator is designed for estimating mutual information between variables that may be either discrete, continuous or a mixture of both (Gao et al., 2017).\n\nnote: Explicitly convert your discrete data to floats\nEven though the GaoKannanOhViswanath estimator is designed to handle discrete data, our implementation demands that all input data are StateSpaceSets whose data points are floats. If you have discrete data, such as strings or symbols, encode them using integers and convert those integers to floats before passing them to mutualinfo.\n\nDescription\n\nThe estimator starts by expressing mutual information in terms of the Radon-Nikodym derivative, and then estimates these derivatives using k-nearest neighbor distances from empirical samples.\n\nThe estimator avoids the common issue of having to add noise to data before analysis due to tied points, which may bias other estimators. Citing their paper, the estimator \"strongly outperforms natural baselines of discretizing the mixed random variables (by quantization) or making it continuous by adding a small Gaussian noise.\"\n\nwarn: Implementation note\nIn Gao et al., (2017), they claim (roughly speaking) that the estimator reduces to the KraskovStögbauerGrassberger1 estimator for continuous-valued data. However, KraskovStögbauerGrassberger1 uses the digamma function, while GaoKannanOhViswanath uses the logarithm instead, so the estimators are not exactly equivalent for continuous data.Moreover, in their algorithm 1, it is clearly not the case that the method falls back on the KSG1 approach. The KSG1 estimator uses k-th neighbor distances in the joint space, while the GaoKannanOhViswanath algorithm selects the maximum k-th nearest distances among the two marginal spaces, which are in general not the same as the k-th neighbor distance in the joint space (unless both marginals are univariate). Therefore, our implementation here differs slightly from algorithm 1 in GaoKannanOhViswanath. We have modified it in a way that mimics KraskovStögbauerGrassberger1 for continous data. Note that because of using the log function instead of digamma, there will be slight differences between the methods. See the source code for more details.\n\nSee also: mutualinfo.\n\n[GaoKannanOhViswanath2017]: Gao, W., Kannan, S., Oh, S., & Viswanath, P. (2017). Estimating mutual information for discrete-continuous mixtures. Advances in neural information processing systems, 30.\n\n\n\n\n\n","category":"type"},{"location":"api/api_mutualinfo/#[GaoOhViswanath](@ref)","page":"Mutual information API","title":"GaoOhViswanath","text":"","category":"section"},{"location":"api/api_mutualinfo/","page":"Mutual information API","title":"Mutual information API","text":"GaoOhViswanath","category":"page"},{"location":"api/api_mutualinfo/#CausalityTools.GaoOhViswanath","page":"Mutual information API","title":"CausalityTools.GaoOhViswanath","text":"GaoOhViswanath <: MutualInformationEstimator\n\nThe GaoOhViswanath mutual information estimator, also called the bias-improved-KSG estimator, or BI-KSG, by Gao et al. (2018)[Gao2018], is given by\n\nbeginalign*\nhatH_GAO(X Y)\n= hatH_KSG(X) + hatH_KSG(Y) - hatH_KZL(X Y) \n= psi(k) +\n    log(N) +\n    log\n        left(\n            dfracc_d_x 2 c_d_y 2c_d_x + d_y 2\n        right)\n     - \n     dfrac1N sum_i=1^N left( log(n_x i 2) + log(n_y i 2) right)\nendalign*\n\nwhere c_d 2 = dfracpi^fracd2Gamma(dfracd2 + 1) is the volume of a d-dimensional unit mathcall_2-ball.\n\n[Gao2018]: Gao, W., Oh, S., & Viswanath, P. (2018). Demystifying fixed k-nearest neighbor information estimators. IEEE Transactions on Information Theory, 64(8), 5629-5661.\n\n\n\n\n\n","category":"type"},{"location":"examples/examples_recurrence/#examples_recurrence","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"","category":"section"},{"location":"examples/examples_recurrence/#Computing-the-[Recurrence](@ref)-measure-for-independent-data","page":"Inferring directional influence using conditional recurrence","title":"Computing the Recurrence measure for independent data","text":"","category":"section"},{"location":"examples/examples_recurrence/","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"The interpretation of the Recurrence measure is that if two variables are symmetrically coupled, then the conditional recurrence in both directions is equal. Two variables that are uncoupled are symmetrically coupled (i.e. no coupling). We therefore expect the difference in conditional recurrence to be around zero.","category":"page"},{"location":"examples/examples_recurrence/","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"using CausalityTools\nusing StableRNGs\nrng = StableRNG(1234)\nx = rand(rng, 300)\ny = rand(rng, 300)\nm = Recurrence(r = 0.5)\nΔ = conditional_recurrence(m, x, y) - conditional_recurrence(m, y, x)","category":"page"},{"location":"examples/examples_recurrence/","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"This value is close to zero. To test if it is significantly indistinguishable from zero, we can use a SurrogateTest (see example below).","category":"page"},{"location":"examples/examples_recurrence/#Independence-test","page":"Inferring directional influence using conditional recurrence","title":"Independence test","text":"","category":"section"},{"location":"examples/examples_recurrence/","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"using CausalityTools\nusing StableRNGs\nrng = StableRNG(1234)\nx = rand(rng, 300)\ny = rand(rng, 300)\ntest = SurrogateTest(Recurrence(r = 0.5); rng, nshuffles = 100, surrogate = RandomShuffle())\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_recurrence/","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"As expected, we can't reject independence. What happens if two variables are coupled?","category":"page"},{"location":"examples/examples_recurrence/","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"using CausalityTools\nusing StableRNGs\nrng = StableRNG(1234)\nx = rand(rng, 300)\nz = x .+ rand(rng, 300)\ntest = SurrogateTest(Recurrence(r = 0.5); rng, nshuffles = 100, surrogate = RandomShuffle())\nindependence(test, x, z)","category":"page"},{"location":"examples/examples_recurrence/","page":"Inferring directional influence using conditional recurrence","title":"Inferring directional influence using conditional recurrence","text":"Now, because the variables are coupled, the evidence in the data support dependence.","category":"page"},{"location":"api/api_conditional_entropy/#Conditional-entropy-API","page":"Conditional entropy API","title":"Conditional entropy API","text":"","category":"section"},{"location":"api/api_conditional_entropy/","page":"Conditional entropy API","title":"Conditional entropy API","text":"The conditional entropy API is defined by","category":"page"},{"location":"api/api_conditional_entropy/","page":"Conditional entropy API","title":"Conditional entropy API","text":"ConditionalEntropy, and its subtypes\nentropy_conditional,","category":"page"},{"location":"api/api_conditional_entropy/","page":"Conditional entropy API","title":"Conditional entropy API","text":"entropy_conditional","category":"page"},{"location":"api/api_conditional_entropy/#CausalityTools.entropy_conditional","page":"Conditional entropy API","title":"CausalityTools.entropy_conditional","text":"entropy_conditional(measure::ConditionalEntropy, c::ContingencyMatrix{T, 2}) where T\n\nEstimate the discrete version of the given ConditionalEntropy measure from its direct (sum) definition, using the probabilities from a pre-computed ContingencyMatrix, constructed from two input variables x and y. This estimation method works for both numerical and categorical data. If measure is not given, then the default is CEShannon().\n\nThe convention is to compute the entropy of the variable in the first column of c conditioned on the variable in the second column of c. To do the opposite, call this function with a new contingency matrix where the order of the variables is reversed.\n\nCompatible measures\n\n ContingencyMatrix\nCEShannon ✓\nCETsallisFuruichi ✓\nCETsallisAbe ✓\n\n\n\n\n\nentropy_conditional([measure::ConditionalEntropy], est::ProbabilitiesEstimator, x, y)\n\nEstimate the entropy of x conditioned on y, using the discrete version of the given conditional entropy (CE) measure. The CE is computed the difference of the joint entropy and the marginal entropy of y, using the ProbabilitiesEstimator est, which must compatible with multivariate data (that is, have an implementation for marginal_encodings). No bias correction is applied. If measure is not given, then the default is CEShannon().\n\nEstimators\n\nJoint and marginal probabilities are computed by jointly discretizing x and y using the approach given by est, and obtaining the marginal distribution for y from the joint distribution.\n\nEstimator Principle CEShannon CETsallisAbe CETsallisFuruichi\nCountOccurrences Frequencies ✓ ✓ x\nValueHistogram Binning (histogram) ✓ ✓ x\nSymbolicPermutation Ordinal patterns ✓ ✓ x\nDispersion Dispersion patterns ✓ ✓ x\n\n\n\n\n\nentropy_conditional([measure::ConditionalEntropy], est::DifferentialEntropyEstimator, x, y)\n\nEstimate the entropy of x conditioned on y, using the differential/continuous version of the given conditional entropy (CE) measure.  The CE is computed the difference of the joint entropy and the marginal entropy of y, using the DifferentialEntropyEstimator est, which must be compatible with multivariate data. No bias correction is applied. If measure is not given, then the default is CEShannon().\n\nEstimators\n\nEstimator Principle CEShannon CETsallisAbe CETsallisFuruichi\nKraskov Nearest neighbors ✓ x x\nZhu Nearest neighbors ✓ x x\nZhuSingh Nearest neighbors ✓ x x\nGao Nearest neighbors ✓ x x\nGoria Nearest neighbors ✓ x x\nLord Nearest neighbors ✓ x x\nLeonenkoProzantoSavani Nearest neighbors ✓ x x\n\n\n\n\n\n","category":"function"},{"location":"api/api_conditional_entropy/#Definitions","page":"Conditional entropy API","title":"Definitions","text":"","category":"section"},{"location":"api/api_conditional_entropy/","page":"Conditional entropy API","title":"Conditional entropy API","text":"ConditionalEntropy\nCEShannon\nCETsallisFuruichi\nCETsallisAbe","category":"page"},{"location":"api/api_conditional_entropy/#CausalityTools.ConditionalEntropy","page":"Conditional entropy API","title":"CausalityTools.ConditionalEntropy","text":"The supertype for all conditional entropies.\n\n\n\n\n\n","category":"type"},{"location":"api/api_conditional_entropy/#CausalityTools.CEShannon","page":"Conditional entropy API","title":"CausalityTools.CEShannon","text":"CEShannon <: ConditionalEntropy\nCEShannon(; base = 2,)\n\nTheShannon conditional entropy measure.\n\nDiscrete definition\n\nSum formulation\n\nThe conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH^S(X  Y) = -sum_x in mathcalX y in mathcalY = p(x y) log(p(x  y))\n\nThis is the definition used when calling entropy_conditional with a ContingencyMatrix.\n\nTwo-entropies formulation\n\nEquivalently, the following difference of entropies hold\n\nH^S(X  Y) = H^S(X Y) - H^S(Y)\n\nwhere H^S(cdot and H^S(cdot  cdot) are the Shannon entropy and Shannon joint entropy, respectively. This is the definition used when calling entropy_conditional with a ProbabilitiesEstimator.\n\nDifferential definition\n\nThe differential conditional Shannon entropy is analogously defined as\n\nH^S(X  Y) = h^S(X Y) - h^S(Y)\n\nwhere h^S(cdot and h^S(cdot  cdot) are the Shannon differential entropy and Shannon joint differential entropy, respectively. This is the definition used when calling entropy_conditional with a DifferentialEntropyEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/api_conditional_entropy/#CausalityTools.CETsallisFuruichi","page":"Conditional entropy API","title":"CausalityTools.CETsallisFuruichi","text":"CETsallisFuruichi <: ConditionalEntropy\nCETsallisFuruichi(; base = 2, q = 1.5)\n\nFuruichi (2006)'s discrete Tsallis conditional entropy measure.\n\nDefinition\n\nFuruichi's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T(X  Y) = -sum_x in mathcalX y in mathcalY\np(x y)^q log_q(p(x  y))\n\nwhen q neq 1. For q = 1, H_q^T(X  Y) reduces to the Shannon conditional entropy:\n\nH_q=1^T(X  Y) = -sum_x in mathcalX y in mathcalY =\np(x y) log(p(x  y))\n\n\n\n\n\n","category":"type"},{"location":"api/api_conditional_entropy/#CausalityTools.CETsallisAbe","page":"Conditional entropy API","title":"CausalityTools.CETsallisAbe","text":"CETsallisAbe <: ConditionalEntropy\nCETsallisAbe(; base = 2, q = 1.5)\n\nAbe & Rajagopal (2001)'s discrete Tsallis conditional entropy measure.\n\nDefinition\n\nAbe & Rajagopal's Tsallis conditional entropy between discrete random variables X and Y with finite ranges mathcalX and mathcalY is defined as\n\nH_q^T_A(X  Y) = dfracH_q^T(X Y) - H_q^T(Y)1 + (1-q)H_q^T(Y)\n\nwhere H_q^T(cdot) and H_q^T(cdot cdot) is the Tsallis entropy and the joint Tsallis entropy.\n\n[Abe2001]: Abe, S., & Rajagopal, A. K. (2001). Nonadditive conditional entropy and its significance for local realism. Physica A: Statistical Mechanics and its Applications, 289(1-2), 157-164.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Probabilities-API","page":"Probabilities API","title":"Probabilities API","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"The probabilities API is defined by","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"ProbabilitiesEstimator, and its subtypes.\nprobabilities\nprobabilities_and_outcomes","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"See also contingency tables for a multivariate version.","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"The probabilities API is re-exported from ComplexityMeasures.jl. Why? Most discrete information theoretic association measures are estimated using some sort of ProbabilitiesEstimators, because their formulas are simply functions of probability mass functions.","category":"page"},{"location":"api/api_probabilities/#Probabilities","page":"Probabilities API","title":"Probabilities","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"ProbabilitiesEstimator\nprobabilities\nprobabilities!\nProbabilities","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.ProbabilitiesEstimator","page":"Probabilities API","title":"ComplexityMeasures.ProbabilitiesEstimator","text":"ProbabilitiesEstimator\n\nThe supertype for all probabilities estimators.\n\nIn ComplexityMeasures.jl, probability distributions are estimated from data by defining a set of possible outcomes Omega = omega_1 omega_2 ldots omega_L , and assigning to each outcome omega_i a probability p(omega_i), such that sum_i=1^N p(omega_i) = 1. It is the role of a ProbabilitiesEstimator to\n\nDefine Omega, the \"outcome space\", which is the set of all possible outcomes over  which probabilities are estimated. The cardinality of this set can be obtained using  total_outcomes.\nDefine how probabilities p_i = p(omega_i) are assigned to outcomes omega_i.\n\nIn practice, probability estimation is done by calling probabilities with some input data and one of the following probabilities estimators. The result is a Probabilities p (Vector-like), where each element p[i] is the probability of the outcome ω[i]. Use probabilities_and_outcomes if you need both the probabilities and the outcomes, and use outcome_space to obtain Omega alone. The element type of Omega varies between estimators, but it is guaranteed to be hashable. This allows for conveniently tracking the probability of a specific event across experimental realizations, by using the outcome as a dictionary key and the probability as the value for that key (or, alternatively, the key remains the outcome and one has a vector of probabilities, one for each experimental realization).\n\nSome estimators can deduce Omega without knowledge of the input, such as SymbolicPermutation. For others, knowledge of input is necessary for concretely specifying Omega, such as ValueHistogram with RectangularBinning. This only matters for the functions outcome_space and total_outcomes.\n\nAll currently implemented probability estimators are listed in a nice table in the probabilities estimators section of the online documentation.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.probabilities","page":"Probabilities API","title":"ComplexityMeasures.probabilities","text":"probabilities(est::ProbabilitiesEstimator, x::Array_or_Dataset) → p::Probabilities\n\nCompute a probability distribution over the set of possible outcomes defined by the probabilities estimator est, given input data x, which is typically an Array or a StateSpaceSet; see Input data for ComplexityMeasures.jl. Configuration options are always given as arguments to the chosen estimator.\n\nTo obtain the outcomes corresponding to these probabilities, use outcomes.\n\nDue to performance optimizations, whether the returned probablities contain 0s as entries or not depends on the estimator. E.g., in ValueHistogram 0s are skipped, while in SymbolicPermutation 0 are not, because we get them for free.\n\nprobabilities(x::Vector_or_Dataset) → p::Probabilities\n\nEstimate probabilities by directly counting the elements of x, assuming that Ω = sort(unique(x)), i.e. that the outcome space is the unique elements of x. This is mostly useful when x contains categorical data.\n\nSee also: Probabilities, ProbabilitiesEstimator.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.probabilities!","page":"Probabilities API","title":"ComplexityMeasures.probabilities!","text":"probabilities!(s, args...)\n\nSimilar to probabilities(args...), but allows pre-allocation of temporarily used containers s.\n\nOnly works for certain estimators. See for example SymbolicPermutation.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.Probabilities","page":"Probabilities API","title":"ComplexityMeasures.Probabilities","text":"Probabilities <: AbstractArray\nProbabilities(x) → p\n\nProbabilities is a simple wrapper around x::AbstractArray{<:Real, N} that ensures its values sum to 1, so that p can be interpreted as N-dimensional probability mass function. In most use cases, p will be a vector.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Estimators","page":"Probabilities API","title":"Estimators","text":"","category":"section"},{"location":"api/api_probabilities/#Overview","page":"Probabilities API","title":"Overview","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Here, we list probabilities estimators that are compatible with CausalityTools.jl. Note that not all probabilities estimators from ComplexityMeasures.jl are included. This is because for the information-based association measures here, the probabilities estimator must be compatible with multivariate data, or have an implementation for marginal_encodings, which discretizes each dimension of the multivariate input data separately.","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Estimator Principle\nContingency Count co-occurrences, optionally discretize first\nCountOccurrences Count of unique elements\nValueHistogram Binning (histogram)\nTransferOperator Binning (transfer operator)\nNaiveKernel Kernel density estimation\nSymbolicPermutation Ordinal patterns\nDispersion Dispersion patterns","category":"page"},{"location":"api/api_probabilities/#Contingency","page":"Probabilities API","title":"Contingency","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Contingency","category":"page"},{"location":"api/api_probabilities/#CausalityTools.Contingency","page":"Probabilities API","title":"CausalityTools.Contingency","text":"Contingency <: ProbabilitiesEstimator\nContingency(est::Union{ProbabilitiesEstimator, Nothing} = nothing)\n\nContingency is a probabilities estimator that transforms input data to a multidimensional probability mass function (internally represented as ContingencyMatrix.\n\nIt works directly on raw discrete/categorical data. Alternatively, if a ProbabilitiesEstimator est for which marginal_encodings is implemented is given, then input data are first discretized before creating the contingency matrix.\n\nnote: Note\nThe Contingency estimator differs from other ProbabilitiesEstimators in that it's not compatible with probabilities and other methods. Instead, it is used to construct ContingencyMatrix, from which probabilities can be computed.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Count-occurrences","page":"Probabilities API","title":"Count occurrences","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"CountOccurrences","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.CountOccurrences","page":"Probabilities API","title":"ComplexityMeasures.CountOccurrences","text":"CountOccurrences()\n\nA probabilities/entropy estimator based on straight-forward counting of distinct elements in a univariate time series or multivariate dataset. This is the same as giving no estimator to probabilities.\n\nOutcome space\n\nThe outcome space is the unique sorted values of the input. Hence, input x is needed for a well-defined outcome_space.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Histograms-(binning)","page":"Probabilities API","title":"Histograms (binning)","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"ValueHistogram\nRectangularBinning\nFixedRectangularBinning","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.ValueHistogram","page":"Probabilities API","title":"ComplexityMeasures.ValueHistogram","text":"ValueHistogram(b::AbstractBinning) <: ProbabilitiesEstimator\n\nA probability estimator based on binning the values of the data as dictated by the binning scheme b and formally computing their histogram, i.e., the frequencies of points in the bins. An alias to this is VisitationFrequency. Available binnings are subtypes of AbstractBinning.\n\nThe ValueHistogram estimator has a linearithmic time complexity (n log(n) for n = length(x)) and a linear space complexity (l for l = dimension(x)). This allows computation of probabilities (histograms) of high-dimensional datasets and with small box sizes ε without memory overflow and with maximum performance. For performance reasons, the probabilities returned never contain 0s and are arbitrarily ordered.\n\nValueHistogram(ϵ::Union{Real,Vector})\n\nA convenience method that accepts same input as RectangularBinning and initializes this binning directly.\n\nOutcomes\n\nThe outcome space for ValueHistogram is the unique bins constructed from b. Each bin is identified by its left (lowest-value) corner, because bins are always left-closed-right-open intervals [a, b). The bins are in data units, not integer (cartesian indices units), and are returned as SVectors, i.e., same type as input data.\n\nFor convenience, outcome_space returns the outcomes in the same array format as the underlying binning (e.g., Matrix for 2D input).\n\nFor FixedRectangularBinning the outcome_space is well-defined from the binning, but for RectangularBinning input x is needed as well.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.RectangularBinning","page":"Probabilities API","title":"ComplexityMeasures.RectangularBinning","text":"RectangularBinning(ϵ, precise = false) <: AbstractBinning\n\nRectangular box partition of state space using the scheme ϵ, deducing the histogram extent and bin width from the input data.\n\nRectangularBinning is a convenience struct. It is re-cast into FixedRectangularBinning once the data are provided, so see that docstring for info on the bin calculation and the meaning of precise.\n\nBinning instructions are deduced from the type of ϵ as follows:\n\nϵ::Int divides each coordinate axis into ϵ equal-length intervals  that cover all data.\nϵ::Float64 divides each coordinate axis into intervals of fixed size ϵ, starting  from the axis minima until the data is completely covered by boxes.\nϵ::Vector{Int} divides the i-th coordinate axis into ϵ[i] equal-length  intervals that cover all data.\nϵ::Vector{Float64} divides the i-th coordinate axis into intervals of fixed size  ϵ[i], starting from the axis minima until the data is completely covered by boxes.\n\nRectangularBinning ensures all input data are covered by extending the created ranges if need be.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.FixedRectangularBinning","page":"Probabilities API","title":"ComplexityMeasures.FixedRectangularBinning","text":"FixedRectangularBinning <: AbstractBinning\nFixedRectangularBinning(ranges::Tuple{<:AbstractRange...}, precise = false)\n\nRectangular box partition of state space where the partition along each dimension is explicitly given by each range ranges, which is a tuple of AbstractRange subtypes. Typically, each range is the output of the range Base function, e.g., ranges = (0:0.1:1, range(0, 1; length = 101), range(2.1, 3.2; step = 0.33)). All ranges must be sorted.\n\nThe optional second argument precise dictates whether Julia Base's TwicePrecision is used for when searching where a point falls into the range. Useful for edge cases of points being almost exactly on the bin edges, but it is exactly four times as slow, so by default it is false.\n\nPoints falling outside the partition do not contribute to probabilities. Bins are always left-closed-right-open: [a, b). This means that the last value of each of the ranges dictates the last right-closing value. This value does not belong to the histogram! E.g., if given a range r = range(0, 1; length = 11), with r[end] = 1, the value 1 is outside the partition and would not attribute any increase of the probability corresponding to the last bin (here [0.9, 1))!\n\nEquivalently, the size of the histogram is histsize = map(r -> length(r)-1, ranges)!\n\nFixedRectangularBinning leads to a well-defined outcome space without knowledge of input data, see ValueHistogram.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Transfer-operator-(binning)","page":"Probabilities API","title":"Transfer operator (binning)","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"TransferOperator","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.TransferOperator","page":"Probabilities API","title":"ComplexityMeasures.TransferOperator","text":"TransferOperator <: ProbabilitiesEstimator\nTransferOperator(b::AbstractBinning)\n\nA probability estimator based on binning data into rectangular boxes dictated by the given binning scheme b, then approximating the transfer (Perron-Frobenius) operator over the bins, then taking the invariant measure associated with that transfer operator as the bin probabilities. Assumes that the input data are sequential (time-ordered).\n\nThis implementation follows the grid estimator approach in Diego et al. (2019)[Diego2019].\n\nOutcome space\n\nThe outcome space for TransferOperator is the set of unique bins constructed from b. Bins are identified by their left (lowest-value) corners, are given in data units, and are returned as SVectors.\n\nBin ordering\n\nBins returned by probabilities_and_outcomes are ordered according to first appearance (i.e. the first time the input (multivariate) timeseries visits the bin). Thus, if\n\nb = RectangularBinning(4)\nest = TransferOperator(b)\nprobs, outcomes = probabilities_and_outcomes(x, est) # x is some timeseries\n\nthen probs[i] is the invariant measure (probability) of the bin outcomes[i], which is the i-th bin visited by the timeseries with nonzero measure.\n\nDescription\n\nThe transfer operator P^Nis computed as an N-by-N matrix of transition probabilities between the states defined by the partition elements, where N is the number of boxes in the partition that is visited by the orbit/points.\n\nIf  x_t^(D) _n=1^L are the L different D-dimensional points over which the transfer operator is approximated,  C_k=1^N  are the N different partition elements (as dictated by ϵ) that gets visited by the points, and  phi(x_t) = x_t+1, then\n\nP_ij = dfrac\n x_n  phi(x_n) in C_j cap x_n in C_i \n x_m  x_m in C_i \n\nwhere  denotes the cardinal. The element P_ij thus indicates how many points that are initially in box C_i end up in box C_j when the points in C_i are projected one step forward in time. Thus, the row P_ik^N where k in 1 2 ldots N  gives the probability of jumping from the state defined by box C_i to any of the other N states. It follows that sum_k=1^N P_ik = 1 for all i. Thus, P^N is a row/right stochastic matrix.\n\nInvariant measure estimation from transfer operator\n\nThe left invariant distribution mathbfrho^N is a row vector, where mathbfrho^N P^N = mathbfrho^N. Hence, mathbfrho^N is a row eigenvector of the transfer matrix P^N associated with eigenvalue 1. The distribution mathbfrho^N approximates the invariant density of the system subject to binning, and can be taken as a probability distribution over the partition elements.\n\nIn practice, the invariant measure mathbfrho^N is computed using invariantmeasure, which also approximates the transfer matrix. The invariant distribution is initialized as a length-N random distribution which is then applied to P^N. The resulting length-N distribution is then applied to P^N again. This process repeats until the difference between the distributions over consecutive iterations is below some threshold.\n\nSee also: RectangularBinning, invariantmeasure.\n\n[Diego2019]: Diego, D., Haaga, K. A., & Hannisdal, B. (2019). Transfer entropy computation using the Perron-Frobenius operator. Physical Review E, 99(4), 042212.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Utility-methods/types","page":"Probabilities API","title":"Utility methods/types","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"For explicit estimation of the transfer operator, see ComplexityMeasures.jl.","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"InvariantMeasure\ninvariantmeasure\ntransfermatrix","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.InvariantMeasure","page":"Probabilities API","title":"ComplexityMeasures.InvariantMeasure","text":"InvariantMeasure(to, ρ)\n\nMinimal return struct for invariantmeasure that contains the estimated invariant measure ρ, as well as the transfer operator to from which it is computed (including bin information).\n\nSee also: invariantmeasure.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.invariantmeasure","page":"Probabilities API","title":"ComplexityMeasures.invariantmeasure","text":"invariantmeasure(x::AbstractStateSpaceSet, binning::RectangularBinning) → iv::InvariantMeasure\n\nEstimate an invariant measure over the points in x based on binning the data into rectangular boxes dictated by the binning, then approximate the transfer (Perron-Frobenius) operator over the bins. From the approximation to the transfer operator, compute an invariant distribution over the bins. Assumes that the input data are sequential.\n\nDetails on the estimation procedure is found the TransferOperator docstring.\n\nExample\n\nusing DynamicalSystems\nhenon_rule(x, p, n) = SVector{2}(1.0 - p[1]*x[1]^2 + x[2], p[2]*x[1])\nhenon = DeterministicIteratedMap(henon_rule, zeros(2), [1.4, 0.3])\norbit, t = trajectory(ds, 20_000; Ttr = 10)\n\n# Estimate the invariant measure over some coarse graining of the orbit.\niv = invariantmeasure(orbit, RectangularBinning(15))\n\n# Get the probabilities and bins\ninvariantmeasure(iv)\n\nProbabilities and bin information\n\ninvariantmeasure(iv::InvariantMeasure) → (ρ::Probabilities, bins::Vector{<:SVector})\n\nFrom a pre-computed invariant measure, return the probabilities and associated bins. The element ρ[i] is the probability of visitation to the box bins[i]. Analogous to binhist.\n\nhint: Transfer operator approach vs. naive histogram approach\nWhy bother with the transfer operator instead of using regular histograms to obtain probabilities?In fact, the naive histogram approach and the transfer operator approach are equivalent in the limit of long enough time series (as n to intfy), which is guaranteed by the ergodic theorem. There is a crucial difference, however:The naive histogram approach only gives the long-term probabilities that orbits visit a certain region of the state space. The transfer operator encodes that information too, but comes with the added benefit of knowing the transition probabilities between states (see transfermatrix).\n\nSee also: InvariantMeasure.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.transfermatrix","page":"Probabilities API","title":"ComplexityMeasures.transfermatrix","text":"transfermatrix(iv::InvariantMeasure) → (M::AbstractArray{<:Real, 2}, bins::Vector{<:SVector})\n\nReturn the transfer matrix/operator and corresponding bins. Here, bins[i] corresponds to the i-th row/column of the transfer matrix. Thus, the entry M[i, j] is the probability of jumping from the state defined by bins[i] to the state defined by bins[j].\n\nSee also: TransferOperator.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#Symbolic-permutations","page":"Probabilities API","title":"Symbolic permutations","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"SymbolicPermutation","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.SymbolicPermutation","page":"Probabilities API","title":"ComplexityMeasures.SymbolicPermutation","text":"SymbolicPermutation <: ProbabilitiesEstimator\nSymbolicPermutation(; m = 3, τ = 1, lt::Function = ComplexityMeasures.isless_rand)\n\nA probabilities estimator based on ordinal permutation patterns.\n\nWhen passed to probabilities the output depends on the input data type:\n\nUnivariate data. If applied to a univariate timeseries (AbstractVector), then the timeseries   is first embedded using embedding delay τ and dimension m, resulting in embedding   vectors  bfx_i _i=1^N-(m-1)tau. Then, for each bfx_i,   we find its permutation pattern pi_i. Probabilities are then   estimated as the frequencies of the encoded permutation symbols   by using CountOccurrences. When giving the resulting probabilities to   entropy, the original permutation entropy is computed [BandtPompe2002].\nMultivariate data. If applied to a an D-dimensional StateSpaceSet,   then no embedding is constructed, m must be equal to D and τ is ignored.   Each vector bfx_i of the dataset is mapped   directly to its permutation pattern pi_i by comparing the   relative magnitudes of the elements of bfx_i.   Like above, probabilities are estimated as the frequencies of the permutation symbols.   The resulting probabilities can be used to compute multivariate permutation   entropy[He2016], although here we don't perform any further subdivision   of the permutation patterns (as in Figure 3 of[He2016]).\n\nInternally, SymbolicPermutation uses the OrdinalPatternEncoding to represent ordinal patterns as integers for efficient computations.\n\nSee SymbolicWeightedPermutation and SymbolicAmplitudeAwarePermutation for estimators that not only consider ordinal (sorting) patterns, but also incorporate information about within-state-vector amplitudes. For a version of this estimator that can be used on spatial data, see SpatialSymbolicPermutation.\n\nnote: Handling equal values in ordinal patterns\nIn Bandt & Pompe (2002), equal values are ordered after their order of appearance, but this can lead to erroneous temporal correlations, especially for data with low amplitude resolution [Zunino2017]. Here, by default, if two values are equal, then one of the is randomly assigned as \"the largest\", using lt = ComplexityMeasures.isless_rand. To get the behaviour from Bandt and Pompe (2002), use lt = Base.isless.\n\nOutcome space\n\nThe outcome space Ω for SymbolicPermutation is the set of length-m ordinal patterns (i.e. permutations) that can be formed by the integers 1, 2, …, m. There are factorial(m) such patterns.\n\nFor example, the outcome [2, 3, 1] corresponds to the ordinal pattern of having the smallest value in the second position, the next smallest value in the third position, and the next smallest, i.e. the largest value in the first position. See also [OrdinalPatternEncoding(@ref).\n\nIn-place symbolization\n\nSymbolicPermutation also implements the in-place probabilities! for StateSpaceSet input (or embedded vector input) for reducing allocations in looping scenarios. The length of the pre-allocated symbol vector must be the length of the dataset. For example\n\nusing ComplexityMeasures\nm, N = 2, 100\nest = SymbolicPermutation(; m, τ)\nx = StateSpaceSet(rand(N, m)) # some input dataset\nπs_ts = zeros(Int, N) # length must match length of `x`\np = probabilities!(πs_ts, est, x)\n\n[BandtPompe2002]: Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for timeseries.\" Physical review letters 88.17 (2002): 174102.\n\n[Zunino2017]: Zunino, L., Olivares, F., Scholkmann, F., & Rosso, O. A. (2017). Permutation entropy based timeseries analysis: Equalities in the input signal can lead to false conclusions. Physics Letters A, 381(22), 1883-1892.\n\n[He2016]: He, S., Sun, K., & Wang, H. (2016). Multivariate permutation entropy and its application for complexity analysis of chaotic systems. Physica A: Statistical Mechanics and its Applications, 461, 812-823.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Dispersion-patterns","page":"Probabilities API","title":"Dispersion patterns","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Dispersion","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.Dispersion","page":"Probabilities API","title":"ComplexityMeasures.Dispersion","text":"Dispersion(; c = 5, m = 2, τ = 1, check_unique = true)\n\nA probability estimator based on dispersion patterns, originally used by Rostaghi & Azami, 2016[Rostaghi2016] to compute the \"dispersion entropy\", which characterizes the complexity and irregularity of a time series.\n\nRecommended parameter values[Li2018] are m ∈ [2, 3], τ = 1 for the embedding, and c ∈ [3, 4, …, 8] categories for the Gaussian symbol mapping.\n\nDescription\n\nAssume we have a univariate time series X = x_i_i=1^N. First, this time series is encoded into a symbol timeseries S using the Gaussian encoding GaussianCDFEncoding with empirical mean μ and empirical standard deviation σ (both determined from X), and c as given to Dispersion.\n\nThen, S is embedded into an m-dimensional time series, using an embedding lag of tau, which yields a total of N - (m - 1)tau delay vectors z_i, or \"dispersion patterns\". Since each element of z_i can take on c different values, and each delay vector has m entries, there are c^m possible dispersion patterns. This number is used for normalization when computing dispersion entropy.\n\nThe returned probabilities are simply the frequencies of the unique dispersion patterns present in S (i.e., the CountOccurences of S).\n\nOutcome space\n\nThe outcome space for Dispersion is the unique delay vectors whose elements are the the symbols (integers) encoded by the Gaussian CDF, i.e., the unique elements of S.\n\nData requirements and parameters\n\nThe input must have more than one unique element for the Gaussian mapping to be well-defined. Li et al. (2018) recommends that x has at least 1000 data points.\n\nIf check_unique == true (default), then it is checked that the input has more than one unique value. If check_unique == false and the input only has one unique element, then a InexactError is thrown when trying to compute probabilities.\n\nnote: Why 'dispersion patterns'?\nEach embedding vector is called a \"dispersion pattern\". Why? Let's consider the case when m = 5 and c = 3, and use some very imprecise terminology for illustration:When c = 3, values clustering far below mean are in one group, values clustered around the mean are in one group, and values clustering far above the mean are in a third group. Then the embedding vector 2 2 2 2 2 consists of values that are close together (close to the mean), so it represents a set of numbers that are not very spread out (less dispersed). The embedding vector 1 1 2 3 3, however, represents numbers that are much more spread out (more dispersed), because the categories representing \"outliers\" both above and below the mean are represented, not only values close to the mean.\n\nFor a version of this estimator that can be used on high-dimensional arrays, see SpatialDispersion.\n\n[Rostaghi2016]: Rostaghi, M., & Azami, H. (2016). Dispersion entropy: A measure for time-series analysis. IEEE Signal Processing Letters, 23(5), 610-614.\n\n[Li2018]: Li, G., Guan, Q., & Yang, H. (2018). Noise reduction method of underwater acoustic signals based on CEEMDAN, effort-to-compress complexity, refined composite multiscale dispersion entropy and wavelet threshold denoising. EntropyDefinition, 21(1), 11.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Kernel-density","page":"Probabilities API","title":"Kernel density","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"NaiveKernel","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.NaiveKernel","page":"Probabilities API","title":"ComplexityMeasures.NaiveKernel","text":"NaiveKernel(ϵ::Real; method = KDTree, w = 0, metric = Euclidean()) <: ProbabilitiesEstimator\n\nEstimate probabilities/entropy using a \"naive\" kernel density estimation approach (KDE), as discussed in Prichard and Theiler (1995) [PrichardTheiler1995].\n\nProbabilities P(mathbfx epsilon) are assigned to every point mathbfx by counting how many other points occupy the space spanned by a hypersphere of radius ϵ around mathbfx, according to:\n\nP_i( X epsilon) approx dfrac1N sum_s B(X_i - X_j  epsilon)\n\nwhere B gives 1 if the argument is true. Probabilities are then normalized.\n\nKeyword arguments\n\nmethod = KDTree: the search structure supported by Neighborhood.jl. Specifically, use KDTree to use a tree-based neighbor search, or BruteForce for the direct distances between all points. KDTrees heavily outperform direct distances when the dimensionality of the data is much smaller than the data length.\nw = 0: the Theiler window, which excludes indices s that are within i - s  w from the given point x_i.\nmetric = Euclidean(): the distance metric.\n\nOutcome space\n\nThe outcome space Ω for NaiveKernel are the indices of the input data, eachindex(x). Hence, input x is needed for a well-defined outcome_space. The reason to not return the data points themselves is because duplicate data points may not get assigned same probabilities (due to having different neighbors).\n\n[PrichardTheiler1995]: Prichard, D., & Theiler, J. (1995). Generalized redundancies for time series analysis. Physica D: Nonlinear Phenomena, 84(3-4), 476-493.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Timescales","page":"Probabilities API","title":"Timescales","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"WaveletOverlap\nPowerSpectrum","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.WaveletOverlap","page":"Probabilities API","title":"ComplexityMeasures.WaveletOverlap","text":"WaveletOverlap([wavelet]) <: ProbabilitiesEstimator\n\nApply the maximal overlap discrete wavelet transform (MODWT) to a signal, then compute probabilities as the (normalized) energies at different wavelet scales. These probabilities are used to compute the wavelet entropy, according to Rosso et al. (2001)[Rosso2001]. Input timeseries x is needed for a well-defined outcome space.\n\nBy default the wavelet Wavelets.WT.Daubechies{12}() is used. Otherwise, you may choose a wavelet from the Wavelets package (it must subtype OrthoWaveletClass).\n\nOutcome space\n\nThe outcome space for WaveletOverlap are the integers 1, 2, …, N enumerating the wavelet scales. To obtain a better understanding of what these mean, we prepared a notebook you can view online. As such, this estimator only works for timeseries input and input x is needed for a well-defined outcome_space.\n\n[Rosso2001]: Rosso et al. (2001). Wavelet entropy: a new tool for analysis of short duration brain electrical signals. Journal of neuroscience methods, 105(1), 65-75.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.PowerSpectrum","page":"Probabilities API","title":"ComplexityMeasures.PowerSpectrum","text":"PowerSpectrum() <: ProbabilitiesEstimator\n\nCalculate the power spectrum of a timeseries (amplitude square of its Fourier transform), and return the spectrum normalized to sum = 1 as probabilities. The Shannon entropy of these probabilities is typically referred in the literature as spectral entropy, e.g. [Llanos2016],[Tian2017].\n\nThe closer the spectrum is to flat, i.e., white noise, the higher the entropy. However, you can't compare entropies of timeseries with different length, because the binning in spectral space depends on the length of the input.\n\nOutcome space\n\nThe outcome space Ω for PowerSpectrum is the set of frequencies in Fourier space. They should be multiplied with the sampling rate of the signal, which is assumed to be 1. Input x is needed for a well-defined outcome_space.\n\n[Llanos2016]: Llanos et al., Power spectral entropy as an information-theoretic correlate of manner of articulation in American English, The Journal of the Acoustical Society of America 141, EL127 (2017)\n\n[Tian2017]: Tian et al, Spectral EntropyDefinition Can Predict Changes of Working Memory Performance Reduced by Short-Time Training in the Delayed-Match-to-Sample Task, Front. Hum. Neurosci.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Diversity","page":"Probabilities API","title":"Diversity","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Diversity","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.Diversity","page":"Probabilities API","title":"ComplexityMeasures.Diversity","text":"Diversity(; m::Int, τ::Int, nbins::Int)\n\nA ProbabilitiesEstimator based on the cosine similarity. It can be used with entropy to compute the diversity entropy of an input timeseries[Wang2020].\n\nThe implementation here allows for τ != 1, which was not considered in the original paper.\n\nDescription\n\nDiversity probabilities are computed as follows.\n\nFrom the input time series x, using embedding lag τ and embedding dimension m,  construct the embedding  Y = bf x_i  = (x_i x_i+tau x_i+2tau ldots x_i+mtau - 1_i = 1^N-mτ.\nCompute D = d(bf x_t bf x_t+1) _t=1^N-mτ-1,  where d(cdot cdot) is the cosine similarity between two m-dimensional  vectors in the embedding.\nDivide the interval [-1, 1] into nbins equally sized subintervals (including the value +1).\nConstruct a histogram of cosine similarities d in D over those subintervals.\nSum-normalize the histogram to obtain probabilities.\n\nOutcome space\n\nThe outcome space for Diversity is the bins of the [-1, 1] interval, and the return configuration is the same as in ValueHistogram (left bin edge).\n\n[Wang2020]: Wang, X., Si, S., & Li, Y. (2020). Multiscale diversity entropy: A novel dynamical measure for fault diagnosis of rotating machinery. IEEE Transactions on Industrial Informatics, 17(8), 5419-5429.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#Utilities","page":"Probabilities API","title":"Utilities","text":"","category":"section"},{"location":"api/api_probabilities/#Outcomes","page":"Probabilities API","title":"Outcomes","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"probabilities_and_outcomes\noutcomes\noutcome_space\ntotal_outcomes\nmissing_outcomes","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.probabilities_and_outcomes","page":"Probabilities API","title":"ComplexityMeasures.probabilities_and_outcomes","text":"probabilities_and_outcomes(est, x)\n\nReturn probs, outs, where probs = probabilities(x, est) and outs[i] is the outcome with probability probs[i]. The element type of outs depends on the estimator. outs is a subset of the outcome_space of est.\n\nSee also outcomes, total_outcomes.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.outcomes","page":"Probabilities API","title":"ComplexityMeasures.outcomes","text":"outcomes(est::ProbabilitiesEstimator, x)\n\nReturn all (unique) outcomes contained in x according to the given estimator. Equivalent with probabilities_and_outcomes(x, est)[2], but for some estimators it may be explicitly extended for better performance.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.outcome_space","page":"Probabilities API","title":"ComplexityMeasures.outcome_space","text":"outcome_space(est::ProbabilitiesEstimator, x) → Ω\n\nReturn a container containing all possible outcomes of est for input x.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to outcome_space(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.total_outcomes","page":"Probabilities API","title":"ComplexityMeasures.total_outcomes","text":"total_outcomes(est::ProbabilitiesEstimator, x)\n\nReturn the length (cardinality) of the outcome space Omega of est.\n\nFor some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to total_outcomes(est). In general it is recommended to use the 2-argument version irrespectively of estimator.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.missing_outcomes","page":"Probabilities API","title":"ComplexityMeasures.missing_outcomes","text":"missing_outcomes(est::ProbabilitiesEstimator, x) → n_missing::Int\n\nEstimate a probability distribution for x using the given estimator, then count the number of missing (i.e. zero-probability) outcomes.\n\nSee also: MissingDispersionPatterns.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#Encodings","page":"Probabilities API","title":"Encodings","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Some probability estimators first \"encode\" input data into an intermediate representation indexed by the positive integers. This intermediate representation is called an \"encoding\".","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"The encodings API is defined by:","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Encoding\nencode\ndecode","category":"page"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"Encoding\nencode\ndecode","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.Encoding","page":"Probabilities API","title":"ComplexityMeasures.Encoding","text":"Encoding\n\nThe supertype for all encoding schemes. Encodings always encode elements of input data into the positive integers. The encoding API is defined by the functions encode and decode. Some probability estimators utilize encodings internally.\n\nCurrent available encodings are:\n\nOrdinalPatternEncoding.\nGaussianCDFEncoding.\nRectangularBinEncoding.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.encode","page":"Probabilities API","title":"ComplexityMeasures.encode","text":"encode(c::Encoding, χ) -> i::Int\n\nEncode an element χ ∈ x of input data x (those given to probabilities) using encoding c.\n\nThe special value of -1 is reserved as a return value for inappropriate elements χ that cannot be encoded according to c.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#ComplexityMeasures.decode","page":"Probabilities API","title":"ComplexityMeasures.decode","text":"decode(c::Encoding, i::Int) -> ω\n\nDecode an encoded element i into the outcome ω ∈ Ω it corresponds to.\n\nΩ is the outcome_space of a probabilities estimator that uses encoding c.\n\n\n\n\n\n","category":"function"},{"location":"api/api_probabilities/#Available-encodings","page":"Probabilities API","title":"Available encodings","text":"","category":"section"},{"location":"api/api_probabilities/","page":"Probabilities API","title":"Probabilities API","text":"OrdinalPatternEncoding\nGaussianCDFEncoding\nRectangularBinEncoding","category":"page"},{"location":"api/api_probabilities/#ComplexityMeasures.OrdinalPatternEncoding","page":"Probabilities API","title":"ComplexityMeasures.OrdinalPatternEncoding","text":"OrdinalPatternEncoding <: Encoding\nOrdinalPatternEncoding(m::Int, lt = ComplexityMeasures.isless_rand)\n\nAn encoding scheme that encodes length-m vectors into their permutation/ordinal patterns and then into the integers based on the Lehmer code. It is used by SymbolicPermutation and similar estimators, see that for a description of the outcome space.\n\nThe ordinal/permutation pattern of a vector χ is simply sortperm(χ), which gives the indices that would sort χ in ascending order.\n\nDescription\n\nThe Lehmer code, as implemented here, is a bijection between the set of factorial(m) possible permutations for a length-m sequence, and the integers 1, 2, …, factorial(m). The encoding step uses algorithm 1 in Berger et al. (2019)[Berger2019], which is highly optimized. The decoding step is much slower due to missing optimizations (pull requests welcomed!).\n\nExample\n\njulia> using ComplexityMeasures\n\njulia> χ = [4.0, 1.0, 9.0];\n\njulia> c = OrdinalPatternEncoding(3);\n\njulia> i = encode(c, χ)\n3\n\njulia> decode(c, i)\n3-element SVector{3, Int64} with indices SOneTo(3):\n 2\n 1\n 3\n\nIf you want to encode something that is already a permutation pattern, then you can use the non-exported permutation_to_integer function.\n\n[Berger2019]: Berger et al. \"Teaching Ordinal Patterns to a Computer: Efficient Encoding Algorithms Based on the Lehmer Code.\" Entropy 21.10 (2019): 1023.\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.GaussianCDFEncoding","page":"Probabilities API","title":"ComplexityMeasures.GaussianCDFEncoding","text":"GaussianCDFEncoding <: Encoding\nGaussianCDFEncoding(; μ, σ, c::Int = 3)\n\nAn encoding scheme that encodes a scalar value into one of the integers sᵢ ∈ [1, 2, …, c] based on the normal cumulative distribution function (NCDF), and decodes the sᵢ into subintervals of [0, 1] (with some loss of information).\n\nNotice that the decoding step does not yield an element of any outcome space of the estimators that use GaussianCDFEncoding internally, such as Dispersion. That is because these estimators additionally delay embed the encoded data.\n\nDescription\n\nGaussianCDFEncoding first maps an input point x  (scalar) to a new real number y_ in 0 1 by using the normal cumulative distribution function (CDF) with the given mean μ and standard deviation σ, according to the map\n\nx to y  y = dfrac1 sigma\n    sqrt2 pi int_-infty^x e^(-(x - mu)^2)(2 sigma^2) dx\n\nNext, the interval [0, 1] is equidistantly binned and enumerated 1 2 ldots c,  and y is linearly mapped to one of these integers using the linear map  y to z  z = textfloor(y(c-1)) + 1.\n\nBecause of the floor operation, some information is lost, so when used with decode, each decoded sᵢ is mapped to a subinterval of [0, 1].\n\nExamples\n\njulia> using ComplexityMeasures, Statistics\n\njulia> x = [0.1, 0.4, 0.7, -2.1, 8.0];\n\njulia> μ, σ = mean(x), std(x); encoding = GaussianCDFEncoding(; μ, σ, c = 5)\n\njulia> es = encode.(Ref(encoding), x)\n5-element Vector{Int64}:\n 2\n 2\n 3\n 1\n 5\n\njulia> decode(encoding, 3)\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.4\n 0.6\n\n\n\n\n\n","category":"type"},{"location":"api/api_probabilities/#ComplexityMeasures.RectangularBinEncoding","page":"Probabilities API","title":"ComplexityMeasures.RectangularBinEncoding","text":"RectangularBinEncoding <: Encoding\nRectangularBinEncoding(binning::RectangularBinning, x)\nRectangularBinEncoding(binning::FixedRectangularBinning)\n\nAn encoding scheme that encodes points χ ∈ x into their histogram bins.\n\nThe first call signature simply initializes a FixedRectangularBinning and then calls the second call signature.\n\nSee FixedRectangularBinning for info on mapping points to bins.\n\n\n\n\n\n","category":"type"},{"location":"examples/examples_mi/#quickstart_mutualinfo","page":"Mutual information","title":"Mutual information","text":"","category":"section"},{"location":"examples/examples_mi/#[MIShannon](@ref)","page":"Mutual information","title":"MIShannon","text":"","category":"section"},{"location":"examples/examples_mi/#example_mi_MutualInformationEstimator","page":"Mutual information","title":"Estimation using MutualInformationEstimators","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"When estimated using a MutualInformationEstimator, some form of bias correction is usually applied. The KraskovStögbauerGrassberger1 and KraskovStögbauerGrassberger2 estimators are perhaps the most popular. A common parametric estimator is GaussianMI.","category":"page"},{"location":"examples/examples_mi/#[MIShannon](@ref)-with-[GaussianMI](@ref)","page":"Mutual information","title":"MIShannon with GaussianMI","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Distributions\nusing Statistics\n\nn = 1000\nusing CausalityTools\nx = randn(1000)\ny = rand(1000) .+ x\nmutualinfo(KSG1(k = 5), x, y)\nmutualinfo(GaussianMI(), x, y) # defaults to `MIShannon()`","category":"page"},{"location":"examples/examples_mi/#[MIShannon](@ref)-with-[KraskovStögbauerGrassberger1](@ref)","page":"Mutual information","title":"MIShannon with KraskovStögbauerGrassberger1","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(KSG1(k = 5), x, y)","category":"page"},{"location":"examples/examples_mi/#[MIShannon](@ref)-with-[KraskovStögbauerGrassberger2](@ref)","page":"Mutual information","title":"MIShannon with KraskovStögbauerGrassberger2","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(KSG2(k = 5), x, y)","category":"page"},{"location":"examples/examples_mi/#[MIShannon](@ref)-with-[GaoKannanOhViswanath](@ref)","page":"Mutual information","title":"MIShannon with GaoKannanOhViswanath","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(GaoKannanOhViswanath(k = 10), x, y)","category":"page"},{"location":"examples/examples_mi/#[MIShannon](@ref)-with-[GaoOhViswanath](@ref)","page":"Mutual information","title":"MIShannon with GaoOhViswanath","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(GaoOhViswanath(k = 10), x, y)","category":"page"},{"location":"examples/examples_mi/#Reproducing-Kraskov-et-al.-(2004)","page":"Mutual information","title":"Reproducing Kraskov et al. (2004)","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"Here, we'll reproduce Figure 4 from Kraskov et al. (2004)'s seminal paper on the nearest-neighbor based mutual information estimator. We'll estimate the mutual information between marginals of a bivariate Gaussian for a fixed time series length of 2000, varying the number of neighbors. Note: in the original paper, they show multiple curves corresponding to different time series length. We only show two single curves: one for the KSG1 estimator and one for the KSG2 estimator.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing LinearAlgebra: det\nusing Distributions: MvNormal\nusing StateSpaceSets: StateSpaceSet\nusing CairoMakie\nusing Statistics\n\nN = 2000\nc = 0.9\nΣ = [1 c; c 1]\nN2 = MvNormal([0, 0], Σ)\nmitrue = -0.5*log(det(Σ)) # in nats\nks = [2; 5; 7; 10:10:70] .* 2\n\nnreps = 30\nmis_ksg1 = zeros(nreps, length(ks))\nmis_ksg2 = zeros(nreps, length(ks))\nfor i = 1:nreps\n    D2 = StateSpaceSet([rand(N2) for i = 1:N])\n    X = D2[:, 1] |> StateSpaceSet\n    Y = D2[:, 2] |> StateSpaceSet\n    measure = MIShannon(; base = ℯ)\n    mis_ksg1[i, :] = map(k -> mutualinfo(measure, KSG1(; k), X, Y), ks)\n    mis_ksg2[i, :] = map(k -> mutualinfo(measure, KSG2(; k), X, Y), ks)\nend\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"k / N\", ylabel = \"Mutual infomation (nats)\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg1, dims = 1) |> vec, label = \"KSG1\")\nscatterlines!(ax, ks ./ N, mean(mis_ksg2, dims = 1) |> vec, label = \"KSG2\")\nhlines!(ax, [mitrue], color = :black, linewidth = 3, label = \"I (true)\")\naxislegend()\nfig","category":"page"},{"location":"examples/examples_mi/#[MutualInformationEstimator](@ref)-comparison","page":"Mutual information","title":"MutualInformationEstimator comparison","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"Most estimators suffer from significant bias when applied to discrete, finite data. One possible resolution is to add a small amount of noise to discrete variables, so that the data becomes continuous in practice.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"Instead of adding noise to your data, you can consider using an estimator that is specifically designed to deal with continuous-discrete mixture data. One example is the GaoKannanOhViswanath estimator.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"Here, we compare its performance to KSG1 on uniformly distributed discrete multivariate data. The true mutual information is zero.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Statistics\nusing StateSpaceSets: StateSpaceSet\nusing Statistics: mean\nusing CairoMakie\n\nfunction compare_ksg_gkov(;\n        k = 5,\n        base = 2,\n        nreps = 15,\n        Ls = [500:100:1000; 1500; 2000; 3000; 4000; 5000; 1000])\n\n    est_gkov = GaoKannanOhViswanath(; k)\n    est_ksg1 = KSG1(; k)\n\n    mis_ksg1_mix = zeros(nreps, length(Ls))\n    mis_ksg1_discrete = zeros(nreps, length(Ls))\n    mis_ksg1_cont = zeros(nreps, length(Ls))\n    mis_gkov_mix = zeros(nreps, length(Ls))\n    mis_gkov_discrete = zeros(nreps, length(Ls))\n    mis_gkov_cont = zeros(nreps, length(Ls))\n\n    for (j, L) in enumerate(Ls)\n        for i = 1:nreps\n            X = StateSpaceSet(float.(rand(1:8, L, 2)))\n            Y = StateSpaceSet(float.(rand(1:8, L, 2)))\n            Z = StateSpaceSet(rand(L, 2))\n            W = StateSpaceSet(rand(L, 2))\n            measure = MIShannon(; base = ℯ)\n            mis_ksg1_discrete[i, j] = mutualinfo(measure, est_ksg1, X, Y)\n            mis_gkov_discrete[i, j] = mutualinfo(measure, est_gkov, X, Y)\n            mis_ksg1_mix[i, j] = mutualinfo(measure, est_ksg1, X, Z)\n            mis_gkov_mix[i, j] = mutualinfo(measure, est_gkov, X, Z)\n            mis_ksg1_cont[i, j] = mutualinfo(measure, est_ksg1, Z, W)\n            mis_gkov_cont[i, j] = mutualinfo(measure, est_gkov, Z, W)\n        end\n    end\n    return mis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n        mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont\nend\n\nfig = Figure()\nax = Axis(fig[1, 1], \n    xlabel = \"Sample size\", \n    ylabel = \"Mutual information (bits)\")\nLs = [100; 200; 500; 1000; 2500; 5000; 10000]\nnreps = 5\nk = 3\nmis_ksg1_mix, mis_ksg1_discrete, mis_ksg1_cont,\n    mis_gkov_mix, mis_gkov_discrete, mis_gkov_cont = \n    compare_ksg_gkov(; nreps, k, Ls)\n\nscatterlines!(ax, Ls, mean(mis_ksg1_mix, dims = 1) |> vec, \n    label = \"KSG1 (mixed)\", color = :black, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_ksg1_discrete, dims = 1) |> vec, \n    label = \"KSG1 (discrete)\", color = :black, \n    linestyle = :dash, marker = '▲')\nscatterlines!(ax, Ls, mean(mis_ksg1_cont, dims = 1) |> vec, \n    label = \"KSG1 (continuous)\", color = :black, \n    linestyle = :dot, marker = '●')\nscatterlines!(ax, Ls, mean(mis_gkov_mix, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (mixed)\", color = :red, \n    marker = :utriangle)\nscatterlines!(ax, Ls, mean(mis_gkov_discrete, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (discrete)\", color = :red, \n    linestyle = :dash, marker = '▲')\nscatterlines!(ax, Ls, mean(mis_gkov_cont, dims = 1) |> vec, \n    label = \"GaoKannanOhViswanath (continuous)\", color = :red, \n    linestyle = :dot, marker = '●')\naxislegend(position = :rb)\nfig","category":"page"},{"location":"examples/examples_mi/#example_mi_DifferentialEntropyEstimator","page":"Mutual information","title":"Estimation using DifferentialEntropyEstimators","text":"","category":"section"},{"location":"examples/examples_mi/#Simple-example","page":"Mutual information","title":"Simple example","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"We can compute MIShannon by naively applying a DifferentialEntropyEstimator. Note that this doesn't apply any bias correction.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nx, y = rand(1000), rand(1000)\nmutualinfo(Kraskov(k = 3), x, y)","category":"page"},{"location":"examples/examples_mi/#[DifferentialEntropyEstimator](@ref)-comparison","page":"Mutual information","title":"DifferentialEntropyEstimator comparison","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"Let's compare the performance of a subset of the implemented mutual information estimators. We'll use example data from Lord et al., where the analytical mutual information is known.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing LinearAlgebra: det\nusing StateSpaceSets: StateSpaceSet\nusing Distributions: MvNormal\nusing LaTeXStrings\nusing CairoMakie\n\n# adapted from https://juliadatascience.io/makie_colors\nfunction new_cycle_theme()\n    # https://nanx.me/ggsci/reference/pal_locuszoom.html\n    my_colors = [\"#D43F3AFF\", \"#EEA236FF\", \"#5CB85CFF\", \"#46B8DAFF\",\n        \"#357EBDFF\", \"#9632B8FF\", \"#B8B8B8FF\"]\n    cycle = Cycle([:color, :linestyle, :marker], covary=true) # alltogether\n    my_markers = [:circle, :rect, :utriangle, :dtriangle, :diamond,\n        :pentagon, :cross, :xcross]\n    my_linestyle = [nothing, :dash, :dot, :dashdot, :dashdotdot]\n    return Theme(\n        fontsize = 22, font=\"CMU Serif\",\n        colormap = :linear_bmy_10_95_c78_n256,\n        palette = (\n            color = my_colors, \n            marker = my_markers, \n            linestyle = my_linestyle,\n        ),\n        Axis = (\n            backgroundcolor= (:white, 0.2), \n            xgridstyle = :dash, \n            ygridstyle = :dash\n        ),\n        Lines = (\n            cycle= cycle,\n        ), \n        ScatterLines = (\n            cycle = cycle,\n        ),\n        Scatter = (\n            cycle = cycle,\n        ),\n        Legend = (\n            bgcolor = (:grey, 0.05), \n            framecolor = (:white, 0.2),\n            labelsize = 13,\n        )\n    )\nend\n\nrun(est; f::Function, # function that generates data\n        base::Real = ℯ, \n        nreps::Int = 10, \n        αs = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], \n        n::Int = 1000) =\n    map(α -> mutualinfo(MIShannon(; base), est, f(α, n)...), αs)\n\nfunction compute_results(f::Function; estimators, k = 5, k_lord = 20,\n        n = 1000, base = ℯ, nreps = 10,\n        as = 7:-1:0,\n        αs = [1/10^(a) for a in as])\n    \n    is = [zeros(length(αs)) for est in estimators]\n    for (k, est) in enumerate(estimators)\n        tmp = zeros(length(αs))\n        for i = 1:nreps\n            tmp .+= run(est; f = f, αs, base, n)\n        end\n        is[k] .= tmp ./ nreps\n    end\n\n    return is\nend\n\nfunction plot_results(f::Function, ftrue::Function; \n        base, estimators, k_lord, k, \n        as = 7:-1:0, αs = [1/10^(a) for a in as], kwargs...\n    )\n    is = compute_results(f; \n        base, estimators, k_lord, k, as, αs, kwargs...)\n    itrue = [ftrue(α; base) for α in αs]\n\n    xmin, xmax = minimum(αs), maximum(αs)\n    \n    ymin = floor(Int, min(minimum(itrue), minimum(Iterators.flatten(is))))\n    ymax = ceil(Int, max(maximum(itrue), maximum(Iterators.flatten(is))))\n    f = Figure()\n    ax = Axis(f[1, 1],\n        xlabel = \"α\", ylabel = \"I (nats)\",\n        xscale = log10, aspect = 1,\n        xticks = (αs, [latexstring(\"10^{$(-a)}\") for a in as]),\n        yticks = (ymin:ymax)\n        )\n    xlims!(ax, (1/10^first(as), 1/10^last(as)))\n    ylims!(ax, (ymin, ymax))\n    lines!(ax, αs, itrue, \n        label = \"I (true)\", linewidth = 4, color = :black)\n    for (i, est) in enumerate(estimators)\n        es = string(typeof(est).name.name)\n        lbl = occursin(\"Lord\", es) ? \"$es (k = $k_lord)\" : \"$es (k = $k)\"\n        scatter!(ax, αs, is[i], label = lbl)\n        lines!(ax, αs, is[i])\n\n    end\n    axislegend()\n    return f\nend\n\nset_theme!(new_cycle_theme())\nk_lord = 20\nk = 5\nbase = ℯ\n\nestimators = [\n    Kraskov(; k), \n    KozachenkoLeonenko(),\n    Zhu(; k), \n    ZhuSingh(; k),\n    Gao(; k),\n    Lord(; k = k_lord),\n    KSG1(; k), \n    KSG2(; k),\n    GaoOhViswanath(; k),\n    GaoKannanOhViswanath(; k),\n    GaussianMI(),\n]","category":"page"},{"location":"examples/examples_mi/#Family-1","page":"Mutual information","title":"Family 1","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"In this system, samples are concentrated around the diagonal X = Y, and the strip of samples gets thinner as alpha to 0.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"function family1(α, n::Int)\n    x = rand(n)\n    v = rand(n)\n    y = x + α * v\n    return StateSpaceSet(x), StateSpaceSet(y)\nend\n\n# True mutual information values for these data\nfunction ifamily1(α; base = ℯ)\n    mi = -log(α) - α - log(2)\n    return mi / log(base, ℯ)\nend\n\nfig = plot_results(family1, ifamily1; \n    k_lord = k_lord, k = k, nreps = 10,\n    estimators = estimators,\n    base = base)","category":"page"},{"location":"examples/examples_mi/#Family-2","page":"Mutual information","title":"Family 2","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"function family2(α, n::Int)\n    Σ = [1 α; α 1]\n    N2 = MvNormal(zeros(2), Σ)\n    D2 = StateSpaceSet([rand(N2) for i = 1:n])\n    X = StateSpaceSet(D2[:, 1])\n    Y = StateSpaceSet(D2[:, 2])\n    return X, Y\nend\n\nfunction ifamily2(α; base = ℯ)\n    return (-0.5 * log(1 - α^2)) / log(ℯ, base)\nend\n\nαs = 0.05:0.05:0.95\nestimators = estimators\nwith_theme(new_cycle_theme()) do\n    f = Figure();\n    ax = Axis(f[1, 1], xlabel = \"α\", ylabel = \"I (nats)\")\n    is_true = map(α -> ifamily2(α), αs)\n    is_est = map(est -> run(est; f = family2, αs, nreps = 20), estimators)\n    lines!(ax, αs, is_true, \n        label = \"I (true)\", color = :black, linewidth = 3)\n    for (i, est) in enumerate(estimators)\n        estname = typeof(est).name.name |> String\n        scatterlines!(ax, αs, is_est[i], label = estname)\n    end\n    axislegend(position = :lt)\n    return f\nend","category":"page"},{"location":"examples/examples_mi/#Family-3","page":"Mutual information","title":"Family 3","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"In this system, we draw samples from a 4D Gaussian distribution distributed as specified in the ifamily3 function below. We let X be the two first variables, and Y be the two last variables.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"function ifamily3(α; base = ℯ)\n    Σ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+α]\n    Σx = Σ[1:2, 1:2]; Σy = Σ[3:4, 3:4]\n    mi = 0.5*log(det(Σx) * det(Σy) / det(Σ))\n    return mi / log(ℯ, base)\nend\n\nfunction family3(α, n::Int)\n    Σ = [7 -5 -1 -3; -5 5 -1 3; -1 -1 3 -1; -3 3 -1 2+α]\n    N4 = MvNormal(zeros(4), Σ)\n    D4 = StateSpaceSet([rand(N4) for i = 1:n])\n    X = D4[:, 1:2]\n    Y = D4[:, 3:4]\n    return X, Y\nend\n\nfig = plot_results(family3, ifamily3; \n    k_lord = k_lord, k = k, nreps = 10,\n    n = 2000,\n    estimators = estimators, base = base)","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"We see that the Lord estimator, which estimates local volume elements using a singular-value decomposition (SVD) of local neighborhoods, outperforms the other estimators by a large margin.","category":"page"},{"location":"examples/examples_mi/#example_mi_ProbabilitiesEstimator","page":"Mutual information","title":"Estimation using ProbabilitiesEstimators","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"We can also use ProbabilitiesEstimator to estimate Shannon mutual information. This does not apply any bias correction.","category":"page"},{"location":"examples/examples_mi/#Discrete-[MIShannon](@ref)-with-[ValueHistogram](@ref)","page":"Mutual information","title":"Discrete MIShannon with ValueHistogram","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"A ValueHistogram estimator can be used to bin the data and compute discrete Shannon mutual information.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 1000)\ny = rand(rng, 1000)\n\n# Use the H3-estimation method with a discrete visitation frequency based \n# probabilities estimator over a fixed grid covering the range of the data,\n# which is on [0, 1].\nest = ValueHistogram(FixedRectangularBinning(0, 1, 5))\nmutualinfo(est, x, y)","category":"page"},{"location":"examples/examples_mi/#Discrete-[MIShannon](@ref)-with-[Contingency](@ref)-(numerical)","page":"Mutual information","title":"Discrete MIShannon with Contingency (numerical)","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"The above example is in fact equivalent to Contingency. However, using the  Contingency estimator is more flexible, because it can also be used on categorical data.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 1000)\ny = rand(rng, 1000)\nest = ValueHistogram(FixedRectangularBinning(0, 1, 5))\nmutualinfo(Contingency(est), x, y)","category":"page"},{"location":"examples/examples_mi/#Discrete-[MIShannon](@ref)-with-[ContingencyMatrix](@ref)-(manual)","page":"Mutual information","title":"Discrete MIShannon with ContingencyMatrix (manual)","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"If you need explicit access to the estimated joint probability mass function, use a ContingencyMatrix directly.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Random; rng = MersenneTwister(1234)\nx = rand(rng, 1000)\ny = rand(rng, 1000)\nc = contingency_matrix(est, x, y)\nest = ValueHistogram(FixedRectangularBinning(0, 1, 5))\nmutualinfo(c)","category":"page"},{"location":"examples/examples_mi/#discrete_mishannon_categorical","page":"Mutual information","title":"Discrete MIShannon with Contingency (categorical)","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"The ContingencyMatrix approach can also be used with categorical data. For example, let's compare the Shannon mutual information between the preferences of a population sample with regards to different foods.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nn = 1000\npreferences = rand([\"neutral\", \"like it\", \"hate it\"], n);\nrandom_foods = rand([\"water\", \"flour\", \"bananas\", \"booze\", \"potatoes\", \"beans\", \"soup\"], n)\nbiased_foods = map(preferences) do preference\n    if cmp(preference, \"neutral\") == 1\n        return rand([\"water\", \"flour\"])\n    elseif cmp(preference, \"like it\") == 1\n        return rand([\"bananas\", \"booze\"])\n    else\n        return rand([\"potatoes\", \"beans\", \"soup\"])\n    end\nend\n\nc_biased = contingency_matrix(preferences, biased_foods) \nc_random = contingency_matrix(preferences, random_foods) \nmutualinfo(c_biased), mutualinfo(c_random)","category":"page"},{"location":"examples/examples_mi/#Longer-example:-AR1-system-and-unidirectionally-coupled-logistic-maps","page":"Mutual information","title":"Longer example: AR1-system and unidirectionally coupled logistic maps","text":"","category":"section"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"In this example we generate realizations of two different systems where we know the strength of coupling between the variables. Our aim is to compute Shannon mutual information I^S(X Y) (MIShannon) between time series of each variable and assess how the magnitude of I^S(X Y) changes as we change the strength of coupling between X and Y. We'll use two systems that ship with CausalityTools.jl:","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"A stochastic system consisting of two unidirectionally coupled first-order autoregressive processes (ar1_unidir)\nA deterministic, chaotic system consisting of two unidirectionally coupled logistic maps (logistic2_unidir)","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"We use the default input parameter values (see AR1Unidir and Logistic2Unidir for details) and below we toggle only the random initial conditions and the coupling strength parameter c_xy. For each value of c_xy we generate 1,000 unique realizations of the system and obtain 500-point time series of the coupled variables.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"To estimate the mutual information, we use the binning-based ValueHistogram estimator. We summarize the distribution of I(X Y) values across all realizations using the median and quantiles encompassing 95 % of the values.","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"using CausalityTools\nusing Statistics\nusing CairoMakie\n\n# Span a range of x-y coupling strengths\nc = 0.0:0.1:1.0\n\n# Number of observations in each time series\nnpts = 500\n\n# Number of unique realizations of each system\nn_realizations = 1000\n\n# Get MI for multiple realizations of two systems, \n# saving three quantiles for each c value\nmi = zeros(length(c), 3, 2)\n\n# Define an estimator for MI\nb = RectangularBinning(4)\nestimator = ValueHistogram(b)\n\nfor i in 1 : length(c)\n    \n    tmp = zeros(n_realizations, 2)\n    \n    for k in 1 : n_realizations\n        \n        # Obtain time series realizations of the two 2D systems \n        # for a given coupling strength and random initial conditions\n        s_logistic = system(Logistic2Unidir(; xi = rand(2), c_xy = c[i]))\n        s_ar = system(AR1Unidir(xi = rand(2), c_xy = c[i]))\n        lmap = trajectory(s_logistic, npts - 1, Ttr = 500)\n        ar1 = trajectory(s_ar, npts - 1)\n        \n        # Compute the MI between the two coupled components of each system\n        tmp[k, 1] = mutualinfo(MIShannon(), estimator, lmap[:, 1], lmap[:, 2])\n        tmp[k, 2] = mutualinfo(MIShannon(), estimator, ar1[:, 1], ar1[:, 2])\n    end\n    \n    # Compute lower, middle, and upper quantiles of MI for each coupling strength\n    mi[i, :, 1] = quantile(tmp[:, 1], [0.025, 0.5, 0.975])\n    mi[i, :, 2] = quantile(tmp[:, 2], [0.025, 0.5, 0.975])\nend\n\n# Plot distribution of MI values as a function of coupling strength for both systems\nfig = with_theme(theme_minimal()) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"Coupling strength\", ylabel = \"Mutual information\")\n    band!(ax, c, mi[:, 1, 1], mi[:, 3, 1], color = (:black, 0.3))\n    lines!(ax, c, mi[:, 2, 1], label = \"2D chaotic logistic maps\", color = :black)\n    band!(ax, c, mi[:, 1, 2], mi[:, 3, 2], color = (:red, 0.3))\n    lines!(ax, c, mi[:, 2, 2],  label = \"2D order-1 autoregressive\", color = :red)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_mi/","page":"Mutual information","title":"Mutual information","text":"As expected, I(X Y) increases with coupling strength in a system-specific manner.","category":"page"},{"location":"api/api_crossmap/#cross_mapping_api","page":"Cross mapping API","title":"Cross mapping API","text":"","category":"section"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"Several cross mapping methods have emerged in the literature Following Sugihara et al. (2012)'s paper on the convergent cross mapping. In CausalityTools.jl, we provide a unified interface for using these cross mapping methods. We indicate the different types of cross mappings by passing an CrossmapMeasure instance as the first argument to crossmap or predict.","category":"page"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"The cross mapping API consists of the following functions.","category":"page"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"predict\ncrossmap","category":"page"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"These functions can dispatch on a CrossmapMeasure, and we currently implement","category":"page"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"ConvergentCrossMapping.\nPairwiseAsymmetricEmbedding.","category":"page"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"crossmap\npredict","category":"page"},{"location":"api/api_crossmap/#CausalityTools.crossmap","page":"Cross mapping API","title":"CausalityTools.crossmap","text":"crossmap(measure::CrossmapMeasure, t::AbstractVector, s::AbstractVector) → ρ::Real\ncrossmap(measure::CrossmapMeasure, est, t::AbstractVector, s::AbstractVector) → ρ::Vector\ncrossmap(measure::CrossmapMeasure, t̄::AbstractVector, S̄::AbstractStateSpaceSet) → ρ\n\nCompute the cross map estimates between between raw time series t and s (and return the real-valued cross-map statistic ρ). If a CrossmapEstimator est is provided, cross mapping is done on random subsamples of the data, where subsampling is dictated by est (a vector of values for ρ is returned).\n\nAlternatively, cross-map between time-aligned time series t̄ and source embedding S̄ that have been constructed by jointly (pre-embedding) some input data.\n\nThis is just a wrapper around predict that simply returns the correspondence measure between the source and the target.\n\n\n\n\n\n","category":"function"},{"location":"api/api_crossmap/#CausalityTools.predict","page":"Cross mapping API","title":"CausalityTools.predict","text":"predict(measure::CrossmapMeasure, t::AbstractVector, s::AbstractVector) → t̂ₛ, t̄, ρ\npredict(measure::CrossmapMeasure, t̄::AbstractVector, S̄::AbstractStateSpaceSet) → t̂ₛ\n\nPerform point-wise cross mappings between source embeddings and target time series according to the algorithm specified by the given cross-map measure (e.g. ConvergentCrossMapping or PairwiseAsymmetricInference).\n\nFirst method: Jointly embeds the target t and source s time series (according to   measure) to obtain time-index aligned target timeseries t̄ and source embedding   S̄ (which is now a StateSpaceSet).   Then calls predict(measure, t̄, S̄) (the first method), and returns both the   predictions t̂ₛ, observations t̄ and their correspondence ρ according to measure.\nSecond method: Returns a vector of predictions t̂ₛ (t̂ₛ := \"predictions of t̄ based   on source embedding S̄\"), where t̂ₛ[i] is the prediction for t̄[i]. It assumes   pre-embedded data which have been correctly time-aligned using a joint embedding   (see embed), i.e. such that t̄[i] and S̄[i] correspond to the same time   index.\n\nDescription\n\nFor each i ∈ {1, 2, …, N} where N = length(t) == length(s), we make the prediction t̂[i] (an estimate of t[i]) based on a linear combination of D + 1 other points in t, where the selection of points and weights for the linear combination are determined by the D+1 nearest neighbors of the point S̄[i]. The details of point selection and weights depend on measure.\n\nNote: Some CrossmapMeasures may define more general mapping procedures. If so, the algorithm is described in their docstring.\n\n\n\n\n\n","category":"function"},{"location":"api/api_crossmap/#Measures","page":"Cross mapping API","title":"Measures","text":"","category":"section"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"CrossmapMeasure","category":"page"},{"location":"api/api_crossmap/#CausalityTools.CrossmapMeasure","page":"Cross mapping API","title":"CausalityTools.CrossmapMeasure","text":"CrossmapMeasure <: AssociationMeasure\n\nThe supertype for all cross-map measures. Concrete subtypes are\n\nConvergentCrossMapping, or CCM for short.\nPairwiseAsymmetricInference, or PAI for short.\n\n\n\n\n\n","category":"type"},{"location":"api/api_crossmap/#Estimators","page":"Cross mapping API","title":"Estimators","text":"","category":"section"},{"location":"api/api_crossmap/","page":"Cross mapping API","title":"Cross mapping API","text":"CrossmapEstimator\nRandomVectors\nRandomSegment\nExpandingSegment","category":"page"},{"location":"api/api_crossmap/#CausalityTools.CrossmapEstimator","page":"Cross mapping API","title":"CausalityTools.CrossmapEstimator","text":"CrossmapEstimator{LIBSIZES, RNG}\n\nA parametric supertype for all cross-map estimators, which are used with predict and crossmap.\n\nBecause the type of the library may differ between estimators, and because RNGs from different packages may be used, subtypes must implement the LIBSIZES and RNG type parameters.\n\nFor efficiency purposes, subtypes may contain mutable containers that can be re-used for ensemble analysis (see Ensemble).\n\nLibraries\n\nA cross-map estimator uses the concept of \"libraries\". A library is essentially just a reference to a set of points, and usually, a library refers to indices of points, not the actual points themselves.\n\nFor example, for timeseries, RandomVectors(libsizes = 50:25:100) produces three separate libraries, where the first contains 50 randomly selected time indices, the second contains 75 randomly selected time indices, and the third contains 100 randomly selected time indices. This of course assumes that all quantities involved can be indexed using the same time indices, meaning that the concept of \"library\" only makes sense after relevant quantities have been jointly embedded, so that they can be jointly indexed. For non-instantaneous prediction, the maximum possible library size shrinks with the magnitude of the index/time-offset for the prediction.\n\nFor spatial analyses (not yet implemented), indices could be more complex and involve multi-indices.\n\n\n\n\n\n","category":"type"},{"location":"api/api_crossmap/#CausalityTools.RandomVectors","page":"Cross mapping API","title":"CausalityTools.RandomVectors","text":"RandomVectors <: CrossmapEstimator\nRandomVectors(; libsizes, replace = false, rng = Random.default_rng())\n\nCross-map over N different libraries, where N = length(libsizes), and the i-th library has cardinality k = libsizes[i]. Points within each library are randomly selected, independently of other libraries, and replace controls whether or not to sample with replacement. A user-specified rng may be specified for reproducibility.\n\nThis is method 3 from Luo et al. (2015)[Luo2015].\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou.\n\nSee also: CrossmapEstimator.\n\n\n\n\n\n","category":"type"},{"location":"api/api_crossmap/#CausalityTools.RandomSegment","page":"Cross mapping API","title":"CausalityTools.RandomSegment","text":"RandomSegment <: CrossmapEstimator\nRandomSegment(; libsizes::Int, rng = Random.default_rng())\n\nIndicatates that cross mapping is performed on contiguous time series segments/windows of length L with a randomly selected starting point.\n\nThis is method 2 from Luo et al. (2015)[Luo2015].\n\n[Luo2015]: \"Questionable causality: Cosmic rays to temperature.\" Proceedings of the National Academy of Sciences Aug 2015, 112 (34) E4638-E4639; DOI: 10.1073/pnas.1510571112 Ming Luo, Holger Kantz, Ngar-Cheung Lau, Wenwen Huang, Yu Zhou.\n\n\n\n\n\n","category":"type"},{"location":"api/api_crossmap/#CausalityTools.ExpandingSegment","page":"Cross mapping API","title":"CausalityTools.ExpandingSegment","text":"ExpandingSegment <: CrossmapEstimator\nExpandingSegment(; libsizes::Int, rng = Random.default_rng())\n\nIndicatates that cross mapping is performed on a contiguous time series segment/window, starting from the first available data point up to the Lth data point.\n\nIf used in an ensemble setting, the estimator is applied to time indices Lmin:step:Lmax of the joint embedding.\n\n\n\n\n\n","category":"type"},{"location":"examples/examples_transferentropy/#examples_transferentropy","page":"Transfer entropy","title":"Transfer entropy","text":"","category":"section"},{"location":"examples/examples_transferentropy/#[TEShannon](@ref)","page":"Transfer entropy","title":"TEShannon","text":"","category":"section"},{"location":"examples/examples_transferentropy/#Estimation-using-[TransferEntropyEstimator](@ref)s","page":"Transfer entropy","title":"Estimation using TransferEntropyEstimators","text":"","category":"section"},{"location":"examples/examples_transferentropy/#Estimator-comparison","page":"Transfer entropy","title":"Estimator comparison","text":"","category":"section"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Let's reproduce Figure 4 from Zhu et al (2015)[Zhu2015], where they test some dedicated transfer entropy estimators on a bivariate autoregressive system. We will test","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The Lindner and Zhu1 dedicated transfer entropy estimators,   which try to eliminate bias.\nThe KSG1 estimator, which computes TE naively as a sum of mutual information   terms (without guaranteed cancellation of biases for the total sum).\nThe Kraskov estimator, which computes TE naively as a sum of entropy    terms (without guaranteed cancellation of biases for the total sum).","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"using CausalityTools\nusing CairoMakie\nusing Statistics\nusing Distributions: Normal\n\nfunction model2(n::Int)\n    𝒩x = Normal(0, 0.1)\n    𝒩y = Normal(0, 0.1)\n    x = zeros(n+2)\n    y = zeros(n+2)\n    x[1] = rand(𝒩x)\n    x[2] = rand(𝒩x)\n    y[1] = rand(𝒩y)\n    y[2] = rand(𝒩y)\n\n    for i = 3:n+2\n        x[i] = 0.45*sqrt(2)*x[i-1] - 0.9*x[i-2] - 0.6*y[i-2] + rand(𝒩x)\n        y[i] = 0.6*x[i-2] - 0.175*sqrt(2)*y[i-1] + 0.55*sqrt(2)*y[i-2] + rand(𝒩y)\n    end\n    return x[3:end], y[3:end]\nend\nte_true = 0.42 # eyeball the theoretical value from their Figure 4.\n\nm = TEShannon(embedding = EmbeddingTE(dT = 2, dS = 2), base = ℯ)\nestimators = [Zhu1(k = 8), Lindner(k = 8), KSG1(k = 8), Kraskov(k = 8)]\nLs = [floor(Int, 2^i) for i in 8.0:0.5:11]\nnreps = 8\ntes_xy = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\ntes_yx = [[zeros(nreps) for i = 1:length(Ls)] for e in estimators]\nfor (k, est) in enumerate(estimators)\n    for (i, L) in enumerate(Ls)\n        for j = 1:nreps\n            x, y = model2(L);\n            tes_xy[k][i][j] = transferentropy(m, est, x, y)\n            tes_yx[k][i][j] = transferentropy(m, est, y, x)\n        end\n    end\nend\n\nymin = minimum(map(x -> minimum(Iterators.flatten(Iterators.flatten(x))), (tes_xy, tes_yx)))\nestimator_names = [\"Zhu1\", \"Lindner\", \"KSG1\", \"Kraskov\"]\nls = [:dash, :dot, :dash, :dot]\nmr = [:rect, :hexagon, :xcross, :pentagon]\n\nfig = Figure(resolution = (800, 350))\nax_xy = Axis(fig[1,1], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"x → y\")\nax_yx = Axis(fig[1,2], xlabel = \"Signal length\", ylabel = \"TE (nats)\", title = \"y → x\")\nfor (k, e) in enumerate(estimators)\n    label = estimator_names[k]\n    marker = mr[k]\n    scatterlines!(ax_xy, Ls, mean.(tes_xy[k]); label, marker)\n    scatterlines!(ax_yx, Ls, mean.(tes_yx[k]); label, marker)\n    hlines!(ax_xy, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black) \n    hlines!(ax_yx, [te_true]; xmin = 0.0, xmax = 1.0, linestyle = :dash, color = :black)\n    linkaxes!(ax_xy, ax_yx)\nend\naxislegend(ax_xy, position = :rb)\n\nfig","category":"page"},{"location":"examples/examples_transferentropy/#Reproducing-Schreiber-(2000)","page":"Transfer entropy","title":"Reproducing Schreiber (2000)","text":"","category":"section"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Let's try to reproduce the results from Schreiber's original paper[Schreiber2000] where he introduced the transfer entropy. We'll use the ValueHistogram estimator, which is visitation frequency based and computes entropies by counting visits of the system's orbit to discrete portions of its reconstructed state space.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"using CausalityTools\nusing DynamicalSystemsBase\nusing CairoMakie\nusing Statistics\nusing Random; Random.seed!(12234);\n\nfunction ulam_system(dx, x, p, t)\n    f(x) = 2 - x^2\n    ε = p[1]\n    dx[1] = f(ε*x[length(dx)] + (1-ε)*x[1])\n    for i in 2:length(dx)\n        dx[i] = f(ε*x[i-1] + (1-ε)*x[i])\n    end\nend\n\nds = DiscreteDynamicalSystem(ulam_system, rand(100) .- 0.5, [0.04])\ntrajectory(ds, 1000; Ttr = 1000);\n\nεs = 0.02:0.02:1.0\nbase = 2\nte_x1x2 = zeros(length(εs)); te_x2x1 = zeros(length(εs))\n# Guess an appropriate bin width of 0.2 for the histogram\nest = ValueHistogram(0.2)\n\nfor (i, ε) in enumerate(εs)\n    set_parameter!(ds, 1, ε)\n    tr = trajectory(ds, 2000; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = transferentropy(TEShannon(; base), est, X1, X2)\n    te_x2x1[i] = transferentropy(TEShannon(; base), est, X2, X1)\nend\n\nfig = with_theme(theme_minimal(), markersize = 2) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\n    scatterlines!(ax, εs, te_x1x2, label = \"X1 to X2\", color = :black, lw = 1.5)\n    scatterlines!(ax, εs, te_x2x1, label = \"X2 to X1\", color = :red, lw = 1.5)\n    axislegend(ax, position = :lt)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"As expected, transfer entropy from X1 to X2 is higher than from X2 to X1 across parameter values for ε. But, by our definition of the ulam system, dynamical coupling only occurs from X1 to X2. The results, however, show nonzero transfer entropy in both directions. What does this mean?","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"Computing transfer entropy from finite time series introduces bias, and so does any particular choice of entropy estimator used to calculate it. To determine whether a transfer entropy estimate should be trusted, we can employ surrogate testing. We'll generate surrogate using TimeseriesSurrogates.jl. One possible way to do so is to use a SurrogateTest with independence, but here we'll do the surrogate resampling manually, so we can plot and inspect the results.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"In the example below, we continue with the same time series generated above. However, at each value of ε, we also compute transfer entropy for nsurr = 50 different randomly shuffled (permuted) versions of the source process. If the original transfer entropy exceeds that of some percentile the transfer entropy estimates of the surrogate ensemble, we will take that as \"significant\" transfer entropy.","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"nsurr = 25 # in real applications, you should use more surrogates\nbase = 2\nte_x1x2 = zeros(length(εs)); te_x2x1 = zeros(length(εs))\nte_x1x2_surr = zeros(length(εs), nsurr); te_x2x1_surr = zeros(length(εs), nsurr)\nest = ValueHistogram(0.2) # use same bin-width as before\n\nfor (i, ε) in enumerate(εs)\n    set_parameter!(ds, 1, ε)\n    tr = trajectory(ds, 500; Ttr = 5000)\n    X1 = tr[:, 1]; X2 = tr[:, 2]\n    @assert !any(isnan, X1)\n    @assert !any(isnan, X2)\n    te_x1x2[i] = transferentropy(TEShannon(; base), est, X1, X2)\n    te_x2x1[i] = transferentropy(TEShannon(; base), est, X2, X1)\n    s1 = surrogenerator(X1, RandomShuffle()); s2 = surrogenerator(X2, RandomShuffle())\n\n    for j = 1:nsurr\n        te_x1x2_surr[i, j] =  transferentropy(TEShannon(; base), est, s1(), X2)\n        te_x2x1_surr[i, j] =  transferentropy(TEShannon(; base), est, s2(), X1)\n    end\nend\n\n# Compute 95th percentiles of the surrogates for each ε\nqs_x1x2 = [quantile(te_x1x2_surr[i, :], 0.95) for i = 1:length(εs)]\nqs_x2x1 = [quantile(te_x2x1_surr[i, :], 0.95) for i = 1:length(εs)]\n\nfig = with_theme(theme_minimal(), markersize = 2) do\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel = \"epsilon\", ylabel = \"Transfer entropy (bits)\")\n    scatterlines!(ax, εs, te_x1x2, label = \"X1 to X2\", color = :black, lw = 1.5)\n    scatterlines!(ax, εs, qs_x1x2, color = :black, linestyle = :dot, lw = 1.5)\n    scatterlines!(ax, εs, te_x2x1, label = \"X2 to X1\", color = :red)\n    scatterlines!(ax, εs, qs_x2x1, color = :red, linestyle = :dot)\n    axislegend(ax, position = :lt)\n    return fig\nend\nfig","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"The plot above shows the original transfer entropies (solid lines) and the 95th percentile transfer entropies of the surrogate ensembles (dotted lines). As expected, using the surrogate test, the transfer entropies from X1 to X2 are mostly significant (solid black line is above dashed black line). The transfer entropies from X2 to X1, on the other hand, are mostly not significant (red solid line is below red dotted line).","category":"page"},{"location":"examples/examples_transferentropy/","page":"Transfer entropy","title":"Transfer entropy","text":"[Schreiber2000]: Schreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.","category":"page"},{"location":"api/api_condmutualinfo/#Conditional-mutual-information-API","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"The condition mutual information (CMI) API is defined by","category":"page"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"ConditionalMutualInformation, and its subtypes.\ncondmutualinfo,\nConditionalMutualInformationEstimator, and its subtypes.","category":"page"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"condmutualinfo","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.condmutualinfo","page":"Conditional mutual information API","title":"CausalityTools.condmutualinfo","text":"condmutualinfo([measure::CMI], est::CMIEstimator, x, y, z) → cmi::Real\n\nEstimate a conditional mutual information (CMI) of some kind (specified by measure), between x and y, given z, using the given dedicated ConditionalMutualInformationEstimator, which may be discrete, continuous or mixed.\n\nEstimators\n\nEstimator Principle CMIShannon CMIRenyiPoczos\nFPVP Nearest neighbors ✓ x\nMesnerShalizi Nearest neighbors ✓ x\nRahimzamani Nearest neighbors ✓ x\nPoczosSchneiderCMI Nearest neighbors x ✓\n\n\n\n\n\ncondmutualinfo([measure::CMI], est::ProbabilitiesEstimator, x, y, z) → cmi::Real ∈ [0, a)\n\nEstimate the conditional mutual information (CMI) measure between x and y given z using a sum of entropy terms, without any bias correction, using the provided ProbabilitiesEstimator est. If measure is not given, then the default is CMIShannon().\n\nWith a ProbabilitiesEstimator, the returned cmi is guaranteed to be non-negative.\n\nEstimators\n\nEstimator Principle CMIShannon CMIRenyiSarbu\nCountOccurrences Frequencies ✓ ✓\nValueHistogram Binning (histogram) ✓ ✓\nSymbolicPermutation Ordinal patterns ✓ ✓\nDispersion Dispersion patterns ✓ ✓\n\n\n\n\n\ncondmutualinfo([measure::CMI], est::DifferentialEntropyEstimator, x, y, z) → cmi::Real\n\nEstimate the mutual information between x and y conditioned on z, using the differential version of the given conditional mutual information (CMI) measure. The DifferentialEntropyEstimator est must must support multivariate data. No bias correction is performed. If measure is not given, then the default is CMIShannon().\n\nnote: Note\nDifferentialEntropyEstimators have their own base field which is not used here. Instead, this method creates a copy of est internally, where est.base is replaced by measure.e.base. Therefore, use measure to control the \"unit\" of the mutual information.\n\nEstimators\n\nEstimator Principle CMIShannon\nKraskov Nearest neighbors ✓\nZhu Nearest neighbors ✓\nGao Nearest neighbors ✓\nGoria Nearest neighbors ✓\nLord Nearest neighbors ✓\nLeonenkoProzantoSavani Nearest neighbors ✓\n\n\n\n\n\ncondmutualinfo([measure::CMI], est::MutualInformationEstimator, x, y, z) → cmi::Real\n\nEstimate the mutual information between x and y conditioned on z, using the given conditional mutual information (CMI) measure, computed as a a difference of mutual information terms (just the chain rule of mutual information)\n\nhatI(X Y  Z) = hatI(X Y Z) - hatI(X Z)\n\nThe MutualInformationEstimator est may be continuous/differential, discrete or mixed. No bias correction in performed, except the bias correction that occurs for each individual mutual information term. If measure is not given, then the default is CMIShannon().\n\nEstimators\n\nEstimator Type Principle CMIShannon\nKraskovStögbauerGrassberger1 Continuous Nearest neighbors ✓\nKraskovStögbauerGrassberger2 Continuous Nearest neighbors ✓\nGaoKannanOhViswanath Mixed Nearest neighbors ✓\nGaoOhViswanath Continuous Nearest neighbors ✓\n\n\n\n\n\n","category":"function"},{"location":"api/api_condmutualinfo/#Definitions","page":"Conditional mutual information API","title":"Definitions","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"ConditionalMutualInformation","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformation","page":"Conditional mutual information API","title":"CausalityTools.ConditionalMutualInformation","text":"ConditionalMutualInformation <: AssociationMeasure\nCMI # alias\n\nThe supertype of all conditional mutual information measures. Concrete subtypes are\n\nCMIShannon\nCMIRenyiJizba\nCMIRenyiPoczos\n\n\n\n\n\n","category":"type"},{"location":"api/api_condmutualinfo/#[ConditionalMutualInformationEstimator](@ref)s","page":"Conditional mutual information API","title":"ConditionalMutualInformationEstimators","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"ConditionalMutualInformationEstimator","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.ConditionalMutualInformationEstimator","page":"Conditional mutual information API","title":"CausalityTools.ConditionalMutualInformationEstimator","text":"ConditionalMutualInformationEstimator <: InformationEstimator\nCMIEstimator # alias\n\nThe supertype of all conditional mutual information estimators.\n\nSubtypes\n\nFPVP.\nPoczosSchneiderCMI.\nRahimzamani.\nMesnerShalizi.\n\n\n\n\n\n","category":"type"},{"location":"api/api_condmutualinfo/#[GaussianCMI](@ref)-(parametric)","page":"Conditional mutual information API","title":"GaussianCMI (parametric)","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"GaussianCMI","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.GaussianCMI","page":"Conditional mutual information API","title":"CausalityTools.GaussianCMI","text":"GaussianCMI <: MutualInformationEstimator\nGaussianCMI(; normalize::Bool = false)\n\nGaussianCMI is a parametric estimator for Shannon conditional mutual information (CMI) (Vejmelka & Paluš)[Vejmelka2008].\n\nDescription\n\nGaussianCMI estimates Shannon CMI through a sum of two mutual information terms that each are estimated using GaussianMI (the normalize keyword is the same as for GaussianMI):\n\nhatI_Gaussian(X Y  Z) = hatI_Gaussian(X Y Z) - hatI_Gaussian(X Z)\n\n[Vejmelka2008]: Vejmelka, M., & Paluš, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E, 77(2), 026214.\n\n\n\n\n\n","category":"type"},{"location":"api/api_condmutualinfo/#[FPVP](@ref)","page":"Conditional mutual information API","title":"FPVP","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"FPVP","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.FPVP","page":"Conditional mutual information API","title":"CausalityTools.FPVP","text":"FPVP <: ConditionalMutualInformationEstimator\nFPVP(k = 1, w = 0)\n\nThe Frenzel-Pompe-Vejmelka-Paluš (or FPVP for short) estimator is used to estimate the differential conditional mutual information using a k-th nearest neighbor approach that is analogous to that of the KraskovStögbauerGrassberger1 mutual information estimator (Frenzel & Pompe, 2007[Frenzel2007]; Vejmelka & Paluš, 2008[Vejmelka2008]).\n\nw is the Theiler window, which controls the number of temporal neighbors that are excluded during neighbor searches.\n\n[Frenzel2007]: Frenzel, S., & Pompe, B. (2007). Partial mutual information for coupling analysis of multivariate time series. Physical review letters, 99(20), 204101. w is the Theiler window.\n\n[Vejmelka2008]: Vejmelka, M., & Paluš, M. (2008). Inferring the directionality of coupling with conditional mutual information. Physical Review E, 77(2), 026214.\n\n\n\n\n\n","category":"type"},{"location":"api/api_condmutualinfo/#[MesnerShalizi](@ref)","page":"Conditional mutual information API","title":"MesnerShalizi","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"MesnerShalizi","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.MesnerShalizi","page":"Conditional mutual information API","title":"CausalityTools.MesnerShalizi","text":"MesnerShalizi <: ConditionalMutualInformationEstimator\nMesnerShalizi(k = 1, w = 0)\n\nThe MesnerShalizi estimator is an estimator for conditional mutual information for data that can be mixtures of discrete and continuous data (Mesner & Shalisi et al., 2020)[MesnerShalizi2020].\n\n[MesnerShalizi2020]: Mesner, O. C., & Shalizi, C. R. (2020). Conditional mutual information estimation for mixed, discrete and continuous data. IEEE Transactions on Information Theory, 67(1), 464-484.\n\n\n\n\n\n","category":"type"},{"location":"api/api_condmutualinfo/#[PoczosSchneiderCMI](@ref)","page":"Conditional mutual information API","title":"PoczosSchneiderCMI","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"PoczosSchneiderCMI","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.PoczosSchneiderCMI","page":"Conditional mutual information API","title":"CausalityTools.PoczosSchneiderCMI","text":"PoczosSchneiderCMI <: ConditionalMutualInformationEstimator\nPoczosSchneiderCMI(k = 1, w = 0)\n\nThe PoczosSchneiderCMI estimator computes various (differential) conditional mutual informations, using a k-th nearest neighbor approach (Póczos & Schneider, 2012)[Póczos2012].\n\n[Póczos2012]: Póczos, B., & Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"api/api_condmutualinfo/#[Rahimzamani](@ref)","page":"Conditional mutual information API","title":"Rahimzamani","text":"","category":"section"},{"location":"api/api_condmutualinfo/","page":"Conditional mutual information API","title":"Conditional mutual information API","text":"Rahimzamani","category":"page"},{"location":"api/api_condmutualinfo/#CausalityTools.Rahimzamani","page":"Conditional mutual information API","title":"CausalityTools.Rahimzamani","text":"Rahimzamani <: ConditionalMutualInformationEstimator\nRahimzamani(k = 1, w = 0)\n\nThe Rahimzamani estimator, short for Rahimzamani-Asnani-Viswanath-Kannan, is an estimator for Shannon conditional mutual information for data that can be mixtures of discrete and continuous data (Rahimzamani et al., 2018)[Rahimzamani2018].\n\nThis is very similar to the GaoKannanOhViswanath mutual information estimator, but has been expanded to the conditional case.\n\n[Rahimzamani2018]: Rahimzamani, A., Asnani, H., Viswanath, P., & Kannan, S. (2018). Estimators for multivariate information measures in general probability spaces. Advances in Neural Information Processing Systems, 31.\n\n\n\n\n\n","category":"type"},{"location":"api/api_predictive_asymmetry/#Predictive-asymmetry-API","page":"Predictive asymmetry API","title":"Predictive asymmetry API","text":"","category":"section"},{"location":"api/api_predictive_asymmetry/","page":"Predictive asymmetry API","title":"Predictive asymmetry API","text":"The predictive asymmetry API is made up of the following functions and types.","category":"page"},{"location":"api/api_predictive_asymmetry/","page":"Predictive asymmetry API","title":"Predictive asymmetry API","text":"PA\nasymmetry.\nPATest, for formal independence testing.","category":"page"},{"location":"api/api_predictive_asymmetry/","page":"Predictive asymmetry API","title":"Predictive asymmetry API","text":"asymmetry","category":"page"},{"location":"api/api_predictive_asymmetry/#CausalityTools.asymmetry","page":"Predictive asymmetry API","title":"CausalityTools.asymmetry","text":"asymmetry(measure::PA, est, x, y, [z]) → ΔA\n\nCompute the predictive asymmetry (PA) from x to y, conditioned on z if given. Returns the distribution of asymmetry values.\n\nCompatible estimators\n\nThe docstring for PA lists compatible estimators.\n\n\n\n\n\n","category":"function"},{"location":"api/#API","page":"APIs and estimators","title":"API","text":"","category":"section"},{"location":"api/#Information-API","page":"APIs and estimators","title":"Information API","text":"","category":"section"},{"location":"api/","page":"APIs and estimators","title":"APIs and estimators","text":"Pages = [\n    \"api/api_information_overview.md\",\n    \"api/api_probabilities.md\",\n    \"api/api_contingency_table.md\",\n    \"api/api_entropies.md\",\n    \"api/api_conditional_entropy.md\",\n    \"api/api_mutualinfo.md\",\n    \"api/api_condmutualinfo.md\",\n    \"api/api_transferentropy.md\",\n    \"api/api_predictive_asymmetry.md\",\n]\nDepth = 3","category":"page"},{"location":"api/#Cross-map-API","page":"APIs and estimators","title":"Cross-map API","text":"","category":"section"},{"location":"api/","page":"APIs and estimators","title":"APIs and estimators","text":"Pages = [\n    \"api/api_crossmap.md\",\n]\nDepth = 3","category":"page"},{"location":"api/#Recurrence-API","page":"APIs and estimators","title":"Recurrence API","text":"","category":"section"},{"location":"api/","page":"APIs and estimators","title":"APIs and estimators","text":"Pages = [\n    \"api/api_recurrence.md\",\n]\nDepth = 3","category":"page"},{"location":"api/api_contingency_table/#contingency_table_api","page":"Contingency table API","title":"Contingency table API","text":"","category":"section"},{"location":"api/api_contingency_table/","page":"Contingency table API","title":"Contingency table API","text":"To estimate discrete information theoretic quantities that are functions of more than one variable, we must estimate empirical joint probability mass functions (pmf). The function contingency_matrix accepts an arbitrary number of equal-length input data and returns the corresponding multidimensional contingency table as a ContingencyMatrix. From this table, we can extract the necessary joint and marginal pmfs for computing any discrete function of multivariate discrete probability distributions. This is essentially the multivariate analogue of Probabilities.","category":"page"},{"location":"api/api_contingency_table/","page":"Contingency table API","title":"Contingency table API","text":"But why would I use a ContingencyMatrix instead of some other indirect estimation method, you may ask. The answer is that ContingencyMatrix allows you to compute any of the information theoretic quantities offered in this package for any type of input data. You input data can literally be any hashable type, for example String, Tuple{Int, String, Int}, or YourCustomHashableDataType.","category":"page"},{"location":"api/api_contingency_table/","page":"Contingency table API","title":"Contingency table API","text":"In the case of numeric data, using a ContingencyMatrix is typically a bit slower than other dedicated estimation procedures. For example, quantities like discrete Shannon-type condmutualinfo are faster to estimate using a formulation based on sums of four entropies (the H4-principle). This is faster because we can both utilize the blazingly fast StateSpaceSet structure directly, and we can avoid explicitly estimating the entire joint pmf, which demands many extra calculation steps. Whatever you use in practice depends on your use case and available estimation methods, but you can always fall back to contingency matrices for any discrete measure.","category":"page"},{"location":"api/api_contingency_table/","page":"Contingency table API","title":"Contingency table API","text":"ContingencyMatrix\ncontingency_matrix","category":"page"},{"location":"api/api_contingency_table/#CausalityTools.ContingencyMatrix","page":"Contingency table API","title":"CausalityTools.ContingencyMatrix","text":"ContingencyMatrix{T, N} <: Probabilities{T, N}\nContingencyMatrix(frequencies::AbstractArray{Int, N})\n\nA contingency matrix is essentially a multivariate analogue of Probabilities that also keep track of raw frequencies.\n\nThe contingency matrix can be constructed directyly from an N-dimensional frequencies array. Alternatively, the contingency_matrix function performs counting for you; this works on both raw categorical data, or by first discretizing data using a a ProbabilitiesEstimator.\n\nDescription\n\nA ContingencyMatrix c is just a simple wrapper around around AbstractArray{T, N}. Indexing c with multiple indices i, j, … returns the (i, j, …)th element of the empirical probability mass function (pmf). The following convencience methods are defined:\n\nfrequencies(c; dims) returns the multivariate raw counts along the given `dims   (default to all available dimensions).\nprobabilities(c; dims) returns a multidimensional empirical   probability mass function (pmf) along the given dims (defaults to all available   dimensions), i.e. the normalized counts.\nprobabilities(c, i::Int) returns the marginal probabilities for the i-th dimension.\noutcomes(c, i::Int) returns the marginal outcomes for the i-th dimension.\n\nOrdering\n\nThe ordering of outcomes are internally consistent, but we make no promise on the ordering of outcomes relative to the input data. This means that if your input data are x = rand([\"yes\", \"no\"], 100); y = rand([\"small\", \"medium\", \"large\"], 100), you'll get a 2-by-3 contingency matrix, but there currently no easy way to determine which outcome the i-j-th row/column of this matrix corresponds to.\n\nSince ContingencyMatrix is intended for use in information theoretic methods that don't care about ordering, as long as the ordering is internally consistent, this is not an issue for practical applications in this package. This may change in future releases.\n\nUsage\n\nContingency matrices is used in the computation of discrete versions of the following quantities:\n\nentropy_joint.\nmutualinfo.\ncondmutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"api/api_contingency_table/#CausalityTools.contingency_matrix","page":"Contingency table API","title":"CausalityTools.contingency_matrix","text":"contingency_matrix(x, y, [z, ...]) → c::ContingencyMatrix\ncontingency_matrix(est::ProbabilitiesEstimator, x, y, [z, ...]) → c::ContingencyMatrix\n\nEstimate a multidimensional contingency matrix c from input data x, y, …, where the input data can be of any and different types, as long as length(x) == length(y) == ….\n\nFor already discretized data, use the first method. For continuous data, you want to discretize the data before computing the contingency table. You can do this manually and then use the first method. Alternatively, you can provide a ProbabilitiesEstimator as the first argument to the constructor. Then the input variables x, y, … are discretized separately according to est (enforcing the same outcome space for all variables), by calling marginal_encodings.\n\n\n\n\n\n","category":"function"},{"location":"api/api_contingency_table/#Utilities","page":"Contingency table API","title":"Utilities","text":"","category":"section"},{"location":"api/api_contingency_table/","page":"Contingency table API","title":"Contingency table API","text":"marginal_encodings","category":"page"},{"location":"api/api_contingency_table/#CausalityTools.marginal_encodings","page":"Contingency table API","title":"CausalityTools.marginal_encodings","text":"marginal_encodings(est::ProbabilitiesEstimator, x::VectorOrStateSpaceSet...)\n\nEncode/discretize each input vector xᵢ ∈ x according to a procedure determined by est. Any xᵢ ∈ X that are multidimensional (StateSpaceSets) will be encoded column-wise, i.e. each column of xᵢ is treated as a timeseries and is encoded separately.\n\nThis is useful for computing any discrete information theoretic quantity, and is used internally by contingency_matrix.\n\nSupported estimators\n\nValueHistogram. Bin visitation frequencies are counted in the joint space XY,   then marginal visitations are obtained from the joint bin visits.   This behaviour is the same for both FixedRectangularBinning and   RectangularBinning (which adapts the grid to the data).   When using FixedRectangularBinning, the range along the first dimension   is used as a template for all other dimensions.\nSymbolicPermutation. Each timeseries is separately encoded according   to its ordinal pattern.\nDispersion. Each timeseries is separately encoded according to its   dispersion pattern.\n\nMany more implementations are possible. Each new implementation gives one new way of estimating the ContingencyMatrix\n\n\n\n\n\n","category":"function"},{"location":"examples/examples_closeness/#Closeness-measures","page":"Closeness measures","title":"Closeness measures","text":"","category":"section"},{"location":"examples/examples_closeness/#quickstart_smeasure","page":"Closeness measures","title":"S-measure","text":"","category":"section"},{"location":"examples/examples_closeness/#Computing-the-s-statistic","page":"Closeness measures","title":"Computing the s-statistic","text":"","category":"section"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = SMeasure(dx = 3, dy = 3)\ns = s_measure(measure, x, y)","category":"page"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"The s statistic is larger when there is stronger coupling and smaller when there is weaker coupling. To check whether s is significant (i.e. large enough to claim directional dependence), we can use a SurrogateTest, like here.","category":"page"},{"location":"examples/examples_closeness/#quickstart_hmeasure","page":"Closeness measures","title":"H-measure","text":"","category":"section"},{"location":"examples/examples_closeness/#Computing-the-h-statistic","page":"Closeness measures","title":"Computing the h-statistic","text":"","category":"section"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = HMeasure(dx = 3, dy = 3)\nh = h_measure(measure, x, y)","category":"page"},{"location":"examples/examples_closeness/#quickstart_mmeasure","page":"Closeness measures","title":"M-measure","text":"","category":"section"},{"location":"examples/examples_closeness/#Computing-the-m-statistic","page":"Closeness measures","title":"Computing the m-statistic","text":"","category":"section"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = MMeasure(dx = 3, dy = 3)\nm = m_measure(measure, x, y)","category":"page"},{"location":"examples/examples_closeness/#quickstart_mmeasure-2","page":"Closeness measures","title":"L-measure","text":"","category":"section"},{"location":"examples/examples_closeness/#Computing-the-l-statistic","page":"Closeness measures","title":"Computing the l-statistic","text":"","category":"section"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = LMeasure(dx = 3, dy = 3)\nl = l_measure(measure, x, y)","category":"page"},{"location":"examples/examples_closeness/#quickstart_jdd","page":"Closeness measures","title":"Joint distance distribution","text":"","category":"section"},{"location":"examples/examples_closeness/#Computing-the-Δ-distribution","page":"Closeness measures","title":"Computing the Δ-distribution","text":"","category":"section"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = JointDistanceDistribution(D = 3, B = 5)\nΔ = jdd(measure, x, y)","category":"page"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"The joint distance distribution measure indicates directional coupling between x and y if Δ is skewed towards positive values. We can use a JointDistanceDistributionTest to formally check this.","category":"page"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"test = JointDistanceDistributionTest(measure)\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_closeness/","page":"Closeness measures","title":"Closeness measures","text":"The p-value is fairly low, and depending on the significance level 1 - α, we cannot reject the null hypothesis that Δ is not skewed towards positive values, and hence we cannot reject that the variables are independent.","category":"page"},{"location":"examples/examples_conditional_mutual_information/#quickstart_mutualinfo","page":"Conditional mutual information","title":"Conditional mutual information","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)","page":"Conditional mutual information","title":"CMIShannon","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/#Estimation-using-[ConditionalMutualInformationEstimator](@ref)s","page":"Conditional mutual information","title":"Estimation using ConditionalMutualInformationEstimators","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"When estimated using a ConditionalMutualInformationEstimator, some form of bias correction is usually applied. The FPVP estimator is a popular choice.","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)-with-[GaussianCMI](@ref)","page":"Conditional mutual information","title":"CMIShannon with GaussianCMI","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = randn(1000)\ny = randn(1000) .+ x\nz = randn(1000) .+ y\ncondmutualinfo(GaussianCMI(), x, z, y) # defaults to `CMIShannon()`","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)-with-[FPVP](@ref)","page":"Conditional mutual information","title":"CMIShannon with FPVP","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = rand(Normal(-1, 0.5), n)\ny = rand(BetaPrime(0.5, 1.5), n) .+ x\nz = rand(Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\ncondmutualinfo(FPVP(k = 5), x, z, y) # defaults to `CMIShannon()`","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)-with-[MesnerShalizi](@ref)","page":"Conditional mutual information","title":"CMIShannon with MesnerShalizi","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = rand(Normal(-1, 0.5), n)\ny = rand(BetaPrime(0.5, 1.5), n) .+ x\nz = rand(Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\ncondmutualinfo(MesnerShalizi(k = 10), x, z, y) # defaults to `CMIShannon()`","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)-with-[Rahimzamani](@ref)","page":"Conditional mutual information","title":"CMIShannon with Rahimzamani","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = rand(Normal(-1, 0.5), n)\ny = rand(BetaPrime(0.5, 1.5), n) .+ x\nz = rand(Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\ncondmutualinfo(CMIShannon(base = 10), Rahimzamani(k = 10), x, z, y)","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIRenyiPoczos](@ref)-with-[PoczosSchneiderCMI](@ref)","page":"Conditional mutual information","title":"CMIRenyiPoczos with PoczosSchneiderCMI","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = rand(Normal(-1, 0.5), n)\ny = rand(BetaPrime(0.5, 1.5), n) .+ x\nz = rand(Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\ncondmutualinfo(CMIRenyiPoczos(base = 2, q = 1.2), PoczosSchneiderCMI(k = 5), x, z, y)","category":"page"},{"location":"examples/examples_conditional_mutual_information/#Estimation-using-[MutualInformationEstimator](@ref)s","page":"Conditional mutual information","title":"Estimation using MutualInformationEstimators","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Any MutualInformationEstimator can also be used to compute conditional mutual information using the chain rule of mutual information. However, the naive application of these estimators don't perform any bias correction when taking the difference of mutual information terms.","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)-with-[KSG1](@ref)","page":"Conditional mutual information","title":"CMIShannon with KSG1","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nusing Statistics\n\nn = 1000\n# A chain X → Y → Z\nx = rand(Normal(-1, 0.5), n)\ny = rand(BetaPrime(0.5, 1.5), n) .+ x\nz = rand(Chisq(100), n)\nz = (z ./ std(z)) .+ y\n\n# We expect zero (in practice: very low) CMI when computing I(X; Z | Y), because\n# the link between X and Z is exclusively through Y, so when observing Y,\n# X and Z should appear independent.\ncondmutualinfo(CMIShannon(base = 2), KSG1(k = 5), x, z, y)","category":"page"},{"location":"examples/examples_conditional_mutual_information/#Estimation-using-[DifferentialEntropyEstimator](@ref)s","page":"Conditional mutual information","title":"Estimation using DifferentialEntropyEstimators","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Any DifferentialEntropyEstimator can also be used to compute conditional mutual information using a sum of entropies. However, the naive application of these estimators don't perform any bias application when taking the sum of entropy terms.","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)-with-[Kraskov](@ref)","page":"Conditional mutual information","title":"CMIShannon with Kraskov","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nn = 1000\n# A chain X → Y → Z\nx = rand(Epanechnikov(0.5, 1.0), n)\ny = rand(Erlang(1), n) .+ x\nz = rand(FDist(5, 2), n)\ncondmutualinfo(CMIShannon(), Kraskov(k = 5), x, z, y)","category":"page"},{"location":"examples/examples_conditional_mutual_information/#Estimation-using-[ProbabilitiesEstimator](@ref)s","page":"Conditional mutual information","title":"Estimation using ProbabilitiesEstimators","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"Any ProbabilitiesEstimator can also be used to compute conditional mutual information using a sum of entropies. However, the naive application of these estimators don't perform any bias application when taking the sum of entropy terms.","category":"page"},{"location":"examples/examples_conditional_mutual_information/#[CMIShannon](@ref)-with-[ValueHistogram](@ref)","page":"Conditional mutual information","title":"CMIShannon with ValueHistogram","text":"","category":"section"},{"location":"examples/examples_conditional_mutual_information/","page":"Conditional mutual information","title":"Conditional mutual information","text":"using CausalityTools\nusing Distributions\nn = 1000\n# A chain X → Y → Z\nx = rand(Epanechnikov(0.5, 1.0), n)\ny = rand(Erlang(1), n) .+ x\nz = rand(FDist(5, 2), n)\nest = ValueHistogram(RectangularBinning(5))\ncondmutualinfo(CMIShannon(), est, x, z, y), condmutualinfo(CMIShannon(), est, x, y, z)","category":"page"},{"location":"api/api_recurrence/#Recurrence-API","page":"Recurrence API","title":"Recurrence API","text":"","category":"section"},{"location":"api/api_recurrence/","page":"Recurrence API","title":"Recurrence API","text":"mcr\nrmcd","category":"page"},{"location":"api/api_recurrence/#CausalityTools.mcr","page":"Recurrence API","title":"CausalityTools.mcr","text":"mcr(m::MCR, x, y)\n\nCompute the association between x and y based on conditional probabilities of recurrence using the given MCR measure, where x and y can be either univariate timeseries or multivariate StateSpaceSets.\n\n\n\n\n\n","category":"function"},{"location":"api/api_recurrence/#CausalityTools.rmcd","page":"Recurrence API","title":"CausalityTools.rmcd","text":"rmcd(measure::RMCD, x, y)\nrmcd(measure::RMCD, x, y, [z, ...])\n\nEstimate the recurrence-based measure of dependence between x and y, conditional on z if given.\n\nParameters for recurrence matrix estimation are given as a RMCD instance. Inputs x, y, z can be either univariate timeseries or multivariate StateSpaceSets.\n\n\n\n\n\n","category":"function"},{"location":"examples/examples_graphs/#Causal-graphs","page":"Causal graphs","title":"Causal graphs","text":"","category":"section"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"Before introducing the causal graph examples, let's create a function that can plot directed graphs that we'll use below.","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"using Graphs, CairoMakie, GraphMakie\n\nfunction plotgraph(g)\n    f, ax, p = graphplot(g,\n        nlabels = repr.(1:nv(g)),\n        nlabels_color = [:red for i in 1:nv(g)],\n    )\n    offsets = 0.05 * (p[:node_pos][] .- p[:node_pos][][1])\n    offsets[1] = Point2f(0, 0.2)\n    p.nlabels_offset[] = offsets\n    autolimits!(ax)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    ax.aspect = DataAspect()\n    return f\nend","category":"page"},{"location":"examples/examples_graphs/#oce_example","page":"Causal graphs","title":"Optimal causation entropy","text":"","category":"section"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"Here, we use the OCE algorithm to infer a time series graph. We use a SurrogateTest for the initial step, and a LocalPermutationTest for the conditional steps.","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"using CausalityTools\nusing StableRNGs\nrng = StableRNG(123)\n\n# An example system where `X → Y → Z → W`.\nsys = system(Logistic4Chain(; rng))\nx, y, z, w = columns(trajectory(sys, 400, Ttr = 10000))\n\n# Independence tests for unconditional and conditional stages.\nutest = SurrogateTest(MIShannon(), KSG2(k = 3, w = 1); rng, nshuffles = 150)\nctest = LocalPermutationTest(CMIShannon(), MesnerShalizi(k = 3, w = 1); rng, nshuffles = 150)\n\n# Infer graph\nalg = OCE(; utest, ctest, α = 0.05, τmax = 1)\nparents = infer_graph(alg, [x, y, z, w])\n\n# Convert to graph and inspect edges\ng = SimpleDiGraph(parents)\ncollect(edges(g))","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"The algorithm nicely recovers the true causal directions. We can also plot the graph using the function we made above.","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"plotgraph(g)","category":"page"},{"location":"examples/examples_graphs/#pc_examples","page":"Causal graphs","title":"PC-algorithm","text":"","category":"section"},{"location":"examples/examples_graphs/#pc_examples_corr","page":"Causal graphs","title":"Correlation-based tests","text":"","category":"section"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"Here, we demonstrate the use of the PC-algorithm with the correlation-based CorrTest both for the pairwise (i.e. using PearsonCorrelation) and conditional (i.e. using PartialCorrelation) case.","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"We'll reproduce the first example from CausalInference.jl, where they also use a parametric correlation test to infer the skeleton graph for some  normally distributed data.","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"using CausalityTools\nusing StableRNGs\nrng = StableRNG(123)\nn = 500\nv = randn(rng, n)\nx = v + randn(rng, n)*0.25\nw = x + randn(rng, n)*0.25\nz = v + w + randn(rng, n)*0.25\ns = z + randn(rng, n)*0.25\nX = [x, v, w, z, s]\n\n# Infer a completed partially directed acyclic graph (CPDAG)\nalg = PC(CorrTest(), CorrTest(); α = 0.05)\nest_cpdag_parametric = infer_graph(alg, X; verbose = false)\n\n# Plot the graph\nplotgraph(est_cpdag_parametric)","category":"page"},{"location":"examples/examples_graphs/#pc_examples_nonparametric","page":"Causal graphs","title":"Nonparametric tests","text":"","category":"section"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"The main difference between the PC algorithm implementation here and in CausalInference.jl is that our implementation automatically works with any compatible and IndependenceTest, and thus any combination of (nondirectional) AssociationMeasure and estimator.","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"Here, we replicate the example above, but using a nonparametric SurrogateTest with the Shannon mutual information MIShannon measure and the GaoOhViswanath estimator for the pairwise independence tests, and a LocalPermutationTest with conditional mutual information CMIShannon and the MesnerShalizi.","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"rng = StableRNG(123)\n\n# Use fewer observations, because MI/CMI takes longer to estimate\nn = 400\nv = randn(rng, n)\nx = v + randn(rng, n)*0.25\nw = x + randn(rng, n)*0.25\nz = v + w + randn(rng, n)*0.25\ns = z + randn(rng, n)*0.25\nX = [x, v, w, z, s]\n\npairwise_test = SurrogateTest(MIShannon(), GaoOhViswanath(k = 10))\ncond_test = LocalPermutationTest(CMIShannon(), MesnerShalizi(k = 10))\nalg = PC(pairwise_test, cond_test; α = 0.05)\nest_cpdag_nonparametric = infer_graph(alg, X; verbose = false)\nplotgraph(est_cpdag_nonparametric)","category":"page"},{"location":"examples/examples_graphs/","page":"Causal graphs","title":"Causal graphs","text":"We get the same graph as with the parametric estimator. However, for general non-gaussian data, the correlation-based tests (which assumes normally distributed data) will not give the same results as other independence tests.","category":"page"},{"location":"independence/#independence_testing","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"independence/#Independence-testing-API","page":"Independence testing","title":"Independence testing API","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"The independence test API is defined by","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"independence\nIndependenceTest","category":"page"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"independence\nIndependenceTest","category":"page"},{"location":"independence/#CausalityTools.independence","page":"Independence testing","title":"CausalityTools.independence","text":"independence(test::IndependenceTest, x, y, [z]) → summary\n\nPerform the given IndependenceTest test on data x, y and z. If only x and y are given, test must provide a bivariate association measure. If z is given too, then test must provide a conditional association measure.\n\nReturns a test summary, whose type depends on test.\n\nCompatible independence tests\n\nSurrogateTest.\nLocalPermutationTest.\nJointDistanceDistributionTest.\n\n\n\n\n\n","category":"function"},{"location":"independence/#CausalityTools.IndependenceTest","page":"Independence testing","title":"CausalityTools.IndependenceTest","text":"IndependenceTest <: IndependenceTest\n\nThe supertype for all independence tests.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[SurrogateTest](@ref)","page":"Independence testing","title":"SurrogateTest","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"SurrogateTest\nSurrogateTestResult","category":"page"},{"location":"independence/#CausalityTools.SurrogateTest","page":"Independence testing","title":"CausalityTools.SurrogateTest","text":"SurrogateTest <: IndependenceTest\nSurrogateTest(measure, [est];\n    nshuffles::Int = 100,\n    surrogate = RandomShuffle(),\n    rng = Random.default_rng(),\n)\n\nA generic (conditional) independence test for assessing whether two variables X and Y are independendent, potentially conditioned on a third variable Z, based on surrogate data.\n\nWhen used with independence, a SurrogateTestResult is returned.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether x and y are independent (given z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between the input variables.\n\nThere are different ways of shuffling, dictated by surrogate, each representing a distinct null hypothesis. For each shuffle, the provided measure is computed (using est, if relevant). This procedure is repeated nshuffles times, and a test summary is returned. The shuffled variable is always the first variable (X). Exceptions are:\n\nIf TransferEntropy measure such as TEShannon,   then the source variable is always shuffled, and the target and conditional   variable are left unshuffled.\n\nCompatible measures\n\nMeasure Pairwise Conditional Requires est\nPearsonCorrelation ✓ ✖ No\nDistanceCorrelation ✓ ✓ No\nSMeasure ✓ ✖ No\nHMeasure ✓ ✖ No\nMMeasure ✓ ✖ No\nLMeasure ✓ ✖ No\nPairwiseAsymmetricInference ✓ ✖ Yes\nConvergentCrossMapping ✓ ✖ Yes\nMIShannon ✓ ✖ Yes\nMIRenyiJizba ✓ ✖ Yes\nMIRenyiSarbu ✓ ✖ Yes\nMITsallisMartin ✓ ✖ Yes\nMITsallisFuruichi ✓ ✖ Yes\nPartialCorrelation ✖ ✓ Yes\nCMIShannon ✖ ✓ Yes\nCMIRenyiJizba ✖ ✓ Yes\nTEShannon ✓ ✓ Yes\nTERenyiJizba ✓ ✓ Yes\n\nExamples\n\nPairwise test, DistanceCorrelation.\nPairwise test, TEShannon.\nConditional test, PartialCorrelation.\nPairwise test, MIShannon, categorical.\nConditional test, CMIShannon, categorical.\n\n\n\n\n\n","category":"type"},{"location":"independence/#CausalityTools.SurrogateTestResult","page":"Independence testing","title":"CausalityTools.SurrogateTestResult","text":"SurrogateTestResult(m, m_surr, pvalue)\n\nHolds the result of a SurrogateTest. m is the measure computed on the original data. m_surr is a vector of the measure computed on permuted data, where m_surr[i] is the measure compute on the i-th permutation. pvalue is the one-sided p-value for the test.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[LocalPermutationTest](@ref)","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"LocalPermutationTest\nLocalPermutationTestResult","category":"page"},{"location":"independence/#CausalityTools.LocalPermutationTest","page":"Independence testing","title":"CausalityTools.LocalPermutationTest","text":"LocalPermutationTest <: IndependenceTest\nLocalPermutationTest(measure, [est];\n    kperm::Int = 5,\n    nshuffles::Int = 100,\n    rng = Random.default_rng(),\n    replace = true,\n    w::Int = 0)\n\nLocalPermutationTest is a generic conditional independence test (Runge, 2018)[Runge2018] for assessing whether two variables X and Y are conditionally independendent given a third variable Z (all of which may be multivariate).\n\nWhen used with independence, a LocalPermutationTestResult is returned.\n\nDescription\n\nThis is a generic one-sided hypothesis test that checks whether X and Y are independent (given Z, if provided) based on resampling from a null distribution assumed to represent independence between the variables. The null distribution is generated by repeatedly shuffling the input data in some way that is intended to break any dependence between x and y, but preserve dependencies between x and z.\n\nThe algorithm is as follows:\n\nCompute the original conditional independence statistic I(X; Y | Z).\nAllocate a scalar valued vector Î with space for nshuffles elements.\nFor k ∈ [1, 2, …, nshuffles], repeat\nFor each zᵢ ∈ Y, let nᵢ be time indices of the kperm nearest neighbors of zᵢ,   excluding the w nearest neighbors of zᵢ from the neighbor query (i.e w is   the Theiler window).\nLet xᵢ⋆ = X[j], where j is randomly sampled from nᵢ with replacement.   This way, xᵢ is replaced with xⱼ only if zᵢ ≈ zⱼ (zᵢ and zⱼ are close).   Repeat for i = 1, 2, …, n and obtain the shuffled X̂ = [x̂₁, x̂₂, …, x̂ₙ].\nCompute the conditional independence statistic Iₖ(X̂; Y | Z).\nLet Î[k] = Iₖ(X̂; Y | Z).\nCompute the p-value as count(Î[k] .<= I) / nshuffles).\n\nIn additional to the conditional variant from Runge (2018), we also provide a pairwise version, where the shuffling procedure is identical, except neighbors in Y are used instead of Z and we I(X; Y) and Iₖ(X̂; Y) instead of I(X; Y | Z) and Iₖ(X̂; Y | Z).\n\nCompatible measures\n\nMeasure Pairwise Conditional Requires est\nPartialCorrelation ✖ ✓ No\nDistanceCorrelation ✖ ✓ No\nCMIShannon ✖ ✓ Yes\nTEShannon ✓ ✓ Yes\n\nThe LocalPermutationTest is only defined for conditional independence testing. Exceptions are for measures like TEShannon, which use conditional measures under the hood even for their pairwise variants, and are therefore compatible with LocalPermutationTest.\n\nThe nearest-neighbor approach in Runge (2018) can be reproduced by using the CMIShannon measure with the FPVP estimator.\n\nExamples\n\nExample using CMIShannon.\nExample using TEShannon.\n\n[Runge2018]: Runge, J. (2018, March). Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information. In International Conference on Artificial Intelligence and Statistics (pp. 938-947). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"independence/#CausalityTools.LocalPermutationTestResult","page":"Independence testing","title":"CausalityTools.LocalPermutationTestResult","text":"LocalPermutationTestResult(m, m_surr, pvalue)\n\nHolds the result of a LocalPermutationTest. m is the measure computed on the original data. m_surr is a vector of the measure computed on permuted data, where m_surr[i] is the measure compute on the i-th permutation. pvalue is the one-sided p-value for the test.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[JointDistanceDistributionTest](@ref)","page":"Independence testing","title":"JointDistanceDistributionTest","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"JointDistanceDistributionTest\nJDDTestResult","category":"page"},{"location":"independence/#CausalityTools.JointDistanceDistributionTest","page":"Independence testing","title":"CausalityTools.JointDistanceDistributionTest","text":"JointDistanceDistributionTest <: IndependenceTest\nJointDistanceDistributionTest(measure::JointDistanceDistribution; rng = Random.default_rng())\n\nAn independence test for two variables based on the JointDistanceDistribution (Amigó & Hirata, 2018)[Amigo2018].\n\nWhen used with independence, a JDDTestResult is returned.\n\nDescription\n\nThe joint distance distribution (labelled Δ in their paper) is used by Amigó & Hirata (2018) to detect directional couplings of the form X to Y or Y to X. JointDistanceDistributionTest formulates their method as an independence test.\n\nFormally, we test the hypothesis H_0 (the variables are independent) against H_1 (there is directional coupling between the variables). To do so, we use a right-sided/upper-tailed t-test to check mean of Δ is skewed towards positive value, i.e.\n\nH_0 = mu(Delta) = 0\nH_1 = mu(Delta)  0.\n\nWhen used with independence, a JDDTestResult is returned, which contains the joint distance distribution and a p-value. If you only need Δ, use jdd directly.\n\nExamples\n\nThis example shows how the JointDistanceDistributionTest can be used in practice.\n\n[Amigo2018]: Amigó, José M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"type"},{"location":"independence/#CausalityTools.JDDTestResult","page":"Independence testing","title":"CausalityTools.JDDTestResult","text":"JDDTestResult(Δjdd, hypothetical_μ, pvalue)\n\nHolds the results of JointDistanceDistributionTest. Δjdd is the Δ-distribution, hypothetical_μ is the hypothetical mean of the Δ-distribution under the null, and pvalue is the p-value for the one-sided t-test.\n\n\n\n\n\n","category":"type"},{"location":"independence/#[CorrTest](@ref)","page":"Independence testing","title":"CorrTest","text":"","category":"section"},{"location":"independence/","page":"Independence testing","title":"Independence testing","text":"CorrTest\nCorrTestResult","category":"page"},{"location":"independence/#CausalityTools.CorrTest","page":"Independence testing","title":"CausalityTools.CorrTest","text":"CorrTest <: IndependenceTest\nCorrTest()\n\nAn independence test based correlation (for two variables) and partial correlation (for three variables), as described in Schmidt et al. (2018)[Schmidt2018]. Uses PearsonCorrelation and PartialCorrelation internally.\n\nAssumes that the input data are (multivariate) normally distributed. Then ρ(X, Y) = 0 implies X ⫫ Y and ρ(X, Y | 𝐙) = 0 implies X ⫫ Y | 𝐙.\n\nDescription\n\nThe null hypothesis is H₀ := ρ(X, Y | 𝐙) = 0. We use the approach in Levy & Narula (1978)[Levy1978] and compute the Z-transformation of the observed (partial) correlation coefficient hatrho_XYbfZ:\n\nZ(hatrho_XYbfZ) =\nlogdfrac1 + hatrho_XYbfZ1 - hatrho_XYbfZ\n\nTo test the null hypothesis against the alternative hypothesis H₁ := ρ(X, Y | 𝐙) > 0, calculate\n\nhatZ = dfrac12dfracZ(hatrho_XYbfZ) - Z(0)sqrt1(n - d - 3)\n\nand compute the two-sided p-value (Schmidt et al., 2018)\n\np(X Y  bfZ) = 2(1 - phi(sqrtn - d - 3Z(hatrho_XYbfZ)))\n\nwhere d is the dimension of bfZ and n is the number of samples. For the pairwise case, the procedure is identical, but set bfZ = emptyset.\n\nExamples\n\nCorrTestfor independence between normally distributed data.\n\n[Levy1978]: Levy, K. J., & Narula, S. C. (1978). Testing hypotheses concerning partial correlations: Some methods and discussion. International Statistical Review/Revue Internationale de Statistique, 215-218.\n\n[Schmidt2018]: Schmidt, C., Huegle, J., & Uflacker, M. (2018, July). Order-independent constraint-based causal structure learning for gaussian distribution models using gpus. In Proceedings of the 30th International Conference on Scientific and Statistical Database Management (pp. 1-10).\n\n\n\n\n\n","category":"type"},{"location":"independence/#CausalityTools.CorrTestResult","page":"Independence testing","title":"CausalityTools.CorrTestResult","text":"CorrTestResult(pvalue, ρ, z)\n\nA simple struct that holds the results of a CorrTest test: the (partial) correlation coefficient ρ, Fisher's z, and pvalue - the two-sided p-value for the test.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#Predefined-coupled-systems","page":"Predefined systems","title":"Predefined coupled systems","text":"","category":"section"},{"location":"coupled_systems/#Systems-definition-API","page":"Predefined systems","title":"Systems definition API","text":"","category":"section"},{"location":"coupled_systems/","page":"Predefined systems","title":"Predefined systems","text":"The systems definition API is defined by","category":"page"},{"location":"coupled_systems/","page":"Predefined systems","title":"Predefined systems","text":"SystemDefinition, DiscreteDefinition, and ContinuousDefinition.\nsystem","category":"page"},{"location":"coupled_systems/","page":"Predefined systems","title":"Predefined systems","text":"SystemDefinition\nDiscreteDefinition\nContinuousDefinition\nsystem","category":"page"},{"location":"coupled_systems/#CausalityTools.SystemDefinition","page":"Predefined systems","title":"CausalityTools.SystemDefinition","text":"SystemDefinition\n\nThe abstract type of all system definitions. Abstract subtypes are DiscreteDefinition and ContinuousSystem.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.DiscreteDefinition","page":"Predefined systems","title":"CausalityTools.DiscreteDefinition","text":"DiscreteDefinition <: SystemDefinition\n\nThe supertype of all discrete system definitions.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.ContinuousDefinition","page":"Predefined systems","title":"CausalityTools.ContinuousDefinition","text":"ContinuousDefinition <: SystemDefinition\n\nThe supertype of all continuous system definitions.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.system","page":"Predefined systems","title":"CausalityTools.system","text":"system(definition::DiscreteDefinition) → s::DiscreteDynamicalSystem\nsystem(definition::ContinuousDefinition) → s::ContinuousDynamicalSystem\n\nInitialize a dynamical system from definition.\n\n\n\n\n\n","category":"function"},{"location":"coupled_systems/#Discrete-systems","page":"Predefined systems","title":"Discrete systems","text":"","category":"section"},{"location":"coupled_systems/","page":"Predefined systems","title":"Predefined systems","text":"Anishchenko\nAR1Unidir\nAR1Bidir\nChaoticMaps3\nHenon2\nHenon3\nIkeda2\nChaoticNoisyLinear2\nLogistic2Unidir\nLogistic2Bidir\nLogistic3CommonDriver\nLogistic4Chain\nNonlinear3\nPeguin2\nUlamLattice\nVar1\nVerdes3","category":"page"},{"location":"coupled_systems/#CausalityTools.Anishchenko","page":"Predefined systems","title":"CausalityTools.Anishchenko","text":"Anishchenko <: DiscreteDefinition\nAnishchenko(;u₀ = rand(2), α =3.277, s=0.1, ω=0.5*(sqrt(5)-1)) → DiscreteDynamicalSystem\n\nInitialise the system defined by eq. 13 in Anishchenko & Strelkova (1998)[Anishchenko1998], which can give strange, nonchaotic attractors.\n\nEquations of motion\n\nbeginaligned\ndx = alpha (1-s cos (2 pi phi )) cdot x(1-x) \ndϕ = (phi + omega ) mod1\nendaligned\n\n[Anishchenko1998]: Anishchenko, Vadim S., and Galina I. Strelkova. \"Irregular attractors.\" Discrete dynamics in Nature and Society 2.1 (1998): 53-72.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.AR1Unidir","page":"Predefined systems","title":"CausalityTools.AR1Unidir","text":"AR1Unidir <: DiscreteDefinition\nAR1Unidir(; ui = [0.2, 0.3], a₁ = 0.90693, b₁ = 0.40693, c_xy = 0.5,\n    nx = Normal(0, 0.40662), ny = Normal(0, 0.40662),\n    rng::R = Random.default_rng())\n\nA bivariate, order one autoregressive model, where x to y (Paluš et al, 2018)[Paluš2018].\n\nEquations of motion\n\nbeginaligned\nx(t+1) = a_1 x(t) + xi_1 \ny(t+1) = b_1 y(t) - c_xy x + xi_2\nendaligned\n\nwhere xi_1 and xi_2 are drawn from normal distributions nx and ny at each iteration.\n\n[Paluš2018]: Paluš, M., Krakovská, A., Jakubík, J., & Chvosteková, M. (2018). Causality, dynamical systems and the arrow of time. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(7), 075307. http://doi.org/10.1063/1.5019944\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.AR1Bidir","page":"Predefined systems","title":"CausalityTools.AR1Bidir","text":"AR1Bidir <: DiscreteDefinition\nAR1Bidir(;xi = [0.2, 0.3], a₁ = 0.5, b₁ = 0.7, c_xy = 0.1, c_yx = 0.2,\n    nx = Normal(0, 0.3), ny = Normal(0, 0.3),\n    rng::R = Random.default_rng())\n\nA system consisting of two mutually coupled first order autoregressive processes.\n\nEquations of motion\n\nbeginaligned\nx(t+1) = a_1x + c_yxy + epsilon_x \ny(t+1) = b_1y + c_xyx + epsilon_y\nendaligned\n\nwhere at each time step, epsilon_x and epsilon_y are drawn from independent normal distributions nx and ny, respectively.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.ChaoticMaps3","page":"Predefined systems","title":"CausalityTools.ChaoticMaps3","text":"ChaoticMaps3() <: DiscreteDefinition\nChaoticMaps3(; ui = [0.2, 0.1, 0.3], r = 3.4, c_xy = 0.5, c_xz = 0.5, c_yz = 0.3)\n\nA model consisting of three coupled 1D maps, where x to y and x to z (Chen et al., 2004)[Chen2004].\n\nEquations of motion\n\nbeginaligned\nx(t) = r x(t-1)( 1 - x(t-1)^2 ) e^-x(t-1)^2 \ny(t) = r y(t-1)( 1 - y(t-1)^2 ) e^-y(t-1)^2 + c_xy x(t-1) \nz(t) = r z(t-1)( 1 - z(t-1)^2 ) e^-z(t-1)^2 + c_xz x(t-1) + c_yz y(t-1)\nendaligned\n\nThe parameters r, c_xy and c_yz do not appear in the original paper, but are added here for explorative purposes.\n\n[Chen2004]: Chen, Yonghong, et al. \"Analyzing multiple nonlinear time series with extended Granger causality.\" Physics Letters A 324.1 (2004): 26-35\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Henon2","page":"Predefined systems","title":"CausalityTools.Henon2","text":"Henon2() <: DiscreteDefinition\nHenon2(;u₀ = [0.1, 0.2, 0.2, 0.3], c_xy = 2.0)\n\nA bivariate system consisting of two identical 1D Henon maps with unidirectional forcing X to Y (Krakovská et al., 2018)[Krakovská2018].\n\nEquations of motion\n\nThe equations of motion are\n\nbeginalign*\nx_1(t+1) = 14 - x_1^2(t) + 03x_2(t) \nx_2(t+1) = x_1(t) \ny_1(t+1) = 14 - c_xy x_1(t) y_1(t) + (1-c_xy) y_1^2(t) + 03 y_2(t) \ny_2(t+1) = y_1(t)\nendalign*\n\n[Krakovská2018]: Krakovská, A., Jakubík, J., Chvosteková, M., Coufal, D., Jajcay, N., & Paluš, M. (2018). Comparison of six methods for the detection of causality in a bivariate time series. Physical Review E, 97(4), 042207.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Henon3","page":"Predefined systems","title":"CausalityTools.Henon3","text":"Henon3() <: DiscreteDefinition\nHenon3(; a = 0.1, b = 0.3, c = 0.1, xi = [0.1, 0.2, 0.3])\n\nHenon3 is a DiscreteDefinition definition for a lagged discrete dynamical system consisting of three coupled 1D Henon maps (Papana et al., 2013)[Papana2013].\n\nEquations of motion\n\nbeginalign*\nx_1(t+1) = a - x_1(t)^2 + b x_1(t-2) \nx_2(t+1) = a - c x_1(t) x_2(t)- (1 - c) x_2(t)^2 + b x_2(t-1) \nx_3(t+1) = c x_2(t) x_3(t) - (1 - c) x_3(t)^2 + b x_3(t-1)\nendalign*\n\nHere c is the coupling constant. The system becomes completely synchronized for c = 07. The initial condition xi is repeated over the first two time steps before iteration starts.\n\n[Papana2013]: Papana, A., Kyrtsou, C., Kugiumtzis, D., & Diks, C. (2013). Simulation study of direct causality measures in multivariate time series. Entropy, 15(7), 2635–2661.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Ikeda2","page":"Predefined systems","title":"CausalityTools.Ikeda2","text":"Ikeda2 <: DiscreteDefinition\nIkeda2(; xi = [0.19, 0.21], c_xy = 1.0, c_yx = 1.0, a = 0.8, b = 12, c = 0.9,\n    r₁ = 0.2, r₂ = 0.15, σ = 0.05, rng = Random.default_rng())\n\nInitialise a discrete two-dimensional Ikeda map system, adapted from Cao et al. (1997)[Cao1997], by adding a noise term and allowing the influences from x to y (c_xy) and from y to x (c_yx) to be adjusted.\n\nThe difference equations are\n\nbeginalign*\nx(t+1) = 1 + mu(x cos(theta) - c_yx y sin(theta)) -\nmin( dfracsigma xi_t^(1)(1-x) xi_t^(2)) \ny(t+1) = mu(y cos(theta) - c_xy x sin(theta)) -\nmin(dfracsigma zeta_t^(1)(1-y) zeta_t^(2))\nendalign*\n\n[Cao1997]: Cao, Liangyue, Alistair Mees, and Kevin Judd. \"Modeling and predicting non-stationary time series.\" International Journal of Bifurcation and Chaos 7.08 (1997): 1823-1831.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.ChaoticNoisyLinear2","page":"Predefined systems","title":"CausalityTools.ChaoticNoisyLinear2","text":"ChaoticNoisyLinear2 <: DiscreteDefinition\nChaoticNoisyLinear2(; xi = [0.1, 0.2], c = 0.5,\n    nx = Normal(0, 0.05), ny = Normal(0, 0.05),\n    rng = Random.default_rng())\n\nA bivariate system of two chaotic maps that are linearly coupled from x → y with coupling strength c.\n\nDefinition\n\nbeginalign*\nx(t+1) = 34 x(t) (1 - x(t)^2) e^-x(t)^2 + 08x(t-1) + xi_x \ny(t+1) = 34 y(t) (1 - y(t)^2) e^-y(t)^2 + 08y(t-1) + xi_y + c x(t-2)\nendalign*\n\nProcess noise xi_x and xi_y is drawn at each iteration from nx and ny.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Logistic2Unidir","page":"Predefined systems","title":"CausalityTools.Logistic2Unidir","text":"Logistic2Unidir <: DiscreteDefinition\nLogistic2Unidir(; xi = [0.5, 0.5], c_xy = 0.1, σ_xy = 0.05, r₁ = 3.78, r₂ = 3.66,\n    rng = Random.default_rng())\n\nA bivariate system consisting of two 1D noisy logistic maps which are undirectionally coupled x → y (Diego et al., 2019)[Diego2019].\n\nEquations of motion\n\nThe equations of motion are\n\nbeginalign*\nx(t+1) = r_1 x(t)(1 - x(t)) \ny(t+1) = r_2 f(xy)(1 - f(xy))\nendalign*\n\nwith\n\nbeginaligned\nf(xy) = dfracy + fracc_xy(x xi )21 + fracc_xy2(1+ sigma_xy )\nendaligned\n\nThe parameter c_xy controls how strong the dynamical forcing is. If σ > 0, dynamical noise masking the influence of  x on y equivalent to sigma_xy cdot xi is added at each iteration. Here,xi is a draw from a flat distribution on 0 1. Thus, setting σ_xy = 0.05 is equivalent to add dynamical noise corresponding to a maximum of 5  of the possible range of values of the logistic map.\n\n[Diego2019]: Diego, David, Kristian Agasøster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Logistic2Bidir","page":"Predefined systems","title":"CausalityTools.Logistic2Bidir","text":"Logistic2Bidir() <: DiscreteDefinition\nLogistic2Bidir(; ui = [0.5, 0.5], c_xy = 0.1, c_yx = 0.1, r₁ = 3.78, r₂ = 3.66,\n    σ_xy = 0.05, σ_yx = 0.05,\n    rng = Random.default_rng())\n\nA bivariate system consisting of two 1D noisy logistic maps which are bidirectionally interacting (Diego et al., 2019)[Diego2019].\n\nEquations of motion\n\nbeginalign*\nx(t+1) = r_1 f_yx^t(1 - f_yx^t) \ny(t+1) = r_2 f_xy^t(1 - f_xy^t) \nf_xy^t = dfracy(t) + c_xy(x(t) + sigma_xy xi_xy^t )1 + c_xy (1 + sigma_xy ) \nf_yx^t = dfracx(t) + c_yx(y(t) + sigma_yx xi_yx^t )1 + c_yx (1 + sigma_yx )\nendalign*\n\nHere, the coupling strength c_xy controls how strongly species x influences species y, and vice versa for c_yx. To simulate time-varying influence of unobserved processes, we use the dynamical noise terms xi_xy^t and xi_yx^t, drawn from a uniform distribution with support on 0 1. If sigma_xy  0, then the influence of x on y is masked by dynamical noise equivalent to sigma_xy xi_xy^t at the t-th iteration of the map, and vice versa for sigma_yx.\n\n[Diego2019]: Diego, David, Kristian Agasøster Haaga, and Bjarte Hannisdal. \"Transfer entropy computation using the Perron-Frobenius operator.\" Physical Review E 99.4 (2019): 042212.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Logistic3CommonDriver","page":"Predefined systems","title":"CausalityTools.Logistic3CommonDriver","text":"Logistic3CommonDriver() <: DiscreteDefinition\nLogistic3CommonDriver(; u₀ = [0.1, 0.2, 0.3],\n    r = 4.0, σx = 0.05, σy = 0.05, σz = 0.05,\n    rng = Random.default_rng())\n\nA discrete dynamical system consisting of three coupled 1D logistic maps representing the response of two independent dynamical variables to the forcing from a common driver (Runge, 2018)[Runge2018]. The dynamical influence goes in the directions Z to X and Z to Y.\n\nEquations of motion\n\nThe equations of motion are\n\nbeginalign*\nx(t+1) = (x(t)(r - r x(t) - z(t) + sigma_x eta_x)) mod 1 \ny(t+1) = (y(t)(r - r y(t) - z(t) + sigma_y eta_y)) mod 1 \nz(t+1) = (z(t)(r - r z(t) + sigma_z eta_z)) mod 1\nendalign*\n\nDynamical noise may be added to each of the dynamical variables by tuning the parameters σz, σx and σz. Default values for the parameters r₁, r₂ and r₃ are set such that the system exhibits chaotic behaviour, with r₁ = r₂ = r₃ = 4.\n\n[Runge2018]: Runge, Jakob. Causal network reconstruction from time series: From theoretical assumptions to practical estimation, Chaos 28, 075310 (2018); doi: 10.1063/1.5025050\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Logistic4Chain","page":"Predefined systems","title":"CausalityTools.Logistic4Chain","text":"Logistic4Chain <: DiscreteDefinition\nLogistic4Chain(; xi = rand(4),\n    rx = 3.9, ry = 3.6, rz = 3.6, rw = 3.8,\n    cxy = 0.4, cyz = 0.4, cyw = 0.35,\n    rng = Random.default_rng())\n\nA variant of Logistic2Bidir where four variables X, Y, Z, W are coupled in a chain X → Y → Z → W with dynamical noise.\n\nDescription\n\nThe equations of motion are\n\nbeginalign*\nx(t+1) = r_x x(t)(1 - x(t))  \ny(t+1) = r_y f_xy^t(1 - f_xy^t) \nz(t+1) = r_z f_yz^t(1 - f_yz^t) \nw(t+1) = r_w f_zw^t(1 - f_zw^t) \nf_xy^t = dfracy(t) + c_xy(x(t) + sigma_xy xi_xy^t )1 + c_xy (1 + sigma_xy ) \nf_yz^t = dfracz(t) + c_yz(y(t) + sigma_yz xi_yz^t )1 + c_yz (1 + sigma_yz ) \nf_zw^t = dfracw(t) + c_zw(z(t) + sigma_zw xi_zw^t )1 + c_zw (1 + sigma_zw )\nendalign*\n\nwhere c_{xy}, c_{yz}, c_{zw} controls the coupling strength from x to y, y to z, and z to w, respectively. The magnitude of dynamical noise in these couplings are controlled by sigma_xy, sigma_yz, and sigma_zw, respectively. xi_xy, xi_yz, and xi_zw are noise terms that each iteration are drawn from independent uniform distributions over the unit interval.\n\nWith default parameters and all dynamical noise terms set to zero, this is the system from Ye et al. (2015)[Ye2015] (but note that for some initial conditions, this system wanders off to pm infty for some of the variables).\n\n[Ye2015]: Ye, Hao, et al. \"Distinguishing time-delayed causal interactions using convergent cross mapping.\" Scientific reports 5 (2015): 14750\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Nonlinear3","page":"Predefined systems","title":"CausalityTools.Nonlinear3","text":"Nonlinear3 <: DiscreteDefinition\nNonlinear3(; xi = rand(3),\n    σ₁ = 1.0, σ₂ = 1.0, σ₃ = 1.0,\n    a₁ = 3.4, a₂ = 3.4, a₃ = 3.4,\n    b₁ = 0.4, b₂ = 0.4, b₃ = 0.4,\n    c₁₂ = 0.5, c₂₃ = 0.3, c₁₃ = 0.5,\n    rng = Random.default_rng())\n\nA 3d nonlinear system with nonlinear couplings x_1 to x_2, x_2 to x_3 and x_1 to x_3. Modified from Gourévitch et al. (2006)[Gourévitch2006].\n\nEquations of motion\n\nbeginaligned\nx_1(t+1) = a_1 x_1 (1-x_1(t))^2  e^-x_2(t)^2 + 04 xi_1(t) \nx_2(t+1) = a_1 x_2 (1-x_2(t))^2  e^-x_2(t)^2 + 04 xi_2(t) + b x_1 x_2 \nx_3(t+1) = a_3 x_3 (1-x_3(t))^2  e^-x_3(t)^2 + 04 xi_3(t) + c x_2(t) + d x_1(t)^2\nendaligned\n\n[Gourévitch2006]:     Gourévitch, B., Le Bouquin-Jeannès, R., & Faucon, G. (2006). Linear and nonlinear     causality between signals: methods, examples and neurophysiological     applications. Biological Cybernetics, 95(4), 349–369.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Peguin2","page":"Predefined systems","title":"CausalityTools.Peguin2","text":"Peguin2 <: DiscreteDefinition\nPeguin2(; xi = [0.5, 0.4], σ₁ = 0.1, σ₂ = 0.1,\n    p₁ = 0.7, p₂ = 0.1, p₃ = 0.4, p₄ = 2.4, p₅ = 0.9, p₆ = 4)\n\nA 2D discrete autoregressive system with nonlinear, nontrivial coupling from [1] . This system is from Péguin-Feissolle & Teräsvirta (1999)[^Péguin-Feissolle1999], and was also studied in Chávez et al. (2003)[Chávez2003].\n\nDescription\n\nThe system is defined by the equations\n\nbeginalign*\nx(t+1) = p_2 + p_3 x(t-2) + c_yxdfracp_4 - p_5 y(t-3)1 + e^-p_6 y(t-3) + xi_1(t) \ny(t+1) = p_1 y(t) + xi_2(t)\nendalign*\n\nHere, xi_12(t) are two independent normally distributed noise processes with zero mean and standard deviations sigma_1 and sigma_2. The xi_12(t) terms represent dynamical noise. The parameters of the original system are here tweakable.\n\n[^Péguin-Feissolle1999]:     Péguin-Feissolle, A., & Teräsvirta, T. (1999). A General Framework for     Testing the Granger Noncausaality Hypothesis. Universites d’Aix-Marseille II     et III. https://www.amse-aixmarseille.fr/sites/default/files/_dt/greqam/99a42.pdf\n\n[Chávez2003]: Chávez, M., Martinerie, J., & Le Van Quyen, M. (2003). Statistical assessment of nonlinear causality: application to epileptic EEG signals. Journal of Neuroscience Methods, 124(2), 113–128. doi:10.1016/s0165-0270(02)00367-9 https://www.sciencedirect.com/science/article/pii/S0165027002003679\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.UlamLattice","page":"Predefined systems","title":"CausalityTools.UlamLattice","text":"UlamLattice <: DiscreteDefinition\nUlamLattice(; D::Int = 10; ui = sin.(1:10), ε::Real = 0.10)\n\nA lattice of D unidirectionally coupled ulam maps[Schreiber2000] defined as\n\nx^m_t+1 = f(epsilon x^m-1_t + (1 - epsilon) x_t^m)\n\nwhere m = 1 2 ldots D and f(x) = 2 - x^2. In this system, information transfer happens only in the direction of increasing m.\n\n[Schreiber2000]: Schreiber, Thomas. \"Measuring information transfer.\" Physical review letters 85.2 (2000): 461.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Var1","page":"Predefined systems","title":"CausalityTools.Var1","text":"Var1 <: DiscreteDefinition\nVar1(; xi = [0.5, 0.5, 0.5],\n    a = 0.5, θ = Normal(0, 1.0), η = Normal(0, 0.2), ϵ = Normal(0, 0.3),\n    rng = Random.default_rng())\n\nA discrete vector autoregressive system where X₁ → X₂ → X₃.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Verdes3","page":"Predefined systems","title":"CausalityTools.Verdes3","text":"Verdes3 <: DiscreteDefinition\nVerdes3(; ui = [0.1, 0.15, 0.2], ωy = 315, ωz = 80, σx = 0.0, σy = 0.0, σz = 0.0)\n\nA 3D system where the response X is a highly nonlinear combination of Y and Z (Verdes, 2005)[Verdes2005]. The forcings Y and Z involve sines and cosines, and have different periods, which controlled by ωy and ωz.\n\nbeginalign*\nx(t+1) = dfracy(t)(18y(t) - 27y(t)^2 + 10)2 + z(t)(1-z(t)) + ηx \ny(t+1) = dfrac(1 - dfraccos(2pi)omega yt)2 + ηy \nz(t+1) = dfrac(1 - dfracsin(2pi)omega zt)2 + ηz\nendalign*\n\nwhere ηx, ηy, ηz is gaussian noise with mean 0 and standard deviation σx, σy and σz.\n\n[Verdes2005]: Verdes, P. F. \"Assessing causality from multivariate time series.\" Physical Review E 72.2 (2005): 026222.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#Continuous-systems","page":"Predefined systems","title":"Continuous systems","text":"","category":"section"},{"location":"coupled_systems/","page":"Predefined systems","title":"Predefined systems","text":"ChuaCircuitsBidir6\nChuaScrollSine3\nHindmarshRose3\nLorenzBidir6\nLorenzForced9\nMediatedLink9\nRepressilator6\nRosslerBidir6\nRosslerForced9\nRosslerLorenzUnidir6\nThomas3","category":"page"},{"location":"coupled_systems/#CausalityTools.ChuaCircuitsBidir6","page":"Predefined systems","title":"CausalityTools.ChuaCircuitsBidir6","text":"ChuaCircuitsBidir6 <: ContinuousDefinition\nChuaCircuitsBidir6(;u₀ = [0.1, 0.1, 0.2, 0.15, 0.15, 0.22],\n    α₁ = 7.0, α₂ = 7.0, β₁ = 14.286, β₂ = 14.286,\n    F₁ = 1.5, F₂ = 1.5, ω₁ = 3.0, ω₂ = 3.0,\n    n1 = Normal(0, 0.1),\n    n2 = Normal(0, 0.1),\n    c12 = 0.1, c21 = 0.1, m₀ = -1/7, m₁ = 2/7)\n\nInitialize a bidirectionally coupled system consisting of two driven Chua circuits, X₁ and X₂ (Murali &  Lakshmanan, 1993)[Murali1993].\n\nDescription\n\nThe subsystems are mutually coupled by a linear resistor, where ϵ12 controls the influence of X₁ on X₂, and c21 controls the influence of X₂ on X₁. The parameters for the subsystems are set equal to each other, as in the original paper, but can here be tuned individually for each subsystem.\n\nbeginalign*\ndfracdx_1dt = alpha_1(y_1 h(x_1)) - alpha_1 epsilon(x_1 - x_2) \ndfracdy_1dt = x_1 - y_1 + z_1 \ndfracdz_1dt = -beta_1 y_1 + F_1 sin(omega_1 t) + epsilon_1 \ndfracdx_2dt = alpha_2 (y_2 h(x_2)) - alpha_2 c_12(x_1 - x_2) \ndfracdy_2dt = x_2 - y_2 + z_2 \ndfracdz_2dt = -beta_2 y_2 + F_2 sin(omega_2 t) + epsilon_2\nendalign*\n\nwhere h(x) = M_1x + 05(M_0 - M_1)(x+1 - x - 1) and epsilon_1 epsilon_2 are noise terms that at each integration step is drawn independently from the normal distributions n1 and n2, respectively.\n\n[Murali1993]: Murali, K., and M. Lakshmanan. \"Chaotic dynamics of the driven Chua's circuit.\" IEEE Transactions on Circuits and Systems I Fundamental Theory and Applications 40.11 (1993): 836-840.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.ChuaScrollSine3","page":"Predefined systems","title":"CausalityTools.ChuaScrollSine3","text":"ChuaScrollSine3 <: ContinuousDefinition\nChuaScrollSine3(; xi = [0.1, 0.2, 0.3],\n    α = 10.814, β = 14, γ = 0, a = 1.3, b = 0.11, c = 2,\n    nx = Normal(0.0, 0.01),\n    ny = Normal(0.0, 0.01)\n    nz = Normal(0.0, 0.01))\n\nAn adjusted Chua system giving rise to n-scroll attractors (Tang et al., 2001)[Tang2001].\n\nDescription\n\nThe dynamics is generated by the following vector field\n\nbeginalign*\ndotx = alpha (y - fx) + eta x \ndoty = x - y + z + eta y \ndotz = -beta y - gamma z + eta z\nendalign*\n\nwhere eta x, eta z, and eta z are drawn independently from normal distributions nx, ny and nz each iteration.\n\nfx is given by the following conditions:\n\nn::Int = c + 1\n\nif x >= 2*a*c\n    fx = (b*pi/2*a)*(x - 2*a*c)\nelseif -2*a*c < x < 2*a*c\n    d = ifelse(isodd(n), pi, 0)\n    fx = -b*sin((pi*x/2*a) + d)\nelseif x <= -2*a*c\n    fx = (b*pi/2*a)*(x + 2*a*c)\nend\n\n[Tang2001]: Tang, Wallace KS, et al. \"Generation of n-scroll attractors via sine function.\" IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications 48.11 (2001): 1369-1372.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.HindmarshRose3","page":"Predefined systems","title":"CausalityTools.HindmarshRose3","text":"HindmarshRose3 <: ContinuousDefinition\nHindmarshRose3(; xi = [0.1, 0.2, 0.3, p)\n\nInitialise a Hindmarsh-Rose system, which is a model of neuronal spiking.\n\nDescription\n\nbeginalign*\ndfracdxdt = y + phi(x) - z + I \ndfracdydt = psi(x) - y \ndfracdzdt = rs(x - x_R) - z\nendalign*\n\nwhere\n\nbeginaligned\nphi(x) = -ax^3+bx^2\npsi(x) = c - dx^2\nendaligned\n\nIf parameters other than the defaults are to be used, they must be provided as a vector [a, b, c, d, r, s, xᵣ, I].\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.LorenzBidir6","page":"Predefined systems","title":"CausalityTools.LorenzBidir6","text":"LorenzBidir6 <: ContinuousDefinition\nLorenzBidir6(; xi = [0.1, 0.05, 0.2, 0.2, 0.25, 0.3],\n    c_xy = 0.2, c_yx = 0.2,\n    a₁ = 10, a₂ = 28, a₃ = 8/3,\n    b₁ = 10, b₂ = 28, b₃ = 9/3)\n\nA bidirectionally coupled Lorenz-Lorenz system, where each subsystem is a 3D Lorenz system (Amigo & Hirata, 2018)[Amigó2018].\n\nDescription\n\nThe dynamics is generated by the following vector field\n\nbeginalign*\ndotx_1 = -a_1 (x_1 - x_2) + c_yx(y_1 - x_1) \ndotx_2 = -x_1 x_3 + a_2 x_1 - x_2 \ndotx_3 = x_1 x_2 - a_3 x_3 \ndoty_1 = -b_1 (y_1 - y_2) + c_xy (x_1 - y_1) \ndoty_2 = -y_1 y_3 + b_2 y_1 - y_2 \ndoty_3 = y_1 y_2 - b_3 y_3\nendalign*\n\nDefault values for the parameters a₁, a₂, a₃, b₁, b₂, b₃ are as in [Amigó2018].\n\n[Amigó2018]: Amigó, José M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.LorenzForced9","page":"Predefined systems","title":"CausalityTools.LorenzForced9","text":"LorenzForced9{V} <: ContinuousDefinition\nLorenzForced9(; xi = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n    c_xy = 0.1, c_yx = 0.1,\n    c_zx = 0.05, c_zy = 0.05,\n    a₁ = 10, a₂ = 28, a₃ = 8/3,\n    b₁ = 10, b₂ = 28, b₃ = 8/3,\n    c₁ = 10, c₂ = 28, c₃ = 8/3)\n\nA system consisting of two bidirectionally coupled 3D Lorenz systems forced by an external 3D Lorenz system (Amigó & Hirata, 2018).\n\nDescription\n\nThe dynamics is generated by the following vector field\n\nbeginalign*\ndotx_1 = - a_1 (x_1 - x_2) + c_yx(y_1 - x_1) + c_zx(z_1 - x_1) \ndotx_2 = - x_1 x_3 + a_2 x_1 - x_2 \ndotx_3 = x_1 x_2 - a_3 x_3 \ndoty_1 = -b_1 (y_1 - y_2) + c_xy (x_1 - y_1) + c_zy(z_1 - y_1) \ndoty_2 = - y_1 y_3 + b_2 y_1 - y_2 \ndoty_3 = y_1 y_2 - b_3 y_3 \ndotz_1 = - c_1 (z_1 - z_2) \ndotz_2 = - z_1 z_3 + c_2 z_1 - z_2 \ndotz_3 = z_1 z_2 - c_3 z_3\nendalign*\n\n[Amigó2018]: Amigó, José M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.MediatedLink9","page":"Predefined systems","title":"CausalityTools.MediatedLink9","text":"MediatedLink9 <: ContinuousDefinition\nMediatedLink9(; xi = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n    ωx = 1.0, ωy = 1.015, ωz = 0.985,\n    k = 0.15, l = 0.2, m = 10.0,\n    c = 0.06) → ContinuousDynamicalSystem\n\nA three-subsystem dynamical system where X and Y are driven by Z (Krakovská, 2018)[Krakovská2018].\n\nDescription\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndx_1 = -omega_x x_2 - x_3 + c*(z_1 - x_1) \ndx_2 = omega_x x_1 + k*x_2  \ndx_3 = l + x_3(x_1 - m)  \ndy_1 = -omega_y y_2 - y_3 + c*(z_1 - y_1)  \ndy_2 = omega_y y_1 + k*y_2  \ndy_3 = l + y_3(y_1 - m)  \ndz_1 = -omega_z z_2 - z_3  \ndz_2 = omega_z z_1 + k*z_2  \ndz_3 = l + z_3(z_1 - m)\nendaligned\n\nAt the default value of the coupling constant c = 0.06, the responses X and Y are already synchronized to the driver Z.\n\n[Krakovská2018]: Krakovská, Anna, et al. \"Comparison of six methods for the detection of causality in a bivariate time series.\" Physical Review E 97.4 (2018): 042207\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Repressilator6","page":"Predefined systems","title":"CausalityTools.Repressilator6","text":"Repressilator6 <: ContinuousDefinition\nRepressilator6(; xi = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], α = 10.0, α₀ = 0.0, β = 100.0,\n    n = 2) → ContinuousDynamicalSystem\n\nA six-dimensional repressilator (or repression-driven oscillator) from Elowitz & Leibler (2000)[Elowitz2000].\n\nUsed in Sun & Bollt (2014)[Sun2014] to study the performance of the causation entropy algorithm.\n\nDescription\n\nbeginalign*\ndfracdm_1dt = -m1 + dfracalpha1 + p_3^n + alpha_0 \ndfracdm_2dt = -m2 + dfracalpha1 + p_1^n + alpha_0 \ndfracdm_3dt = -m3 + dfracalpha1 + p_2^n + alpha_0 \ndfracdp_1dt = -beta(p_1 - m_1) \ndfracdp_2dt = -beta(p_2 - m_2) \ndfracdp_3dt = -beta(p_3 - m_3) \nendalign*\n\n[Elowitz2000]: Elowitz, M. B., & Leibler, S. (2000). A synthetic oscillatory network of transcriptional regulators. Nature, 403(6767), 335-338.\n\n[Sun2014]: Sun, J., Cafaro, C., & Bollt, E. M. (2014). Identifying the coupling structure in complex systems through the optimal causation entropy principle. Entropy, 16(6), 3416-3433.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.RosslerBidir6","page":"Predefined systems","title":"CausalityTools.RosslerBidir6","text":"RosslerBidir6 <: ContinuousDefinition\nRosslerBidir6(; xi = [0.1, 0.1, 0.2, 0.3, 0.3, 0.4],\n    a = 0.1, b = 0.1, c = 14.0, ϵ₁ = 0.0, ϵ₂ = 0.0,\n    ω₁ = 1 + 0.015, ω₂ = 1 - 0.015)\n\nA bidirectionally coupled 6D Rossler system from Krakovská et al. (2018)[Krakovská2018].\n\nDescription\n\nThe system consists of two separate subsystems, each being a 3D Rossler attractor. The subsystems are bidirectionally coupled, influencing each other through variables x_1 and `x_2.\n\nbeginalign*\ndfracdx_1dt = omega_1 (-y_1) - z_1 + c_21*(x_1 - x_2) \ndfracdy_1dt = omega_1 x_1 + a y_1 \ndfracdz_1dt = b + z_1 (x_1 - c) \ndfracdx_2dt = omega_2 (-y_2) - z_2 + c_12 (x_2 - x_1) \ndfracdy_2dt = omega_2*x_2 + a*y_2 \ndfracdz_2dt = b + z_2 (x_2 - c) \nendalign*\n\nwith c_12 geq 0 and c_21 geq 0.\n\n[Krakovská2018]: Krakovská, A., Jakubík, J., Chvosteková, M., Coufal, D., Jajcay, N., & Paluš, M. (2018). Comparison of six methods for the detection of causality in a bivariate time series. Physical Review E, 97(4), 042207.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.RosslerForced9","page":"Predefined systems","title":"CausalityTools.RosslerForced9","text":"RosslerForced9 <: ContinuousDefinition\nRosslerForced9(; xi = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n    ω₁ = 1.015, ω₂ = 0.985, ω₃ = 0.95,\n    c_xy = 0.1, c_yx = 0.1,\n    c_zx = 0.05, c_zy = 0.05,\n    a₁ = 0.15, a₂ = 0.2, a₃ = 10,\n    b₁ = 0.15, b₂ = 0.2, b₃ = 10,\n    c₁ = 0.15, c₂ = 0.2, c₃ = 10)\n\nEquations of motion for a system consisting of three coupled 3D Rössler systems (X, Y, Z), giving a 9D system (Amigó & Hirata, 2018)[Amigó2018].\n\nDescription\n\nThe dynamics is generated by the following vector field\n\nbeginaligned\ndotx_1 = -omega_1 (x_2 + x_3) + c_yx(y_1 - x_1) + c_zx(z_1 - x_1) \ndotx_2 = omega_1 x_1 + a_1 x_2 \ndotx_3 = a_2 + x_3 (x_1 - a_3) \ndoty_1 = -omega_1 (y_2 + y_3) + c_xy(x_1 - y_1) + c_zy(z_1 - y_1) \ndotx_2 = omega_2 y_1 + b_1 y_2 \ndotx_3 = b_2 + x_3 (y_1 - b_3) \ndoty_1 = -omega_2 (z_2  + z_3) \ndotx_2 = omega_2 z_1 + c_1 z_2 \ndotx_3 = c_2 + z_3 (z_1 - c_3)\nendaligned\n\nThe external system Z influences both X and Y (controlled by c_zx and c_zy). Simultaneously, the subsystems  X and Y bidirectionally influences each other (controlled by c_xy and c_yx). The X and Y subsystems are mostly synchronized for c_xy > 0.1 or c_yx > 0.1.\n\n[Amigó2018]: Amigó, José M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.RosslerLorenzUnidir6","page":"Predefined systems","title":"CausalityTools.RosslerLorenzUnidir6","text":"RosslerLorenzUnidir6 <: ContinuousDefinition\nRosslerLorenzUnidir6(; xi = [0.1, 0.2, 0.3, 0.05, 0.1, 0.15],\n    a₁ = 6, a₂ = 6, a₃ = 2.0, b₁ = 10, b₂ = 28, b₃ = 8/3, c_xy = 1.0)\n\nInitialise a Rössler-Lorenz system consisting of two independent 3D subsystems: one Rössler system and one Lorenz system. They are coupled such that the second component (x₂) of the Rössler system unidirectionally forces the second component (y₂) of the Lorenz system.\n\nThe parameter c_xy controls the coupling strength. The implementation here also allows for tuning the parameters of each subsystem by introducing the constants a₁, a₂, a₃, b₁, b₂, b₃. Default values for these parameters are as in [1].\n\nEquations of motion\n\nThe dynamics is generated by the following vector field\n\nDescription\n\nbeginalign*\ndot x_1 = -a_1(x_2 + x_3) \ndot x_2 = a_2(x_1 + a_2x_2) \ndot x_3 = a_1(a_2 + x_3(x_1 - a_3)) \ndot y_1 = b_1(y_2 - y_1) \ndot y_2 = y_1(b_2 - y_3) - y_2 +c_xy(x_2)^2 \ndot y_3 = y_1 y_2 - b_3y_3\nendalign*\n\nwith the coupling constant c_xy geq 0.\n\n\n\n\n\n","category":"type"},{"location":"coupled_systems/#CausalityTools.Thomas3","page":"Predefined systems","title":"CausalityTools.Thomas3","text":"Thomas3 <: ContinuousDefinition\nThomas3(; xi = [0.11, 0.09, 0.10], b = 0.20)\n\nThomas' cyclically symmetric attractor is a continuous dynamical system with three variables. It has a single free parameter b, for which interesting behaviour occurs when b ∈ (0, 1). In particular, the system is chaotic whenever b < 0.20.\n\nDefinition\n\nbeginalign*\ndfracdxdt = sin(y) - bx \ndfracdydt = sin(z) - by \ndfracdzdt = sin(x) - bz\nendalign*\n\n\n\n\n\n","category":"type"},{"location":"examples/examples_conditional_entropy/#examples_condentropy","page":"Conditional entropy","title":"Conditional entropy","text":"","category":"section"},{"location":"examples/examples_conditional_entropy/#Discrete:-example-from-Cover-and-Thomas","page":"Conditional entropy","title":"Discrete: example from Cover & Thomas","text":"","category":"section"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"This is essentially example 2.2.1 in Cover & Thomas (2006), where they use the following contingency table as an example. We'll take their example and manually construct a ContingencyMatrix that we can use to compute the conditional entropy. The ContingencyMatrix constructor takes the probabilities as the first argument and the raw frequencies as the second argument. Note also that Julia is column-major, so we need to transpose their example. Then their X is in the first dimension of our contingency matrix (along columns) and their Y is our second dimension (rows).","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"using CausalityTools\nfreqs_yx = [1//8 1//16 1//32 1//32; \n    1//16 1//8  1//32 1//32;\n    1//16 1//16 1//16 1//16; \n    1//4  0//1  0//1  0//1];\nfreqs_xy = transpose(freqs_yx);\nprobs_xy = freqs_xy ./ sum(freqs_xy)\nc_xy = ContingencyMatrix(probs_xy, freqs_xy)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"The marginal distribution for x (first dimension) is","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probabilities(c_xy, 1)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"The marginal distribution for y (second dimension) is","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probabilities(c_xy, 2)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"And the Shannon conditional entropy H^S(X  Y)","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"ce_x_given_y = entropy_conditional(CEShannon(), c_xy) |> Rational","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"This is the same as in their example. Hooray! To compute H^S(Y  X), we just need to flip the contingency matrix.","category":"page"},{"location":"examples/examples_conditional_entropy/","page":"Conditional entropy","title":"Conditional entropy","text":"probs_yx = freqs_yx ./ sum(freqs_yx);\nc_yx = ContingencyMatrix(probs_yx, freqs_yx);\nce_y_given_x = entropy_conditional(CEShannon(), c_yx) |> Rational","category":"page"},{"location":"examples/examples_independence/#examples_independence","page":"Independence testing","title":"Independence testing","text":"","category":"section"},{"location":"examples/examples_independence/#quickstart_jddtest","page":"Independence testing","title":"JointDistanceDistributionTest","text":"","category":"section"},{"location":"examples/examples_independence/#Bidirectionally-coupled-logistic-maps","page":"Independence testing","title":"Bidirectionally coupled logistic maps","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Let's use the built-in logistic2_bidir discrete dynamical system to create a pair of bidirectionally coupled time series and use the JointDistanceDistributionTest to see if we can confirm from observed time series that these variables are bidirectionally coupled. We'll use a significance level of 1 - α = 0.99, i.e. α = 0.01.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We start by generating some time series and configuring the test.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nsys = system(Logistic2Bidir(c_xy = 0.5, c_yx = 0.4))\nx, y = columns(trajectory(sys, 2000, Ttr = 10000))\nmeasure = JointDistanceDistribution(D = 5, B = 5)\ntest = JointDistanceDistributionTest(measure)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Now, we test for independence in both directions.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, y, x)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the null hypothesis is rejected in both directions at the pre-determined  significance level, and hence we detect directional coupling in both directions.","category":"page"},{"location":"examples/examples_independence/#Non-coupled-logistic-maps","page":"Independence testing","title":"Non-coupled logistic maps","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"What happens in the example above if there is no coupling?","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"sys = system(Logistic2Bidir(c_xy = 0.00, c_yx = 0.0))\nx, y = columns(trajectory(sys, 1000, Ttr = 10000));\nrxy = independence(test, x, y)\nryx = independence(test, y, x)\npvalue(rxy), pvalue(ryx)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"At significance level 0.99, we can't reject the null in either direction, hence there's not enough evidence in the data to suggest directional coupling.","category":"page"},{"location":"examples/examples_independence/#[LocalPermutationTest](@ref)","page":"Independence testing","title":"LocalPermutationTest","text":"","category":"section"},{"location":"examples/examples_independence/#example_localpermtest_cmishannon","page":"Independence testing","title":"Conditional mutual information (Shannon, differential)","text":"","category":"section"},{"location":"examples/examples_independence/#Chain-of-random-variables-X-\\to-Y-\\to-Z","page":"Independence testing","title":"Chain of random variables X to Y to Z","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we'll create a three-variable scenario where X and Z are connected through Y, so that I(X Z  Y) = 0 and I(X Y  Z)  0. We'll test for conditional independence using Shannon conditional mutual information (CMIShannon). To estimate CMI, we'll use the Kraskov differential entropy estimator, which naively computes CMI as a sum of entropy terms without guaranteed bias cancellation.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\n\nX = randn(1000)\nY = X .+ randn(1000) .* 0.4\nZ = randn(1000) .+ Y\nx, y, z = StateSpaceSet.((X, Y, Z))\ntest = LocalPermutationTest(CMIShannon(base = 2), Kraskov(k = 10), nshuffles = 30)\ntest_result = independence(test, x, y, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We expect there to be a detectable influence from X to Y, if we condition on Z or not, because Z doesn't influence neither X nor Y. The null hypothesis is that the first two variables are conditionally independent given the third, which we reject with a very low p-value. Hence, we accept the alternative hypothesis that the first two variables X and Y. are conditionally dependent given Z.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test_result = independence(test, x, z, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we cannot reject the null hypothesis that X and Z are conditionally independent given Y, because Y is the variable that transmits information from X to Z.","category":"page"},{"location":"examples/examples_independence/#example_localpermtest_teshannon","page":"Independence testing","title":"Transfer entropy (Shannon, differential)","text":"","category":"section"},{"location":"examples/examples_independence/#Chain-of-random-variables-X-\\to-Y-\\to-Z-to-W","page":"Independence testing","title":"Chain of random variables X to Y to Z to W","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we demonstrate LocalPermutationTest with the TEShannon measure with default parameters and the FPVP estimator. We'll use a system of four coupled logistic maps that are linked X → Y → Z → W.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nusing Random; rng = Random.default_rng()\ns = system(Logistic4Chain(; xi = rand(4)))\nx, y, z, w = columns(trajectory(s, 2000))\ntest = LocalPermutationTest(TEShannon(), FPVP(), nshuffles = 50)\ntest_result = independence(test, x, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"There is significant transfer entropy from X → Z. We should expect this transfer entropy to be non-significant when conditioning on Y, because all information from X to Z is transferred through Y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test_result = independence(test, x, z, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we cannot reject the null hypothesis that X and Z are conditionally independent given Y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The same goes for variables one step up the chain","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test_result = independence(test, y, w, z)","category":"page"},{"location":"examples/examples_independence/#examples_surrogatetest","page":"Independence testing","title":"SurrogateTest","text":"","category":"section"},{"location":"examples/examples_independence/#examples_surrogatetest_distancecorrelation","page":"Independence testing","title":"Distance correlation","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nx = randn(1000)\ny = randn(1000) .+ 0.5x\nindependence(SurrogateTest(DistanceCorrelation()), x, y)","category":"page"},{"location":"examples/examples_independence/#examples_surrogatetest_partialcorrelation","page":"Independence testing","title":"Partial correlation","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nx = randn(1000)\ny = randn(1000) .+ 0.5x\nz = randn(1000) .+ 0.8y\nindependence(SurrogateTest(PartialCorrelation()), x, z, y)","category":"page"},{"location":"examples/examples_independence/#examples_surrogatetest_mishannon_categorical","page":"Independence testing","title":"Mutual information (MIShannon, categorical)","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"In this example, we expect the preference and the food variables to be independent.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\n# Simulate \nn = 1000\npreference = rand([\"yes\", \"no\"], n)\nfood = rand([\"veggies\", \"meat\", \"fish\"], n)\ntest = SurrogateTest(MIShannon(), Contingency())\nindependence(test, preference, food)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, there's not enough evidence to reject the null hypothesis that the variables are independent.","category":"page"},{"location":"examples/examples_independence/#examples_surrogatetest_cmishannon_categorical","page":"Independence testing","title":"Conditional mutual information (CMIShannon, categorical)","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Here, we simulate a survey at a ski resort. The data are such that the place a person grew up is associated with how many times they fell while going skiing. The control happens through an intermediate variable preferred_equipment, which indicates what type of physical activity the person has engaged with in the past. Some activities like skateboarding leads to better overall balance, so people that are good on a skateboard also don't fall, and people that to less challenging activities fall more often.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We should be able to reject places ⫫ experience, but not reject places ⫫ experience | preferred_equipment.  Let's see if we can detect these relationships using (conditional) mutual information.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nn = 10000\n\nplaces = rand([\"city\", \"countryside\", \"under a rock\"], n);\npreferred_equipment = map(places) do place\n    if cmp(place, \"city\") == 1\n        return rand([\"skateboard\", \"bmx bike\"])\n    elseif cmp(place, \"countryside\") == 1\n        return rand([\"sled\", \"snowcarpet\"])\n    else\n        return rand([\"private jet\", \"ferris wheel\"])\n    end\nend;\nexperience = map(preferred_equipment) do equipment\n    if equipment ∈ [\"skateboard\", \"bmx bike\"]\n        return \"didn't fall\"\n    elseif equipment ∈ [\"sled\", \"snowcarpet\"]\n        return \"fell 3 times or less\"\n    else\n        return \"fell uncontably many times\"\n    end\nend;\n\ntest_mi = independence(SurrogateTest(MIShannon(), Contingency()), places, experience)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the evidence favors the alternative hypothesis that places and  experience are dependent.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test_cmi = independence(SurrogateTest(CMIShannon(), Contingency()), places, experience, preferred_equipment)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Again, as expected, when conditioning on the mediating variable, the dependence disappears, and we can't reject the null hypothesis of independence.","category":"page"},{"location":"examples/examples_independence/#Transfer-entropy-([TEShannon](@ref))","page":"Independence testing","title":"Transfer entropy (TEShannon)","text":"","category":"section"},{"location":"examples/examples_independence/#examples_surrogatetest_teshannon","page":"Independence testing","title":"Pairwise","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We'll see if we can reject independence for two unidirectionally coupled timeseries where x drives y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nsys = system(Logistic2Unidir(c_xy = 0.5)) # x affects y, but not the other way around.\nx, y = columns(trajectory(sys, 1000, Ttr = 10000))\n\ntest = SurrogateTest(TEShannon(), KSG1(k = 4))\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, we can reject the null hypothesis that the future of y is independent of x, because x does actually influence y. This doesn't change if we compute partial (conditional) transfer entropy with respect to some random extra time series, because it doesn't influence any of the other two variables.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(test, x, y, rand(length(x)))","category":"page"},{"location":"examples/examples_independence/#examples_surrogatetest_smeasure","page":"Independence testing","title":"SMeasure","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nx, y = randn(3000), randn(3000)\nmeasure = SMeasure(dx = 3, dy = 3)\ns = s_measure(measure, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The s statistic is larger when there is stronger coupling and smaller when there is weaker coupling. To check whether s is significant (i.e. large enough to claim directional dependence), we can use a SurrogateTest, like here.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"test = SurrogateTest(measure)\nindependence(test, x, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The p-value is high, and we can't reject the null at any reasonable significance level. Hence, there isn't evidence in the data to support directional coupling from x to y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"What happens if we use coupled variables?","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"z = x .+ 0.1y\nindependence(test, x, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Now we can confidently reject the null (independence), and conclude that there is evidence in the data to support directional dependence from x to z.","category":"page"},{"location":"examples/examples_independence/#examples_patest","page":"Independence testing","title":"PATest","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"The following example demonstrates how to compute the significance of the PA directional dependence measure using a PATest. We'll use timeseries from a chain of unidirectionally coupled logistic maps that are coupled X to Y to Z to W.","category":"page"},{"location":"examples/examples_independence/#Conditional-analysis","page":"Independence testing","title":"Conditional analysis","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"What happens if we computeDelta A_X to Z? We'd maybe expect there to be  some information transfer X to Z, even though the variables are not directly linked, because information is transferred through Y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nusing DelayEmbeddings\nusing Random\nrng = MersenneTwister(1234)\n\nsys = system(Logistic4Chain(xi = [0.1, 0.2, 0.3, 0.4]; rng))\nx, y, z, w = columns(trajectory(sys, 1000))\nτx = estimate_delay(x, \"mi_min\")\nτy = estimate_delay(y, \"mi_min\")\ntest = PATest(PA(ηT = 1:10, τS = estimate_delay(x, \"mi_min\")), FPVP())\nΔA_xz = independence(test, x, z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the distribution is still significantly skewed towards positive values. To determine whether the information flow between x and z is mediated by y, we can compute the conditional distribution Delta A_X to Z  Y. If these values are still positively skewed, we conclude that Y is not a mediating variable. If conditioning on Y causes Delta A_X to Z  Y to not be skewed towards positive values any more, then we conclude that Y is a mediating variable and that X and Z are linked X to Y to Z.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"measure = PA(ηT = 1:10, τS = estimate_delay(x, \"mi_min\"), τC = estimate_delay(y, \"mi_min\"))\ntest = PATest(measure, FPVP())\nΔA_xzy = independence(test, x, z, y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"We can't reject independence when conditioning on Y, so we conclude that Y is a variable responsible for transferring information from X to Z.","category":"page"},{"location":"examples/examples_independence/#examples_corrtest","page":"Independence testing","title":"CorrTest","text":"","category":"section"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"using CausalityTools\nusing StableRNGs\nrng = StableRNG(1234)\n\n# Some normally distributed data\nX = randn(rng, 1000) \nY = 0.5*randn(rng, 1000) .+ X\nZ = 0.5*randn(rng, 1000) .+ Y\nW = randn(rng, 1000);","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"Let's test a few independence relationships. For example, we expect that X ⫫ W. We also expect dependence X !⫫ Z, but this dependence should vanish when conditioning on the intermediate variable, so we expect X ⫫ Z | Y.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(CorrTest(), X, W)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the outcome is that we can't reject the null hypothesis that X ⫫ W.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(CorrTest(), X, Z)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"However, we can reject the  null hypothesis that X ⫫ Z, so the evidence favors the alternative hypothesis X !⫫ Z.","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"independence(CorrTest(), X, Z, Y)","category":"page"},{"location":"examples/examples_independence/","page":"Independence testing","title":"Independence testing","text":"As expected, the correlation between X and Z significantly vanishes when conditioning on Y, because Y is solely responsible for the observed correlation between X and Y.","category":"page"},{"location":"experimental/#[Experimental](@ref-experimental_methods)","page":"Experimental","title":"Experimental","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"Here we list implemented methods that do not yet appear in peer-reviewed journals, but are found, for example, in pre-print servers like arXiv.","category":"page"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"The API for these methods, and their return values, may change at any time until they appear as part of the public API. Use them wisely.","category":"page"},{"location":"experimental/#Predictive-asymmetry","page":"Experimental","title":"Predictive asymmetry","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"predictive_asymmetry","category":"page"},{"location":"experimental/#CausalityTools.predictive_asymmetry","page":"Experimental","title":"CausalityTools.predictive_asymmetry","text":"predictive_asymmetry(estimator::TransferEntropyEstimator, ηs; s, t, [c],\n    dTf = 1, dT = 1, dS = 1, τT = -1, τS = -1, [dC = 1, τC = -1],\n    normalize::Bool = false, f::Real = 1.0, base = 2) → Vector{Float64}\n\nCompute the predictive asymmetry[Haaga2020] 𝔸(s → t) for source time series s and target time series t over prediction lags ηs, using the given estimator and embedding parameters dTf, dT, dS, τT, τS (see also EmbeddingTE)\n\nIf a conditional time series c is provided, compute 𝔸(s → t | c). Then, dC and τC controls the embedding dimension and embedding lag for the conditional variable.\n\nReturns\n\nReturns a vector containing the predictive asymmetry for each value of ηs.\n\nNormalization (hypothesis test)\n\nIf normalize == true (the default), then compute the normalized predictive asymmetry 𝒜. In this case, for each eta in ηs, compute 𝒜(η) by normalizing 𝔸(η) to some fraction f of the mean transfer entropy over prediction lags -eta  eta (exluding lag 0). Haaga et al. (2020)[Haaga2020] uses a normalization with f=1.0 as a built-in hypothesis test, avoiding more computationally costly surrogate testing.\n\nEstimators\n\nAny estimator that works for transferentropy will also work with predictive_asymmetry. Check the online documentation for compatiable estimators.\n\nExamples\n\nusing CausalityTools\n# Some example time series\nx, y = rand(100), rand(100)\n# 𝔸(x → y) over prediction lags 1:5\n𝔸reg  = predictive_asymmetry(x, y, VisitationFrequency(RectangularBinning(5)), 1:5)\n\ninfo: Experimental!\nThis is a method that does not yet appear in a peer-reviewed scientific journal. Feel free to use, but consider it experimental for now. It will reappear in a 2.X release in new form once published in a peer-reviewed journal.\n\n[Haaga2020]: Haaga, Kristian Agasøster, David Diego, Jo Brendryen, and Bjarte Hannisdal. \"A simple test for causality in complex systems.\" arXiv preprint arXiv:2005.01860 (2020).\n\n\n\n\n\n","category":"function"},{"location":"experimental/#[PATest](@ref)","page":"Experimental","title":"PATest","text":"","category":"section"},{"location":"experimental/","page":"Experimental","title":"Experimental","text":"PATest\nPATestResult","category":"page"},{"location":"experimental/#CausalityTools.PATest","page":"Experimental","title":"CausalityTools.PATest","text":"PATest <: IndependenceTest\nPATest(measure::PA, est)\n\nA test for directional (conditional) dependence based on the modified PA measure, estimated using the given estimator est. Compatible estimators are listed in the docstring for PA.\n\nWhen used with independence, a PATestResult summary is returned.\n\nnote: Note\nThis is an experimental test. It is part of an ongoing paper submission revision, but is provided here for convenience.\n\n\n\n\n\n","category":"type"},{"location":"experimental/#CausalityTools.PATestResult","page":"Experimental","title":"CausalityTools.PATestResult","text":"PATestResult(n_vars, ΔA, ttest, pvalue)\n\nHolds the result of a SurrogateTest. n_vars is the number of variables used for the test (2 for pairwise, 3 for conditional). ΔA is the distribution of asymmetries, one for each η. ttest is a one-sample t-test, and pvalue is the right-tailed p-value for the test.\n\n\n\n\n\n","category":"type"},{"location":"measures/#association_measure","page":"Association measures","title":"Association measures","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"AssociationMeasure","category":"page"},{"location":"measures/#CausalityTools.AssociationMeasure","page":"Association measures","title":"CausalityTools.AssociationMeasure","text":"AssociationMeasure\n\nThe supertype of all association measures.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Overview","page":"Association measures","title":"Overview","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"Type Measure Pairwise Conditional Function version\nCorrelation PearsonCorrelation ✓ ✖ pearson_correlation\nCorrelation DistanceCorrelation ✓ ✓ distance_correlation\nCloseness SMeasure ✓ ✖ s_measure\nCloseness HMeasure ✓ ✖ h_measure\nCloseness MMeasure ✓ ✖ m_measure\nCloseness (ranks) LMeasure ✓ ✖ l_measure\nCloseness JointDistanceDistribution ✓ ✖ jdd\nCross-mapping PairwiseAsymmetricInference ✓ ✖ crossmap\nCross-mapping ConvergentCrossMapping ✓ ✖ crossmap\nConditional recurrence MCR ✓ ✖ mcr\nConditional recurrence RMCD ✓ ✓ rmcd\nShared information MIShannon ✓ ✖ mutualinfo\nShared information MIRenyiJizba ✓ ✖ mutualinfo\nShared information MIRenyiSarbu ✓ ✖ mutualinfo\nShared information MITsallisFuruichi ✓ ✖ mutualinfo\nShared information PartialCorrelation ✖ ✓ partial_correlation\nShared information CMIShannon ✖ ✓ condmutualinfo\nShared information CMIRenyiSarbu ✖ ✓ condmutualinfo\nShared information CMIRenyiJizba ✖ ✓ condmutualinfo\nInformation transfer TEShannon ✓ ✓ transferentropy\nInformation transfer TERenyiJizba ✓ ✓ transferentropy\nInformation asymmetry PA ✓ ✓ asymmetry","category":"page"},{"location":"measures/#Correlation-measures","page":"Association measures","title":"Correlation measures","text":"","category":"section"},{"location":"measures/#Pearson-correlation","page":"Association measures","title":"Pearson correlation","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"PearsonCorrelation\npearson_correlation","category":"page"},{"location":"measures/#CausalityTools.PearsonCorrelation","page":"Association measures","title":"CausalityTools.PearsonCorrelation","text":"PearsonCorrelation\n\nThe Pearson correlation of two variables.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with pearson_correlation to compute the raw correlation coefficient.\n\nDescription\n\nThe sample Pearson correlation coefficient for real-valued random variables X and Y with associated samples x_i_i=1^N and y_i_i=1^N is defined as\n\nrho_xy = dfracsum_i=1^n (x_i - barx)(y_i - bary) sqrtsum_i=1^N (x_i - barx)^2sqrtsum_i=1^N (y_i - bary)^2\n\nwhere barx and bary are the means of the observations x_k and y_k, respectively.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.pearson_correlation","page":"Association measures","title":"CausalityTools.pearson_correlation","text":"pearson_correlation(x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)\n\nCompute the PearsonCorrelation between x and y, which must each be 1-dimensional.\n\n\n\n\n\n","category":"function"},{"location":"measures/#Partial-correlation","page":"Association measures","title":"Partial correlation","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"PartialCorrelation\npartial_correlation","category":"page"},{"location":"measures/#CausalityTools.PartialCorrelation","page":"Association measures","title":"CausalityTools.PartialCorrelation","text":"PartialCorrelation <: AssociationMeasure\n\nThe correlation of two variables, with the effect of a set of conditioning variables removed.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for   conditional dependence.\nUse with partial_correlation to compute the raw correlation coefficient.\n\nDescription\n\nThere are several ways of estimating the partial correlation. We follow the matrix inversion method, because for StateSpaceSets, we can very efficiently compute the required joint covariance matrix Sigma for the random variables.\n\nFormally, let X_1 X_2 ldots X_n be a set of n real-valued random variables. Consider the joint precision matrix,P = (p_ij) = Sigma^-1. The partial correlation of any pair of variables (X_i X_j), given the remaining variables bfZ = X_k_i=1 i neq i j^n, is defined as\n\nrho_X_i X_j  bfZ = -dfracp_ijsqrt p_ii p_jj \n\nIn practice, we compute the estimate\n\nhatrho_X_i X_j  bfZ =\n-dfrachatp_ijsqrt hatp_ii hatp_jj \n\nwhere hatP = hatSigma^-1 is the sample precision matrix.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.partial_correlation","page":"Association measures","title":"CausalityTools.partial_correlation","text":"partial_correlation(x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet,\n    z::VectorOrStateSpaceSet...)\n\nCompute the PartialCorrelation between x and y, given z.\n\n\n\n\n\n","category":"function"},{"location":"measures/#Distance-correlation","page":"Association measures","title":"Distance correlation","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"DistanceCorrelation\ndistance_correlation","category":"page"},{"location":"measures/#CausalityTools.DistanceCorrelation","page":"Association measures","title":"CausalityTools.DistanceCorrelation","text":"DistanceCorrelation\n\nThe distance correlation (Székely et al., 2007)[Székely2007] measure quantifies potentially nonlinear associations between pairs of variables. If applied to three variables, the partial distance correlation (Székely and Rizzo, 2014)[Székely2014] is computed.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for   pairwise dependence.\nUse with distance_correlation to compute the raw distance correlation   coefficient.\n\nwarn: Warn\nA partial distance correlation distance_correlation(X, Y, Z) = 0 doesn't always guarantee conditional independence X ⫫ Y | Z. See Székely and Rizzo (2014) for in-depth discussion.\n\n[Székely2007]: Székely, G. J., Rizzo, M. L., & Bakirov, N. K. (2007). Measuring and testing dependence by correlation of distances. The annals of statistics, 35(6), 2769-2794.\n\n[Székely2014]: Székely, G. J., & Rizzo, M. L. (2014). Partial distance correlation with methods for dissimilarities.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.distance_correlation","page":"Association measures","title":"CausalityTools.distance_correlation","text":"distance_correlation(x, y) → dcor ∈ [0, 1]\ndistance_correlation(x, y, z) → pdcor\n\nCompute the empirical/sample distance correlation (Székely et al., 2007)[Székely2007], here called dcor, between StateSpaceSets x and y. Alternatively, compute the partial distance correlation pdcor (Székely and Rizzo, 2014)[Székely2014].\n\nSee also: DistanceCorrelation.\n\n[Székely2007]: Székely, G. J., Rizzo, M. L., & Bakirov, N. K. (2007). Measuring and testing dependence by correlation of distances. The annals of statistics, 35(6), 2769-2794.\n\n[Székely2014]: Székely, G. J., & Rizzo, M. L. (2014). Partial distance correlation with methods for dissimilarities.\n\n\n\n\n\n","category":"function"},{"location":"measures/#Closeness-measures","page":"Association measures","title":"Closeness measures","text":"","category":"section"},{"location":"measures/#Joint-distance-distribution","page":"Association measures","title":"Joint distance distribution","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"JointDistanceDistribution\njdd","category":"page"},{"location":"measures/#CausalityTools.JointDistanceDistribution","page":"Association measures","title":"CausalityTools.JointDistanceDistribution","text":"JointDistanceDistribution <: AssociationMeasure end\nJointDistanceDistribution(; metric = Euclidean(), B = 10, D = 2, τ = -1, μ = 0.0)\n\nThe joint distance distribution (JDD) measure (Amigó & Hirata, 2018)[Amigo2018].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for directional dependence.\nUse with jdd to compute the joint distance distribution Δ from Amigó & Hirata (2018).\n\nKeyword arguments\n\ndistance_metric::Metric: An instance of a valid distance metric from Distances.jl.   Defaults to Euclidean().\nB::Int: The number of equidistant subintervals to divide the interval [0, 1] into   when comparing the normalised distances.\nD::Int: Embedding dimension.\nτ::Int: Embedding delay. By convention, τ is negative.\nμ: The hypothetical mean value of the joint distance distribution if there   is no coupling between x and y (default is μ = 0.0).\n\nDescription\n\nFrom input time series x(t) and y(t), we first construct the delay embeddings (note the positive sign in the embedding lags; therefore the input parameter τ is by convention negative).\n\nbeginalign*\nbfx_i  = (x_i x_i+tau ldots x_i+(d_x - 1)tau)  \nbfy_i  = (y_i y_i+tau ldots y_i+(d_y - 1)tau)  \nendalign*\n\nThe algorithm then proceeds to analyze the distribution of distances between points of these embeddings, as described in Amigó & Hirata (2018)[Amigo2018].\n\nExamples\n\nComputing the JDD\nIndependence testing using JDD\n\n[Amigo2018]: Amigó, José M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.jdd","page":"Association measures","title":"CausalityTools.jdd","text":"jdd(measure::JointDistanceDistribution, source, target) → Δ\n\nCompute the joint distance distribution (Amigó & Hirata, 2018[Amigo2018]) from source to target using the given JointDistanceDistribution measure.\n\nReturns the distribution Δ from the paper directly (example). Use JointDistanceDistributionTest to perform a formal indepencence test.\n\n[Amigo2018]: Amigó, José M., and Yoshito Hirata. \"Detecting directional couplings from multivariate flows by the joint distance distribution.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.7 (2018): 075302.\n\n\n\n\n\n","category":"function"},{"location":"measures/#S-measure","page":"Association measures","title":"S-measure","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"SMeasure\ns_measure","category":"page"},{"location":"measures/#CausalityTools.SMeasure","page":"Association measures","title":"CausalityTools.SMeasure","text":"SMeasure < AssociationMeasure\nSMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nSMeasure is a bivariate association measure from Grassberger et al. (1999)[Grassberger1999] and Quiroga et al. (2000) [Quiroga2000] that measure directional dependence between two input (potentially multivariate) time series.\n\nNote that τx and τy are negative; see explanation below.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for directional dependence.\nUse with s_measure to compute the raw s-measure statistic.\n\nDescription\n\nThe steps of the algorithm are:\n\nFrom input time series x(t) and y(t), construct the delay embeddings (note  the positive sign in the embedding lags; therefore inputs parameters  τx and τy are by convention negative).\n\nbeginalign*\nbfx_i  = (x_i x_i+tau_x ldots x_i+(d_x - 1)tau_x)  \nbfy_i  = (y_i y_i+tau_y ldots y_i+(d_y - 1)tau_y)  \nendalign*\n\nLet r_ij and s_ij be the indices of the K-th nearest neighbors  of bfx_i and bfy_i, respectively. Neighbors closed than w time indices  are excluded during searches (i.e. w is the Theiler window).\nCompute the the mean squared Euclidean distance to the K nearest neighbors  for each x_i, using the indices r_i j.\n\nR_i^(k)(x) = dfrac1k sum_i=1^k(bfx_i bfx_r_ij)^2\n\nCompute the y-conditioned mean squared Euclidean distance to the K nearest   neighbors for each x_i, now using the indices s_ij.\n\nR_i^(k)(xy) = dfrac1k sum_i=1^k(bfx_i bfx_s_ij)^2\n\nDefine the following measure of independence, where 0 leq S leq 1, and   low values indicate independence and values close to one occur for   synchronized signals.\n\nS^(k)(xy) = dfrac1N sum_i=1^N dfracR_i^(k)(x)R_i^(k)(xy)\n\nInput data\n\nThe algorithm is slightly modified from [Grassberger1999] to allow univariate timeseries as input.\n\nIf x and y are StateSpaceSets then use x and y as is and ignore the parameters   dx/τx and dy/τy.\nIf x and y are scalar time series, then create dx and dy dimensional embeddings,   respectively, of both x and y, resulting in N different m-dimensional embedding points   X = x_1 x_2 ldots x_N  and Y = y_1 y_2 ldots y_N .   τx and τy control the embedding lags for x and y.\nIf x is a scalar-valued vector and y is a StateSpaceSet, or vice versa,   then create an embedding of the scalar timeseries using parameters dx/τx or dy/τy.\n\nIn all three cases, input StateSpaceSets are length-matched by eliminating points at the end of the longest StateSpaceSet (after the embedding step, if relevant) before analysis.\n\n[Quiroga2000]: Quian Quiroga, R., Arnhold, J. & Grassberger, P. [2000] “Learning driver-response relationships from synchronization patterns,” Phys. Rev. E61(5), 5142–5148.\n\n[Grassberger1999]: Arnhold, J., Grassberger, P., Lehnertz, K., & Elger, C. E. (1999). A robust method for detecting interdependences: application to intracranially recorded EEG. Physica D: Nonlinear Phenomena, 134(4), 419-430.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.s_measure","page":"Association measures","title":"CausalityTools.s_measure","text":"s_measure(measure::SMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)\n\nCompute the SMeasure from source x to target y.\n\n\n\n\n\ns_measure(measure::SMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet) → s ∈ [0, 1]\n\nCompute the given measure to quantify the directional dependence between univariate/multivariate time series x and y.\n\nReturns a scalar s where s = 0 indicates independence between x and y, and higher values indicate synchronization between x and y, with complete synchronization for s = 1.0.\n\nExample\n\nusing CausalityTools\n\n# A two-dimensional Ulam lattice map\nsys = ulam(2)\n\n# Sample 1000 points after discarding 5000 transients\norbit = trajectory(sys, 1000, Ttr = 5000)\nx, y = orbit[:, 1], orbit[:, 2]\n\n# 4-dimensional embedding for `x`, 5-dimensional embedding for `y`\nm = SMeasure(dx = 4, τx = 3, dy = 5, τy = 1)\ns_measure(m, x, y)\n\n\n\n\n\n","category":"function"},{"location":"measures/#H-measure","page":"Association measures","title":"H-measure","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"HMeasure\nh_measure","category":"page"},{"location":"measures/#CausalityTools.HMeasure","page":"Association measures","title":"CausalityTools.HMeasure","text":"HMeasure <: AssociationMeasure\nHMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nThe HMeasure (Grassberger et al., 1999)[Grassberger1999] is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.\n\nNote that τx and τy are negative by convention. See docstring for SMeasure for an explanation.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for directional dependence.\nUse with h_measure to compute the raw h-measure statistic.\n\nDescription\n\nThe HMeasure (Grassberger et al., 1999)[Grassberger1999] is similar to the SMeasure, but the numerator of the formula is replaced by R_i(x), the mean squared Euclidean distance to all other points, and there is a log-term inside the sum:\n\nH^(k)(xy) = dfrac1N sum_i=1^N\nlog left( dfracR_i(x)R_i^(k)(xy) right)\n\nParameters are the same and R_i^(k)(xy) is computed as for SMeasure.\n\n[Grassberger1999]: Arnhold, J., Grassberger, P., Lehnertz, K., & Elger, C. E. (1999). A robust method for detecting interdependences: application to intracranially recorded EEG. Physica D: Nonlinear Phenomena, 134(4), 419-430.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.h_measure","page":"Association measures","title":"CausalityTools.h_measure","text":"h_measure(measure::HMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)\n\nCompute the HMeasure from source x to target y.\n\n\n\n\n\n","category":"function"},{"location":"measures/#M-measure","page":"Association measures","title":"M-measure","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MMeasure\nm_measure","category":"page"},{"location":"measures/#CausalityTools.MMeasure","page":"Association measures","title":"CausalityTools.MMeasure","text":"MMeasure <: AssociationMeasure\nMMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nThe MMeasure (Andrzejak et al., 2003)[Andrzejak2003] is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.\n\nNote that τx and τy are negative by convention. See docstring for SMeasure for an explanation.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for directional dependence.\nUse with m_measure to compute the raw m-measure statistic.\n\nDescription\n\nThe MMeasure is based on SMeasure and HMeasure. It is given by\n\nM^(k)(xy) = dfrac1N sum_i=1^N\nlog left( dfracR_i(x) - R_i^(k)(xy)R_i(x) - R_i^k(x) right)\n\nwhere R_i(x) is computed as for HMeasure, while R_i^k(x) and R_i^(k)(xy) is computed as for SMeasure. Parameters also have the same meaning as for SMeasure/HMeasure.\n\n[Andrzejak2003]: Andrzejak, R. G., Kraskov, A., Stögbauer, H., Mormann, F., & Kreuz, T. (2003). Bivariate surrogate techniques: necessity, strengths, and caveats. Physical review E, 68(6), 066202.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.m_measure","page":"Association measures","title":"CausalityTools.m_measure","text":"m_measure(measure::MMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)\n\nCompute the MMeasure from source x to target y.\n\n\n\n\n\n","category":"function"},{"location":"measures/#L-measure","page":"Association measures","title":"L-measure","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"LMeasure\nl_measure","category":"page"},{"location":"measures/#CausalityTools.LMeasure","page":"Association measures","title":"CausalityTools.LMeasure","text":"LMeasure <: AssociationMeasure\nLMeasure(; K::Int = 2, dx = 2, dy = 2, τx = - 1, τy = -1, w = 0)\n\nThe LMeasure (Chicharro & Andrzejak, 2009)[^^Chicharro20093] is a pairwise association measure. It quantifies the probability with which close state of a target timeseries/embedding are mapped to close states of a source timeseries/embedding.\n\nNote that τx and τy are negative by convention. See docstring for SMeasure for an explanation.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for directional dependence.\nUse with l_measure to compute the raw l-measure statistic.\n\nDescription\n\nLMeasure is similar to MMeasure, but uses distance ranks instead of the raw distances.\n\nLet bfx_i be an embedding vector, and let g_ij denote the rank that the distance between bfx_i and some other vector bfx_j in a sorted ascending list of distances between bfx_i and bfx_i neq j In other words, g_ij this is just the N-1 nearest neighbor distances sorted )\n\nLMeasure is then defined as\n\nL^(k)(xy) = dfrac1N sum_i=1^N\nlog left( dfracG_i(x) - G_i^(k)(xy)G_i(x) - G_i^k(x) right)\n\nwhere G_i(x) = fracN2 and G_i^K(x) = frack+12 are the mean and minimal rank, respectively.\n\nThe y-conditioned mean rank is defined as\n\nG_i^(k)(xy) = dfrac1Ksum_j=1^K g_iw_i j\n\nwhere w_ij is the index of the j-th nearest neighbor of bfy_i.\n\n[Chicharro2009]: Chicharro, D., & Andrzejak, R. G. (2009). Reliable detection of directional couplings using rank statistics. Physical Review E, 80(2), 026217.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.l_measure","page":"Association measures","title":"CausalityTools.l_measure","text":"l_measure(measure::LMeasure, x::VectorOrStateSpaceSet, y::VectorOrStateSpaceSet)\n\nCompute the LMeasure from source x to target y.\n\n\n\n\n\n","category":"function"},{"location":"measures/#Cross-map-measures","page":"Association measures","title":"Cross-map measures","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"See also the cross mapping API for estimators.","category":"page"},{"location":"measures/#Convergent-cross-mapping","page":"Association measures","title":"Convergent cross mapping","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"ConvergentCrossMapping","category":"page"},{"location":"measures/#CausalityTools.ConvergentCrossMapping","page":"Association measures","title":"CausalityTools.ConvergentCrossMapping","text":"ConvergentCrossMapping <: CrossmapMeasure\nConvergentCrossMapping(; d::Int = 2, τ::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe convergent cross mapping (CCM) measure (Sugihara et al., 2012)[Sugihara2012]).\n\nSpecifies embedding dimension d, embedding lag τ to be used, as described below, with predict or crossmap. The Theiler window w controls how many temporal neighbors are excluded during neighbor searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nLet S(i) be the source time series variable and T(i) be the target time series variable. This version produces regular embeddings with fixed dimension d and embedding lag τ as follows:\n\n( S(i) S(i+tau) S(i+2tau) ldots S(i+(d-1)tau T(i))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D-1 variables, while the last (D-th) variable is to be predicted.\n\nWith this convention, τ < 0 implies \"past/present values of source used to predict target\", and τ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if τ > 0 (embed_warn = false turns off warnings).\n\n[Sugihara2012]: Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Pairwise-asymmetric-inference","page":"Association measures","title":"Pairwise asymmetric inference","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"PairwiseAsymmetricInference","category":"page"},{"location":"measures/#CausalityTools.PairwiseAsymmetricInference","page":"Association measures","title":"CausalityTools.PairwiseAsymmetricInference","text":"PairwiseAsymmetricInference <: CrossmapMeasure\nPairwiseAsymmetricInference(; d::Int = 2, τ::Int = -1, w::Int = 0,\n    f = Statistics.cor, embed_warn = true)\n\nThe pairwise asymmetric inference (PAI) cross mapping measure (McCracken & Weigel (2014)[McCracken2014]) is a version of ConvergentCrossMapping that searches for neighbors in mixed embeddings (i.e. both source and target variables included); otherwise, the algorithms are identical.\n\nSpecifies embedding dimension d, embedding lag τ to be used, as described below, with predict or crossmap. The Theiler window w controls how many temporal neighbors are excluded during neighbor searches (w = 0 means that only the point itself is excluded). f is a function that computes the agreement between observations and predictions (the default, f = Statistics.cor, gives the Pearson correlation coefficient).\n\nEmbedding\n\nThere are many possible ways of defining the embedding for PAI. Currently, we only implement the \"add one non-lagged source timeseries to an embedding of the target\" approach, which is used as an example in McCracken & Weigel's paper. Specifically: Let S(i) be the source time series variable and T(i) be the target time series variable. PairwiseAsymmetricInference produces regular embeddings with fixed dimension d and embedding lag τ as follows:\n\n(S(i) T(i+(d-1)tau ldots T(i+2tau) T(i+tau) T(i)))_i=1^N-(d-1)tau\n\nIn this joint embedding, neighbor searches are performed in the subspace spanned by the first D variables, while the last variable is to be predicted.\n\nWith this convention, τ < 0 implies \"past/present values of source used to predict target\", and τ > 0 implies \"future/present values of source used to predict target\". The latter case may not be meaningful for many applications, so by default, a warning will be given if τ > 0 (embed_warn = false turns off warnings).\n\n[McCracken2014]: McCracken, J. M., & Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E, 90(6), 062903.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Recurrence-based","page":"Association measures","title":"Recurrence-based","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MCR\nRMCD","category":"page"},{"location":"measures/#CausalityTools.MCR","page":"Association measures","title":"CausalityTools.MCR","text":"MCR <: AssociationMeasure\nMCR(; r, metric = Euclidean())\n\nAn association measure based on mean conditional probabilities of recurrence (MCR) introduced by Romano et al. (2007)[Romano2007].\n\nr is  mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of AbstractRecurrenceType from RecurrenceAnalysis.jl. To use any r that is not a real number, you have to do using RecurrenceAnalysis first. The metric is any valid metric from Distances.jl.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise   association.\nUse with mcr to compute the raw MCR for pairwise association.\n\nDescription\n\nFor input variables X and Y, the conditional probability of recurrence is defined as\n\nM(X  Y) = dfrac1N sum_i=1^N p(bfy_i  bfx_i) =\ndfrac1N sum_i=1^N dfracsum_i=1^N J_R_i j^X Ysum_i=1^N R_i j^X\n\nwhere R_i j^X is the recurrence matrix and J_R_i j^X Y is the joint recurrence matrix, constructed using the given metric. The measure M(Y  X) is defined analogously.\n\nRomano et al. (2007)'s interpretation of this quantity is that if X drives Y, then M(X|Y) > M(Y|X), if Y drives X, then M(Y|X) > M(X|Y), and if coupling is symmetric,  then M(Y|X) = M(X|Y).\n\nInput data\n\nX and Y can be either both univariate timeseries, or both multivariate StateSpaceSets.\n\n[Romano2007]: Romano, M. C., Thiel, M., Kurths, J., & Grebogi, C. (2007). Estimation of the direction of the coupling by conditional probabilities of recurrence. Physical Review E, 76(3), 036211.\n\n\n\n\n\n","category":"type"},{"location":"measures/#CausalityTools.RMCD","page":"Association measures","title":"CausalityTools.RMCD","text":"RMCD <: AssociationMeasure\nRMCD(; r, metric = Euclidean(), base = 2)\n\nThe recurrence measure of conditional dependence, or RMCD (Ramos et al., 2017)[Ramos2017], is a recurrence-based measure that mimics the conditional mutual information, but uses recurrence probabilities.\n\nr is a mandatory keyword which specifies the recurrence threshold when constructing recurrence matrices. It can be instance of any subtype of AbstractRecurrenceType from RecurrenceAnalysis.jl. To use any r that is not a real number, you have to do using RecurrenceAnalysis first. The metric is any valid metric from Distances.jl.\n\nBoth the pairwise and conditional RMCD is non-negative, but due to round-off error, negative values may occur. If that happens, an RMCD value of 0.0 is returned.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise   or conditional association.\nUse with rmcd to compute the raw RMCD for pairwise or conditional association.\n\nDescription\n\nThe RMCD measure is defined by\n\nI_RMCD(X Y  Z) = dfrac1N\nsum_i left\ndfrac1N sum_j R_ij^X Y Z\nlog left(\n    dfracsum_j R_ij^X Y Z sum_j R_ij^Z sum_j sum_j R_ij^X Z sum_j sum_j R_ij^Y Z\n    right)\nright\n\nwhere  base controls the base of the logarithm. I_RMCD(X Y  Z) is zero when Z = X, Z = Y or when X, Y and Z are mutually independent.\n\nOur implementation allows dropping the third/last argument, in which case the following mutual information-like quantitity is computed (not discussed in Ramos et al., 2017).\n\n\nI_RMCD(X Y) = dfrac1N\nsum_i left\ndfrac1N sum_j R_ij^X Y\nlog left(\n    dfracsum_j R_ij^X  R_ij^Y sum_j R_ij^X Y\n    right)\nright\n\n[Ramos2017]: Ramos, A. M., Builes-Jaramillo, A., Poveda, G., Goswami, B., Macau, E. E., Kurths, J., & Marwan, N. (2017). Recurrence measure of conditional dependence and applications. Physical Review E, 95(5), 052206.\n\n\n\n\n\n","category":"type"},{"location":"measures/#information_measures","page":"Association measures","title":"Information measures","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"Association measures that are information-based are listed here. Available estimators are listed in the information API.","category":"page"},{"location":"measures/#Mutual-information-(Shannon)","page":"Association measures","title":"Mutual information (Shannon)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MIShannon","category":"page"},{"location":"measures/#CausalityTools.MIShannon","page":"Association measures","title":"CausalityTools.MIShannon","text":"MIShannon <: MutualInformation\nMIShannon(; base = 2)\n\nThe Shannon mutual information I^S(X Y).\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information.\n\nDiscrete definition\n\nThere are many equivalent formulations of discrete Shannon mutual information. In this package, we currently use the double-sum and the three-entropies formulations.\n\nDouble sum formulation\n\nAssume we observe samples barbfX_1N_y = barbfX_1 ldots barbfX_n  and barbfY_1N_x = barbfY_1 ldots barbfY_n  from two discrete random variables X and Y with finite supports mathcalX =  x_1 x_2 ldots x_M_x  and mathcalY = y_1 y_2 ldots x_M_y. The double-sum estimate is obtained by replacing the double sum\n\nhatI_DS(X Y) =\n sum_x_i in mathcalX y_i in mathcalY p(x_i y_j) log left( dfracp(x_i y_i)p(x_i)p(y_j) right)\n\nwhere  hatp(x_i) = fracn(x_i)N_x, hatp(y_i) = fracn(y_j)N_y, and hatp(x_i x_j) = fracn(x_i)N, and N = N_x N_y. This definition is used by mutualinfo when called with a ContingencyMatrix.\n\nThree-entropies formulation\n\nAn equivalent formulation of discrete Shannon mutual information is\n\nI^S(X Y) = H^S(X) + H_q^S(Y) - H^S(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint discrete Shannon entropies. This definition is used by mutualinfo when called with a ProbabilitiesEstimator.\n\nDifferential mutual information\n\nOne possible formulation of differential Shannon mutual information is\n\nI^S(X Y) = h^S(X) + h_q^S(Y) - h^S(X Y)\n\nwhere h^S(cdot) and h^S(cdot cdot) are the marginal and joint differential Shannon entropies. This definition is used by mutualinfo when called with a DifferentialEntropyEstimator.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(Tsallis,-Furuichi)","page":"Association measures","title":"Mutual information (Tsallis, Furuichi)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MITsallisFuruichi","category":"page"},{"location":"measures/#CausalityTools.MITsallisFuruichi","page":"Association measures","title":"CausalityTools.MITsallisFuruichi","text":"MITsallisFuruichi <: MutualInformation\nMITsallisFuruichi(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Furuichi (2006)[Furuichi2006], which in that paper is called the mutual entropy.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information.\n\nDescription\n\nFuruichi's Tsallis mutual entropy between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_q^T(X Y) = H_q^T(X) - H_q^T(X  Y) = H_q^T(X) + H_q^T(Y) - H_q^T(X Y)\n\nwhere H^T(cdot) and H^T(cdot cdot) are the marginal and joint Tsallis entropies, and q is the Tsallis-parameter. ```\n\n[Furuichi2006]: Furuichi, S. (2006). Information theoretical properties of Tsallis entropies. Journal of Mathematical Physics, 47(2), 023302.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(Tsallis,-Martin)","page":"Association measures","title":"Mutual information (Tsallis, Martin)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MITsallisMartin","category":"page"},{"location":"measures/#CausalityTools.MITsallisMartin","page":"Association measures","title":"CausalityTools.MITsallisMartin","text":"MITsallisMartin <: MutualInformation\nMITsallisMartin(; base = 2, q = 1.5)\n\nThe discrete Tsallis mutual information from Martin et al. (2005)[Martin2004].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information. \n\nDescription\n\nMartin et al.'s Tsallis mutual information between variables X in mathbbR^d_X and Y in mathbbR^d_Y is defined as\n\nI_textMartin^T(X Y q) = H_q^T(X) + H_q^T(Y) - (1 - q) H_q^T(X) H_q^T(Y) - H_q(X Y)\n\nwhere H^S(cdot) and H^S(cdot cdot) are the marginal and joint Shannon entropies, and q is the Tsallis-parameter.\n\n[Martin2004]: Martin, S., Morison, G., Nailon, W., & Durrani, T. (2004). Fast and accurate image registration using Tsallis entropy and simultaneous perturbation stochastic approximation. Electronics Letters, 40(10), 1.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(Rényi,-Sarbu)","page":"Association measures","title":"Mutual information (Rényi, Sarbu)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MIRenyiSarbu","category":"page"},{"location":"measures/#CausalityTools.MIRenyiSarbu","page":"Association measures","title":"CausalityTools.MIRenyiSarbu","text":"MIRenyiSarbu <: MutualInformation\nMIRenyiSarbu(; base = 2, q = 1.5)\n\nThe discrete Rényi mutual information from Sarbu (2014)[Sarbu2014].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information.\n\nDescription\n\nSarbu (2014) defines discrete Rényi mutual information as the Rényi alpha-divergence between the conditional joint probability mass function p(x y) and the product of the conditional marginals, p(x) cdot p(y):\n\nI(X Y)^R_q =\ndfrac1q-1\nlog left(\n    sum_x in X y in Y\n    dfracp(x y)^qleft( p(x)cdot p(y) right)^q-1\nright)\n\n[Sarbu2014]: Sarbu, S. (2014, May). Rényi information transfer: Partial Rényi transfer entropy and partial Rényi mutual information. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 5666-5670). IEEE.\n\nSee also: mutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Mutual-information-(Rényi,-Jizba)","page":"Association measures","title":"Mutual information (Rényi, Jizba)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"MIRenyiJizba","category":"page"},{"location":"measures/#CausalityTools.MIRenyiJizba","page":"Association measures","title":"CausalityTools.MIRenyiJizba","text":"MIRenyiJizba <: MutualInformation\n\nThe Rényi mutual information I_q^R_J(X Y) defined in Jizba et al. (2012)[Jizba2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with mutualinfo to compute the raw mutual information. \n\nDefinition\n\nI_q^R_J(X Y) = S_q^R(X) + S_q^R(Y) - S_q^R(X Y)\n\nwhere S_q^R(cdot) and S_q^R(cdot cdot) the Rényi entropy and the joint Rényi entropy.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). Rényi's information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Conditional-mutual-information-(Shannon)","page":"Association measures","title":"Conditional mutual information (Shannon)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"CMIShannon","category":"page"},{"location":"measures/#CausalityTools.CMIShannon","page":"Association measures","title":"CausalityTools.CMIShannon","text":"CMIShannon <: ConditionalMutualInformation\nCMIShannon(; base = 2)\n\nThe Shannon conditional mutual information (CMI) I^S(X Y  Z).\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with condmutualinfo to compute the raw conditional mutual information.\n\nSupported definitions\n\nConsider random variables X in mathbbR^d_X and Y in mathbbR^d_Y, given Z in mathbbR^d_Z. The Shannon conditional mutual information is defined as\n\nbeginalign*\nI(X Y  Z)\n= H^S(X Z) + H^S(Y z) - H^S(X Y Z) - H^S(Z) \n= I^S(X Y Z) + I^S(X Y)\nendalign*\n\nwhere I^S(cdot cdot) is the Shannon mutual information MIShannon, and H^S(cdot) is the Shannon entropy.\n\nDifferential Shannon CMI is obtained by replacing the entropies by differential entropies.\n\nSee also: condmutualinfo.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Conditional-mutual-information-(Rényi,-Jizba)","page":"Association measures","title":"Conditional mutual information (Rényi, Jizba)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"CMIRenyiJizba","category":"page"},{"location":"measures/#CausalityTools.CMIRenyiJizba","page":"Association measures","title":"CausalityTools.CMIRenyiJizba","text":"CMIRenyiJizba <: ConditionalMutualInformation\n\nThe Rényi conditional mutual information I_q^R_J(X Y  Z defined in Jizba et al. (2012)[Jizba2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with condmutualinfo to compute the raw conditional mutual information.\n\nDefinition\n\nI_q^R_J(X Y  Z) = I_q^R_J(X Y Z) - I_q^R_J(X Z)\n\nwhere I_q^R_J(X Z) is the MIRenyiJizba mutual information.\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). Rényi’s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Conditional-mutual-information-(Rényi,-Poczos)","page":"Association measures","title":"Conditional mutual information (Rényi, Poczos)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"CMIRenyiPoczos","category":"page"},{"location":"measures/#CausalityTools.CMIRenyiPoczos","page":"Association measures","title":"CausalityTools.CMIRenyiPoczos","text":"CMIRenyiPoczos <: ConditionalMutualInformation\n\nThe differential Rényi conditional mutual information I_q^R_P(X Y  Z) defined in (Póczos & Schneider, 2012)[Póczos2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise dependence.\nUse with condmutualinfo to compute the raw conditional mutual information. \n\nDefinition\n\nbeginalign*\nI_q^R_P(X Y  Z) = dfrac1q-1\nint int int dfracp_Z(z) p_X Y  Z^q( p_XZ(xz) p_YZ(yz) )^q-1 \nmathbbE_X Y Z sim p_X Y Z\nleft dfracp_X Z^1-q(X Z) p_Y Z^1-q(Y Z) p_X Y Z^1-q(X Y Z) p_Z^1-q(Z) right\nendalign*\n\n[Póczos2012]: Póczos, B., & Schneider, J. (2012, March). Nonparametric estimation of conditional information and divergences. In Artificial Intelligence and Statistics (pp. 914-923). PMLR.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Transfer-entropy-(Shannon)","page":"Association measures","title":"Transfer entropy (Shannon)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"TEShannon","category":"page"},{"location":"measures/#CausalityTools.TEShannon","page":"Association measures","title":"CausalityTools.TEShannon","text":"TEShannon <: TransferEntropy\nTEShannon(; base = 2; embedding = EmbeddingTE()) <: TransferEntropy\n\nThe Shannon-type transfer entropy measure.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise   and conditional dependence.\nUse with transferentropy to compute the raw transfer entropy.\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I^S(T^+ S^-  T^-) \nTE(S to T  C) = I^S(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I(T^+ S^-  T^-) is the Shannon conditional mutual information (CMIShannon). The variables T^+, T^-, S^- and C^- are described in the docstring for transferentropy.\n\nCompatible estimators\n\nShannon-type transfer entropy can be estimated using a range of different estimators, which all boil down to computing conditional mutual information, except for TransferEntropyEstimator, which compute transfer entropy using some direct method.\n\nEstimator Type Principle TEShannon\nCountOccurrences ProbabilitiesEstimator Frequencies ✓\nValueHistogram ProbabilitiesEstimator Binning (histogram) ✓\nSymbolicPermuation ProbabilitiesEstimator Ordinal patterns ✓\nDispersion ProbabilitiesEstimator Dispersion patterns ✓\nKraskov DifferentialEntropyEstimator Nearest neighbors ✓\nZhu DifferentialEntropyEstimator Nearest neighbors ✓\nZhuSingh DifferentialEntropyEstimator Nearest neighbors ✓\nGao DifferentialEntropyEstimator Nearest neighbors ✓\nGoria DifferentialEntropyEstimator Nearest neighbors ✓\nLord DifferentialEntropyEstimator Nearest neighbors ✓\nLeonenkoProzantoSavani DifferentialEntropyEstimator Nearest neighbors ✓\nGaussanMI MutualInformationEstimator Parametric ✓\nKSG1 MutualInformationEstimator Continuous ✓\nKSG2 MutualInformationEstimator Continuous ✓\nGaoKannanOhViswanath MutualInformationEstimator Mixed ✓\nGaoOhViswanath MutualInformationEstimator Continuous ✓\nFPVP ConditionalMutualInformationEstimator Nearest neighbors ✓\nMesnerShalizi ConditionalMutualInformationEstimator Nearest neighbors ✓\nRahimzamani ConditionalMutualInformationEstimator Nearest neighbors ✓\nZhu1 TransferEntropyEstimator Nearest neighbors ✓\nLindner TransferEntropyEstimator Nearest neighbors ✓\n\n\n\n\n\n","category":"type"},{"location":"measures/#Transfer-entropy-(Rényi,-Jizba)","page":"Association measures","title":"Transfer entropy (Rényi, Jizba)","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"TERenyiJizba","category":"page"},{"location":"measures/#CausalityTools.TERenyiJizba","page":"Association measures","title":"CausalityTools.TERenyiJizba","text":"TERenyiJizba() <: TransferEntropy\n\nThe Rényi transfer entropy from Jizba et al. (2012)[Jizba2012].\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise   and conditional dependence.\nUse with transferentropy to compute the raw transfer entropy.\n\nDescription\n\nThe transfer entropy from source S to target T, potentially conditioned on C is defined as\n\nbeginalign*\nTE(S to T) = I_q^R_J(T^+ S^-  T^-) \nTE(S to T  C) = I_q^R_J(T^+ S^-  T^- C^-)\nendalign*\n\nwhere I_q^R_J(T^+ S^-  T^-) is Jizba et al. (2012)'s definition of conditional mutual information (CMIRenyiJizba). The variables T^+, T^-, S^- and C^- are described in the docstring for transferentropy.\n\nCompatible estimators\n\nJizba's formulation of Renyi-type transfer entropy can currently be estimated using selected probabilities estimators and differential entropy estimators, which under the hood compute the transfer entropy as Jizba's formulation of Rényi conditional mutual information.\n\nEstimator Type Principle TERenyiJizba\nCountOccurrences ProbabilitiesEstimator Frequencies ✓\nValueHistogram ProbabilitiesEstimator Binning (histogram) ✓\nLeonenkoProzantoSavani DifferentialEntropyEstimator Nearest neighbors ✓\n\n[Jizba2012]: Jizba, P., Kleinert, H., & Shefaat, M. (2012). Rényi’s information transfer between financial time series. Physica A: Statistical Mechanics and its Applications, 391(10), 2971-2989.\n\n\n\n\n\n","category":"type"},{"location":"measures/#Predictive-asymmetry","page":"Association measures","title":"Predictive asymmetry","text":"","category":"section"},{"location":"measures/","page":"Association measures","title":"Association measures","text":"PA","category":"page"},{"location":"measures/#CausalityTools.PA","page":"Association measures","title":"CausalityTools.PA","text":"PA <: CausalityTools.AssociationMeasure\nPA(ηT = 1:5, τS = 1, τC = 1)\n\nThe modified predictive asymmetry measure (Haaga et al., in revision).\n\nnote: Note\nThis is an experimental measure. It is part of an ongoing paper submission revision, but is provided here for convenience.\n\nUsage\n\nUse with independence to perform a formal hypothesis test for pairwise   or conditional directional dependence.\nUse with asymmetry to compute the raw asymmetry distribution.\n\nKeyword arguments\n\nηT. The prediction lags for the target variable.\nτS. The embedding delay(s) for the source variable.\nτC. The embedding delay(s) for the conditional variable(s).\n\nAll parameters are given as a single integer or multiple integers greater than zero.\n\nCompatible estimators\n\nPA/asymmetry uses condmutualinfo under the hood. Any estimator that can be used for ConditionalMutualInformation can therefore, in principle, be used with the predictive asymmetry. We recommend to use FPVP, or one of the other dedicated conditional mutual information estimators.\n\nEstimator Type Principle Pairwise Conditional\nCountOccurrences ProbabilitiesEstimator Frequencies ✓ ✓\nValueHistogram ProbabilitiesEstimator Binning (histogram) ✓ ✓\nDispersion ProbabilitiesEstimator Dispersion patterns ✓ ✓\nKraskov DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nZhu DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nZhuSingh DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nGao DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nGoria DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nLord DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nLeonenkoProzantoSavani DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nGaussanMI MutualInformationEstimator Parametric ✓ ✓\nKSG1 MutualInformationEstimator Continuous ✓ ✓\nKSG2 MutualInformationEstimator Continuous ✓ ✓\nGaoKannanOhViswanath MutualInformationEstimator Mixed ✓ ✓\nGaoOhViswanath MutualInformationEstimator Continuous ✓ ✓\nFPVP ConditionalMutualInformationEstimator Nearest neighbors ✓ ✓\nMesnerShalizi ConditionalMutualInformationEstimator Nearest neighbors ✓ ✓\nRahimzamani ConditionalMutualInformationEstimator Nearest neighbors ✓ ✓\n\nExamples\n\nComputing the asymmetry distribution.\n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#Transfer-entropy-API","page":"Transfer entropy API","title":"Transfer entropy API","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"The transfer entropy API is made up of the following functions and types.","category":"page"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"TransferEntropy, and its subtypes.\ntransferentropy.\nTransferEntropyEstimator, and its subtypes.","category":"page"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"transferentropy","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.transferentropy","page":"Transfer entropy API","title":"CausalityTools.transferentropy","text":"transferentropy([measure::TEShannon], est, s, t, [c])\ntransferentropy(measure::TERenyiJizba, est, s, t, [c])\n\nEstimate the transfer entropy TE^*(S to T) or TE^*(S to T  C) if c is given, using the provided estimator est, where * indicates the given measure. If measure is not given, then TEShannon(; base = 2) is the default.\n\nArguments\n\nmeasure: The transfer entropy measure, e.g. TEShannon or   TERenyi, which dictates which formula is computed.   Embedding parameters are stored in measure.embedding, and   is represented by an EmbeddingTE instance. If calling transferentropy   without giving measure, then the embedding is optimized by finding   suitable delay embedding parameters using the \"traditional\"   approach from DynamicalSystems.jl.\ns: The source timeseries.\nt: The target timeseries.\nc: Optional. A conditional timeseries.\n\nDescription\n\nThe Shannon transfer entropy is defined as TE^S(S to T  C) = I^S(T^+ S^-  T^- C^-), where I^S(T^+ S^-  T^- C^-) is CMIShannon, and marginals for the CMI are constructed as described in EmbeddingTE. The definition is analogous for TERenyiJizba.\n\nIf s, t, and c are univariate timeseries, then the the marginal embedding variables T^+ (target future), T^- (target present/past), S^- (source present/past) and C^- (present/past of conditioning variables) are constructed by first jointly embedding  s, t and c with relevant delay embedding parameters, then subsetting relevant columns of the embedding.\n\nSince estimates of TE^*(S to T) and TE^*(S to T  C) are just a special cases of conditional mutual information where input data are marginals of a particular form of delay embedding, any combination of variables, e.g. S = (A B), T = (C D), C = (D E F) are valid inputs (given as StateSpaceSets). In practice, however, s, t and c are most often timeseries, and if  s, t and c are StateSpaceSets, it is assumed that the data are pre-embedded and the embedding step is skipped.\n\nCompatible estimators\n\ntransferentropy is just a simple wrapper around condmutualinfo that constructs an appropriate delay embedding from the input data before CMI is estimated. Consequently, any estimator that can be used for ConditionalMutualInformation is, in principle, also a valid transfer entropy estimator. TransferEntropyEstimators are the exception - they compute transfer entropy directly.\n\nEstimator Type Principle TEShannon TERenyiJizba\nCountOccurrences ProbabilitiesEstimator Frequencies ✓ ✓\nValueHistogram ProbabilitiesEstimator Binning (histogram) ✓ ✓\nDispersion ProbabilitiesEstimator Dispersion patterns ✓ ✖\nKraskov DifferentialEntropyEstimator Nearest neighbors ✓ ✖\nZhu DifferentialEntropyEstimator Nearest neighbors ✓ ✖\nZhuSingh DifferentialEntropyEstimator Nearest neighbors ✓ ✖\nGao DifferentialEntropyEstimator Nearest neighbors ✓ ✖\nGoria DifferentialEntropyEstimator Nearest neighbors ✓ ✖\nLord DifferentialEntropyEstimator Nearest neighbors ✓ ✖\nLeonenkoProzantoSavani DifferentialEntropyEstimator Nearest neighbors ✓ ✓\nGaussanMI MutualInformationEstimator Parametric ✓ ✖\nKSG1 MutualInformationEstimator Continuous ✓ ✖\nKSG2 MutualInformationEstimator Continuous ✓ ✖\nGaoKannanOhViswanath MutualInformationEstimator Mixed ✓ ✖\nGaoOhViswanath MutualInformationEstimator Continuous ✓ ✖\nFPVP ConditionalMutualInformationEstimator Nearest neighbors ✓ ✖\nMesnerShalizi ConditionalMutualInformationEstimator Nearest neighbors ✓ ✖\nRahimzamani ConditionalMutualInformationEstimator Nearest neighbors ✓ ✖\nZhu1 TransferEntropyEstimator Nearest neighbors ✓ ✖\nLindner TransferEntropyEstimator Nearest neighbors ✓ ✖\nHilbert TransferEntropyEstimator Hilbert transform ✓ ✖\nSymbolicTransferEntropy TransferEntropyEstimator Hilbert transform ✓ ✖\n\n\n\n\n\n","category":"function"},{"location":"api/api_transferentropy/#Definitions","page":"Transfer entropy API","title":"Definitions","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"TransferEntropy","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.TransferEntropy","page":"Transfer entropy API","title":"CausalityTools.TransferEntropy","text":"TransferEntropy <: AssociationMeasure\n\nThe supertype of all transfer entropy measures. Concrete subtypes are\n\nTEShannon\nTERenyiJizba\n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#[TransferEntropyEstimator](@ref)s","page":"Transfer entropy API","title":"TransferEntropyEstimators","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"TransferEntropyEstimator","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.TransferEntropyEstimator","page":"Transfer entropy API","title":"CausalityTools.TransferEntropyEstimator","text":"The supertype of all dedicated transfer entropy estimators.\n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#[Zhu1](@ref)","page":"Transfer entropy API","title":"Zhu1","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"Zhu1","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.Zhu1","page":"Transfer entropy API","title":"CausalityTools.Zhu1","text":"Zhu1 <: TransferEntropyEstimator\nZhu1(k = 1, w = 0, base = MathConstants.e)\n\nThe Zhu1 transfer entropy estimator (Zhu et al., 2015)[Zhu2015].\n\nAssumes that the input data have been normalized as described in (Zhu et al., 2015).\n\nThis estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. Entropy, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#[Lindner](@ref)","page":"Transfer entropy API","title":"Lindner","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"Lindner","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.Lindner","page":"Transfer entropy API","title":"CausalityTools.Lindner","text":"Lindner <: TransferEntropyEstimator\nLindner(k = 1, w = 0, base = 2)\n\nThe Lindner transfer entropy estimator (Lindner et al., 2011)[Lindner2011], which is also used in the Trentool MATLAB toolbox, and is based on nearest neighbor searches.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nFor a given points in the joint embedding space jᵢ, this estimator first computes the distance dᵢ from jᵢ to its k-th nearest neighbor. Then, for each point mₖ[i] in the k-th marginal space, it counts the number of points within radius dᵢ.\n\nThe transfer entropy is then computed as\n\nTE(X to Y) =\npsi(k) + dfrac1N sum_i^n\nleft\n    sum_k=1^3 left( psi(m_ki + 1) right)\nright\n\nwhere the index k references the three marginal subspaces T, TTf and ST for which neighbor searches are performed.\n\n[Lindner2011]: Lindner, M., Vicente, R., Priesemann, V., & Wibral, M. (2011). TRENTOOL: A Matlab open source toolbox to analyse information flow in time series data with transfer entropy. BMC neuroscience, 12(1), 1-22.\n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#Convenience","page":"Transfer entropy API","title":"Convenience","text":"","category":"section"},{"location":"api/api_transferentropy/#[SymbolicTransferEntropy](@ref)","page":"Transfer entropy API","title":"SymbolicTransferEntropy","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"SymbolicTransferEntropy","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.SymbolicTransferEntropy","page":"Transfer entropy API","title":"CausalityTools.SymbolicTransferEntropy","text":"SymbolicTransferEntropy <: TransferEntropyEstimator\nSymbolicTransferEntropy(; m = 3, τ = 1, lt = ComplexityMeasures.isless_rand\n\nA convenience estimator for symbolic transfer entropy (Stanieck & Lenertz, 2008)[Stanieck2008].\n\nDescription\n\nSymbolic transfer entropy consists of two simple steps. First, the input time series are embedded with embedding lag m and delay τ. The ordinal patterns of the embedding vectors are then encoded using SymbolicPermutation with marginal_encodings. This transforms the input time series into integer time series using OrdinalPatternEncoding.\n\nTransfer entropy is then estimated as usual on the encoded timeseries with transferentropy and the CountOccurrences naive frequency estimator.\n\n[Stanieck2008]: Staniek, M., & Lehnertz, K. (2008). Symbolic transfer entropy. Physical review letters, 100(15), 158101.\n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#[Hilbert](@ref)","page":"Transfer entropy API","title":"Hilbert","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"Hilbert\nPhase\nAmplitude","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.Hilbert","page":"Transfer entropy API","title":"CausalityTools.Hilbert","text":"Hilbert(est;\n    source::InstantaneousSignalProperty = Phase(),\n    target::InstantaneousSignalProperty = Phase(),\n    cond::InstantaneousSignalProperty = Phase())\n) <: TransferDifferentialEntropyEstimator\n\nCompute transfer entropy on instantaneous phases/amplitudes of relevant signals, which are obtained by first applying the Hilbert transform to each signal, then extracting the phases/amplitudes of the resulting complex numbers[Palus2014]. Original time series are thus transformed to instantaneous phase/amplitude time series. Transfer entropy is then estimated using the provided est on those phases/amplitudes (use e.g. VisitationFrequency, or SymbolicPermutation).\n\ninfo: Info\nDetails on estimation of the transfer entropy (conditional mutual information) following the phase/amplitude extraction step is not given in Palus (2014). Here, after instantaneous phases/amplitudes have been obtained, these are treated as regular time series, from which transfer entropy is then computed as usual.\n\nSee also: Phase, Amplitude.\n\n[Palus2014]: Paluš, M. (2014). Cross-scale interactions and information transfer. Entropy, 16(10), 5263-5289.\n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#CausalityTools.Phase","page":"Transfer entropy API","title":"CausalityTools.Phase","text":"Phase <: InstantaneousSignalProperty\n\nIndicates that the instantaneous phases of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#CausalityTools.Amplitude","page":"Transfer entropy API","title":"CausalityTools.Amplitude","text":"Amplitude <: InstantaneousSignalProperty\n\nIndicates that the instantaneous amplitudes of a signal should be used. \n\n\n\n\n\n","category":"type"},{"location":"api/api_transferentropy/#Utilities","page":"Transfer entropy API","title":"Utilities","text":"","category":"section"},{"location":"api/api_transferentropy/","page":"Transfer entropy API","title":"Transfer entropy API","text":"optimize_marginals_te\nEmbeddingTE","category":"page"},{"location":"api/api_transferentropy/#CausalityTools.optimize_marginals_te","page":"Transfer entropy API","title":"CausalityTools.optimize_marginals_te","text":"optimize_marginals_te([scheme = OptimiseTraditional()], s, t, [c]) → EmbeddingTE\n\nOptimize marginal embeddings for transfer entropy computation from source time series s to target time series t, conditioned on c if c is given, using the provided optimization scheme.\n\n\n\n\n\n","category":"function"},{"location":"api/api_transferentropy/#CausalityTools.EmbeddingTE","page":"Transfer entropy API","title":"CausalityTools.EmbeddingTE","text":"EmbeddingTE(; dS = 1, dT = 1, dTf = 1, dC = 1, τS = -1, τT = -1, ηTf = 1, τC = -1)\nEmbeddingTE(opt::OptimiseTraditional, s, t, [c])\n\nEmbeddingTE provide embedding parameters for transfer entropy analysis using either TEShannon, TERenyi, or in general any subtype of TransferEntropy, which in turns dictates the embedding used with transferentropy.\n\nThe second method finds parameters using the \"traditional\" optimised embedding techniques from DynamicalSystems.jl\n\nConvention for generalized delay reconstruction\n\nWe use the following convention. Let s(i) be time series for the source variable, t(i) be the time series for the target variable and c(i) the time series for the conditional variable. To compute transfer entropy, we need the following marginals:\n\nbeginaligned\nT^+ = t(i+eta^1) t(i+eta^2) ldots (t(i+eta^d_T^+)  \nT^- =  (t(i+tau^0_T) t(i+tau^1_T) t(i+tau^2_T) ldots t(t + tau^d_T - 1_T))  \nS^- =  (s(i+tau^0_S) s(i+tau^1_S) s(i+tau^2_S) ldots s(t + tau^d_S - 1_S))  \nC^- =  (c(i+tau^0_C) c(i+tau^1_C) c(i+tau^2_C) ldots c(t + tau^d_C - 1_C)) \nendaligned\n\nDepending on the application, the delay reconstruction lags tau^k_T leq 0, tau^k_S leq 0, and tau^k_C leq 0 may be equally spaced, or non-equally spaced. The same applied to the prediction lag(s), but typically only a only a single predictions lag eta^k is used (so that d_T^+ = 1).\n\nFor transfer entropy, traditionally at least one tau^k_T, one tau^k_S and one tau^k_C equals zero. This way, the T^-, S^- and C^- marginals always contains present/past states, while the mathcal T marginal contain future states relative to the other marginals. However, this is not a strict requirement, and modern approaches that searches for optimal embeddings can return embeddings without the intantaneous lag.\n\nCombined, we get the generalized delay reconstruction mathbbE = (T^+_(d_T^+) T^-_(d_T) S^-_(d_S) C^-_(d_C)). Transfer entropy is then computed as\n\nbeginaligned\nTE_S rightarrow T  C = int_mathbbE P(T^+ T^- S^- C^-)\nlog_bleft(fracP(T^+  T^- S^- C^-)P(T^+  T^- C^-)right)\nendaligned\n\nor, if conditionals are not relevant,\n\nbeginaligned\nTE_S rightarrow T = int_mathbbE P(T^+ T^- S^-)\nlog_bleft(fracP(T^+  T^- S^-)P(T^+  T^-)right)\nendaligned\n\nHere,\n\nT^+ denotes the d_T^+-dimensional set of vectors furnishing the future   states of T (almost always equal to 1 in practical applications),\nT^- denotes the d_T-dimensional set of vectors furnishing the past and   present states of T,\nS^- denotes the d_S-dimensional set of vectors furnishing the past and   present of S, and\nC^- denotes the d_C-dimensional set of vectors furnishing the past and   present of C.\n\nKeyword arguments\n\ndS, dT, dC, dTf (f for future) are the dimensions of the S^-,   T^-, C^- and T^+ marginals. The parameters dS, dT, dC and dTf   must each be a positive integer number.\nτS, τT, τC are the embedding lags for S^-, T^-, C^-.   Each parameter are integers ∈ 𝒩⁰⁻, or a vector of integers ∈ 𝒩⁰⁻, so   that S^-, T^-, C^- always represents present/past values.   If e.g. τT is an integer, then for the T^- marginal is constructed using   lags tau_T = 0 tau 2tau ldots (d_T- 1)tau_T .   If is a vector, e.g. τΤ = [-1, -5, -7], then the dimension dT must match the lags,   and precisely those lags are used: tau_T = -1 -5 -7 .\nThe prediction lag(s) ηTf is a positive integer. Combined with the requirement   that the other delay parameters are zero or negative, this ensures that we're   always predicting from past/present to future. In typical applications,   ηTf = 1 is used for transfer entropy.\n\nExamples\n\nSay we wanted to compute the Shannon transfer entropy TE^S(S to T) = I^S(T^+ S^-  T^-). Using some modern procedure for determining optimal embedding parameters using methods from DynamicalSystems.jl, we find that the optimal embedding of T^- is three-dimensional and is given by the lags [0, -5, -8]. Using the same procedure, we find that the optimal embedding of S^- is two-dimensional with lags -1 -8. We want to predicting a univariate version of the target variable one time step into the future (ηTf = 1). The total embedding is then the set of embedding vectors\n\nE_TE =  (T(i+1) S(i-1) S(i-8) T(i) T(i-5) T(i-8)) . Translating this to code, we get:\n\nusing CausalityTools\njulia> EmbeddingTE(dT=3, τT=[0, -5, -8], dS=2, τS=[-1, -4], ηTf=1)\n\n# output\nEmbeddingTE(dS=2, dT=3, dC=1, dTf=1, τS=[-1, -4], τT=[0, -5, -8], τC=-1, ηTf=1)\n\n\n\n\n\n","category":"type"},{"location":"causal_graphs/#causal_graphs","page":"Causal graphs","title":"Inferring causal graphs","text":"","category":"section"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"Directed causal graphical models, estimated on observed data, is an incredibly useful framework for causal inference. There exists a plethora of methods for estimating such models.","category":"page"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"Useful reading:","category":"page"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"Pearl, J. Glymour, M., & Jewell, N. P. (2016). Causal inference in statistics:   A primer. John Wiley & Sons**. An excellent introductory book, suitable for anyone   interested, from a beginners to experts.\nGlymour, C., Zhang, K., & Spirtes, P. (2019). Review of causal discovery methods   based on graphical models. Frontiers in genetics, 10, 524. The authoritative   overview of causal discovery from graphical models. Many more methods have emerged   since this paper, and our goal is to provide a library of these methods.","category":"page"},{"location":"causal_graphs/#Causal-graph-API","page":"Causal graphs","title":"Causal graph API","text":"","category":"section"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"The API for inferring causal graphs is defined by:","category":"page"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"infer_graph\nGraphAlgorithm, and its subtypes","category":"page"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"infer_graph\nGraphAlgorithm","category":"page"},{"location":"causal_graphs/#CausalityTools.infer_graph","page":"Causal graphs","title":"CausalityTools.infer_graph","text":"infer_graph(algorithm::GraphAlgorithm, x) → g\n\nInfer graph from input data x using the given algorithm.\n\nReturns g, whose type depends on algorithm.\n\n\n\n\n\n","category":"function"},{"location":"causal_graphs/#CausalityTools.GraphAlgorithm","page":"Causal graphs","title":"CausalityTools.GraphAlgorithm","text":"GraphAlgorithm\n\nThe supertype of all causal graph inference algorithms.\n\nConcrete implementations\n\nOCE. The optimal causation entropy algorithm for time series graphs.\n\n\n\n\n\n","category":"type"},{"location":"causal_graphs/#[Optimal-causation-entropy](@ref)","page":"Causal graphs","title":"Optimal causation entropy","text":"","category":"section"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"OCE","category":"page"},{"location":"causal_graphs/#CausalityTools.OCE","page":"Causal graphs","title":"CausalityTools.OCE","text":"OCE <: GraphAlgorithm\nOCE(; utest::IndependenceTest = SurrogateTest(MIShannon(), KSG2(k = 3, w = 3)),\n      ctest::C = LocalPermutationTest(CMIShannon(), MesnerShalizi(k = 3, w = 3)),\n      τmax::T = 1, α = 0.05)\n\nThe optimal causation entropy (OCE) algorithm for causal discovery (Sun et al., 2015)[Sun2015].\n\nDescription\n\nThe OCE algorithm has three steps to determine the parents of a variable xᵢ.\n\nPerform pairwise independence tests using utest and select the variable xⱼ(-τ)  that has the highest significant (i.e. with associated p-value below α)  association with xᵢ(0). Assign it to the set of selected parents P.\nPerform conditional independence tests using ctest, finding the parent  Pₖ that has the highest association with xᵢ given the already selected parents,  and add it to P.  Repeat until no more variables with significant association are found.\nBackwards elimination of parents Pₖ of xᵢ(0) for which xᵢ(0) ⫫ Pₖ | P - {Pₖ},  where P is the set of parent nodes found in the previous steps.\n\nτmax indicates the maximum lag τ between the target variable xᵢ(0) and its potential parents xⱼ(-τ). Sun et al. 2015's method is based on τmax = 1.\n\nReturns\n\nWhen used with infer_graph, it returns a vector p, where p[i] are the parents for each input variable. This result can be converted to a SimpleDiGraph from Graphs.jl (see example).\n\nExamples\n\nInferring time series graph from a chain of logistic maps\n\n[Sun2015]: Sun, J., Taylor, D., & Bollt, E. M. (2015). Causal network inference by optimal causation entropy. SIAM Journal on Applied Dynamical Systems, 14(1), 73-106.\n\n\n\n\n\n","category":"type"},{"location":"causal_graphs/#[PC](@ref)","page":"Causal graphs","title":"PC","text":"","category":"section"},{"location":"causal_graphs/","page":"Causal graphs","title":"Causal graphs","text":"PC","category":"page"},{"location":"causal_graphs/#CausalityTools.PC","page":"Causal graphs","title":"CausalityTools.PC","text":"PC <: GraphAlgorithm\nPC(pairwise_test, conditional_test;\n    α = 0.05, max_depth = Inf, maxiters_orient = Inf)\n\nThe PC algorithm (Spirtes et al., 2000)[Spirtes2000], which is named named after the first names of the authors, Peter Spirtes and Clark Glymour, which is implemented as described in Kalisch & Bühlmann (2008)[Kalisch2008].\n\nArguments\n\npairwise_test: An IndependenceTest that uses a pairwise,   nondirectional AssociationMeasure measure (e.g. a parametric   CorrTest, or SurrogateTest with the MIShannon measure).\nconditional_test: An IndependenceTest that uses a conditional,   nondirectional AssociationMeasure (e.g. CorrTest,   or SurrogateTest with the CMIShannon measure).\n\nKeyword arguments\n\nα::Real. The significance level of the test.\nmax_depth. The maximum level of conditional indendence tests to be   performed. By default, there is no limit (i.e. max_depth = Inf), meaning that   maximum depth is N - 2, where N is the number of input variables.\nmaxiters_orient::Real. The maximum number of times to apply the orientation   rules. By default, there is not limit (i.e. maxiters_orient = Inf).\n\ninfo: Directional measures will not give meaningful answers\nDuring the skeleton search phase, if a significance association between two nodes are is found, then a bidirectional edge is drawn between them. The generic implementation of PC therefore doesn't currently handle directional measures such as TEShannon. The reason is that if a  directional relationship X → Y exists between two nodes X and Y, then the algorithm would first draw a bidirectional arrow between X and Y when analysing the direction X → Y, and then removing it again when analysing in the direction Y → X (a similar situation would also occur for the conditional stage). This will be fixed in a future release. For now, use nondirectional measures, e.g. MIShannon and CMIShannon!\n\nDescription\n\nWhen used with infer_graph on some input data x, the PC algorithm performs the following steps:\n\nInitialize an empty fully connected graph g with N nodes, where N is the number  of variables and x[i] is the data for the i-th node.\nReduce the fully connected g to a skeleton graph by performing pairwise  independence tests between all vertices using pairwise_test. Remove  any edges where adjacent vertices are found to be independent according to the test  (i.e. the null hypothesis of independence cannot be rejected at significance level  1 - α).\nThin the skeleton g by conditional independence testing. If  x[i] ⫫ x[j] | x[Z] for some set of variables Z (not including i and j)  according to conditional_test (i.e. the null hypothesis of conditional independence  cannot be rejected at significance level 1 - α), then the edge between i and j is  removed, and we record the separating set S(i, j) = Z. Independence tests are first  performed for conditioning sets of size 1, and repeated for conditioning sets of  increasing size, which in most cases limits the number of tests needed.  The separating  sets S(i, j), which records which variables were in the conditioning set that  rendered variables i and j independent, are recorded.  If max_depth is an integer, then this procedure is performed on conditioning  sets of sizes 1:max_depth, and if max_depth == nothing, then all possible  conditioning set sizes are potentially used.\nCreate a directed graph dg from g by replacing every  undirected edge X - Y in g by the bidirectional edge X ↔ Y (i.e.  construct two directional edges X → Y and Y → X). Orientiation rules 0-3  are then repeatedly applied to dg until no more edges can be oriented:\nRule 0 (orients v-structures): X ↔ Y ↔ Z becomes X → Y ← Z if Y is not in the   separating set S(X, Z).\nRule 1 (prevents new v-structures): X → Y ↔ Z becomes X → Y → Z if X and Z   are not adjacent.\nRule 2 (avoids cycles): X → Y → Z ↔ X becomes X → Y → Z ← X.\nRule 3: To avoid creating cycles or new v-structures, whenever X - Y → Z,   X - W → Z, and X - Z but there is no edge between Y and W, turn the   undirected X - Z edge into the directed edge X → Z.\n\nThe resulting directed graph (a SimpleDiGraph from Graphs.jl) is then returned.\n\nExamples\n\nPC algorithm with parametric independence tests\nPC algorithm with nonparametric independence tests\n\n[Kalisch2008]: Kalisch, M., & Bühlmann, P. (2008). Robustification of the PC-algorithm for directed acyclic graphs. Journal of Computational and Graphical Statistics, 17(4), 773-789.\n\n[Spirtes2000]: Spirtes, P., Glymour, C. N., Scheines, R., & Heckerman, D. (2000). Causation, prediction, and search. MIT press.\n\n\n\n\n\n","category":"type"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/#Association-measures","page":"Examples","title":"Association measures","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Pages = [\n    \"examples/examples_mi.md\",\n    \"examples/examples_conditional_mutual_information.md\",\n    \"examples/examples_transferentropy.md\",\n    \"examples/examples_cross_mappings.md\",\n    \"examples/examples_closeness.md\",\n    \"examples/examples_predictive_asymmetry.md\",\n]\nDepth = 3","category":"page"},{"location":"examples/#Independence-tests","page":"Examples","title":"Independence tests","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Pages = [\n    \"examples/examples_independence.md\",\n]\nDepth = 3","category":"page"},{"location":"examples/#Causal-graphs","page":"Examples","title":"Causal graphs","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Pages = [\n    \"examples/examples_graphs.md\",\n]\nDepth = 3","category":"page"},{"location":"examples/#Miscellaneous","page":"Examples","title":"Miscellaneous","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Pages = [\n    \"examples/examples_entropy.md\",\n    \"examples/examples_conditional_entropy.md\",\n]\nDepth = 3","category":"page"},{"location":"api/api_information_overview/#information_api","page":"Information API","title":"Information API","text":"","category":"section"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"This page outlines the information API. It contains a lot of information, so for convenience, we list all concrete implementation of pairwise and conditional association measures here.","category":"page"},{"location":"api/api_information_overview/#information_measures_design","page":"Information API","title":"Design","text":"","category":"section"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"We have taken great care to make sure that information estimators are reusable and modular. Functions have the following general form.","category":"page"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"f([measure], estimator, input_data...)\n\n# Some examples\nmutualinfo(MIShannon(base = ℯ), Kraskov(k = 1), x, y)\nmutualinfo(MITsallisFuruichi(base = ℯ), KozachenkoLeonenko(k = 3), x, y)\ncondmutualinfo(CMIShannon(base = 2), ValueHistogram(3), x, y, z)\ncondmutualinfo(CMIRenyiJizba(base = 2), KSG2(k = 5), x, y, z)\ncondmutualinfo(CMIRenyiPoczos(base = 2), PoczosSchneiderCMI(k = 10), x, y, z)","category":"page"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"This modular design really shines when it comes to independence testing and causal graph inference. You can essentially test the performance of any independence measure with any estimator, as long as their combination is implemented (and if it's not, please submit a PR or issue!). We hope that this will both ease reproduction of existing literature results, and spawn new research. Please let us know if you use the package for something useful, or publish something based on it!","category":"page"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"Information measures are either estimated using one of the following basic estimator types,","category":"page"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"ProbabilitiesEstimators,\nDifferentialEntropyEstimators,","category":"page"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"or using measure-specific estimators:","category":"page"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"MutualInformationEstimators are used with mutualinfo\nConditionalMutualInformationEstimators are used with condmutualinfo\nTransferEntropyEstimators are used with transferentropy","category":"page"},{"location":"api/api_information_overview/#Naming-convention:-The-same-name-for-different-things","page":"Information API","title":"Naming convention: The same name for different things","text":"","category":"section"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"Upon doing a literature review on the possible variants of information theoretic measures, it become painstakingly obvious that authors use the same name for different concepts. For novices, and experienced practitioners too, this can be confusing. Our API clearly distinguishes between methods that are conceptually the same but named differently in the literature due to differing estimation strategies, from methods that actually have different definitions.","category":"page"},{"location":"api/api_information_overview/","page":"Information API","title":"Information API","text":"Multiple, equivalent definitions occur for example for the Shannon mutual   information (MI; MIShannon), which has both a discrete and continuous version, and there there are multiple equivalent mathematical formulas for them: a direct sum/integral   over a joint probability mass function (pmf), as a sum of three entropy terms, and as   a Kullback-Leibler divergence between the joint pmf and the product of the marginal   distributions. Since these definitions are all equivalent, we only need once type   (MIShannon) to represent them.\nBut Shannon MI is not the  only type of mutual information! For example, \"Tsallis mutual information\"   has been proposed in different variants by various authors. Despite sharing the   same name, these are actually nonequivalent definitions. We've thus assigned   them entirely different measure names (e.g. MITsallisFuruichi and   MITsallisMartin), with the author name at the end.","category":"page"},{"location":"api/api_entropies/#entropies","page":"Entropies API","title":"Entropies API","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"The entropies API is defined by","category":"page"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"EntropyDefinition\nentropy\nDifferentialEntropyEstimator","category":"page"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"The entropies API is re-exported from ComplexityMeasures.jl. Why? Continuous/differential versions of many information theoretic association measures can be written as a function of differential entropy terms, and can thus be estimated using DifferentialEntropyEstimators.","category":"page"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"ComplexityMeasures.entropy","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.entropy","page":"Entropies API","title":"ComplexityMeasures.entropy","text":"entropy([e::DiscreteEntropyEstimator,] probs::Probabilities)\nentropy([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x)\n\nCompute the discrete entropy h::Real ∈ [0, ∞), using the estimator e, in one of two ways:\n\nDirectly from existing Probabilities probs.\nFrom input data x, by first estimating a probability mass function using the provided ProbabilitiesEstimator, and then computing the entropy from that mass fuction using the provided DiscreteEntropyEstimator.\n\nInstead of providing a DiscreteEntropyEstimator, an EntropyDefinition can be given directly, in which case MLEntropy is used as the estimator. If e is not provided, Shannon() is used by default.\n\nMaximum entropy and normalized entropy\n\nAll discrete entropies have a well defined maximum value for a given probability estimator. To obtain this value one only needs to call the entropy_maximum. Or, one can use entropy_normalized to obtain the normalized form of the entropy (divided by the maximum).\n\nExamples\n\nx = [rand(Bool) for _ in 1:10000] # coin toss\nps = probabilities(x) # gives about [0.5, 0.5] by definition\nh = entropy(ps) # gives 1, about 1 bit by definition\nh = entropy(Shannon(), ps) # syntactically equivalent to above\nh = entropy(Shannon(), CountOccurrences(x), x) # syntactically equivalent to above\nh = entropy(SymbolicPermutation(;m=3), x) # gives about 2, again by definition\nh = entropy(Renyi(2.0), ps) # also gives 1, order `q` doesn't matter for coin toss\n\n\n\n\n\nentropy(est::DifferentialEntropyEstimator, x) → h\n\nApproximate the differential entropy h::Real using the provided DifferentialEntropyEstimator and input data x. This method doesn't involve explicitly computing (discretized) probabilities first.\n\nThe overwhelming majority of entropy estimators estimate the Shannon entropy. If some estimator can estimate different definitions of entropy (e.g., Tsallis), this is provided as an argument to the estimator itself.\n\nSee the table of differential entropy estimators in the docs for all differential entropy estimators.\n\nExamples\n\nA standard normal distribution has a base-e differential entropy of 0.5*log(2π) + 0.5 nats.\n\nest = Kraskov(k = 5, base = ℯ) # Base `ℯ` for nats.\nh = entropy(est, randn(1_000_000))\nabs(h - 0.5*log(2π) - 0.5) # ≈ 0.001\n\n\n\n\n\n","category":"function"},{"location":"api/api_entropies/#Definitions","page":"Entropies API","title":"Definitions","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"EntropyDefinition\nShannon\nRenyi\nTsallis\nKaniadakis\nCurado\nStretchedExponential","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.EntropyDefinition","page":"Entropies API","title":"ComplexityMeasures.EntropyDefinition","text":"EntropyDefinition\n\nEntropyDefinition is the supertype of all types that encapsulate definitions of (generalized) entropies. These also serve as estimators of discrete entropies, see description below.\n\nCurrently implemented entropy definitions are:\n\nRenyi.\nTsallis.\nShannon, which is a subcase of the above two in the limit q → 1.\nKaniadakis.\nCurado.\nStretchedExponential.\n\nThese types can be given as inputs to entropy or entropy_normalized.\n\nDescription\n\nMathematically speaking, generalized entropies are just nonnegative functions of probability distributions that verify certain (entropy-type-dependent) axioms. Amigó et al.'s[Amigó2018] summary paper gives a nice overview.\n\nHowever, for a software implementation computing entropies in practice, definitions is not really what matters; estimators matter. Because in the practical sense, one needs to estimate a definition from finite data, and different ways of estimating a quantity come with their own pros and cons.\n\nThat is why the type DiscreteEntropyEstimator exists, which is what is actually given to entropy. Some ways to estimate a discrete entropy only apply to a specific entropy definition. For estimators that can be applied to various entropy definitions, this is specified by providing an instance of EntropyDefinition to the estimator.\n\n[Amigó2018]: Amigó, J. M., Balogh, S. G., & Hernández, S. (2018). A brief review of generalized entropies. Entropy, 20(11), 813.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#ComplexityMeasures.Shannon","page":"Entropies API","title":"ComplexityMeasures.Shannon","text":"Shannon <: EntropyDefinition\nShannon(; base = 2)\n\nThe Shannon[Shannon1948] entropy, used with entropy to compute:\n\nH(p) = - sum_i pi log(pi)\n\nwith the log at the given base.\n\nThe maximum value of the Shannon entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#ComplexityMeasures.Renyi","page":"Entropies API","title":"ComplexityMeasures.Renyi","text":"Renyi <: EntropyDefinition\nRenyi(q, base = 2)\nRenyi(; q = 1.0, base = 2)\n\nThe Rényi[Rényi1960] generalized order-q entropy, used with entropy to compute an entropy with units given by base (typically 2 or MathConstants.e).\n\nDescription\n\nLet p be an array of probabilities (summing to 1). Then the Rényi generalized entropy is\n\nH_q(p) = frac11-q log left(sum_i pi^qright)\n\nand generalizes other known entropies, like e.g. the information entropy (q = 1, see [Shannon1948]), the maximum entropy (q=0, also known as Hartley entropy), or the correlation entropy (q = 2, also known as collision entropy).\n\nThe maximum value of the Rényi entropy is log_base(L), which is the entropy of the uniform distribution with L the total_outcomes.\n\n[Rényi1960]: A. Rényi, Proceedings of the fourth Berkeley Symposium on Mathematics, Statistics and Probability, pp 547 (1960)\n\n[Shannon1948]: C. E. Shannon, Bell Systems Technical Journal 27, pp 379 (1948)\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#ComplexityMeasures.Tsallis","page":"Entropies API","title":"ComplexityMeasures.Tsallis","text":"Tsallis <: EntropyDefinition\nTsallis(q; k = 1.0, base = 2)\nTsallis(; q = 1.0, k = 1.0, base = 2)\n\nThe Tsallis[Tsallis1988] generalized order-q entropy, used with entropy to compute an entropy.\n\nbase only applies in the limiting case q == 1, in which the Tsallis entropy reduces to Shannon entropy.\n\nDescription\n\nThe Tsallis entropy is a generalization of the Boltzmann-Gibbs entropy, with k standing for the Boltzmann constant. It is defined as\n\nS_q(p) = frackq - 1left(1 - sum_i pi^qright)\n\nThe maximum value of the Tsallis entropy is ``k(L^1 - q - 1)(1 - q), with L the total_outcomes.\n\n[Tsallis1988]: Tsallis, C. (1988). Possible generalization of Boltzmann-Gibbs statistics. Journal of statistical physics, 52(1), 479-487.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#ComplexityMeasures.Kaniadakis","page":"Entropies API","title":"ComplexityMeasures.Kaniadakis","text":"Kaniadakis <: EntropyDefinition\nKaniadakis(; κ = 1.0, base = 2.0)\n\nThe Kaniadakis entropy (Tsallis, 2009)[Tsallis2009], used with entropy to compute\n\nH_K(p) = -sum_i=1^N p_i f_kappa(p_i)\n\nf_kappa (x) = dfracx^kappa - x^-kappa2kappa\n\nwhere if kappa = 0, regular logarithm to the given base is used, and 0 probabilities are skipped.\n\n[Tsallis2009]: Tsallis, C. (2009). Introduction to nonextensive statistical mechanics: approaching a complex world. Springer, 1(1), 2-1.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#ComplexityMeasures.Curado","page":"Entropies API","title":"ComplexityMeasures.Curado","text":"Curado <: EntropyDefinition\nCurado(; b = 1.0)\n\nThe Curado entropy (Curado & Nobre, 2004)[Curado2004], used with entropy to compute\n\nH_C(p) = left( sum_i=1^N e^-b p_i right) + e^-b - 1\n\nwith b ∈ ℛ, b > 0, where the terms outside the sum ensures that H_C(0) = H_C(1) = 0.\n\nThe maximum entropy for Curado is L(1 - exp(-bL)) + exp(-b) - 1 with L the total_outcomes.\n\n[Curado2004]: Curado, E. M., & Nobre, F. D. (2004). On the stability of analytic entropic forms. Physica A: Statistical Mechanics and its Applications, 335(1-2), 94-106.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#ComplexityMeasures.StretchedExponential","page":"Entropies API","title":"ComplexityMeasures.StretchedExponential","text":"StretchedExponential <: EntropyDefinition\nStretchedExponential(; η = 2.0, base = 2)\n\nThe stretched exponential, or Anteneodo-Plastino, entropy (Anteneodo & Plastino, 1999[Anteneodo1999]), used with entropy to compute\n\nS_eta(p) = sum_i = 1^N\nGamma left( dfraceta + 1eta - log_base(p_i) right) -\np_i Gamma left( dfraceta + 1eta right)\n\nwhere eta geq 0, Gamma(cdot cdot) is the upper incomplete Gamma function, and Gamma(cdot) = Gamma(cdot 0) is the Gamma function. Reduces to Shannon entropy for η = 1.0.\n\nThe maximum entropy for StrechedExponential is a rather complicated expression involving incomplete Gamma functions (see source code).\n\n[Anteneodo1999]: Anteneodo, C., & Plastino, A. R. (1999). Maximum entropy approach to stretched exponential probability distributions. Journal of Physics A: Mathematical and General, 32(7), 1089.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#[DifferentialEntropyEstimator](@ref)s","page":"Entropies API","title":"DifferentialEntropyEstimators","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"CausalityTools.jl reexports DifferentialEntropyEstimators from ComplexityMeasures.jl. Why? Any information-based measure that can be written as a function of differential entropies can be estimated using a DifferentialEntropyEstimators. ","category":"page"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"DifferentialEntropyEstimator","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.DifferentialEntropyEstimator","page":"Entropies API","title":"ComplexityMeasures.DifferentialEntropyEstimator","text":"DifferentialEntropyEstimator\nDiffEntropyEst # alias\n\nThe supertype of all differential entropy estimators. These estimators compute an entropy value in various ways that do not involve explicitly estimating a probability distribution.\n\nSee the table of differential entropy estimators in the docs for all differential entropy estimators.\n\nSee entropy for usage.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#Overview","page":"Entropies API","title":"Overview","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Only estimators compatible with multivariate data are applicable to the multi-argument measures provided by CausalityTools. Hence, some entropy estimators are missing from the overview here (see ComplexityMeasures.jl for details).","category":"page"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Each DifferentialEntropyEstimators uses a specialized technique to approximate relevant densities/integrals, and is often tailored to one or a few types of generalized entropy. For example, Kraskov estimates the Shannon entropy.","category":"page"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Estimator Principle Shannon\nKozachenkoLeonenko Nearest neighbors ✓\nKraskov Nearest neighbors ✓\nZhu Nearest neighbors ✓\nZhuSingh Nearest neighbors ✓\nGao Nearest neighbors ✓\nGoria Nearest neighbors ✓\nLord Nearest neighbors ✓","category":"page"},{"location":"api/api_entropies/#[Kraskov](@ref)","page":"Entropies API","title":"Kraskov","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Kraskov","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.Kraskov","page":"Entropies API","title":"ComplexityMeasures.Kraskov","text":"Kraskov <: DiffEntropyEst\nKraskov(; k::Int = 1, w::Int = 1, base = 2)\n\nThe Kraskov estimator computes the Shannon differential entropy of a multi-dimensional StateSpaceSet using the k-th nearest neighbor searches method from [Kraskov2004] at the given base.\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Kraskov estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Kraskov2004]: Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical review E, 69(6), 066138.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#[KozachenkoLeonenko](@ref)","page":"Entropies API","title":"KozachenkoLeonenko","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"KozachenkoLeonenko","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.KozachenkoLeonenko","page":"Entropies API","title":"ComplexityMeasures.KozachenkoLeonenko","text":"KozachenkoLeonenko <: DiffEntropyEst\nKozachenkoLeonenko(; w::Int = 0, base = 2)\n\nThe KozachenkoLeonenko estimator computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nusing the nearest neighbor method from Kozachenko & Leonenko (1987)[KozachenkoLeonenko1987], as described in Charzyńska and Gambin[Charzyńska2016].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nIn contrast to Kraskov, this estimator uses only the closest neighbor.\n\nSee also: entropy, Kraskov, DifferentialEntropyEstimator.\n\n[Charzyńska2016]: Charzyńska, A., & Gambin, A. (2016). Improvement of the k-NN entropy estimator with applications in systems biology. EntropyDefinition, 18(1), 13.\n\n[KozachenkoLeonenko1987]: Kozachenko, L. F., & Leonenko, N. N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9-16.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#[Zhu](@ref)","page":"Entropies API","title":"Zhu","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Zhu","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.Zhu","page":"Entropies API","title":"ComplexityMeasures.Zhu","text":"Zhu <: DiffEntropyEst\nZhu(; k = 1, w = 0, base = 2)\n\nThe Zhu estimator (Zhu et al., 2015)[Zhu2015] is an extension to KozachenkoLeonenko, and computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Zhu estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby approximating densities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. w is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, KozachenkoLeonenko, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#[ZhuSingh](@ref)","page":"Entropies API","title":"ZhuSingh","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"ZhuSingh","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.ZhuSingh","page":"Entropies API","title":"ComplexityMeasures.ZhuSingh","text":"ZhuSingh <: DiffEntropyEst\nZhuSingh(; k = 1, w = 0, base = 2)\n\nThe ZhuSingh estimator (Zhu et al., 2015)[Zhu2015] computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. ZhuSingh estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nLike Zhu, this estimator approximates probabilities within hyperrectangles surrounding each point xᵢ ∈ x using using k nearest neighbor searches. However, it also considers the number of neighbors falling on the borders of these hyperrectangles. This estimator is an extension to the entropy estimator in Singh et al. (2003).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nSee also: entropy, DifferentialEntropyEstimator.\n\n[Zhu2015]: Zhu, J., Bellanger, J. J., Shu, H., & Le Bouquin Jeannès, R. (2015). Contribution to transfer entropy estimation via the k-nearest-neighbors approach. EntropyDefinition, 17(6), 4173-4201.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#[Gao](@ref)","page":"Entropies API","title":"Gao","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Gao","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.Gao","page":"Entropies API","title":"ComplexityMeasures.Gao","text":"Gao <: DifferentialEntropyEstimator\nGao(; k = 1, w = 0, base = 2, corrected = true)\n\nThe Gao estimator (Gao et al., 2015) computes the Shannon differential entropy, using a k-th nearest-neighbor approach based on Singh et al. (2003)[Singh2003].\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nGao et al., 2015 give two variants of this estimator. If corrected == false, then the uncorrected version is used. If corrected == true, then the corrected version is used, which ensures that the estimator is asymptotically unbiased.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. KozachenkoLeonenko estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\n[Gao2015]: Gao, S., Ver Steeg, G., & Galstyan, A. (2015, February). Efficient estimation of mutual information for strongly dependent variables. In Artificial intelligence and     statistics (pp. 277-286). PMLR.\n\n[Singh2003]: Singh, H., Misra, N., Hnizdo, V., Fedorowicz, A., & Demchuk, E. (2003). Nearest neighbor estimates of entropy. American journal of mathematical and management sciences, 23(3-4), 301-321.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#[Goria](@ref)","page":"Entropies API","title":"Goria","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Goria","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.Goria","page":"Entropies API","title":"ComplexityMeasures.Goria","text":"Goria <: DifferentialEntropyEstimator\nGoria(; k = 1, w = 0, base = 2)\n\nThe Goria estimator computes the Shannon differential entropy of a multi-dimensional StateSpaceSet in the given base.\n\nDescription\n\nAssume we have samples bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density functionf  mathbbR^d to mathbbR. Goria estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nSpecifically, let bfn_1 bfn_2 ldots bfn_N be the distance of the samples bfx_1 bfx_2 ldots bfx_N  to their k-th nearest neighbors. Next, let the geometric mean of the distances be\n\nhatrho_k = left( prod_i=1^N right)^dfrac1N\n\nGoria et al. (2005)[Goria2005]'s estimate of Shannon differential entropy is then\n\nhatH = mhatrho_k + log(N - 1) - psi(k) + log c_1(m)\n\nwhere c_1(m) = dfrac2pi^fracm2m Gamma(m2) and psi is the digamma function.\n\n[Goria2005]: Goria, M. N., Leonenko, N. N., Mergel, V. V., & Novi Inverardi, P. L. (2005). A new class of random vector entropy estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17(3), 277-297.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#[Lord](@ref)","page":"Entropies API","title":"Lord","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"Lord","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.Lord","page":"Entropies API","title":"ComplexityMeasures.Lord","text":"Lord <: DifferentialEntropyEstimator\nLord(; k = 10, w = 0, base = 2)\n\nLord estimates the Shannon differential entropy using a nearest neighbor approach with a local nonuniformity correction (LNC).\n\nw is the Theiler window, which determines if temporal neighbors are excluded during neighbor searches (defaults to 0, meaning that only the point itself is excluded when searching for neighbours).\n\nDescription\n\nAssume we have samples barX = bfx_1 bfx_2 ldots bfx_N  from a continuous random variable X in mathbbR^d with support mathcalX and density function f  mathbbR^d to mathbbR. Lord estimates the Shannon differential entropy\n\nH(X) = int_mathcalX f(x) log f(x) dx = mathbbE-log(f(X))\n\nby using the resubstitution formula\n\nhatbarX k = -mathbbElog(f(X))\napprox sum_i = 1^N log(hatf(bfx_i))\n\nwhere hatf(bfx_i) is an estimate of the density at bfx_i constructed in a manner such that hatf(bfx_i) propto dfrack(x_i)  NV_i, where k(x_i) is the number of points in the neighborhood of bfx_i, and V_i is the volume of that neighborhood.\n\nWhile most nearest-neighbor based differential entropy estimators uses regular volume elements (e.g. hypercubes, hyperrectangles, hyperspheres) for approximating the local densities hatf(bfx_i), the Lord estimator uses hyperellopsoid volume elements. These hyperellipsoids are, for each query point xᵢ, estimated using singular value decomposition (SVD) on the k-th nearest neighbors of xᵢ. Thus, the hyperellipsoids stretch/compress in response to the local geometry around each sample point. This makes Lord a well-suited entropy estimator for a wide range of systems.\n\n[Lord2015]: Lord, W. M., Sun, J., & Bollt, E. M. (2018). Geometric k-nearest neighbor estimation of entropy and mutual information. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(3), 033114.\n\n\n\n\n\n","category":"type"},{"location":"api/api_entropies/#Utilities","page":"Entropies API","title":"Utilities","text":"","category":"section"},{"location":"api/api_entropies/","page":"Entropies API","title":"Entropies API","text":"entropy_maximum\nentropy_normalized","category":"page"},{"location":"api/api_entropies/#ComplexityMeasures.entropy_maximum","page":"Entropies API","title":"ComplexityMeasures.entropy_maximum","text":"entropy_maximum(e::EntropyDefinition, est::ProbabilitiesEstimator, x)\n\nReturn the maximum value of a discrete entropy with the given probabilities estimator and input data x. Like in outcome_space, for some estimators the concrete outcome space is known without knowledge of input x, in which case the function dispatches to entropy_maximum(e, est).\n\nentropy_maximum(e::EntropyDefinition, L::Int)\n\nSame as above, but computed directly from the number of total outcomes L.\n\n\n\n\n\n","category":"function"},{"location":"api/api_entropies/#ComplexityMeasures.entropy_normalized","page":"Entropies API","title":"ComplexityMeasures.entropy_normalized","text":"entropy_normalized([e::DiscreteEntropyEstimator,] est::ProbabilitiesEstimator, x) → h̃\n\nReturn h̃ ∈ [0, 1], the normalized discrete entropy of x, i.e. the value of entropy divided by the maximum value for e, according to the given probabilities estimator.\n\nInstead of a discrete entropy estimator, an EntropyDefinition can be given as first argument. If e is not given, it defaults to Shannon().\n\nNotice that there is no method entropy_normalized(e::DiscreteEntropyEstimator, probs::Probabilities), because there is no way to know the amount of possible events (i.e., the total_outcomes) from probs.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Overview","title":"Overview","text":"(Image: CausalityTools.jl static logo)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"CausalityTools","category":"page"},{"location":"#CausalityTools","page":"Overview","title":"CausalityTools","text":"CausalityTools\n\n(Image: CI) (Image: ) (Image: ) (Image: codecov) (Image: DOI)\n\nCausalityTools.jl is a package for quantifying associations and dynamical coupling between datasets, independence testing and causal inference.\n\nAll further information is provided in the documentation, which you can either find online or build locally by running the docs/make.jl file.\n\nKey features\n\nAssociation measures from conventional statistics, information theory and dynamical   systems theory, for example distance correlation, mutual information, transfer entropy,   convergent cross mapping and a lot more!\nA dedicated API for independence testing, which comes with automatic compatibility with   every measure-estimator combination you can think of. For example, we offer the generic   SurrogateTest, which is fully compatible with   TimeseriesSurrogates.jl,   and the LocalPermutationTest for conditional indepencence testing.\nA dedicated API for causal network inference based on these measures and independence   tests.\n\nInstallation\n\nTo install the package, run import Pkg; Pkg.add(\"CausalityTools\").\n\n\n\n\n\n","category":"module"},{"location":"#Goals","page":"Overview","title":"Goals","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Causal inference, and quantification of association in general, is fundamental to most scientific disciplines. There exists a multitude of bivariate and multivariate association measures in the scientific literature. However, beyond the most basic measures, most methods aren't readily available for practical use. Most scientific papers don't provide code, which makes reproducing them difficult or impossible, without investing significant time and resources into deciphering and understanding the original papers to the point where an implementation is possible. To make reliable inferences, proper independence tests are also crucial.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Our main goal with this package is to provide an easily extendible library of association measures, a as-complete-as-possible set of their estimators. We also want to lower the entry-point to the field of association quantification, independence testing and causal inference, by providing well-documented implementations of literature methods with runnable code examples.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"The core function for quantifying associations is independence, which performs either a parametric or nonparametric (conditional) IndependenceTest using some form of association measure. These tests, in turn, can be used with some GraphAlgorithm and infer_graph to infer causal graphs.","category":"page"},{"location":"#Input-data","page":"Overview","title":"Input data","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"Input data for CausalityTools are given as:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Univariate timeseries, which are given as standard Julia Vectors.\nMultivariate timeseries, StateSpaceSets, or state space sets, which are given as   StateSpaceSets. Many methods convert timeseries inputs to StateSpaceSet   for faster internal computations.\nCategorical data can be used with ContingencyMatrix to compute various   information theoretic measures and is represented using any iterable whose elements   can be any arbitrarily complex data type (as long as it's hashable), for example   Vector{String}, {Vector{Int}}, or Vector{Tuple{Int, String}}.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"StateSpaceSet","category":"page"},{"location":"#StateSpaceSets.StateSpaceSet","page":"Overview","title":"StateSpaceSets.StateSpaceSet","text":"StateSpaceSet{D, T} <: AbstractStateSpaceSet{D,T}\n\nA dedicated interface for sets in a state space. It is an ordered container of equally-sized points of length D. Each point is represented by SVector{D, T}. The data are a standard Julia Vector{SVector}, and can be obtained with vec(ssset::StateSpaceSet). Typically the order of points in the set is the time direction, but it doesn't have to be.\n\nWhen indexed with 1 index, StateSpaceSet is like a vector of points. When indexed with 2 indices it behaves like a matrix that has each of the columns be the timeseries of each of the variables. When iterated over, it iterates over its contained points. See description of indexing below for more.\n\nStateSpaceSet also supports almost all sensible vector operations like append!, push!, hcat, eachrow, among others.\n\nDescription of indexing\n\nIn the following let i, j be integers, typeof(X) <: AbstractStateSpaceSet and v1, v2 be <: AbstractVector{Int} (v1, v2 could also be ranges, and for performance benefits make v2 an SVector{Int}).\n\nX[i] == X[i, :] gives the ith point (returns an SVector)\nX[v1] == X[v1, :], returns a StateSpaceSet with the points in those indices.\nX[:, j] gives the jth variable timeseries (or collection), as Vector\nX[v1, v2], X[:, v2] returns a StateSpaceSet with the appropriate entries (first indices being \"time\"/point index, while second being variables)\nX[i, j] value of the jth variable, at the ith timepoint\n\nUse Matrix(ssset) or StateSpaceSet(matrix) to convert. It is assumed that each column of the matrix is one variable. If you have various timeseries vectors x, y, z, ... pass them like StateSpaceSet(x, y, z, ...). You can use columns(dataset) to obtain the reverse, i.e. all columns of the dataset in a tuple.\n\n\n\n\n\n","category":"type"},{"location":"#Pull-requests-and-issues","page":"Overview","title":"Pull requests and issues","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"This package has been and is under heavy development. Don't hesitate to submit an issue if you find something that doesn't work or doesn't make sense, or if there's some functionality that you're missing. Pull requests are also very welcome!","category":"page"},{"location":"#Maintainers-and-contributors","page":"Overview","title":"Maintainers and contributors","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"The CausalityTools.jl software is maintained by Kristian Agasøster Haaga, who also curates and writes this documentation. Significant contributions to the API and documentation design has been made by George Datseris, which also co-authors ComplexityMeasures.jl, which we develop in tandem with this package.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"A complete list of contributors to this repo are listed on the main Github page. Some important contributions are:","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Norbert Genera contributed bug reports and   investigations that led to subsequent improvements for the pairwise asymmetric   inference algorithm and an improved cross mapping API.\nDavid Diego's contributions were   invaluable in the initial stages of development. His MATLAB code provided the basis   for several transfer entropy methods and binning-related code.\nGeorge Datseris also ported KSG1 and KSG2 mutual   information estimators to Neighborhood.jl.\nBjarte Hannisdal provided tutorials for mutual information.\nTor Einar Møller contributed to cross-mapping methods in initial stages of development.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Many individuals has contributed code to other packages in the JuliaDynamics ecosystem which we use here. Contributors are listed in the respective GitHub repos and webpages.","category":"page"},{"location":"#Related-packages","page":"Overview","title":"Related packages","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"TransferEntropy.jl previously   provided mutual infromation and transfer entropy estimators. These have been   re-implemented from scratch and moved here.","category":"page"},{"location":"examples/examples_cross_mappings/#examples_crossmappings","page":"Cross mappings","title":"Cross mappings","text":"","category":"section"},{"location":"examples/examples_cross_mappings/#[ConvergentCrossMapping](@ref)","page":"Cross mappings","title":"ConvergentCrossMapping","text":"","category":"section"},{"location":"examples/examples_cross_mappings/#[ConvergentCrossMapping](@ref)-directly","page":"Cross mappings","title":"ConvergentCrossMapping directly","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nx, y = rand(200), rand(100)\ncrossmap(CCM(), x, y)","category":"page"},{"location":"examples/examples_cross_mappings/#[ConvergentCrossMapping](@ref)-with-[RandomVectors](@ref)","page":"Cross mappings","title":"ConvergentCrossMapping with RandomVectors","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"When cross-mapping with the RandomVectors estimator, a single random subsample of time indices (i.e. not in any particular order) of length l is drawn for each library size l, and cross mapping is performed using the embedding vectors corresponding to those time indices.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 200), randn(rng, 200)\n\n# We'll draw a single sample at each `l ∈ libsizes`. Sampling with replacement is then\n# necessary, because our 200-pt timeseries will result in embeddings with\n# less than 200 points.\nest = RandomVectors(; libsizes = 50:10:200, replace = true, rng)\ncrossmap(CCM(), est, x, y)","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"To generate a distribution of cross-map estimates for each l ∈ libsizes, just call crossmap repeatedly, e.g.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 200), randn(rng, 200)\nest = RandomVectors(; libsizes = 50:10:200, replace = true, rng)\nρs = [crossmap(CCM(), est, x, y) for i = 1:80]\nM = hcat(ρs...)","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Now, the k-th row of M contains 80 estimates of the correspondence measure ρ at library size libsizes[k].","category":"page"},{"location":"examples/examples_cross_mappings/#[ConvergentCrossMapping](@ref)-with-[RandomSegments](@ref)","page":"Cross mappings","title":"ConvergentCrossMapping with RandomSegments","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"When cross-mapping with the RandomSegments estimator, a single random subsample of continguous, ordered time indices of length l is drawn for each library size l, and cross mapping is performed using the embedding vectors corresponding to those time indices.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 200), randn(rng, 200)\n\n# We'll draw a single sample at each `l ∈ libsizes`. We limit the library size to 100, \n# because drawing segments of the data longer than half the available data doesn't make\n# much sense.\nest = RandomSegment(; libsizes = 50:10:100, rng)\ncrossmap(CCM(), est, x, y)","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"As above, to generate a distribution of cross-map estimates for each l ∈ libsizes, just call crossmap repeatedly, e.g.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Random; rng = MersenneTwister(1234)\nx, y = randn(rng, 200), randn(rng, 200)\nest = RandomSegment(; libsizes = 50:10:100, rng)\nρs = [crossmap(CCM(), est, x, y) for i = 1:80]\nM = hcat(ρs...)","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Now, the k-th row of M contains 80 estimates of the correspondence measure ρ at library size libsizes[k].","category":"page"},{"location":"examples/examples_cross_mappings/#Reproducing-Sugihara-et-al.-(2012)","page":"Cross mappings","title":"Reproducing Sugihara et al. (2012)","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"note: Run blocks consecutively\nIf copying these examples and running them locally, make sure the relevant packages (given in the first block) are loaded first.","category":"page"},{"location":"examples/examples_cross_mappings/#Figure-3A","page":"Cross mappings","title":"Figure 3A","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's reproduce figure 3A too, focusing only on ConvergentCrossMapping this time. In this figure, they compute the cross mapping for libraries of increasing size, always starting at time index 1. This approach - which we here call the ExpandingSegment estimator - is one of many ways of estimating the correspondence between observed and predicted value.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"For this example, they use a bidirectional system with asymmetrical coupling strength.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n\nfunction eom_logistic_sugi(u, p, t)\n    (; rx, ry, βxy, βyx) = p\n    (; x, y) = u\n\n    dx = x*(rx - rx*x - βxy*y)\n    dy = y*(ry - ry*y - βyx*x)\n    return SVector{2}(dx, dy)\nend\n\n# βxy := effect on x of y\n# βyx := effect on y of x\nfunction logistic_sugi(; u0 = rand(2), rx, ry, βxy, βyx)\n    p = @LArray [rx, ry, βxy, βyx] (:rx, :ry, :βxy, :βyx)\n    DiscreteDynamicalSystem(eom_logistic_sugi, u0, p)\nend\n\n# Used in `reproduce_figure_3A_naive`, and `reproduce_figure_3A_ensemble` below.\nfunction add_to_fig!(fig_pos, libsizes, ρs_x̂y, ρs_ŷx; title = \"\", quantiles = false)\n    ax = Axis(fig_pos; title, aspect = 1,\n        xlabel = \"Library size\", ylabel = \"Correlation (ρ)\")\n    ylims!(ax, (-1, 1))\n    hlines!([0], linestyle = :dash, alpha = 0.5, color = :grey)\n    scatterlines!(libsizes, median.(ρs_x̂y), label = \"x̂|y\", color = :blue)\n    scatterlines!(libsizes, median.(ρs_ŷx), label = \"ŷ|x\", color = :red)\n    if quantiles\n        band!(libsizes, quantile.(ρs_x̂y, 0.05), quantile.(ρs_x̂y, 0.95), color = (:blue, 0.5))\n        band!(libsizes, quantile.(ρs_ŷx, 0.05), quantile.(ρs_ŷx, 0.95), color = (:red, 0.5))\n    end\n    axislegend(ax, position = :rb)\nend\n\nfunction reproduce_figure_3A_naive(measure::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, βxy = 0.02, βyx = 0.32);\n    x, y = columns(trajectory(sys_bidir, 3100, Ttr = 10000));\n    libsizes = [20:2:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:3000]\n    est = ExpandingSegment(; libsizes);\n    ρs_x̂y = crossmap(measure, est, x, y)\n    ρs_ŷx = crossmap(measure, est, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, ρs_x̂y, ρs_ŷx; title = \"`ExpandingSegment`\")\n        fig\n    end\nend\n\nreproduce_figure_3A_naive(ConvergentCrossMapping(d = 3))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Hm. This looks a bit like the paper, but the curve is not smooth. We can do better!","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"It is not clear from the paper exactly what they plot in their Figure 3A, if they plot an average of some kind, or precisely what parameters and initial conditions they use. However, we can get a smoother plot by using a Ensemble. Combined with a CrossmapEstimator, it uses Monte Carlo resampling on subsets of the input data to compute an ensemble of ρs that we here use to compute the median and 90-th percentile range for each library size.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"function reproduce_figure_3A_ensemble(measure::CrossmapMeasure)\n    sys_bidir = logistic_sugi(; u0 = [0.4, 0.2], rx = 3.8, ry = 3.5, βxy = 0.02, βyx = 0.1);\n    x, y = columns(trajectory(sys_bidir, 10000, Ttr = 10000));\n    # Note: our time series are 1000 points long. When embedding, some points are\n    # lost, so we must use slightly less points for the segments than \n    # there are points in the original time series.\n    libsizes = [20:5:50; 55:5:200; 300:50:500; 600:100:900; 1000:500:3000]\n    # No point in doing more than one rep, because there data are always the same\n    # for `ExpandingSegment.`\n    ensemble_ev = Ensemble(measure, ExpandingSegment(; libsizes); nreps = 1)\n    ensemble_rs = Ensemble(measure, RandomSegment(; libsizes); nreps = 30)\n    ensemble_rv = Ensemble(measure, RandomVectors(; libsizes); nreps = 30)\n    ρs_x̂y_es = crossmap(ensemble_ev, x, y)\n    ρs_ŷx_es = crossmap(ensemble_ev, y, x)\n    ρs_x̂y_rs = crossmap(ensemble_rs, x, y)\n    ρs_ŷx_rs = crossmap(ensemble_rs, y, x)\n    ρs_x̂y_rv = crossmap(ensemble_rv, x, y)\n    ρs_ŷx_rv = crossmap(ensemble_rv, y, x)\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure(resolution = (800, 300))\n        add_to_fig!(fig[1, 1], libsizes, ρs_x̂y_es, ρs_ŷx_es; title = \"`ExpandingSegment`\", quantiles = false) # quantiles make no sense for `ExpandingSegment`\n        add_to_fig!(fig[1, 2], libsizes, ρs_x̂y_rs, ρs_ŷx_rs; title = \"`RandomSegment`\", quantiles = true)\n        add_to_fig!(fig[1, 3], libsizes, ρs_x̂y_rv, ρs_ŷx_rv; title = \"`RandomVector`\", quantiles = true)\n        fig\n    end\nend\n\nreproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, τ = -1))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"With the RandomVectors estimator, the mean of our ensemble ρs seem to look pretty much identical to Figure 3A in Sugihara et al. The RandomSegment estimator also performs pretty well, but since subsampled segments are contiguous, there are probably some autocorrelation effects at play.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"We can avoid the autocorrelation issue by tuning the w parameter of the ConvergentCrossMapping measure, which is the  Theiler window. Setting the Theiler window to w > 0, we can exclude neighbors of a query point p that are close to p in time, and thus deal with autocorrelation issues that way (the default w = 0 excludes only the point itself). Let's re-do the analysis with w = 5, just for fun.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"reproduce_figure_3A_ensemble(ConvergentCrossMapping(d = 3, τ = -1, w = 5))","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"There wasn't really that much of a difference, since for the logistic map, the autocorrelation function flips sign for every lag increase. However, for examples from other systems, tuning w may be important.","category":"page"},{"location":"examples/examples_cross_mappings/#Figure-3B","page":"Cross mappings","title":"Figure 3B","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"What about figure 3B? Here they generate time series of length 400 for a range of values for both coupling parameters, and plot the dominant direction Delta = rho(hatx  y) - rho(haty  x).","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"In the paper, they use a 1000 different parameterizations for the logistic map parameters, but don't state what is summarized in the plot. For simplicity, we'll therefore just stick to rx = ry = 3.7, as in the examples above, and just loop over the coupling strengths in either direction.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"function reproduce_figure_3B()\n    βxys = 0.0:0.02:0.4\n    βyxs = 0.0:0.02:0.4\n    ρx̂ys = zeros(length(βxys), length(βyxs))\n    ρŷxs = zeros(length(βxys), length(βyxs))\n\n    for (i, βxy) in enumerate(βxys)\n        for (j, βyx) in enumerate(βyxs)\n            sys_bidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.7, βxy, βyx);\n            # Generate 1000 points. Randomly select a 400-pt long segment.\n            x, y = columns(trajectory(sys_bidir, 400, Ttr = 10000));\n            ensemble = Ensemble(CCM(d = 3, w = 5, τ = -1), RandomVectors(libsizes = 100), nreps = 50)\n            ρx̂ys[i, j] = mean(crossmap(ensemble, x, y))\n            ρŷxs[i, j] = mean(crossmap(ensemble, y, x))\n        end\n    end\n    Δ = ρŷxs .- ρx̂ys\n\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        fig = Figure();\n        ax = Axis(fig[1, 1], xlabel = \"βxy\", ylabel = \"βyx\")\n        cont = contourf!(ax, Δ, levels = range(-1, 1, length = 10),\n            colormap = :curl)\n        ax.xticks = 1:length(βxys), string.([i % 2 == 0 ? βxys[i] : \"\" for i in 1:length(βxys)])\n        ax.yticks = 1:length(βyxs), string.([i % 2 == 0 ? βyxs[i] : \"\" for i in 1:length(βyxs)])\n        Colorbar(fig[1 ,2], cont, label = \"Δ (ρ(ŷ|x) - ρ(x̂|y))\")\n        tightlimits!(ax)\n        fig\n    end\nend\n\nreproduce_figure_3B()","category":"page"},{"location":"examples/examples_cross_mappings/#Figures-3C-and-3D","page":"Cross mappings","title":"Figures 3C and 3D","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's reproduce figures 3C and 3D in Sugihara et al. (2012)[Sugihara2012], which introduced the ConvergentCrossMapping measure. Equations and parameters can be found in their supplementary material. Simulatenously, we also compute the PairwiseAsymmetricInference measure from McCracken & Weigel (2014)[McCracken2014], which is a related method, but uses a slightly different embedding.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"[Sugihara2012]: Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"[McCracken2014]: McCracken, J. M., & Weigel, R. S. (2014). Convergent cross-mapping and pairwise asymmetric inference. Physical Review E, 90(6), 062903.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing Statistics\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\n\n\n# -----------------------------------------------------------------------------------------\n# Create 500-point long time series for Sugihara et al. (2012)'s example for figure 3.\n# -----------------------------------------------------------------------------------------\nsys_unidir = logistic_sugi(; u0 = [0.2, 0.4], rx = 3.7, ry = 3.700001, βxy = 0.00, βyx = 0.32);\nx, y = columns(trajectory(sys_unidir, 500, Ttr = 10000));\n\n# -----------------------------------------------------------------------------------------\n# Cross map.\n# -----------------------------------------------------------------------------------------\nm_ccm = ConvergentCrossMapping(d = 2)\nm_pai = PairwiseAsymmetricInference(d = 2)\n# Make predictions x̂y, i.e. predictions `x̂` made from embedding of y (AND x, if PAI)\nt̂ccm_x̂y, tccm_x̂y, ρccm_x̂y = predict(m_ccm, x, y)\nt̂pai_x̂y, tpai_x̂y, ρpai_x̂y = predict(m_pai, x, y);\n# Make predictions ŷx, i.e. predictions `ŷ` made from embedding of x (AND y, if PAI)\nt̂ccm_ŷx, tccm_ŷx, ρccm_ŷx = predict(m_ccm, y, x)\nt̂pai_ŷx, tpai_ŷx, ρpai_ŷx = predict(m_pai, y, x);\n\n# -----------------------------------------------------------------------------------------\n# Plot results\n# -----------------------------------------------------------------------------------------\nρs = (ρccm_x̂y, ρpai_x̂y, ρccm_ŷx, ρpai_ŷx)\nsccm_x̂y, spai_x̂y, sccm_ŷx, spai_ŷx = (map(ρ -> (@sprintf \"%.3f\" ρ), ρs)...,)\n\nρs = (ρccm_x̂y, ρpai_x̂y, ρccm_ŷx, ρpai_ŷx)\nsccm_x̂y, spai_x̂y, sccm_ŷx, spai_ŷx = (map(ρ -> (@sprintf \"%.3f\" ρ), ρs)...,)\n\nwith_theme(theme_minimal(),\n    markersize = 5) do\n    fig = Figure();\n    ax_ŷx = Axis(fig[2,1], aspect = 1, xlabel = \"y(t) (observed)\", ylabel = \"ŷ(t) | x (predicted)\")\n    ax_x̂y = Axis(fig[2,2], aspect = 1, xlabel = \"x(t) (observed)\", ylabel = \"x̂(t) | y (predicted)\")\n    xlims!(ax_ŷx, (0, 1)), ylims!(ax_ŷx, (0, 1))\n    xlims!(ax_x̂y, (0, 1)), ylims!(ax_x̂y, (0, 1))\n    ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n    scatterlines!(ax_ts, x[1:300], label = \"x\")\n    scatterlines!(ax_ts, y[1:300], label = \"y\")\n    axislegend()\n    scatter!(ax_ŷx, tccm_ŷx, t̂ccm_ŷx, label = \"CCM (ρ = $sccm_ŷx)\", color = :black)\n    scatter!(ax_ŷx, tpai_ŷx, t̂pai_ŷx, label = \"PAI (ρ = $spai_ŷx)\", color = :red)\n    axislegend(ax_ŷx, position = :lt)\n    scatter!(ax_x̂y, tccm_x̂y, t̂ccm_x̂y, label = \"CCM (ρ = $sccm_x̂y)\", color = :black)\n    scatter!(ax_x̂y, tpai_x̂y, t̂pai_x̂y, label = \"PAI (ρ = $spai_x̂y)\", color = :red)\n    axislegend(ax_x̂y, position = :lt)\n    fig\nend","category":"page"},{"location":"examples/examples_cross_mappings/#[PairwiseAsymmetricInference](@ref)","page":"Cross mappings","title":"PairwiseAsymmetricInference","text":"","category":"section"},{"location":"examples/examples_cross_mappings/#Reproducing-McCracken-and-Weigel-(2014)","page":"Cross mappings","title":"Reproducing McCracken & Weigel (2014)","text":"","category":"section"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"Let's try to reproduce figure 8 from McCracken & Weigel (2014)'s[McCracken2014] paper on PairwiseAsymmetricInference (PAI). We'll start by defining the their example B (equations 6-7). This system consists of two variables X and Y, where X drives Y.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"After we have computed the PAI in both directions, we define a measure of directionality as the difference between PAI in the X to Y direction and in the Y to X direction, so that if X drives Y, then Delta  0.","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"using CausalityTools\nusing LabelledArrays\nusing StaticArrays\nusing DynamicalSystemsBase\nusing StateSpaceSets\nusing CairoMakie, Printf\nusing Distributions: Normal\nusing Statistics: mean, std\n\nfunction eom_nonlinear_sindriver(dx, x, p, n)\n    a, b, c, t, Δt = (p...,)\n    x, y = x[1], x[2]\n    𝒩 = Normal(0, 1)\n    \n    dx[1] = sin(t)\n    dx[2] = a*x * (1 - b*x) + c* rand(𝒩)\n    p[end-1] += 1 # update t\n\n    return\nend\n\nfunction nonlinear_sindriver(;u₀ = rand(2), a = 1.0, b = 1.0, c = 2.0, Δt = 1)\n    DiscreteDynamicalSystem(eom_nonlinear_sindriver, u₀, [a, b, c, 0, Δt])\nend\n\nfunction reproduce_figure_8_mccraken(; \n        c = 2.0, Δt = 0.2,\n        as = 0.25:0.25:5.0,\n        bs = 0.25:0.25:5.0)\n    # -----------------------------------------------------------------------------------------\n    # Generate many time series for many different values of the parameters `a` and `b`,\n    # and compute PAI. This will replicate the upper right panel of \n    # figure 8 in McCracken & Weigel (2014).\n    # -----------------------------------------------------------------------------------------\n    \n    measure = PairwiseAsymmetricInference(d = 3)\n\n    # Manually resample `nreps` length-`L` time series and use mean ρ(x̂|X̄y) - ρ(ŷ|Ȳx)\n    # for each parameter combination.\n    nreps = 50\n    L = 300 # length of timeseries\n    Δ = zeros(length(as), length(bs))\n    for (i, a) in enumerate(as)\n        for (j, b) in enumerate(bs)\n            s = nonlinear_sindriver(; a, b, c,  Δt)\n            x, y = columns(trajectory(s, 1000, Ttr = 10000))\n            Δreps = zeros(nreps)\n            for i = 1:nreps\n                # Ensure we're subsampling at the same time indices. \n                ind_start = rand(1:(1000-L))\n                r = ind_start:(ind_start + L)\n                Δreps[i] = @views crossmap(measure, y[r], x[r]) - \n                    crossmap(measure, x[r], y[r])\n            end\n            Δ[i, j] = mean(Δreps)\n        end\n    end\n\n    # -----------------------------------------------------------------------------------------\n    # An example time series for plotting.\n    # -----------------------------------------------------------------------------------------\n    sys = nonlinear_sindriver(; a = 1.0, b = 1.0, c, Δt)\n    npts = 500\n    orbit = trajectory(sys, npts, Ttr = 10000)\n    x, y = columns(orbit)\n    with_theme(theme_minimal(),\n        markersize = 5) do\n        \n        X = x[1:300]\n        Y = y[1:300]\n        fig = Figure();\n        ax_ts = Axis(fig[1, 1:2], xlabel = \"Time (t)\", ylabel = \"Value\")\n        scatterlines!(ax_ts, (X .- mean(X)) ./ std(X), label = \"x\")\n        scatterlines!(ax_ts, (Y .- mean(Y)) ./ std(Y), label = \"y\")\n        axislegend()\n\n        ax_hm = Axis(fig[2, 1:2], xlabel = \"a\", ylabel = \"b\")\n        ax_hm.yticks = (1:length(as), string.([i % 2 == 0 ? as[i] : \"\" for i = 1:length(as)]))\n        ax_hm.xticks = (1:length(bs), string.([i % 2 == 0 ? bs[i] : \"\" for i = 1:length(bs)]))\n        hm = heatmap!(ax_hm, Δ,  colormap = :viridis)\n        Colorbar(fig[2, 3], hm; label = \"Δ' = ρ(ŷ | yx) - ρ(x̂ | xy)\")\n        fig\n    end\nend\n\nreproduce_figure_8_mccraken()","category":"page"},{"location":"examples/examples_cross_mappings/","page":"Cross mappings","title":"Cross mappings","text":"As expected, Delta  0 for all parameter combinations, implying that X \"PAI drives\" Y.","category":"page"},{"location":"examples/examples_entropy/#examples_entropy","page":"Entropy","title":"Entropy","text":"","category":"section"},{"location":"examples/examples_entropy/#Differential-entropy:-estimator-comparison","page":"Entropy","title":"Differential entropy: estimator comparison","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Here, we'll test the different nearest-neighbor based differential entropy estimators on a three-dimensional normal distribution mathcalN (mu Sigma) with zero means and covariance matrix Sigma = diag(r_1 r_2 r_3) with r_1 = r_2 = r_3 = 05.  The analytical entropy for multivariate Gaussian is H(mathcalN (mu Sigma)) = dfrac12log(det(2pi e Sigma)). In our case, Sigma is diagonal, so det(Sigma) = (05)^3 and H = 05log(2pi e (05)^3)approx 3217.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Several of these estimators have been shown to convergence to the true entropy with an increasing number of samples. Therefore, we test the  estimators on samples of increasing size N, where N ranges from 1000 to 30000. Since we're estimating entropy from samples of a normal distribution, we don't expect the estimates to perfectly match the analytical entropy every time. On average, however, they should hit the target when the sample size gets large enough.","category":"page"},{"location":"examples/examples_entropy/#Analytical-and-estimated-entropies","page":"Entropy","title":"Analytical and estimated entropies","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"We'll first make two helper functions.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"analytical_entropy(estimators, Ls; d::Int, r, base = 2): Computes the analytical     Shannon differential entropy to the given base of a multivariate normal distribution   with covariance matrix with diagonal elements r and zeros on the off-diagonal.   Does so for each of the given estimators for each   sample size in Ls.\nmvnormal_entropies(; d::Int, r, base = 2, kwargs...): Estimates  the Shannon    entropy to the given base of samples from a multivariate normal distribution as   specified as above.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"using CausalityTools\nusing Distributions: MvNormal\nusing LinearAlgebra\nusing Statistics: quantile\nusing Random; rng = MersenneTwister(12345678)\nusing CairoMakie\n\nanalytical_entropy(; d::Int, r, base = 2) = \n    0.5*log(det(2*pi*ℯ*diagm(repeat([r], d)))) / log(ℯ, base) # convert to desired base\n\nfunction mvnormal_entropies(estimators, Ls; \n        d = 3,\n        base = 2,\n        nreps = 50,\n        r = 0.5,\n    )\n    μ = zeros(d)\n    Σ = diagm(repeat([r], d))\n    N = MvNormal(μ, Σ)    \n    Hs = [[zeros(nreps) for L in Ls] for est in estimators]\n    data = [StateSpaceSet([rand(rng, N) for i = 1:maximum(Ls)]) for i = 1:nreps]\n    for (e, est) in enumerate(estimators)\n        for (l, L) in enumerate(Ls)\n            for i = 1:nreps\n                Hs[e][l][i] = entropy(Shannon(; base), est, data[i][1:L])\n            end\n        end\n    end\n    return Hs\nend;","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"We'll also need a function to summarize the estimates.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"# A helper to get the estimator name for plotting.\ngetname(est::DifferentialEntropyEstimator) = typeof(est).name.name  |> string\nfunction medians_and_quantiles(Hs, Ls; q = 0.95)\n    medians = [zeros(length(Ls)) for est in estimators]\n    lb = [zeros(length(Ls)) for est in estimators]\n    ub = [zeros(length(Ls)) for est in estimators]\n\n    for (e, est) in enumerate(estimators)\n        for (l, L) in enumerate(Ls)\n            ĥs = Hs[e][l] # nreps estimates for this combinations of e and l\n            medians[e][l] = quantile(ĥs, 0.5)\n            lb[e][l] = quantile(ĥs, (1 - q) / 2)\n            ub[e][l] = quantile(ĥs, 1 - ((1 - q) / 2))\n        end\n    end\n\n    return medians, lb, ub\nend;","category":"page"},{"location":"examples/examples_entropy/#Plotting-utilities","page":"Entropy","title":"Plotting utilities","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Now, make some plotting helper functions.","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"struct Cyclor{T} <: AbstractVector{T}\n    c::Vector{T}\n    n::Int\nend\nCyclor(c) = Cyclor(c, 0)\n\nBase.length(c::Cyclor) = length(c.c)\nBase.size(c::Cyclor) = size(c.c)\nBase.iterate(c::Cyclor, state=1) = Base.iterate(c.c, state)\nBase.getindex(c::Cyclor, i) = c.c[(i-1)%length(c.c) + 1]\nBase.getindex(c::Cyclor, i::AbstractArray) = c.c[i]\nfunction Base.getindex(c::Cyclor)\n    c.n += 1\n    c[c.n]\nend\nBase.iterate(c::Cyclor, i = 1) = iterate(c.c, i)\n\nCOLORSCHEME = [\n    \"#D43F3AFF\", \"#EEA236FF\", \"#5CB85CFF\", \"#46B8DAFF\",\n    \"#357EBDFF\", \"#9632B8FF\", \"#B8B8B8FF\",\n]\n\nCOLORS = Cyclor(COLORSCHEME)\nLINESTYLES = Cyclor(string.([\"--\", \".-\", \".\", \"--.\", \"---...\"]))\nMARKERS = Cyclor(string.([:circle, :rect, :utriangle, :dtriangle, :diamond,\n    :pentagon, :cross, :xcross]))\n\nfunction plot_entropy_estimates(Hs, Ls, Htrue)\n    # Summarize data (medians[e][l]) is the median of the e-th estimator for the \n    # l-th sample size).\n    medians, lbs, ubs = medians_and_quantiles(Hs, Ls);\n\n    fig = Figure(resolution = (800, 1000))\n    ymax = (vcat(Hs...) |> Iterators.flatten |> maximum) * 1.1\n    ymin = (vcat(Hs...) |> Iterators.flatten |> minimum) * 0.9\n\n    # We have 9 estimators, so place them on a 5-by-2 grid\n    positions = (Tuple(c) for c in CartesianIndices((5, 2)))\n    for (i, (est, c)) in enumerate(zip(estimators, positions))\n        ax = Axis(fig[first(c), last(c)],\n            xlabel = \"Sample size (L)\",\n            ylabel = \"Ĥ (bits)\",\n            title = getname(est)\n        )\n        ylims!(ax, (ymin, ymax))\n        # Ground truth\n        hlines!(ax, [Htrue], \n            linestyle = :dash, \n            color = :black,\n            linewidth = 2,\n        )\n        # Estimates\n        band!(ax, Ls, lbs[i], ubs[i], color = (COLORS[i], 0.5))\n        lines!(ax, Ls, medians[i], \n            label = getname(est),\n            linestyle = LINESTYLES[i],\n            color = COLORS[i],\n            marker = MARKERS[i],\n            linewidth = 2\n        )\n    end\n    fig\nend;","category":"page"},{"location":"examples/examples_entropy/#Results","page":"Entropy","title":"Results","text":"","category":"section"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"Now, we can finally run an ensemble of tests and plot the confidence bands against the ground truth. This","category":"page"},{"location":"examples/examples_entropy/","page":"Entropy","title":"Entropy","text":"k = 4\nestimators = [\n    Kraskov(; k), \n    KozachenkoLeonenko(), \n    Gao(; k),\n    ZhuSingh(; k),\n    Zhu(; k),\n    Goria(; k),\n    LeonenkoProzantoSavani(; k),\n    Lord(; k = k*5)\n]\n\nLs = [100:100:1000 |> collect; 2500:2500:5000 |> collect]\nd = 3\nr = 0.5\nnreps = 30\nHs = mvnormal_entropies(estimators, Ls; d, r, nreps)\nHtrue = analytical_entropy(; d, r)\nplot_entropy_estimates(Hs, Ls, Htrue)","category":"page"}]
}
