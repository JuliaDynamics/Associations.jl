
import TransferEntropy: TransferEntropyEstimator, EmbeddingTE, transferentropy
export predictive_asymmetry

functionÂ causalbalance(lags,Â tes)
    posindsÂ =Â findall(lagsÂ .>Â 0)
    negindsÂ =Â findall(lagsÂ .<Â 0)
    sum(tes[posinds])Â -Â sum(tes[neginds])Â Â 
end

function verified_prediction_lags(lag::Int)
    # If only one Î· is provided (either positive or negative), just make a prediction 
    # for that lag.
    Î·s = [-abs(lag), abs(lag)]
end

function verified_prediction_lags(lags)
    Î·s = sort(lags[lags .!= 0])

    if length(Î·s) % 2 != 0
        throw(ArgumentError("Need an even number of lags (as many negative as positive prediction lags)"))
    end

    Î·s_neg = Î·s[Î·s .< 0]
    Î·s_pos = Î·s[Î·s .> 0]

    if length(Î·s_neg) != length(Î·s_pos) 
        throw(ArgumentError("There must be as many negative as positive prediction lags. Got $Î·s_neg and $Î·s_pos"))
    end

    Î·s_neg_sort = sort(abs.(Î·s_neg))
    Î·s_pos_sort = sort(Î·s_pos)
    if !(all(Î·s_neg_sort .== Î·s_pos_sort))
        throw(ArgumentError("Negative amd positive prediction lags must be symmetric. Got $Î·s_neg and $Î·s_pos"))
    endÂ 

    return Î·s
end

"""
## General interface

    predictive_asymmetry(s, t, [c], 
        estimator::TransferEntropyEstimator, Î·s; 
        dğ’¯ = 1, dT = 1, dS = 1, Ï„T = -1, Ï„S = -1, 
        [dC = 1, Ï„C = -1,],
        normalize::Bool = false, f::Real = 1.0) â†’ Vector{Float64}

Compute the predictive asymmetry[^Haaga2020] ğ”¸(`s` â†’ `t`) for source time series `s` and 
target time series `t` over prediction lags `Î·s`, using the given `estimator` and embedding 
parameters `dğ’¯`, `dT`, `dS`, `Ï„T`, `Ï„S`. 

If a conditional time series `c` is provided, compute ğ”¸(`s` â†’ `t` |Â `c`). Then, `dC` and 
`Ï„C` controls the embedding dimension and embedding lag for the conditional variable.

## Returns

Returns a vector containing the predictive asymmetry for each value of `Î·s`.

## Normalization (hypothesis test)

If `normalize == true` (the default), then compute the normalized predictive asymmetry ğ’œ. 

In this case, for each ``\\eta`` in `Î·s`, compute ğ’œ(Î·) by normalizing ğ”¸(Î·) to some fraction `f` of the 
mean transfer entropy over prediction lags ``-\\eta, ..., \\eta`` (exluding lag 0). 

Haaga et al. (2020)[^Haaga2020] uses a normalization with `f=1.0` as a built-in hypothesis test, 
avoiding more computationally costly surrogate testing.

## Estimators

Any estimator that works for [`transferentropy`](@ref) will work with 
`predictive_asymmetry`. It is recommended to use either the rectangular 
binning-based methods or the symbolic estimators for the fastest computations. 

### Binning based

### [`VisitationFrequency`](@ref)

    predictive_asymmetry(s, t, [c],
        estimator::VisitationFrequency{RectangularBinning}, Î·s; kwargs...) â†’ Vector{Float64}

Estimate (normalized) ğ”¸(`s` â†’ `t`) or ğ”¸(`s` â†’ `t` |Â `c`) 
using visitation frequencies over a rectangular binning. 

    predictive_asymmetry(s, t, [c],
        estimator::TransferOperator{RectangularBinning}, Î·s; kwargs...) â†’ Vector{Float64}

Estimate (normalized) ğ”¸(`s` â†’ `t`) or ğ”¸(`s` â†’ `t` |Â `c`) 
using an approximation to the transfer operator over a rectangular binning.

See also: [`VisitationFrequency`](@ref), [`RectangularBinning`](@ref).

### Nearest neighbor based

    predictive_asymmetry(s, t, [c],
        estimator::Kraskov, Î·s; kwargs...)
    predictive_asymmetry(s, t, [c],
        estimator::KozachenkoLeonenko, Î·s; kwargs...) â†’ Vector{Float64}

Estimate (normalized) ğ”¸(`s` â†’ `t`) or ğ”¸(`s` â†’ `t` |Â `c`)
using naive nearest neighbor estimators.

*Note: only Shannon entropy is possible to use for nearest neighbor estimators, so the 
keyword `q` cannot be provided; it is hardcoded as `q = 1`.*

See also [`Kraskov`](@ref), [`KozachenckoLeonenko`](@ref).

### Kernel density based

    predictive_asymmetry(s, t, [c],
        estimator::NaiveKernel{Union{TreeDistance, DirectDistance}}, Î·s; 
        kwargs...) â†’ Vector{Float64}

Estimate (normalized) ğ”¸(`s` â†’ `t`) or ğ”¸(`s` â†’ `t` |Â `c`)
using a naive kernel density estimator.

See also [`NaiveKernel`](@ref), [`TreeDistance`](@ref), [`DirectDistance`](@ref).

### Hilbert

    predictive_asymmetry(s, t, [c],
        estimator::Hilbert{VisitationFrequency{RectangularBinning}}, Î·s; 
        kwargs...) â†’ Vector{Float64}

Estimate (normalized) ğ”¸(`s` â†’ `t`) or ğ”¸(`s` â†’ `t` |Â `c`) by first 
applying the Hilbert transform to `s`, `t` (`c`) and then estimating transfer entropy.

See also [`Hilbert`](@ref), [`Amplitude`](@ref), [`Phase`](@ref).

## Examples

```julia
using CausalityTools 

# Some example time series
x, y, z = rand(100), rand(100), rand(100)

# Define prediction lags and estimation method
Î·s = 1:5
method = VisitationFrequency(RectangularBinning(5))

# ğ”¸(x â†’ y) and  ğ”¸(x â†’ y |Â z)
ğ”¸reg  = predictive_asymmetry(x, y, method, Î·s, normalize = false)
ğ”¸cond = predictive_asymmetry(x, y, z, method, Î·s, normalize = false)

# ğ’œ(x â†’ y) and ğ’œ(x â†’ y |Â z), using different normalization factors
ğ’œreg = predictive_asymmetry(x, y, method, Î·s, normalize = true, f = 1.0)
ğ’œcond = predictive_asymmetry(x, y, z, method, Î·s, normalize = true, f = 1.5)
```

### [`SymbolicPermutation`](@ref)

For the symbolic estimators, make sure that the maximum prediction lag Î· stays 
small. This is because the symbolization procedure uses delay embedding vectors 
of dimension `m` if the motif length is `m` (so the actual maximum prediction lag 
used is `maximum(Î·s)*m`, which may be too large if `maximum(Î·s)` is too large).


```julia
# Some example time series
x, y, z = rand(100), rand(100), rand(100)

# Define prediction lags and estimation method
Î·s = 1:3 # small prediction lags
method = SymbolicPermutation()

# ğ’œ(x â†’ y)
predictive_asymmetry(x, y, method, Î·s, normalize = true) 

# ğ’œ(x â†’ y |Â z)
predictive_asymmetry(x, y, z, method, Î·s, normalize = true) 
```

## Description

The predictive asymmetry method is from Haaga et al. (2020) [^Haaga2020].


[^Haaga2020]: Haaga, Kristian AgasÃ¸ster, David Diego, Jo Brendryen, and Bjarte Hannisdal. "A simple test for causality in complex systems." arXiv preprint arXiv:2005.01860 (2020).
"""
function predictive_asymmetry end

function check_Î·s(Î·s)
    all(Î·s .> 0) || throw(ArgumentError("all Î·s must be >= 1, got $(Î·s)"))
    issorted(Î·s) || throw(ArgumentError("Î·s must be provided in increasing order, got $(Î·s)"))
end

function predictive_asymmetry(source, target, estimator, Î·s; 
        normalize = false, f::Real = 1.0,
        dğ’¯ = 1, dT = 1, dS = 1, Ï„T = -1, Ï„S = -1)
    
    check_Î·s(Î·s)
    NÎ· = length(Î·s)

    te_fws = zeros(NÎ·)
    te_bws = zeros(NÎ·)
    ğ”¸s = zeros(NÎ·)

    for (i, Î·) in enumerate(Î·s)
        te_fws[i] = transferentropy(source, target, estimator, dğ’¯ = dğ’¯, dT = dT, dS = dS, Ï„T = Ï„T, Ï„S = Ï„S, Î·ğ’¯ = Î·)
        te_bws[i] = transferentropy(source, target, estimator, dğ’¯ = dğ’¯, dT = dT, dS = dS, Ï„T = Ï„T, Ï„S = Ï„S, Î·ğ’¯ = -Î·)
        
        if normalize
            ğ”¸áµ¢ = (sum(te_fws[1:i]) - sum(te_bws[1:i])) / Î·
            avg_te = (sum(te_fws[1:i]) + sum(te_bws[1:i])) / (2*Î·)
            ğ”¸s[i] = ğ”¸áµ¢ / (f*avg_te)
        else
            ğ”¸áµ¢ = (sum(te_fws[1:i]) - sum(te_bws[1:i])) / Î·
            ğ”¸s[i] = ğ”¸áµ¢
        end
    end
    
    return ğ”¸s
end

function predictive_asymmetry(source, target, cond, estimator, Î·s; 
        normalize = false, f::Real = 1.0,
        dğ’¯ = 1, dT = 1, dS = 1, dC = 1, Ï„T = -1, Ï„S = -1, Ï„C = -1)
    
    check_Î·s(Î·s)
    NÎ· = length(Î·s)

    te_fws = zeros(NÎ·)
    te_bws = zeros(NÎ·)
    ğ”¸s = zeros(NÎ·)
    
    for (i, Î·) in enumerate(Î·s)
        te_fws[i] = transferentropy(source, target, cond, estimator, dğ’¯ = dğ’¯, dT = dT, dS = dS, Ï„T = Ï„T, Ï„S = Ï„S, dC = dC, Ï„C = Ï„C, Î·ğ’¯ = Î·)
        te_bws[i] = transferentropy(source, target, cond, estimator, dğ’¯ = dğ’¯, dT = dT, dS = dS, Ï„T = Ï„T, Ï„S = Ï„S, dC = dC, Ï„C = Ï„C, Î·ğ’¯ = -Î·)
        if normalize
            ğ”¸áµ¢ = (sum(te_fws[1:i]) - sum(te_bws[1:i])) / Î·
            avg_te = (sum(te_fws[1:i]) + sum(te_bws[1:i])) / (2*Î·)
            ğ”¸s[i] = ğ”¸áµ¢ / (f*avg_te)
        else
            ğ”¸áµ¢ = (sum(te_fws[1:i]) - sum(te_bws[1:i])) / Î·
            ğ”¸s[i] = ğ”¸áµ¢
        end
    end
    
    return ğ”¸s
end