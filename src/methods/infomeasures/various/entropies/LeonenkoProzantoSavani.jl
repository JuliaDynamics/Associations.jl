using SpecialFunctions: gamma
using Neighborhood: bulksearch
using Neighborhood: Euclidean, Theiler
import ComplexityMeasures: DifferentialEntropyEstimator
import ComplexityMeasures: entropy

export LeonenkoProzantoSavani

"""
    LeonenkoProzantoSavani <: DifferentialEntropyEstimator
    LeonenkoProzantoSavani(k = 1, w = 0)

The `LeonenkoProzantoSavani` estimator computes the [`Shannon`](@ref), [`Renyi`](@ref), or
[`Tsallis`](@ref) [`entropy`](@ref) using the `k`-th nearest-neighbor approach
from Leonenko et al. (2008)[^LeonenkoProsantoSavani2008].

`w` is the Theiler window, which determines if temporal neighbors are excluded
during neighbor searches (defaults to `0`, meaning that only the point itself is excluded
when searching for neighbours).

[^LeonenkoProsantoSavani2008]:
    Leonenko, N., Pronzato, L., & Savani, V. (2008). A class of RÃ©nyi information
    estimators for multidimensional densities. The Annals of Statistics, 36(5), 2153-2182.
"""
Base.@kwdef struct LeonenkoProzantoSavani <: DifferentialEntropyEstimator
    k::Int = 1
    w::Int = 0
end

function entropy(e::Shannon, est::LeonenkoProzantoSavani, x::AbstractDataset{D}) where D
    h = IÌ‚(1.0, est, x) # measured in nats
    return h / log(e.base, â„¯) # convert to desired base.
end

function entropy(e::Renyi, est::LeonenkoProzantoSavani, x::AbstractDataset{D}) where D
    if e.q â‰ˆ 1.0
        h = IÌ‚(e.q, est, x) # measured in nats
    else
        h = log(IÌ‚(e.q, est, x)) / (1 - e.q) # measured in nats
    end
    return h / log(e.base, â„¯) # convert to desired base.
end

function entropy(e::Tsallis, est::LeonenkoProzantoSavani, x::AbstractDataset{D}) where D
    if e.q â‰ˆ 1.0
        h = IÌ‚(e.q, est, x) # measured in nats
    else
        h = (IÌ‚(e.q, est, x) - 1) / (1 - e.q) # measured in nats
    end
    return h / log(e.base, â„¯) # convert to desired base.
end

# TODO: this gives nan??
# Use notation from original paper
function IÌ‚(q, est::LeonenkoProzantoSavani, x::AbstractDataset{D}) where D
    (; k, w) = est
    N = length(x)
    Vâ‚˜ = ball_volume(D)
    Câ‚– = (gamma(k) / gamma(k + 1 - q))^(1 / (1 - q))
    tree = KDTree(x, Euclidean())
    idxs, ds = bulksearch(tree, x, NeighborNumber(k), Theiler(w))
    if q â‰ˆ 1.0 # equations 3.9 & 3.10 in Leonenko et al. (2008)
        h = (1 / N) * sum(log.(Î¾áµ¢_shannon(last(dáµ¢), Vâ‚˜, N, D, k) for dáµ¢ in ds))
    else # equations 3.1 & 3.2 in Leonenko et al. (2008)
        h = (1 / N) * sum(Î¾áµ¢_renyi_tsallis(last(dáµ¢), Câ‚–, Vâ‚˜, N, D)^(1 - q) for dáµ¢ in ds)
    end
    return h
end
Î¾áµ¢_renyi_tsallis(dáµ¢, Câ‚–, Vâ‚˜, N::Int, D::Int) = (N - 1) * Câ‚– * Vâ‚˜ * (dáµ¢)^D
Î¾áµ¢_shannon(dáµ¢, Vâ‚˜, N::Int, D::Int, k) = (N - 1) * exp(-digamma(k)) * Vâ‚˜ * (dáµ¢)^D

using Distributions: MvNormal
import Distributions.entropy as dentropy
function entropy(e::Renyi, ð’©::MvNormal; base = 2)
    q = e.q
    if q â‰ˆ 1.0
        h = dentropy(ð’©)
    else
        Î£ = ð’©.Î£
        D = length(ð’©.Î¼)
        h = dentropy(ð’©) - (D / 2) * (1 + log(q) / (1 - q))
    end
    return h / log(base, â„¯)
end

# Eq. 15 in Nielsen & Nock (2011); https://arxiv.org/pdf/1105.3259.pdf
function entropy(e::Tsallis, ð’©::MvNormal; base = 2)
    q = e.q
    Î£ = ð’©.Î£
    D = length(ð’©.Î¼)
    hr = entropy(Renyi(q = q), ð’©; base)
    h = (exp((1 - q) * hr) - 1) / (1 - q)
    return h / log(base, â„¯)
end
